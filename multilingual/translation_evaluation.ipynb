{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Translation Evaluation Notebook\n",
    "\n",
    "This notebook evaluates machine translation (MT) quality using various NLP metrics:\n",
    "- **BLEU**: Measures precision of n-grams.\n",
    "- **METEOR**: Considers synonymy and stemming.\n",
    "- **ROUGE**: Compares overlap with reference translations.\n",
    "- **COMET**: Neural-based evaluation metric.\n",
    "- **BERTScore**: Uses contextual embeddings.\n",
    "\n",
    "Ensure that `multilingual_labelled_translated.csv` is available before running the notebook.\n"
   ],
   "id": "3ae06fc246d50fca"
  },
  {
   "cell_type": "code",
   "id": "3cb11adf",
   "metadata": {},
   "source": [
    "import json\n",
    "from bert_score import score\n",
    "import sacrebleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import csv\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "899fbdcf",
   "metadata": {},
   "source": [
    "def eval_bert_score(references, hypothesis):\n",
    "    P, R, F1 = score(\n",
    "        hypothesis,\n",
    "        references,\n",
    "        model_type=\"microsoft/deberta-xlarge-mnli\"\n",
    "    )\n",
    "    return {'P': P.numpy().tolist(), 'R': R.numpy().tolist(), 'F': F1.numpy().tolist()}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3613757",
   "metadata": {},
   "source": [
    "def eval_meteor(references, hypothesis):\n",
    "    res = []\n",
    "    for i, item in enumerate(hypothesis):\n",
    "        score = meteor_score([references[i].split(' ')], item.split(' '))\n",
    "        res.append(score)\n",
    "    return res\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d3773c9",
   "metadata": {},
   "source": [
    "def eval_bleu(references, hypothesis):\n",
    "    res = []\n",
    "    for i, item in enumerate(hypothesis):\n",
    "        bleu = sacrebleu.corpus_bleu([item], [[references[i]]])\n",
    "        res.append(bleu.score)\n",
    "    return res\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01c3c419",
   "metadata": {},
   "source": [
    "def eval_comet(references, hypothesis, sources):\n",
    "    model_path = download_model(\"wmt20-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    data = [{\"src\": src, \"mt\": mt, \"ref\": ref} for src, mt, ref in zip(sources, hypothesis, references)]\n",
    "    predictions = model.predict(data, batch_size=2, gpus=0, num_workers=1)\n",
    "    return predictions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d95573c3",
   "metadata": {},
   "source": [
    "def eval_rouge(references, hypothesis):\n",
    "    res = []\n",
    "    rouge_types = [\"rougeL\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
    "    for i, item in enumerate(references):\n",
    "        scores = scorer.score(item, hypothesis[i])\n",
    "        res.append(scores)\n",
    "    return res\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50627940",
   "metadata": {},
   "source": [
    "# Load CSV file\n",
    "file_path = \"multilingual_labelled_translated.csv\"  # Replace with your file path\n",
    "with open(file_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)  # Reads as a list of dictionaries\n",
    "    data = [row for row in reader]\n",
    "\n",
    "mt_apis = ['gpt_translation', 'deepL_translation', 'aws_translation']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c09c726d",
   "metadata": {},
   "source": [
    "## BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "5997791c",
   "metadata": {},
   "source": [
    "for mt in mt_apis:\n",
    "    drctr = mt.replace('translation', 'evaluation')\n",
    "    print(f\"Evaluating BLEU for\", mt)\n",
    "\n",
    "    ref, hypo, sources = [], [], []\n",
    "    for item in data:\n",
    "        ref.append(item['translation'])\n",
    "        hypo.append(item.get(mt, ''))\n",
    "\n",
    "    res = eval_bleu(ref, hypo)\n",
    "    with open(f'{drctr}/BLEU_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "        \n",
    "print(\"BLEU evaluation complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ab7cc2e",
   "metadata": {},
   "source": [
    "## METEOR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e372a14",
   "metadata": {},
   "source": [
    "for mt in mt_apis:\n",
    "    drctr = mt.replace('translation', 'evaluation')\n",
    "    print(f\"Evaluating METEOR for\", mt)\n",
    "\n",
    "    ref, hypo, sources = [], [], []\n",
    "    for item in data:\n",
    "        ref.append(item['translation'])\n",
    "        hypo.append(item.get(mt, ''))\n",
    "\n",
    "    res = eval_meteor(ref, hypo)\n",
    "    \n",
    "    with open(f'{drctr}/METEOR_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "        \n",
    "print(\"METEOR evaluation complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "02246ad8",
   "metadata": {},
   "source": [
    "## ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "7179f621",
   "metadata": {},
   "source": [
    "for mt in mt_apis:\n",
    "    drctr = mt.replace('translation', 'evaluation')\n",
    "    print(f\"Evaluating ROUGE for\", mt)\n",
    "\n",
    "    ref, hypo, sources = [], [], []\n",
    "    for item in data:\n",
    "        ref.append(item['translation'])\n",
    "        hypo.append(item.get(mt, ''))\n",
    "\n",
    "    res = eval_rouge(ref, hypo)\n",
    "    \n",
    "    with open(f'{drctr}/ROUGE_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "        \n",
    "print(\"ROUGE evaluation complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffea25c7",
   "metadata": {},
   "source": "## BERTScore Evaluation\n"
  },
  {
   "cell_type": "code",
   "id": "6c889190",
   "metadata": {},
   "source": [
    "for mt in mt_apis:\n",
    "    drctr = mt.replace('translation', 'evaluation')\n",
    "    print(f\"Evaluating BERTScore for\", mt)\n",
    "\n",
    "    ref, hypo, sources = [], [], []\n",
    "    for item in data:\n",
    "        ref.append(item['translation'])\n",
    "        hypo.append(item.get(mt, ''))\n",
    "\n",
    "    res = eval_bert_score(ref, hypo)\n",
    "\n",
    "    with open(f'{drctr}/BERTScore_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "\n",
    "print(\"BERTScore evaluation complete.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f2769bc6",
   "metadata": {},
   "source": [
    "## COMET Evaluation\n",
    "Run COMET individually after commenting out all the imports in cell #1 and only importing comet specific library.\n",
    "There is some method override that throws error when comet is run with all other imports from above."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf83071b",
   "metadata": {},
   "source": [
    "for mt in mt_apis:\n",
    "    drctr = mt.replace('translation', 'evaluation')\n",
    "    print(f\"Evaluating COMET for\", mt)\n",
    "\n",
    "    ref, hypo, sources = [], [], []\n",
    "    for item in data:\n",
    "        ref.append(item['translation'])\n",
    "        hypo.append(item.get(mt, ''))\n",
    "        sources.append(item['body'])\n",
    "\n",
    "    res = eval_comet(ref, hypo, sources)\n",
    "\n",
    "    with open(f'{drctr}/COMET_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, indent=4)\n",
    "\n",
    "print(\"COMET evaluation complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
