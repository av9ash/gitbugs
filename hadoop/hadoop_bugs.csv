Summary,Issue id,Status,Priority,Resolution,Created,Resolved,Affects Version/s,Description
JAR in conflict with timestamp check causes AM errors,13404344,Resolved,Blocker,Duplicate,30/Sep/21 17:20,20/Jul/22 20:51,2.9.2,"After an init action pulls down a new JAR and the check of a JAR's timestamp is performed  [1]we can sometimes cause an incorrect error if the timestamp does not match. In order to address this you can perform workarounds like:

record old timestamp at the beginning before the connector is changed
local -r old_file_time=$(date -r ${dataproc_common_lib_dir}/gcs-connector.jar ""+%m%d%H%M.00"")

# at end of script.
touch -t ""${old_file_time}"" <any jar they changed>
touch -h -t ""${old_file_time}"" <symlinks they changed>


We should instead of checking the date be comparing version compatibility tests.

 

 

1. https://github.com/apache/hadoop/blob/release-2.7.3-RC2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java#L255-L258"
Fix Hadoop build on Debian 10,13400058,Resolved,Blocker,Fixed,08/Sep/21 17:46,18/Sep/21 18:52,3.4.0,"We're using *Debian testing* as one of the package sources to get the latest packages. It seems to be broken at the moment. The CI fails to create the build environment for the Debian 10 platform -

{code}
[2021-09-08T00:21:11.596Z] #13 [ 8/14] RUN apt-get -q update     && apt-get -q install -y --no-install-recommends python3     && apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:10)     && apt-get clean     && rm -rf /var/lib/apt/lists/*
...
[2021-09-08T00:21:22.744Z] #13 11.28 Preparing to unpack .../libc6_2.31-17_amd64.deb ...

[2021-09-08T00:21:23.260Z] #13 11.46 Checking for services that may need to be restarted...

[2021-09-08T00:21:23.260Z] #13 11.48 Checking init scripts...

[2021-09-08T00:21:23.260Z] #13 11.50 Unpacking libc6:amd64 (2.31-17) over (2.28-10) ...

[2021-09-08T00:21:26.290Z] #13 14.38 Setting up libc6:amd64 (2.31-17) ...

[2021-09-08T00:21:26.290Z] #13 14.42 /usr/bin/perl: error while loading shared libraries: libcrypt.so.1: cannot open shared object file: No such file or directory

[2021-09-08T00:21:26.290Z] #13 14.42 dpkg: error processing package libc6:amd64 (--configure):

[2021-09-08T00:21:26.290Z] #13 14.42  installed libc6:amd64 package post-installation script subprocess returned error exit status 127

[2021-09-08T00:21:26.291Z] #13 14.43 Errors were encountered while processing:

[2021-09-08T00:21:26.291Z] #13 14.43  libc6:amd64

[2021-09-08T00:21:26.291Z] #13 14.46 E: Sub-process /usr/bin/dpkg returned an error code (1)

[2021-09-08T00:21:27.867Z] #13 ERROR: executor failed running [/bin/bash -o pipefail -c apt-get -q update     && apt-get -q install -y --no-install-recommends python3     && apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:10)     && apt-get clean     && rm -rf /var/lib/apt/lists/*]: exit code: 100

[2021-09-08T00:21:27.867Z] ------

[2021-09-08T00:21:27.867Z]  > [ 8/14] RUN apt-get -q update     && apt-get -q install -y --no-install-recommends python3     && apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:10)     && apt-get clean     && rm -rf /var/lib/apt/lists/*:

[2021-09-08T00:21:27.867Z] ------

[2021-09-08T00:21:27.867Z] executor failed running [/bin/bash -o pipefail -c apt-get -q update     && apt-get -q install -y --no-install-recommends python3     && apt-get -q install -y --no-install-recommends $(pkg-resolver/resolve.py debian:10)     && apt-get clean     && rm -rf /var/lib/apt/lists/*]: exit code: 100

[2021-09-08T00:21:27.867Z] ERROR: Docker failed to build yetus/hadoop:ef5dbc7283a.

[2021-09-08T00:21:27.867Z] 
{code}

The above log lines are copied from - https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3388/3/pipeline"
Filter deps with release labels,13401369,Resolved,Blocker,Fixed,15/Sep/21 11:42,16/Sep/21 16:19,3.4.0,We need to add the ability to filter the dependencies listed in packages.json file based on the specified release label. This is helpful for maintaining dependencies across different releases for a platform.
ABFS AbfsDelegationTokenManager to generate canonicalServiceName if DT plugin doesn't,13401382,Open,Critical,,15/Sep/21 12:43,,3.3.1,"Currently in {{AbfsDelegationTokenManager}}, any {{CustomDelegationTokenManager}} only provides a canonical service name if it
implements {{BoundDTExtension}} and its {{getCanonicalServiceName()}} method.

If this doesn't hold, {{AbfsDelegationTokenManager}} returns null, which causes {{AzureBlobFileSystem.getCanonicalServiceName()}}
to call {{super.getCanonicalServiceName()}} *which resolves the IP address of the abfs endpoint, and then the FQDN of that IPAddr

If a storage account is served over >1 endpoint, then the DT will only have a valid service name for one of the possible
endpoints, so _only work if all process get the same IP address when the look up the storage account address_

Fix

# DT plugins SHOULD generate the canonical service name
#  If they don't, and DTs are enabled: {{AbfsDelegationTokenManager}} to create a default one
# and {{AzureBlobFileSystem.getCanonicalServiceName()}} MUST NOT call superclass.


The default canonical service name of a store will be {{abfs:// + FsURI.getHost() + ""/""}}, so all containers in same storage account has the same service name

{code}
abfs://bucket@stevel-testing.dfs.core.windows.net/path
{code}

maps to 
{code}
abfs://stevel-testing.dfs.core.windows.net/ 
{code}

This will mean that only one DT will be created per storage a/c; Applications will not need to list all containers which deployed processes will wish to interact with. Today's behaviour, based on rDNS lookup of storage account, is possibly slightly broader in that all storage accounts which map to the same IPAddr share a DT. The proposed scheme will still be much broader than that of S3A, where every bucket has its unique service name, so apps need to list all target filesystems at launch time (easy for MR, source of trouble in spark).

Fix: straightforward. 

Test
* no DTs: service name == null
* DTs: will match proposed pattern, even if extension returns null."
BuiltInGzipCompressor header and trailer should not be static variables,13397934,Resolved,Critical,Fixed,28/Aug/21 06:00,09/Sep/21 04:25,3.4.0,"In the newly added BuiltInGzipCompressor, we should not let header and trailer as static variables as they are for different instances."
Performance degradation in Text.append() after HADOOP-16951,13399976,Resolved,Critical,Fixed,08/Sep/21 11:49,10/Sep/21 23:03,3.4.0,"We discovered a serious performance degradation in {{Text.append()}}.

The problem is that the logic which intends to increase the size of the backing array does not work as intended.
It's very difficult to spot, so I added extra logs to see what happens.

Let's add 4096 bytes of textual data in a loop:
{noformat}
  public static void main(String[] args) {
    Text text = new Text();
    String toAppend = RandomStringUtils.randomAscii(4096);

    for(int i = 0; i < 100; i++) {
      text.append(toAppend.getBytes(), 0, 4096);
    }
  }
{noformat}

With some debug printouts, we can observe:
{noformat}
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:append(251)) - length: 24576,  len: 4096, utf8ArraySize: 4096, bytes.length: 30720
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:append(253)) - length + (length >> 1): 36864
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:append(254)) - length + len: 28672
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:ensureCapacity(287)) - >>> enhancing capacity from 30720 to 36864
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:append(251)) - length: 28672,  len: 4096, utf8ArraySize: 4096, bytes.length: 36864
2021-09-08 13:35:29,528 INFO  [main] io.Text (Text.java:append(253)) - length + (length >> 1): 43008
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:append(254)) - length + len: 32768
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:ensureCapacity(287)) - >>> enhancing capacity from 36864 to 43008
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:append(251)) - length: 32768,  len: 4096, utf8ArraySize: 4096, bytes.length: 43008
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:append(253)) - length + (length >> 1): 49152
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:append(254)) - length + len: 36864
2021-09-08 13:35:29,529 INFO  [main] io.Text (Text.java:ensureCapacity(287)) - >>> enhancing capacity from 43008 to 49152
...
{noformat}

After a certain number of {{append()}} calls, subsequent capacity increments are small.

It's because the difference between two {{length + (length >> 1)}} values is always 6144 bytes. Because the size of the backing array is trailing behind the calculated value, the increment will also be 6144 bytes. This means that new arrays are constantly created.

Suggested solution: don't calculate the capacity in advance based on length. Instead, pass the required minimum to {{ensureCapacity()}}. Then the increment should depend on the actual size of the byte array if the desired capacity is larger."
Run optional CI for changes in C,13392051,Resolved,Critical,Fixed,27/Jul/21 07:38,05/Aug/21 12:19,3.4.0,We need to ensure that we run the CI for all the platforms when there are changes in C files.
Run CI for Centos 7,13391728,Resolved,Critical,Fixed,25/Jul/21 12:00,29/Jul/21 17:51,3.4.0,Need to run the CI for Centos 7 platform since it's a supported platform. The CI will run on this platform only when there's C++ file/C++ build/platform related changes.
Use separate source dir for platform builds,13389964,Resolved,Critical,Fixed,16/Jul/21 06:08,26/Jul/21 20:10,3.4.0,"The multi-platform build stages run on the checkout of the source directory, one after the other. For those platforms that are marked as optional (Centos 8 and Debian 10 currently), the condition to run CI on the platform is determined by inspecting the git commit history and checking if there's any C++ file/C++ build/platform related changes.

It seems like after YETUS runs on one platform, it's clearing up the git branch information. This is causing the build to not get triggered on the optional platforms. Please note that those platforms not marked optional (Ubuntu focal) isn't affected by this since CI runs for this platform irrespective of any C++ changes.

We can see this in the Jenkins UI page -

CI runs for Centos 8 -
 !image-2021-07-16-11-36-26-698.png! 

Subsequently, the CI for Debian 10 gets skipped -
 !image-2021-07-16-11-36-55-495.png! 

However, CI for Ubuntu focal runs since it's not marked as optional -
 !image-2021-07-16-11-37-56-923.png! 

Thus, we need to ensure that each platform builds on its own copy of the source code checkout so that whatever changes one platform makes doesn't affect the other."
ShellBasedUnixGroupsMapping: group name containing space can be used to inject group memberships,13402174,Open,Critical,,20/Sep/21 12:57,,3.3.1,"Group names available from identity management systems, for example sssd, may contain space characters when used with for example Active Directory. such a group name can be used to inject group memberships granting permission to basically any targeted group.

 

Suppose following scenario:

a) centralized identity management system is used, where organization's responsible roles are defined to allow access to their named groups.

b) group ""hdfs"" grants hdfs-admin permissions and is managed by authorized personnel only.

c) attacker orders creation of a group named as ""uploaderformy hdfs"" and the attacker's user account ""attacker1"" as member of that group.

 

This will lead to the scenario where ShellBasedUnixGroupsMapping executes group lookup and returns groups uploaderformy and hdfs for the ""attacker1"" username as TOKEN_SEPARATOR_REGEX contains space character in addition others (""[ \t\n\r\f]"").

This bug was found during our own solution based on the ShellBasedUnixGroupsMapping for [https://github.com/teragrep/]

 

Other versions may be affected as well.

 "
Run CI for Ubuntu 18.04,13393001,Open,Critical,,02/Aug/21 06:11,,3.4.0,Need to run the CI for Ubuntu 18.04 since it's a supported platform. The CI will run on this platform only when there's C++ file/C++ build/platform related changes.
Run CI for Fedora 33,13393000,Open,Critical,,02/Aug/21 06:09,,3.4.0,Need to run the CI for Fedora 33 since it's a supported platform. The CI will run on this platform only when there's C++ file/C++ build/platform related changes.
Add s3a tool to convert S3 server logs to avro/csv files,13403878,In Progress,Major,,28/Sep/21 15:26,,3.3.2,"Add s3a tool to convert S3 server logs to avro/csv files

With S3A Auditing, we have code in hadoop-aws to parse s3 log entries, including splitting up the referrer into its fields.

But we don't have an easy way of using it. I've done some early work in spark but as well as that code not working ([https://github.com/hortonworks-spark/cloud-integration/blob/master/spark-cloud-integration/src/main/scala/com/cloudera/spark/cloud/s3/S3LogRecordParser.scala]), it doesn't do the audit splitting.


 And, given that the S3 audit logs can be small on a lightly loaded store, not always justified.

Proposed

we add
 # utility parser class to take a row and split it into a record
 # which can be saved to avro through a schema we define
 # or exported to CSV with/without headers. (with: easy to understand, without: can cat files)
 # add a mapper so this can be used in MR jobs (could even make it committer test ..)
 # and a ""hadoop s3guard/hadoop s3"" entry point so you can do it on the cli

{code:java}
hadoop s3 parselogs -format avro -out s3a://dest/path -recursive s3a://stevel-london/logs/bucket1/*
{code}
would take all files under the path, load, parse and emit the output.

design issues
 * would you combine all files, or emit a new .avro or .csv file for each one?
 * what's a good avro schema to cope with new context attributes
 * CSV nuances: tabs vs spaces, use opencsv or implement the (escaping?) writer ourselves.
 me: TSV and do a minimal escaping and quoting emitter. Can use opencsv in the test suite.
 * would you want an initial filter during processing? especially for exit codes?
 me: no, though I could see the benefit for 503s. Best to let you load it into a notebook or spreadsheet and go from there."
S3A DeleteOperation to parallelize POSTing of bulk deletes,13398683,Open,Major,,01/Sep/21 13:38,,3.4.0,"Once the need to update the DDB tables is removed, we can't go from a single POSTed delete at a time to posting a large set of bulk delete operations in parallel.

The current design is to support incremental update of S3Guard tables, including handling partial failures. Not a problem anymore.

This will significantly improve delete() performance on directory trees with many many children/descendants, as it goes from a sequence of children/1000 POSTs to parallel writes. As each file deleted is still throttled, we will be limited to 3500 deletes/second with throttling, so throwing a large pool of workers at the problem would be counter-productive and potentially cause problems for other applications trying to write down the same directory tree. But we can do better than one-POST at a time.

Proposed
* if parallel delete is off: no limit
* parallel delete is on, limit #of parallel to 3000/page-size: you'll never have more updates pending than the write limit of a single shard."
S3A: Allow SSE configurations per object path,13396008,Open,Major,,19/Aug/21 02:04,,3.3.1,"Currently, we can map the SSE configurations at bucket level only:
{code:java}
<property>
  <name>fs.s3a.bucket.ireland-dev.server-side-encryption-algorithm</name>
  <value>SSE-KMS</value>
</property>

<property>
  <name>fs.s3a.bucket.ireland-dev.server-side-encryption.key</name>
  <value>arn:aws:kms:eu-west-1:98067faff834c:key/071a86ff-8881-4ba0-9230-95af6d01ca01</value>
</property>
{code}
But sometimes we want to encrypt data in different paths with different keys within the same bucket. For example, a partitioned table might benefit from encrypting each partition with a different key when the partition represents a customer or a country.

[S3 already can encrypt using different keys/configurations at the object level|https://aws.amazon.com/premiumsupport/knowledge-center/s3-encrypt-specific-folder/], so what we need to do on Hadoop is to provide a way to map which key to use. One idea could be mapping them in the XML config:

 
{code:java}
<property>
  <name>fs.s3a.server-side-encryption.paths</name>
  <value>s3://bucket/my_table/country=ireland,s3://bucket/my_table/country=uk, s3://bucket/my_table/country=germany</value>
</property>

<property>
  <name>fs.s3a.server-side-encryption.path-keys</name>
  <value>arn:aws:kms:eu-west-1:90ireland09:key/ireland-key,arn:aws:kms:eu-west-1:980uk0993c:key/uk-key,arn:aws:kms:eu-west-1:98germany089:key/germany-key</value>
</property>
{code}
Or potentially fetch the mappings from the filesystem:

 
{code:java}
<property>
  <name>fs.s3a.server-side-encryption.mappings</name>
  <value>s3://bucket/configs/encryption_mappings.json</value>
</property> {code}
where encryption_mappings.json could be something like this:

 
{code:java}
{ 
   ""path"": ""s3://bucket/customer_table/customerId=abc123"", 
   ""algorithm"": ""SSE-KMS"",
   ""key"": ""arn:aws:kms:eu-west-1:933993746:key/abc123-key""
}
...
{ 
   ""path"": ""s3://bucket/customer_table/customerId=xyx987"", 
   ""algorithm"": ""SSE-KMS"",
   ""key"": ""arn:aws:kms:eu-west-1:933993746:key/xyx987-key""
}
{code}
 

 "
RawLocalFileSystem cannot mkdir/chmod paths with emojis. ☹️,13399600,Open,Major,,06/Sep/21 14:48,,3.3.1,"*Bug description:*

`fs.mkdirs` command for `RawLocalFileSystem` doesn't work in Hadoop 3 with NativeIO enabled.

The failure was happening when doing the native `chmod` command to the file (the `mkdir` command itself is working).

Stacktrace:

{{ENOENT: No such file or directory  ENOENT: No such file or directory at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmodImpl(Native Method) at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmod(NativeIO.java:382) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:974) at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660) at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700) at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)}}

 

*To reproduce:*
 * Add `fs.mkdirs` in RawLocalFileSystem with NativeIO enabled.
 * Sample: [https://github.com/apache/hadoop/pull/3391]"
ABFS: Transient failure of TestAbfsClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting,13392896,Open,Major,,01/Aug/21 01:44,,3.4.0,"Transient failure of the below test observed for HNS OAuth, AppendBlob HNS OAuth and Non-HNS SharedKey combinations. The value denoted by ""actual value"" below varies across failures, and exceeds the upper limit of the expected range.

_TestAbfsClientThrottlingAnalyzer.testManySuccessAndErrorsAndWaiting:171->fuzzyValidate:49 The actual value 10 is not within the expected range: [5.60, 8.40]._

Verified failure with client and server in the same region to rule out network issues."
ABFS: Fix transient failures in ITestAbfsStreamStatistics and ITestAbfsRestOperationException,13397564,Reopened,Major,,26/Aug/21 13:11,,3.3.1,"To address transient failures in the following test classes:
 * ITestAbfsStreamStatistics: Uses a filesystem level instance to record read/write statistics, which also tracks these operations in other tests. running parallelly. To be marked for sequential run only to avoid transient failure
 * ITestAbfsRestOperationException: The use of a static member to track retry count causes transient failures when two tests of this class happen to run together. Switch to non-static variable for assertions on retry count"
ABFS: Refactor read flow to include ReadRequestParameter,13397530,Open,Major,,26/Aug/21 11:23,,3.4.0,"This Jira is to facilitate upcoming work as part of adding an alternate connection :
 HADOOP-17853 ABFS: Enable optional store connectivity over azure specific protocol for data egress - ASF JIRA (apache.org)

The scope of the change is to introduce a ReadRequestParameter that will include the various inputs needed for the read request to AbfsClient class."
ABFS: Enable optional store connectivity over azure specific protocol for data egress,13395708,Open,Major,,17/Aug/21 15:25,,3.4.0,This Jira is to provide an option to enable store access on read path over an Azure specific protocol. This will only work on Azure VMs and hence will be disabled by default.
ABFS: Fork AbfsHttpOperation to add alternate connection,13397162,Open,Major,,25/Aug/21 12:03,,3.4.0,"This Jira is to facilitate upcoming work as part of adding an alternate connection :
[HADOOP-17853] ABFS: Enable optional store connectivity over azure specific protocol for data egress - ASF JIRA (apache.org)



The scope of the change is to make AbfsHttpOperation as abstract class and create a child class AbfsHttpConnection. Future connection types will be added as child of AbfsHttpOperation."
ABFS: Refactor HTTP request handling code,13399255,Open,Major,,03/Sep/21 13:36,,3.4.0,Aims at Http request handling code refactoring.
WASB: Fix Compiler Warnings,13397424,Open,Major,,26/Aug/21 06:18,,3.3.1,"To track a list of WASB compiler warnings, which are also intermittently affecting yetus runs on ABFS driver PRs.
 # hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java:210:29:[deprecation] HFLUSH in StreamCapabilities has been deprecated"
ABFS: Modifications to TracingContext,13387754,Open,Major,,05/Jul/21 15:23,,3.3.1,"TracingContext introduced in the PR  [HADOOP-17290|https://github.com/apache/hadoop/pull/2520] tracks a request with identifiers as it passes through ABFS layers. Creating this Jira to track suggested modifications and improvements to the tracing structure.
 * rename Listener; remove it from TracingContext constructor, confine to setter
 * make fields (identifiers) immutable; eliminate retry count field => reduce instance cloning
 * introduce child classes of TracingContext to handle stream/continuation ops"
Bump aliyun-sdk-oss to 3.13.0,13393311,Resolved,Major,Fixed,03/Aug/21 18:10,14/Aug/21 12:20,3.2.3,"Bump aliyun-sdk-oss to 3.13.0 in order to remove transitive dependency on jdom 1.1.

Ref: https://issues.apache.org/jira/browse/HADOOP-17820?focusedCommentId=17390206&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17390206."
implement non-guava Precondition checkState,13402753,Resolved,Major,Fixed,22/Sep/21 17:52,07/Oct/21 02:11,3.2.3,"As part In order to replace Guava Preconditions, we need to implement our own versions of the API.
 This Jira is to add the implementation {{checkState}} to the existing class {{org.apache.hadoop.util.Preconditions}}

+The plan is as follows+
 * implement {{org.apache.hadoop.util.Preconditions.checkState}} with the minimum set of interface used in the current hadoop repo.
 * we can replace {{guava.Preconditions}} by {{org.apache.hadoop.util.Preconditions}} once all the interfaces have been implemented (both this jira and HADOOP-17929 are complete).
 * We need the change to be easily to be backported in 3.x.

previous jiras:
 * HADOOP-17126 was created to implement CheckNotNull.
 * HADOOP-17929 implementing checkArgument.

CC: [~stevel@apache.org], [~vjasani]"
implement non-guava Precondition checkArgument,13402746,Resolved,Major,Fixed,22/Sep/21 17:49,01/Oct/21 07:17,3.2.3,"As part In order to replace Guava Preconditions, we need to implement our own versions of the API.
 This Jira is to add the implementation {{checkArgument}} to the existing class {{org.apache.hadoop.util.Preconditions}}

+The plan is as follows+
 * implement {{org.apache.hadoop.util.Preconditions.checkArgument}} with the minimum set of interface used in the current hadoop repo.
 * we can replace {{guava.Preconditions}} by {{org.apache.hadoop.util.Preconditions}} once all the interfaces have been implemented.
 * We need the change to be easily to be backported in 3.x.

A previous jira HADOOP-17126 was created to replace CheckNotNull. HADOOP-17930 is created to implement checkState.

CC: [~stevel@apache.org], [~vjasani]"
[JDK 17] TestNetUtils fails,13401290,Resolved,Major,Fixed,15/Sep/21 08:34,27/Sep/21 01:13,3.4.0,"TestNetUtils#testInvalidAddress fails.
{noformat}
[INFO] Running org.apache.hadoop.net.TestNetUtils
[ERROR] Tests run: 48, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.469 s <<< FAILURE! - in org.apache.hadoop.net.TestNetUtils
[ERROR] testInvalidAddress(org.apache.hadoop.net.TestNetUtils)  Time elapsed: 0.386 s  <<< FAILURE!
java.lang.AssertionError: 
 Expected to find 'invalid-test-host:0' but got unexpected exception: java.net.UnknownHostException: invalid-test-host/<unresolved>:0
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:592)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:551)
	at org.apache.hadoop.net.TestNetUtils.testInvalidAddress(TestNetUtils.java:109)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

	at org.apache.hadoop.test.GenericTestUtils.assertExceptionContains(GenericTestUtils.java:396)
	at org.apache.hadoop.test.GenericTestUtils.assertExceptionContains(GenericTestUtils.java:373)
	at org.apache.hadoop.net.TestNetUtils.testInvalidAddress(TestNetUtils.java:116)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.UnknownHostException: invalid-test-host/<unresolved>:0
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:592)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:551)
	at org.apache.hadoop.net.TestNetUtils.testInvalidAddress(TestNetUtils.java:109)
	... 30 more
{noformat}"
Provide fallbacks for identity/cost providers and backoff enable,13391688,Resolved,Major,Fixed,24/Jul/21 18:54,28/Jul/21 17:11,3.4.0,"This sub-task is to provide default properties for identity-provider.impl, cost-provider.impl and backoff.enable such that if properties with port is not configured, we can fallback to default property (port-less)."
TestLocalFSCopyFromLocal.testDestinationFileIsToParentDirectory failure after reverting HADOOP-16878,13403037,Resolved,Major,Fixed,24/Sep/21 03:00,28/Sep/21 16:49,3.3.2,"After reverting HADOOP-16878 from branch-3.3, test {{TestLocalFSCopyFromLocal.testDestinationFileIsToParentDirectory}} started to fail because it expects an exception but the copying succeeded."
Add more test for the BuiltInGzipCompressor,13397449,Resolved,Major,Fixed,26/Aug/21 07:40,22/Sep/21 15:00,3.4.0,We added BuiltInGzipCompressor recently. It is better to add more compatibility tests for the compressor.
Upgrade Kafka to 2.8.1,13403462,Resolved,Major,Fixed,27/Sep/21 08:48,28/Sep/21 04:37,3.2.3,"Kafka 2.4.0 has the following vulnerability.

[https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-38153]"
lz4-java and snappy-java should be excluded from relocation in shaded Hadoop libraries,13399415,Resolved,Major,Fixed,04/Sep/21 21:33,14/Sep/21 18:17,3.3.1,"lz4-java is a provided dependency. So in the shaded Hadoop libraries, e.g. hadoop-client-api, if we don't exclude lz4 dependency, the downstream will still see the exception even they include lz4 dependency.
{code:java}
[info]   Cause: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.net.jpountz.lz4.LZ4Factory
[info]   at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
[info]   at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
[info]   at org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66)
[info]   at org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119)
[info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152)
[info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168)
 {code}

Currently snappy-java is included and relocated in Hadoop shaded client libraries. But as it includes native methods, it should not be relocated too due to JNI method resolution. The downstream will see the exception:

{code}
[info]   Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I
[info]   at org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)                                                                                                 
[info]   at org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)                                                                                                        
[info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)
[info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)

{code}
"
Test Result Not Working In Jenkins Result,13400141,Resolved,Major,Fixed,09/Sep/21 04:32,10/Sep/21 23:46,3.4.0,"The jenkins used to show the Test Results, now the list is broken and No PR has that working.

eg:

[https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3404/1/testReport/]

 

For Daily Build No Test Result as well:

https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/622/"
Upgrade ant to 1.10.11,13398785,Resolved,Major,Fixed,01/Sep/21 19:33,02/Sep/21 21:24,2.10.2,"Vulnerabilities reported in org.apache.ant:ant:1.10.9
 * [CVE-2021-36374|https://nvd.nist.gov/vuln/detail/CVE-2021-36374] moderate severity
 * [CVE-2021-36373|https://nvd.nist.gov/vuln/detail/CVE-2021-36373] moderate severity

suggested: org.apache.ant:ant ~> 1.10.11"
Upgrade JSON smart to 2.4.7,13394607,Resolved,Major,Fixed,11/Aug/21 10:06,14/Aug/21 11:32,3.2.3,"Currently we are using JSON Smart 2.4.2 version which is vulnerable to - CVE-2021-31684.

We can upgrade the version to 2.4.7 (2.4.5 or later)."
Upgrade jetty version to 9.4.43,13389310,Resolved,Major,Fixed,13/Jul/21 04:34,22/Jul/21 08:38,3.2.2,"https://github.com/eclipse/jetty.project/security/advisories/GHSA-m6cp-vxjx-65j6
https://github.com/eclipse/jetty.project/security/advisories/GHSA-gwcr-j4wh-j3cq"
Better token validation,13388312,Resolved,Major,Fixed,08/Jul/21 07:36,10/Jul/21 06:08,2.10.2,`MessageDigest.isEqual()` should be used for checking tokens.
Modify Text.ensureCapacity() to efficiently max out the backing array size,13400214,Resolved,Major,Fixed,09/Sep/21 11:06,30/Sep/21 00:26,3.4.0,"This is a continuation of HADOOP-17901.

Right now we use a factor of 1.5x to increase the byte array if it's full. However, if the size reaches a certain point, the increment is only (current size + length). This can cause performance issues if the textual data which we intend to store is beyond this point.

Instead, let's max out the array to the maximum. Based on different sources, a safe choice seems to be Integer.MAX_VALUE - 8 (see ArrayList, AbstractCollection, HashTable, etc)."
Move ClusterStorageCapacityExceededException to Public from LimitedPrivate,13399949,Resolved,Major,Fixed,08/Sep/21 10:24,13/Sep/21 17:21,3.4.0,"As of now the exception is marked limited private
{code:java}
@InterfaceAudience.LimitedPrivate({ ""HDFS"", ""MapReduce"", ""Tez"" })
{code}
Doesn't allow other projects, Rather than individually adding project, Make it Public itself.

This exception can be used to act as a fail-fast marker for different operations."
Remove GzipOutputStream,13399051,Resolved,Major,Fixed,03/Sep/21 01:43,09/Sep/21 04:24,3.4.0,"As we provide built-in gzip compressor, we can use it in compressor stream. The wrapper GzipOutputStream can be removed now."
Check real user ACLs in addition to proxied user ACLs,13396187,Resolved,Major,Fixed,19/Aug/21 18:34,08/Sep/21 15:29,2.10.1,"In a secure cluster, it is possible to configure the services to allow a super-user to proxy to a regular user and perform actions on behalf of the proxied user (see [Proxy user - Superusers Acting On Behalf Of Other Users|https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html]).

This is useful for automating server access for multiple different users in a multi-tenant cluster. For example, this can be used by a super user submitting jobs to a YARN queue, accessing HDFS files, scheduling Oozie workflows, etc, which will then execute the service as the proxied user.

Usually when these services check ACLs to determine if the user has access to the requested resources, the service only needs to check the ACLs for the proxied user. However, it is sometimes desirable to allow the proxied user to have access to the resources when only the real user has open ACLs.

For instance, let's say the user {{adm}} is the only user with submit ACLs to the {{dataload}} queue, and the {{adm}} user wants to submit apps to the {{dataload}} queue on behalf of users {{headless1}} and {{headless2}}. In addition, we want to be able to bill {{headless1}} and {{headless2}} separately for the YARN resources used in the {{dataload}} queue. In order to do this, the apps need to run in the {{dataload}} queue as the respective headless users. We could open up the ACLs to the {{dataload}} queue to allow {{headless1}} and {{headless2}} to submit apps. But this would allow those users to submit any app to that queue, and not be limited to just the data loading apps, and we don't trust the {{headless1}} and {{headless2}} owners to honor that restriction.

This JIRA proposes that we define a way to set up ACLs to restrict a resource's access to a  super-user, but when the access happens, run it as the proxied user."
"hadoop-aws landsat-pds test bucket will be deleted after Jul 1, 2021",13386947,Resolved,Major,Duplicate,01/Jul/21 04:55,30/Jan/24 20:00,,"I found an anouncement that landsat-pds buket will be deleted on July 1, 2021

(https://registry.opendata.aws/landsat-8/)

and  I think this bucket  is used in th test of hadoop-aws module use

[https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java#L93]

 

At this time I can access the bucket but we might have to change the test bucket in someday."
Replace IOUtils#closeQuietly usages,13387240,Resolved,Major,Fixed,02/Jul/21 09:22,08/Jul/21 07:05,3.4.0,"IOUtils#closeQuietly is deprecated since 2.6 release of commons-io without any replacement. Since we already have good replacement available in Hadoop's own IOUtils, we should use it."
Provide fallbacks for callqueue.impl and scheduler.impl,13388702,Resolved,Major,Fixed,09/Jul/21 16:27,14/Jul/21 11:59,3.4.0,"As mentioned in parent Jira, we should provide default properties for callqueue.impl and scheduler.impl such that if properties with port is not configured, we can fallback to default property. If ""ipc.8020.callqueue.impl"" is not present, fallback property could be ""ipc.callqueue.impl"" (without port). We can take up rest of the callqueue properties in separate sub-tasks."
Checkstyle - Allow line length: 100,13391234,Resolved,Major,Fixed,22/Jul/21 02:45,22/Jul/21 21:57,3.3.2,"Update the checkstyle rule to allow for 100 or 120 characters.

Discussion thread: [https://lists.apache.org/thread.html/r69c363fb365d4cfdec44433e7f6ec7d7eb3505067c2fcb793765068f%40%3Ccommon-dev.hadoop.apache.org%3E]"
Use CuratorCache implementation instead of PathChildrenCache / TreeCache,13393429,Resolved,Major,Fixed,04/Aug/21 09:45,07/Aug/21 02:21,3.4.0,"As we have moved to Curator 5.2.0 for Hadoop 3.4.0, we should start using new CuratorCache service implementation in place of deprecated PathChildrenCache and TreeCache usecases."
Avoid possible class loading deadlock with VerifierNone initialization,13396628,Resolved,Major,Fixed,23/Aug/21 08:17,24/Aug/21 13:48,3.3.2,"Superclass Verifier has a static initializer VERIFIER_NONE that initializes sub-class VerifierNone. This reference can result in deadlock during class loading as per [https://docs.oracle.com/javase/specs/jls/se8/html/jls-12.html#jls-12.4.2].

As of today, only RpcProgram use this instance and hence it is safe but if more clients start using this (specifically static ones), it has potential to bring deadlock. We should break this referencing before it is late."
ExceptionsHandler to add terse/suppressed Exceptions in thread-safe manner,13397644,Resolved,Major,Fixed,26/Aug/21 19:46,03/Sep/21 02:06,3.3.2,"Even though we have explicit comments stating that we have thread-safe replacement of terseExceptions and suppressedExceptions, in reality we don't have it. As we can't guarantee only non-concurrent addition of Exceptions at a time from any Server implementation, we should make this thread-safe."
Add Hadoop code formatter in dev-support,13399432,Resolved,Major,Fixed,05/Sep/21 07:58,23/Sep/21 04:34,3.4.0,We should add Hadoop code formatter xml to dev-support specifically for new developers to refer to.
Provide alternative to Guava VisibleForTesting,13404264,Resolved,Major,Fixed,30/Sep/21 09:54,05/Oct/21 17:21,3.3.2,"In an attempt to reduce the dependency on Guava, we should remove VisibleForTesting annotation usages as it has very high usage in our codebase. This Jira is to provide Hadoop's own alternative and use it in hadoop-common-project modules."
ABFS: Support for Encryption Context,13401361,Resolved,Major,Fixed,15/Sep/21 10:55,01/Jan/24 19:10,3.3.1,"Support for customer-provided encryption keys at the file level, superceding the global (account-level) key use in HADOOP-17536.

ABFS driver will support an ""EncryptionContext"" plugin for retrieving encryption information, the implementation for which should be provided by the client. The keys/context retrieved will be sent via request headers to the server, which will store the encryption context. Subsequent REST calls to server that access data/user metadata of the file will require fetching the encryption context through a GetFileProperties call and retrieving the key from the custom provider, before sending the request."
Improve the GitHub pull request template,13389501,Resolved,Major,Fixed,14/Jul/21 01:47,14/Aug/21 12:17,,"The current Hadoop pull request template can be improved.

- Require some information (e.g. https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)
- Checklists (e.g. https://github.com/apache/nifi/blob/main/.github/PULL_REQUEST_TEMPLATE.md)
- Move current notice to comment (i.e. surround with <!-- and  -->)"
improve YARN Registry DNS Server qps,13396833,Open,Major,,24/Aug/21 06:55,,3.1.2,"There are some points to improve the performance of YARN Registry DNS Server.

- Do not print unnecessary logs (It just needs change log4j.properties)
-- log4j.logger.org.apache.hadoop.registry.server.dns=WARN
- Change some loglevels at points which can affect performance degradation.
- Use ""newFixedThreadPool"" instead of ""newCachedThreadPool"" to prevent OOM
- Use Blocking on TCP handler. Using non-blocking and sleeping some time(""Thread.sleep(500)"") is meaningless.

In our environment, QPS of original yarn dns server is about 5000 (UDP), 100 (TCP).
Now, QPS of our improved yarn dns server is about 47000 (UDP), 500 (TCP).


I will make a pull request at https://github.com/apache/hadoop soon.
"
fs.s3a.acl.default not working after S3A Audit feature added,13392716,Resolved,Major,Fixed,30/Jul/21 09:58,02/Aug/21 14:36,3.3.2,"
After HADOOP-17511 the fs.s3a.acl.default propperty isn't being passed through to S3 PUT/COPY requests.

The new RequestFactory is being given the acl values from the S3A FS instance, but the factory is being created before the acl settings are loaded from the configuration.

Fix, and ideally, test (if the getXAttr lets us see this now)"
ABFS: Stabilize openFile withStatus,13399638,Resolved,Major,Duplicate,07/Sep/21 00:49,30/Nov/22 14:51,3.3.1,"Add support for more FileStatus types at OpenFile, and address minor concerns with HADOOP-17682 PR."
S3AInstrumentation Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics,13395121,Open,Major,,13/Aug/21 13:24,,3.2.1,"When using hadoop s3a file upload for spark event Logs, the logs were queued up and not uploaded before the process is shut down:
{code:java}
// 21/08/13 12:22:39 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
21/08/13 12:22:39 WARN S3AInstrumentation: Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{blocksSubmitted=1, blocksInQueue=1, blocksActive=0, blockUploadsCompleted=0, blockUploadsFailed=0, bytesPendingUpload=106716, bytesUploaded=0, blocksAllocated=1, blocksReleased=1, blocksActivelyAllocated=0, exceptionsInMultipartFinalize=0, transferDuration=0 ms, queueDuration=0 ms, averageQueueTime=0 ms, totalUploadDuration=0 ms, effectiveBandwidth=0.0 bytes/s}{code}
details see logs attached"
"Upgrade third party protobuf-java-2.5.0.jar to address vulnerabilities #CVE-2015-5237, CVE-2019-15544,",13396667,Open,Major,,23/Aug/21 11:31,,,"Third party jar protobuf-java-2.5.0.jar reports vulnerabilities # CVE-2015-5237, CVE-2019-15544 and need to be upgraded.

CVE-2019-15544：

Vulnerability Description：An issue was discovered in the protobuf crate before 2.6.0 for Rust. Attackers can exhaust all memory via Vec::reserve calls.

CVE-2015-5237：

Vulnerability Description：protobuf allows remote authenticated attackers to cause a heap-based buffer overflow.

 

Please review  and let me know if you have any concerns or would like to add more details to upgrade."
Support building on Apple Silicon ,13403386,Resolved,Major,Fixed,26/Sep/21 19:39,27/Sep/21 12:28,3.4.0,
distcp to use openFile() with sequential IO; ranges of reads,13398730,Resolved,Major,Fixed,01/Sep/21 15:07,19/Aug/22 09:46,3.4.0,"once openFile adds standard options for sequential access, distcp to adopt so as to enforce sequential reads on all uploads/backups"
Add ListWithIOStats<T> wrapper to return IOStats from a list.,13389886,Resolved,Major,Won't Fix,15/Jul/21 17:28,27/Jul/22 13:36,3.3.1,"Just as RemoteIterators now adds the ability to add an IOStats result, it'd be handy to do the same for a java.util.List. This makes it possible to return IO stats from static methods which return them"
No error message reported when bucket doesn't exist in S3AFS,13389573,Resolved,Major,Fixed,14/Jul/21 07:28,22/Jul/21 08:33,3.3.1,"No error message is shown at INFO level, when the bucket trying to access, doesn't exist. 

While translating the exception when the bucket doesn't exist(error code: NoSuchBucket, status code: 404), we are setting the ""path"" as the error message rather than ""message"", the actual error message. 

for eg: 
{code:java}
❯ bin/hadoop fs -ls s3a://mmt-no-bucket/
2021-07-14 12:58:26,169 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2021-07-14 12:58:26,392 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2021-07-14 12:58:26,448 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2021-07-14 12:58:26,448 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started
ls: s3a://mmt-no-bucket/
2021-07-14 12:58:27,712 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...
2021-07-14 12:58:27,713 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.
2021-07-14 12:58:27,713 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete. {code}
 "
Spark job stuck in S3A StagingCommitter::setupJob,13403020,Resolved,Major,Cannot Reproduce,23/Sep/21 21:24,03/Jun/22 15:33,3.2.1,"This is using the S3A directory staging committer, the Spark driver gets stuck in a retry loop inside setupJob. Here's a stack trace:


{noformat}
org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)
org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)
org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$1753/2105635903.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)
org.apache.spark.sql.DataFrameWriter$$Lambda$1752/114484787.apply(Unknown Source)
org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)
org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)
org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85) => holding Monitor(org.apache.spark.sql.execution.QueryExecution@705144571})
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.sql.execution.SparkPlan$$Lambda$1574/1384254911.apply(Unknown Source)
org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)
org.apache.spark.sql.execution.SparkPlan$$Lambda$1573/696771575.apply(Unknown Source)
org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)
org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104) => holding Monitor(org.apache.spark.sql.execution.command.DataWritingCommandExec@539925125})
org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:170)
org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)
org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:163)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter.setupJob(DirectoryStagingCommitter.java:65)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter.setupJob(StagingCommitter.java:458)
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:355)
org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:2062)
org.apache.hadoop.fs.s3a.S3AFileSystem.innerMkdirs(S3AFileSystem.java:2129)
org.apache.hadoop.fs.s3a.S3AFileSystem.createFakeDirectory(S3AFileSystem.java:2808)
org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:2833)
org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
org.apache.hadoop.fs.s3a.Invoker$$Lambda$232/695085082.execute(Unknown Source)
org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
org.apache.hadoop.fs.s3a.S3AFileSystem$$Lambda$1932/855044548.execute(Unknown Source)
org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$createEmptyObject$13(S3AFileSystem.java:2835)
org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:1589)
org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:2751)
org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2785)
org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1717)
org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1457)
org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
org.apache.hadoop.fs.s3a.S3AFileSystem$$Lambda$1933/1245120662.execute(Unknown Source)
org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1461)
com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2136)
com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)
com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)
com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1058)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1191)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.pauseBeforeRetry(AmazonHttpClient.java:1653)
com.amazonaws.http.AmazonHttpClient$RequestExecutor.doPauseBeforeRetry(AmazonHttpClient.java:1679)
{noformat}


Another thing of note in this setup, is the staging committer is using an S3 bucket to track pending commits (used to use HDFS for this, but switched to S3 once it became strongly consistent)."
FsShell to use openFile(sequential) for reads,13398732,Resolved,Major,Duplicate,01/Sep/21 15:09,24/Apr/22 16:39,,"FsShell commands to use openFile(sequential) for reading data, for better performance in clusters where the default s3a fs policy == random"
"openFile() to add standard options for read policies, start/end ranges",13398731,Resolved,Major,Duplicate,01/Sep/21 15:08,24/Apr/22 16:39,3.3.1,"Define standard openFile() options for read policies, start and end ranges.

* document
* include in contract tests
"
Support IPV6 with IP for internal and external communication,13394500,In Progress,Major,,10/Aug/21 20:23,,,"Support IPV6 with IP for internal and external communication 

WebAppProxy Service & embedded webapp mode"
Support IPv6 for ChecksumFileSystem,13394611,Open,Major,,11/Aug/21 10:22,,,"org.apache.hadoop.fs.ChecksumFileSystem#getChecksumFile() fail to parse on IPv6 URL. This needs to handle conditionally.

[~Hemanth Boyina]"
Always use GitHub PR rather than JIRA to review patches,13389498,Resolved,Major,Done,14/Jul/21 01:32,27/Mar/22 18:18,,"Now there are 2 types of precommit jobs in https://ci-hadoop.apache.org/
(1) Precommit-(HADOOP|HDFS|MAPREDUCE|YARN)-Build jobs that try to download patches from JIRA and test them.
(2) hadoop-multibranch job for GitHub PR

The problems are:
- The build configs are separated. The (2) config is in Jenkinsfile, and the (1) configs are in the Jenkins. When we update Jenkinsfile, I had to manually update the configs of the 4 precommit jobs via Jenkins Web UI.
- The (1) build configs are static. We cannot use separate config for each branch. This may cause some build failures.
- GitHub Actions cannot be used in the (1) jobs.

Therefore I want to disable the (1) jobs and always use GitHub PR to review patches.

How to do this:

1. Update the wiki: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute#HowToContribute-Provideapatch
2. Disable the Precommit-(HADOOP|HDFS|MAPREDUCE|YARN)-Build jobs."
ABFS: Fix unchecked cast compiler warning for AbfsListStatusRemoteIterator,13397061,Resolved,Major,Fixed,25/Aug/21 04:46,01/Mar/22 13:50,3.3.1,"Hadoop yetus run shows a java compiler warning for unchecked casting of Object to Iterator<FileStatus> in a method of AbfsListStatusRemoteIterator class. This can be resolved by introducing a new class to hold the iterator and exception thrown when applicable.

 

hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsListStatusRemoteIterator.java:97:38:[unchecked] unchecked cast

 

This task will also add logging to facilitate better investigation of the transient failure tracked in HADOOP-17797."
Add extensions to ProtobufRpcEngine RequestHeaderProto,13392180,Resolved,Major,Fixed,27/Jul/21 17:38,28/Jul/21 23:18,,The header used in ProtobufRpcEngine messages doesn't allow for new properties to be added by child classes. We can add a range of extensions that can be useful for proto classes that need to extend RequestHeaderProto.
NullPointerException when no HTTP response set on AbfsRestOperation,13403017,Resolved,Major,Fixed,23/Sep/21 20:45,30/Sep/21 12:40,3.3.1,"Seen when running HBase 2.2 on top of ABFS with Hadoop 3.1ish:
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.fs.azurebfs.services.AbfsClient.renameIdempotencyCheckOp(AbfsClient.java:382)
        at org.apache.hadoop.fs.azurebfs.services.AbfsClient.renamePath(AbfsClient.java:348)
        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.rename(AzureBlobFileSystemStore.java:722)
        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.rename(AzureBlobFileSystem.java:327)
        at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:249)
        at org.apache.hadoop.hbase.regionserver.HRegionFileSystem.rename(HRegionFileSystem.java:1115) {noformat}
Digging in, it looks like the {{AbfsHttpOperation}} inside of {{AbfsRestOperation}} may sometimes be null, but {{AbfsClient}} will try to unwrap it (and read the status code from the HTTP call). I'm not sure why we sometimes _get_ the null HttpOperation but it seems pretty straightforward to not get the NPE.

HBase got wedged after this, but I'm not sure if it's because of this NPE or (perhaps) we weren't getting any responses from ABFS itself (i.e. there was some ABFS outage/unavailability or the node itself couldn't talk to ABFS)."
ITestS3AContractDistCp failing IllegalStateException: WFOPENSSL0030 Running handshake with buffered unwrapped data,13392111,Resolved,Major,Cannot Reproduce,27/Jul/21 12:28,05/Jan/22 16:01,3.3.2,"Test failure against s3 london with ddb for s3guard.: IllegalStateException: WFOPENSSL0030 Running handshake with buffered unwrapped data

This is on a system with openssl (LibreSSL 2.8.3) , wildfly possibly using it"
Build Hadoop on Centos 7,13398363,Resolved,Major,Fixed,31/Aug/21 05:30,08/Nov/21 12:16,2.10.2,"Getting Hadoop to build on Centos 7 will greatly benefit the community. Here, we aim to provide a Dockerfile that builds out the image with all the dependencies needed to build Hadoop on Centos 7.

 "
s3a: set fs.s3a.downgrade.syncable.exceptions = true by default,13402524,Resolved,Major,Fixed,21/Sep/21 17:15,09/Nov/21 13:56,3.3.1,"HADOOP-17597 set policy of reacting to hsync() on an s3 output stream to be one of :Fail, warn, with default == fail.

I propose downgrading this to warn. We've done it internally, after having it on fail long enough to identify which processes were doing either of
* having unrealistic expectations about the output stream (fix: move off s3)
* were using hflush() as a variant of flush(), with the failure being an over-reaction


"
Run junit in Jenkins only if surefire reports exist,13395792,Resolved,Major,Fixed,18/Aug/21 03:30,24/Aug/21 17:27,,"We need to check if some xml files exist under surefire-reports before running junit in Jenkins. 

 !image-2021-08-18-08-59-14-022.png|thumbnail! "
Refactor fetching of credentials in Jenkins,13387080,Resolved,Major,Fixed,01/Jul/21 14:32,06/Aug/21 18:56,3.4.0,Need to refactor fetching of credentials in Jenkinsfile.
Lookup old S3 encryption configs for JCEKS,13402150,Resolved,Major,Fixed,20/Sep/21 10:35,05/Oct/21 11:18,3.4.0,HADOOP-17871 introduces new set of S3 encryption configs which are replaced by old property names during look-up. We need to look-up for both the properties since either could be set in a JCEKS file.
JsonSerialization raises EOFException reading JSON data stored on google GCS,13404043,Resolved,Major,Fixed,29/Sep/21 10:39,02/Nov/21 10:11,3.3.1,"The JsonSerialization<> load code doesn't work on gcs as it uses ""stream.available()"" to fail with a meaningful message if the stream is empty.

But that method is meant to say how much data is available without blocking, something we actually get wrong ourselves. Google GCS team didn't get it wrong, so on a read(), if there's no local buffer, an EOFException is raised"
HADOOP-17817. S3A to raise IOE if both S3-CSE and S3Guard enabled,13392085,Resolved,Major,Fixed,27/Jul/21 10:28,28/Jul/21 14:36,3.4.0,Throw an exception if S3Guard and S3 Client-side encryption are enabled on a bucket. Follow-up to HADOOP-13887. 
S3A Tests to skip if S3Guard and S3-CSE are enabled.,13392748,Resolved,Major,Fixed,30/Jul/21 11:54,05/Aug/21 10:46,3.4.0,Skip S3A tests when S3Guard and S3-CSE are enabled since it causes PathIOE otherwise.
Distcp file length comparison have no effect,13402856,Resolved,Major,Fixed,23/Sep/21 06:14,18/Oct/21 10:22,3.3.1,"the params for compareFileLengthsAndChecksums in RetriableFileCopyCommand have no effect

current is
{code:java}
        DistCpUtils.compareFileLengthsAndChecksums(source.getLen(), sourceFS,
                sourcePath, sourceChecksum, targetFS,
                targetPath, skipCrc, source.getLen());{code}
{code:java}
public static void compareFileLengthsAndChecksums(long srcLen,
           FileSystem sourceFS, Path source, FileChecksum sourceChecksum,
           FileSystem targetFS, Path target, boolean skipCrc,
           long targetLen) throws IOException {
  if (srcLen != targetLen) {
    throw new IOException(
        DistCpConstants.LENGTH_MISMATCH_ERROR_MSG + source + "" ("" + srcLen
            + "") and target:"" + target + "" ("" + targetLen + "")"");
  }
{code}
so, compare source.getLen() with source.getLen()...

It should be like below in history view
{code:java}
        DistCpUtils.compareFileLengthsAndChecksums(source.getLen(), sourceFS,
                sourcePath, sourceChecksum, targetFS,
                targetPath, skipCrc, offset + bytesRead);
{code}
 

 "
Upgrade JSON smart to 1.3.3 on branch-2.10,13398783,Resolved,Major,Fixed,01/Sep/21 18:54,02/Sep/21 21:40,2.10.0,"Currently branch-2.10 is using JSON Smart 1.3.1 version which is vulnerable to [link CVE-2021-27568|https://nvd.nist.gov/vuln/detail/CVE-2021-27568].

We can upgrade the version to 1.3.1.

+Description of the vulnerability:+

{quote}An issue was discovered in netplex json-smart-v1 through 2015-10-23 and json-smart-v2 through 2.4. An exception is thrown from a function, but it is not caught, as demonstrated by NumberFormatException. When it is not caught, it may cause programs using the library to crash or expose sensitive information.{quote}"
DistCp derives if XAttr is supported wrongly,13402138,Patch Available,Major,,20/Sep/21 08:49,,,"{color:#0033b3}public static void {color}{color:#00627a}checkFileSystemXAttrSupport{color}({color:#000000}FileSystem {color}fs)
 {color:#0033b3}throws {color}{color:#000000}XAttrsNotSupportedException {color}{
 {color:#0033b3}try {color}{
 fs.getXAttrs({color:#0033b3}new {color}Path({color:#000000}Path{color}.{color:#871094}SEPARATOR{color}));
 } {color:#0033b3}catch {color}({color:#000000}Exception {color}e) {
 {color:#0033b3}throw new {color}XAttrsNotSupportedException({color:#067d17}""XAttrs not supported for file system: ""
{color} + fs.getUri());
 }
}

IOExecption will also be treated as if Xattr is not supported, which would be wrong.

 "
Some Hadoop 3.3.1 jdiff files are missing,13404003,Open,Major,,29/Sep/21 06:02,,,"When releasing 3.3.1, HDFS jdiff file is added by https://github.com/apache/hadoop/commit/a77bf7cf07189911da99e305e3b80c589edbbfb5, but the other jdiff files are missing.

In addition, the base version is still 2.7.2 in hadoop-yarn and hadoop-mapreduce when running jdiff. The version should be upgraded."
Backport HADOOP-15993 to branch-3.2 which address CVE-2014-4611,13401679,Resolved,Major,Fixed,16/Sep/21 14:57,24/Sep/21 06:00,,"Now the version is 0.8.2.1 and it has net.jpountz.lz4:lz4:1.2.0 dependency, which is vulnerable. ([https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-4611])

 

cc./ [~aajisaka]"
Upgrade OpenSSL to 1.1.1l for vulnerability fix,13402206,Open,Major,,20/Sep/21 14:00,,,"A vulnerability scan reported the following CVEs in openSSL 1.1.1k in Hadoop:

CVE-2021-3711

CVE-2021-3712

Affects jars :

libssl.so.1.1

libcrypto.so.1.1

libcrypto.a

libssl.a"
Improve PrometheusSink for Namenode TopMetrics,13399445,Resolved,Major,Fixed,05/Sep/21 11:18,21/Sep/21 01:48,3.4.0,"HADOOP-16398 added exporter for hadoop metrics to prometheus. But some of metrics can't be exported  validly. For example like these metrics, 

1.  -queue metrics for ResourceManager-
{code:java}
queue_metrics_max_capacity{queue=""root.queue1"",context=""yarn"",hostname=""rm_host1""} 1
// queue2's metric can't be exported queue_metrics_max_capacity{queue=""root.queue2"",context=""yarn"",hostname=""rm_host1""} 2
{code}
-It always exported  only one queue's metric because PrometheusMetricsSink$metricLines only cache one metric  if theses metrics have the same name no matter these metrics has different metric tags.-

 

2. -rpc metrics for Namenode-

-Namenode may have rpc metrics with multi port like service-rpc. But because  the same reason  as  Issue 1, it wiil lost some rpc metrics if we use PrometheusSink.-
{code:java}
rpc_rpc_queue_time300s90th_percentile_latency{port=""9000"",servername=""ClientNamenodeProtocol"",context=""rpc"",hostname=""nnhost""} 0
// rpc port=9005 metric can't be exported 
rpc_rpc_queue_time300s90th_percentile_latency{port=""9005"",servername=""ClientNamenodeProtocol"",context=""rpc"",hostname=""nnhost""} 0
{code}
3. TopMetrics for Namenode

org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics is a special metric. And I think It is essentially a Summary metric type. TopMetrics record name will according to different user and op ,  which means that these metric will always exist in PrometheusMetricsSink$metricLines and it may cause the risk of its memory leak. We e need to treat it special. 
{code:java}
// invaild topmetric export
# TYPE nn_top_user_op_counts_window_ms_1500000_op_safemode_get_user_hadoop_client_ip_test_com_count counter
nn_top_user_op_counts_window_ms_1500000_op_safemode_get_user_hadoop_client_ip_test_com_count{context=""dfs"",hostname=""nn_host"",op=""safemode_get"",user=""hadoop/client-ip@TEST.COM""} 10

// it should be 
# TYPE nn_top_user_op_counts_window_ms_1500000_count counter
nn_top_user_op_counts_window_ms_1500000_count{context=""dfs"",hostname=""nn_host"",op=""safemode_get"",user=""hadoop/client-ip@TEST.COM""} 10{code}"
memory field in SequenceFile.Sorter java int overflow,13401763,Open,Major,,17/Sep/21 02:41,,3.3.1,"memory field in SequenceFile.Sorter, can cause run method in SequenceFile.Sorter.SortPass enter an endless loop.


If you set the ""io.sort.mb"" attribute to a value greater than 2050, as a result of
{code:java}
this.memory = conf.getInt(CommonConfigurationKeys.IO_SORT_MB_KEY, CommonConfigurationKeys.SEQ_IO_SORT_MB_DEFAULT) * 1024 * 1024;{code}
, memory field java int overflow, may become negative, so memoryLimit field in SequenceFile.Sorter.SortPass will be negative too, lead to run method in SequenceFile.Sorter.SortPass enter an endless loop."
fs.s3a.connection.maximum should be bigger than fs.s3a.threads.max,13397455,Resolved,Major,Fixed,26/Aug/21 07:58,30/Aug/21 17:32,3.3.0,
Refactor pkg-resolver installation,13401782,Open,Major,,17/Sep/21 06:02,,3.4.0,"In the view of keeping the Dockerfiles free from installation logic, we need to move the code for installing pkg-resolver to a script file and only run this script file from Dockerfile. Right now, we're installing the dependencies of pkg-resolver directly in the Dockerfile itself."
Fix compilation error of ITUseHadoopCodecs with -DskipShade,13401591,Resolved,Major,Duplicate,16/Sep/21 07:49,16/Sep/21 16:46,,"{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-client-integration-tests: Compilation failure: Compilation failure:
[ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[34,28] cannot find symbol
[ERROR]   symbol:   class RandomDatum
[ERROR]   location: package org.apache.hadoop.io
[ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[100,16] package RandomDatum does not exist
[ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[100,54] package RandomDatum does not exist
[ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[103,7] cannot find symbol
[ERROR]   symbol:   class RandomDatum
[ERROR]   location: class org.apache.hadoop.example.ITUseHadoopCodecs
[ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[104,7] cannot find symbol
[ERROR]   symbol:   class RandomDatum
[ERROR]   location: class org.apache.hadoop.example.ITUseHadoopCodecs
{noformat}"
FileUtil#fullyDelete deletes contents of sym-linked directory when symlink cannot be deleted because of local fs fault,13400907,Resolved,Major,Fixed,14/Sep/21 03:04,15/Sep/21 17:59,,"As discussed in HADOOP-6536, FileUtil#fullyDelete should not delete the contents of the sym-linked directory when we pass a symlink parameter. Currently we try to delete the resource first by calling deleteImpl, and if deleteImpl is failed, we regard it as non-empty directory and remove all its contents and then itself. This logic behaves wrong when local file system cannot delete symlink to a directory because of faulty disk, local system's error, etc. When we cannot delete it in the first time, hadoop will try to remove all the contents of the directory it pointed to and leave an empty dir.
So, we should add an isSymlink checking before we call fullyDeleteContents to prevent such behavior.

 "
Avoid using implicit dependency on junit-jupiter-api,13399896,Resolved,Major,Fixed,08/Sep/21 05:08,08/Sep/21 09:25,3.3.2,"Compilation of branch-3.3 fails due to lack of transitive dependency existing in trunk.
{noformat}
[ERROR] /home/centos/srcs/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[29,29] package org.junit.jupiter.api does not exist
[ERROR] /home/centos/srcs/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[47,4] cannot find symbol
  symbol:   class BeforeEach
  location: class org.apache.hadoop.fs.http.TestHttpFileSystem
[INFO] 2 errors
{noformat}"
CredentialProviderFactory.getProviders() recursion loading JCEKS file from s3a,13399586,Resolved,Major,Fixed,06/Sep/21 13:16,10/Sep/21 16:01,3.3.1,"{{CredentialProviderFactory.getProviders()}} will be called recursively if the FS being instantiated as a source of a JCECKs file calls {{Configuration.getPassword()}}.
 S3A FileSystem does this when looking at encryption settings, even when authentication
 is not being done by hadoop config options.

Recursion is straightforward to detect, leaving a choice of actions
 # Fail
 # Fix by having getPassword() return null; this implicitly skips use of the credential providers for the nested FS instance.
 # fix by skipping the specific credential provider factory with the problem (warn first?)

My preference is for #3."
Prometheus metrics only include the last set of labels,13389868,Resolved,Major,Fixed,15/Jul/21 16:02,09/Sep/21 08:12,3.3.1,"A prometheus endpoint was added in https://issues.apache.org/jira/browse/HADOOP-16398, but the logic that puts them into a map based on the ""key"" incorrectly hides any metrics with the same key but different labels. The relevant code is here: [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java#L55|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java#L55.]

The labels/tags need to be taken into account, as different tags mean different metrics. For example, I came across this while trying to scrape metrics for all the queues in our scheduler. Only the last queue is included because all the metrics have the same ""key"" but a different ""queue"" label/tag."
Add BuiltInGzipCompressor,13392845,Resolved,Major,Fixed,31/Jul/21 03:12,16/Aug/21 17:09,,"Currently, GzipCodec only supports BuiltInGzipDecompressor, if native zlib is not loaded. So, without Hadoop native codec installed, saving SequenceFile using GzipCodec will throw exception like ""SequenceFile doesn't work with GzipCodec without native-hadoop code!""

Same as other codecs which we migrated to using prepared packages (lz4, snappy), it will be better if we support GzipCodec generally without Hadoop native codec installed. Similar to BuiltInGzipDecompressor, we can use Java Deflater to support BuiltInGzipCompressor.

"
javadoc broken in branch-2.10 root,13400079,Open,Major,,08/Sep/21 18:37,,2.10.2,"I went through some of the qbt-reports email reports.
I noticed that javadoc root was failing for quite sometime.


{code:bash}
[INFO] --- maven-javadoc-plugin:3.3.0:javadoc (default-cli) @ hadoop-main ---
[WARNING] Error injecting: org.apache.maven.plugins.javadoc.JavadocReport
java.lang.TypeNotPresentException: Type org.apache.maven.plugins.javadoc.JavadocReport not present
	at org.eclipse.sisu.space.URLClassSpace.loadClass(URLClassSpace.java:147)
	at org.eclipse.sisu.space.NamedClass.load(NamedClass.java:46)
	at org.eclipse.sisu.space.AbstractDeferredClass.get(AbstractDeferredClass.java:48)
	at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:81)
	at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:53)
	at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:65)
	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:115)
	at org.eclipse.sisu.bean.BeanScheduler$Activator.onProvision(BeanScheduler.java:176)
	at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:126)
	at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68)
	at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:63)
	at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:45)
	at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016)
	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)
	at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012)
	at org.eclipse.sisu.inject.Guice4$1.get(Guice4.java:162)
	at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:81)
	at org.eclipse.sisu.plexus.LazyPlexusBean.getValue(LazyPlexusBean.java:51)
	at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:263)
	at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:255)
	at org.apache.maven.plugin.internal.DefaultMavenPluginManager.getConfiguredMojo(DefaultMavenPluginManager.java:517)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:121)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:607)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: java.lang.UnsupportedClassVersionError: org/apache/maven/plugins/javadoc/JavadocReport : Unsupported major.minor version 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:808)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:443)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:65)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:349)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:348)
	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClassFromSelf(ClassRealm.java:401)
	at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:42)
	at org.codehaus.plexus.classworlds.realm.ClassRealm.unsynchronizedLoadClass(ClassRealm.java:271)
	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:247)
	at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:239)
	at org.eclipse.sisu.space.URLClassSpace.loadClass(URLClassSpace.java:139)
	... 42 more
{code}
"
Upgrade BouncyCastle to 1.69,13399825,Resolved,Major,Duplicate,07/Sep/21 16:50,08/Sep/21 10:50,2.10.2,"Vulnerabilities reported in BouncyCastle:
[CVE-2020-26939|https://nvd.nist.gov/vuln/detail/CVE-2020-26939] moderate severity
[CVE-2020-15522|https://nvd.nist.gov/vuln/detail/CVE-2020-15522] moderate severity

Affecting releases before 1.66.
 
Upgrade to latest 1.69.

 "
Add config for Aliyun OSS cnam support,13397747,Open,Major,,27/Aug/21 08:50,,,"Add new config for Aliyun OSS cname support, enable user to close cname"
Unable to use custom SAS tokens for accessing files from ADLS gen2 storage accounts with hierarchical namespace enabled.,13398150,Open,Major,,30/Aug/21 07:46,,,"I have some parquet files in abfss://con@sa1.dfs.core.windows.net/folder1/. I generated the User Delegation SAS token with the following permission on 'folder1'

SAS_SIGNED_PERMISSIONS -> ""racwdxltmeop""

But when I read from ""abfss://con@sa1.dfs.core.windows.net/folder1/""

I get a *HTTP 403 error*, I believe this happens when ABFSS driver makes use of `*getACLStatus*` API call to determine whether the storage service has hierarchical namespace enabled or not.

 

I found a workaround, ie to set *fs.azure.account.hns.enabled* to *true* which would skip get ACL API call and as folder level SAS only works for HNS enabled accounts. May I know if this behavior is expected and the workaround I am using is stable for production use and if there are any hidden implications?

 

Thank you in advance. "
[KMS] FileNotFoundException due to inability to access zookeeper znode,13397689,Open,Major,,27/Aug/21 03:52,,,"A mysterious FileNotFoundException was thrown by KMS client. The KMS server did not log anything related to it. After extensive investigation it was realized it could be caused by either of the two Zookeeper errors:

(1) unable to access the znode /zkdtsm due to Zookeeper ACL or other problems.
(2) if there's a runtime dependency mismatch. In in case, our RangerKMS (where the HadoopKMS runtime is embedded), threw a NoSuchMethodError during curator initialization (the RangerKMS used an incompatible curator-recipes version at runtime). Curator was not initialized successfully and therefore unable to access the znode.

In any case, it would be great to surface up the zookeeper issue to make troubleshooting easier."
[KMS] Some error messages are not logged in the KMS server,13397407,Open,Major,,26/Aug/21 04:30,,,"While troubleshooting a recent KMS issue, it was found a certain exceptions are not logged in the server side. When client got an error, for example, ""token xxx not found in the cache"", we wanted to find out more details from the server side (stack trace, exact exception...etc) but it was not found."
Hadoop NativeAzureFileSystem append removes ownership set on the file,13395325,Open,Major,,15/Aug/21 11:24,,3.3.1,"*Repro:* Create Operation sets ownership whereas append operation removes the same.

Create:

*// -rw-r--r-- 1 root supergroup 1 2021-08-15 11:02 /tmp/dummyfile*

Append:

*// -rwxrwxrwx 1 <NULL>  <NULL> 2 2021-08-15 11:04 /tmp/dummyfile*
{code:java}
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.permission.FsPermission;

public class Wasb {

 private static final short FILE_LOG_PERMISSIONS = 0640;
 
 public static void main(String[] args) throws Exception {
 
    Configuration fsConf = new Configuration();
    fsConf.set(""fs.azure.enable.append.support"", ""true"");

Path filePath = new Path(""/tmp/dummyfile"");

FileSystem fs = FileSystem.newInstance(filePath.toUri(), fsConf);

FSDataOutputStream stream = fs.create(filePath, false);
stream.write(12345);
stream.close();

stream = fs.append(filePath);
stream.write(888);
stream.close();

fs.close();
 }
}
{code}"
Move Ozone to related projects section,13392647,Resolved,Major,Fixed,30/Jul/21 03:37,20/Aug/21 09:03,,"Hi all, as Ozone was spun to TLP, it has individual web site.

Now on Modules part of Hadoop [website|https://hadoop.apache.org/], the link of Ozone website is old page.

IMHO there are two ways to fix it :
1. update it to new page.
2. move Ozone to Related projects part on Hadoop website

Please feel free to give me some feedback, thanks"
NPE in S3AInputStream read() after failure to reconnect to store,13391226,Resolved,Major,Fixed,22/Jul/21 01:04,30/Jul/21 19:06,3.2.2,"when [reading from S3a storage|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L450], SSLException (which extends IOException) happens, which will trigger [onReadFailure|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L458].

onReadFailure calls ""reopen"". it will first close the original *wrappedStream* and set *wrappedStream = null*, and then it will try to [re-get *wrappedStream*|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L184]. But what if the previous code [obtaining S3Object|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L183] throw exception, then ""wrappedStream"" will be null.

And the [retry|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L446] mechanism may re-execute the [wrappedStream.read|https://github.com/apache/hadoop/blob/rel/release-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L450] and cause NPE.

 

For more details, please refer to [https://github.com/NVIDIA/spark-rapids/issues/2915]"
Upgrade ZooKeeper to 3.4.14 in branch-3.2,13395454,Resolved,Major,Fixed,16/Aug/21 10:50,17/Aug/21 04:37,3.2.2,Upgrade ZooKeeper 3.4.14 to fix CVE-2019-0201 (https://zookeeper.apache.org/security.html). That way the ZooKeeper version will be consistent with BigTop 3.0.0 (BIGTOP-3471).
Exclude spotbugs-annotations from transitive dependencies on branch-3.2,13395417,Resolved,Major,Fixed,16/Aug/21 08:00,16/Aug/21 21:04,3.2.2,"Building Hadoop in dist profile with ZooKeeper 3.4.14 fails on hadoop-client-check-test-invariants. Excluding com.github.spotbugs:spotbugs-annotation from transitive dependencies should fix this for users needing zookeeer-3.4.14. Since the dependency is provided/optional on ZooKeeper 3.5.x, branch-3.3 and above are not affected."
LocalFS to support ability to disable permission get/set; remove need for winutils,13393877,Open,Major,,06/Aug/21 11:11,,3.3.1,"
Setting FS permissions on windows has always been a paint point for people running spark standalone, creating files through localFS required WINUTILS.EX To do this

I'm seeing some other demand for disabling setting permissions, primarily because if you mount storage (including azure storage) to a VM, you cant call setPermissions; things fail. 

Proposed: 
1. we add an option to disable permission setting
2. we add a path capability, which for hdfs, abfs is always true (not sure about wasb as it varies on file vs dir)
3. we add the option to turn off permission get/set for RawLocalFileSystem and downgrade to a no-op on write, ug+rw on read (or even make configurable?)

(Distcp should maybe probe the path and downgrade -p options if the dest store doesn't do perms properly, which argues for wasb to return true, so file perms are set)"
Remove dependency on jdom,13392588,Resolved,Major,Won't Do,29/Jul/21 18:24,30/Jul/21 00:07,,"It doesn't seem that jdom is referenced anywhere in the code base now, yet it exists in the distribution.

{code}
$ find . -name ""*jdom*.jar""
./hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/jdom-1.1.jar
{code}

There is recently [CVE-2021-33813|https://github.com/advisories/GHSA-2363-cqg2-863c] issued for jdom. Let's remove the binary from the dist if not useful."
Replacing native lib with their Java wrappers,13393148,Resolved,Major,Later,02/Aug/21 20:43,03/Aug/21 00:10,,This is umbrella ticker covering all works for replacing native lib with their Java wrappers.
Parallelize stages in Jenkins,13387022,Open,Major,,01/Jul/21 10:23,,3.4.0,Jenkins now builds for multiple environments as different stages. Need to parallelize them.
Upgrade log4j to fix critical vulnerability,13393040,Resolved,Major,Duplicate,02/Aug/21 09:44,02/Aug/21 12:35,3.3.1,"CVE-2019-17571 - log4j-1.2.17 - (Fix available in log4j-2.8.2)

Please upgrade to log4j-2.8.2 to fix vulnerability"
Add Dockerfile for Ubuntu 18.04,13392999,Open,Major,,02/Aug/21 06:07,,3.4.0,Adding a Dockerfile for building on Ubuntu 18.04 since there are a lot of users in the community using this distro.
Add Dockerfile for Fedora 33,13392997,Open,Major,,02/Aug/21 06:06,,3.4.0,Adding a Dockerfile for building on Fedora 33 since there are a lot of users in the community using this distro.
WARN security.LdapGroupsMapping: Failed to get groups for user,13391077,Open,Major,,21/Jul/21 07:12,,3.0.0,"I run hadoop with ldap

 

hadoop.security.group.mapping=org.apache.hadoop.security.LdapGroupsMapping

hadoop.security.group.mapping.ldap.bind.user=uid=ldapadmin,ou=people,dc=join,dc=com

hadoop.security.group.mapping.ldap.bind.password=00000

hadoop.security.group.mapping.ldap.base=dc=join,dc=com

hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=posixAccount)(uid=\{0}))

hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=posixGroup)

hadoop.security.group.mapping.ldap.search.attr.member=memberUid

hadoop.security.group.mapping.ldap.search.attr.group.name=cn

 

but I run spark,I get a WARN

 

*(LdapGroupsMapping:290)2021-07-13 13:02:45,523 WARN  - [pool-2-thread-4:] ~ Failed to get groups for user jztwk (retry=0) by javax.naming.OperationNotSupportedException: [LDAP: error code 53 - unauthenticated bind (DN with no password) disallowed]*

 

and I *ldapsearch -x -D ""uid=ldapadmin,ou=people,dc=join,dc=com"" -W -b ""dc=join,dc=com""*

can get result

 

 

*hdfs groups yarn*
*yarn : hadoop spark yarn*

 

so How can I fix it

 

CDH 6.3.2"
Remove WARN logging from LoggingAuditor when executing a request outside an audit span ,13389816,Resolved,Major,Fixed,15/Jul/21 10:50,16/Jul/21 14:36,3.3.2,"Removing WARN logging when executing a request outside an audit span, since it would be present in DEBUG level. "
CLONE - Uber-JIRA: Hadoop should support IPv6,13389522,Open,Major,,14/Jul/21 04:27,,,"Hadoop currently treats IPv6 as unsupported. Track related smaller issues to support IPv6.

(Current case here is mainly HBase on HDFS, so any suggestions about other test cases/workload are really appreciated.)

Please see [Here | https://issues.apache.org/jira/browse/HADOOP-11890?focusedCommentId=17379845&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17379845] for more details."
Add a sample configuration to use ZKDelegationTokenSecretManager in Hadoop KMS,13388337,Resolved,Major,Fixed,08/Jul/21 08:48,09/Jul/21 13:54,,"The following parameters should be documented in 
https://hadoop.apache.org/docs/stable/hadoop-kms/index.html#Delegation_Tokens

* hadoop.kms.authentication.zk-dt-secret-manager.enable
* hadoop.kms.authentication.zk-dt-secret-manager.kerberos.keytab
* hadoop.kms.authentication.zk-dt-secret-manager.kerberos.principal
* hadoop.kms.authentication.zk-dt-secret-manager.zkConnectionString
* hadoop.kms.authentication.zk-dt-secret-manager.znodeWorkingPath
* hadoop.kms.authentication.zk-dt-secret-manager.zkAuthType"
"""hadoop.security.token.service.use_ip"" should be documented",13388168,Resolved,Major,Duplicate,07/Jul/21 11:42,08/Jul/21 06:45,,hadoop.security.token.service.use_ip is not documented in core-default.xml. It should be documented.
S3A Xattr/getXAttr to handle directories without markers,13389614,Open,Minor,,14/Jul/21 12:13,,3.3.1,"The XAttr support in S3 raises FNFE if given a directory as a path when that directory has children but no marker.

It should somehow downgrade (e.g. FNFE -> probe for a dir, and if found return an empty set of headers)"
ABFS: Test with 100MB buffer size in ITestAbfsReadWriteAndSeek times out ,13395707,Open,Minor,,17/Aug/21 15:23,,3.3.1,"testReadAndWriteWithDifferentBufferSizesAndSeek with buffer size above 100 MB is failing with timeout. It is delaying the whole test run by 15-30 mins. 

[ERROR] testReadAndWriteWithDifferentBufferSizesAndSeek[Size=104,857,600](org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek) Time elapsed: 1,800.041 s <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 1800000 milliseconds
 at sun.misc.Unsafe.park(Native Method)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
 at java.util.concurrent.FutureTask.get(FutureTask.java:191)
 at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.waitForAppendsToComplete(AbfsOutputStream.java:515)
 at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.flushWrittenBytesToService(AbfsOutputStream.java:533)
 at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.flushInternal(AbfsOutputStream.java:377)
 at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.close(AbfsOutputStream.java:337)
 at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)
 at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
 at org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek.testReadWriteAndSeek(ITestAbfsReadWriteAndSeek.java:81)
 at org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek(ITestAbfsReadWriteAndSeek.java:66)"
ABFS to log @ debug each config option *And where it comes from*,13397810,Open,Minor,,27/Aug/21 12:05,,3.3.1,"as with the other stores, there's a lot of options in abfs related to authentication, and when they are misconfigured, nothing works.

cloudstore's storediag can print out the options and their origins, but it's a command line tool, so cannot debug what applications like spark or hive are getting. 

Proposed: as each config option is resolved, print out where it came from, including which jcecks credential provider. "
AbstractAbfsIntegrationTest.createFilesystemForSASTests() leaks FS instances,13400246,Open,Minor,,09/Sep/21 13:48,,3.3.1,"I've noticed in MAPREDUCE-7341 that ABFS ITests had a lot of FS instances closed in the shutdown hook. 

Tracked down to the fact that  {{AbstractAbfsIntegrationTest.createFilesystemForSASTests()}} creates a temp FS via newInstance(), and doesn't close it...the instance is kept in the cache until shutdown.

I'm going to fix it in MAPREDUCE-7341, though it's something minor which could be pulled out for backporting"
transient failure of ITestAbfsListStatusRemoteIterator.testWithAbfsIteratorDisabledWithoutHasNext,13389406,Open,Minor,,13/Jul/21 14:29,,3.4.0,"assert failure of {{ITestAbfsListStatusRemoteIterator.estWithAbfsIteratorDisabledWithoutHasNext}}  on a (probably) overloaded parallel test run.

Did not occur standalone. 

Before identifying/fixing causes, the test case could be improved for more informative error reporting

* report when the fileNames.remove(pathStr) call returned false: the listing had found an  unexpected value
* tune the assert which did fail to include the unknown element.

This may just be a bug from duplicate filenames, in which case the patch in progress for that should make it ""go away"""
Update commons-lang to 3.12.0,13404150,Resolved,Minor,Fixed,29/Sep/21 21:49,26/Oct/21 01:20,3.3.2,"our commons-lang3 dependency is currently 3.7, which is nearly 4 years old. latest right now is 3.12 and there are at least some fixes that would make us more robust on JDKs newer than openjdk8 (e.g. LANG-1384. [release notes indicate 3.9 is the first to support jdk11|https://commons.apache.org/proper/commons-lang/changes-report.html])."
Fix typos in usage message in winutils.exe,13402809,Resolved,Minor,Fixed,22/Sep/21 22:37,27/Sep/21 20:42,3.4.0,"The usage message for task creation in winutils.exe has a few typos:
* OPTOINS
* cup rate"
The error of Constant  annotation in AzureNativeFileSystemStore.java,13398869,Resolved,Minor,Fixed,02/Sep/21 06:53,18/Oct/21 04:01,3.4.0,
Update xerces to 2.12.1,13403646,Resolved,Minor,Fixed,27/Sep/21 18:45,29/Sep/21 09:54,3.3.1,Update xerces due to CVE-2012-0881
Print RPC response length in the exception message,13401370,Resolved,Minor,Fixed,15/Sep/21 11:42,17/Sep/21 06:49,3.4.0,"To facilitate problem tracking, we can print RPC Response Length in the exception message."
Make it easier to debug UnknownHostExceptions from NetUtils.connect,13393766,Resolved,Minor,Fixed,05/Aug/21 19:30,06/Aug/21 16:30,3.3.2,"Most UnknownHostExceptions thrown throughout hadoop include a useful message, either the hostname that was not found or some other descriptor of the problem. The UnknownHostException thrown from NetUtils.connect only includes the [message of the underlying UnresolvedAddressException|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java#L592]. If you take a look at the source for UnresolvedAddressException, [it only has a no-args constructor|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/channels/UnresolvedAddressException.html] (java11, but same is true in other versions). So it never has a message, meaning the UnknownHostException message is empty.

We should include the endpoint.toString() in the UnknownHostException thrown by NetUtils.connect"
ipc.Client not setting interrupt flag after catching InterruptedException,13390918,Resolved,Minor,Fixed,20/Jul/21 12:01,06/Aug/21 02:44,3.3.2,"ipc.Client is swallowing InterruptedException at a couple of places:
 # While waiting on all connections to be closed
 # While waiting to retrieve some RPC response

We should at least set the interrupt signal and also log the InterruptedException caught."
Remove ListenerHandle from Hadoop registry,13394094,Resolved,Minor,Fixed,08/Aug/21 17:38,09/Aug/21 08:58,3.4.0,"As part of HADOOP-17835 (replacing PathChildrenCache/TreeCache by CuratorCache), realized that although registerPathListener() of CuratorService returns ListenerHandle, it is not used by RegistryDNSServer. We can remove ListenerHandle from hadoop-registry as it is not Public/LP interface."
Fix command line example in Hadoop Cluster Setup documentation,13401775,Resolved,Minor,Fixed,17/Sep/21 05:14,17/Sep/21 13:37,3.3.1,"About Hadoop cluster setup documentation ([https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html])

The option <cluster_name> is specified in the following example, but HDFS command ignores it.
{noformat}
`[hdfs]$ $HADOOP_HOME/bin/hdfs namenode -format <cluster_name>`
{noformat}"
ITestCustomSigner fails with NPE against private endpoint,13392813,Resolved,Minor,Cannot Reproduce,30/Jul/21 19:00,19/Jan/24 14:07,3.3.1,ITestCustomSigner fails when the tester is pointed at a private endpoint
Improve Magic Committer Performance,13393249,Resolved,Minor,Fixed,03/Aug/21 10:41,21/Jun/22 09:51,3.3.1,"Magic committer tasks can be slow because every file created with overwrite=false triggers a HEAD (verify there's no file) and a LIST (that there's no dir). And because of delayed manifestations, it may not behave as expected.

ParquetOutputFormat is one example of a library which does this.

we could fix parquet to use overwrite=true, but (a) there may be surprises in other uses (b) it'd still leave the list and (c) do nothing for other formats call

Proposed: createFile() under a magic path to skip all probes for file/dir at end of path

Only a single task attempt Will be writing to that directory and it should know what it is doing. If there is conflicting file names and parts across tasks that won't even get picked up at this point. Oh and none of the committers ever check for this: you'll get the last file manifested (s3a) or renamed (file)

If we skip the checks we will save 2 HTTP requests/file.
"
S3a parquet reads slow with Spark on Kubernetes (EKS),13394128,Resolved,Minor,Works for Me,09/Aug/21 04:13,09/Aug/21 13:38,3.2.0,"I am trying to read parquet saved in S3 via Spark on EKS using hadoop-AWS 3.2.0. There are 112 partitions (each around 130MB) for a particular month.

 

The data is being read but very very slowly. I just keep seeing below and very small dataset actually being fetched.

 

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: Values passed - text: read on s3a://uat1-prp-rftu-25-045552507264-us-east-1/xxxx/yyyy/zzzz/table_fact_mtd_c/ptn_val_txt=20200229/part-00012-32dbfb10-b43c-4066-a70e-d3575ea530d5-c000.snappy.parquet, idempotent: true, Retried: org.apache.hadoop.fs.s3a.S3AFileSystem$$Lambda$1199/2130521693@5259f9d0, Operation:org.apache.hadoop.fs.s3a.Invoker$$Lambda$1239/37396157@454de3d3

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: retryUntranslated begin

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: Values passed - text: lazySeek on s3a://uat1-prp-rftu-25-045552507264-us-east-1/xxxx/yyyy/zzzz/table_fact_mtd_c/ptn_val_txt=20200229/part-00012-32dbfb10-b43c-4066-a70e-d3575ea530d5-c000.snappy.parquet, idempotent: true, Retried: org.apache.hadoop.fs.s3a.S3AFileSystem$$Lambda$1199/2130521693@5259f9d0, Operation:org.apache.hadoop.fs.s3a.Invoker$$Lambda$1239/37396157@3776ef6c

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: retryUntranslated begin

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: Values passed - text: read on s3a://uat1-prp-rftu-25-045552507264-us-east-1/xxxx/yyyy/zzzz/table_fact_mtd_c/ptn_val_txt=20200229/part-00012-32dbfb10-b43c-4066-a70e-d3575ea530d5-c000.snappy.parquet, idempotent: true, Retried: org.apache.hadoop.fs.s3a.S3AFileSystem$$Lambda$1199/2130521693@5259f9d0, Operation:org.apache.hadoop.fs.s3a.Invoker$$Lambda$1239/37396157@3602676a

21/08/09 05:07:05 DEBUG Executor task launch worker for task 60.0 in stage 3.0 (TID 63) Invoker: retryUntranslated begin

 

Here is the spark config for hadoop-aws.
|spark.hadoop.fs.s3a.assumed.role.sts.endpoint: https://sts.amazonaws.com|
|spark.hadoop.fs.s3a.assumed.role.sts.endpoint.region: us-east-1|
|spark.hadoop.fs.s3a.attempts.maximum: 20|
|spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider|
|spark.hadoop.fs.s3a.block.size: 128M|
|spark.hadoop.fs.s3a.connection.establish.timeout: 50000|
|spark.hadoop.fs.s3a.connection.maximum: 50|
|spark.hadoop.fs.s3a.connection.ssl.enabled: true|
|spark.hadoop.fs.s3a.connection.timeout: 2000000|
|spark.hadoop.fs.s3a.endpoint: s3.us-east-1.amazonaws.com|
|spark.hadoop.fs.s3a.etag.checksum.enabled: false|
|spark.hadoop.fs.s3a.experimental.input.fadvise: normal|
|spark.hadoop.fs.s3a.fast.buffer.size: 1048576|
|spark.hadoop.fs.s3a.fast.upload: true|
|spark.hadoop.fs.s3a.fast.upload.active.blocks: 8|
|spark.hadoop.fs.s3a.fast.upload.buffer: bytebuffer|
|spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem|
|spark.hadoop.fs.s3a.list.version: 2|
|spark.hadoop.fs.s3a.max.total.tasks: 30|
|spark.hadoop.fs.s3a.metadatastore.authoritative: false|
|spark.hadoop.fs.s3a.metadatastore.impl: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore|
|spark.hadoop.fs.s3a.multiobjectdelete.enable: true|
|spark.hadoop.fs.s3a.multipart.purge: true|
|spark.hadoop.fs.s3a.multipart.purge.age: 86400|
|spark.hadoop.fs.s3a.multipart.size: 32M|
|spark.hadoop.fs.s3a.multipart.threshold: 64M|
|spark.hadoop.fs.s3a.paging.maximum: 5000|
|spark.hadoop.fs.s3a.readahead.range: 65536|
|spark.hadoop.fs.s3a.retry.interval: 500ms|
|spark.hadoop.fs.s3a.retry.limit: 20|
|spark.hadoop.fs.s3a.retry.throttle.interval: 500ms|
|spark.hadoop.fs.s3a.retry.throttle.limit: 20|
|spark.hadoop.fs.s3a.s3.client.factory.impl: org.apache.hadoop.fs.s3a.DefaultS3ClientFactory|
|spark.hadoop.fs.s3a.s3guard.ddb.background.sleep: 25|
|spark.hadoop.fs.s3a.s3guard.ddb.max.retries: 20|
|spark.hadoop.fs.s3a.s3guard.ddb.region: us-east-1|
|spark.hadoop.fs.s3a.s3guard.ddb.table: s3-data-guard-master|
|spark.hadoop.fs.s3a.s3guard.ddb.table.capacity.read: 500|
|spark.hadoop.fs.s3a.s3guard.ddb.table.capacity.write: 100|
|spark.hadoop.fs.s3a.s3guard.ddb.table.create: true|
|spark.hadoop.fs.s3a.s3guard.ddb.throttle.retry.interval: 1s|
|spark.hadoop.fs.s3a.socket.recv.buffer: 8388608|
|spark.hadoop.fs.s3a.socket.send.buffer: 8388608|
|spark.hadoop.fs.s3a.threads.keepalivetime: 60|
|spark.hadoop.fs.s3a.threads.max: 50|

 

Not sure if you need it - still putting it across (other spark configuration)
|spark.app.id: spark-b97cb651f3f14c6cb3197079376a74c7|
|spark.app.startTime: 1628476986471|
|spark.blockManager.port: 0|
|spark.broadcast.compress: true|
|spark.checkpoint.compress: true|
|spark.cleaner.periodicGC.interval: 2min|
|spark.cleaner.referenceTracking: true|
|spark.cleaner.referenceTracking.blocking: true|
|spark.cleaner.referenceTracking.blocking.shuffle: true|
|spark.cleaner.referenceTracking.cleanCheckpoints: true|
|spark.cores.max: 5|
|spark.driver.bindAddress: 28.132.124.86|
|spark.driver.blockManager.port: 0|
|spark.driver.cores: 5|
|spark.driver.extraJavaOptions: -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'|
|spark.driver.host: xxx-xxxx-xxx-8be6777b28caacc7-driver-svc.default.svc|
|spark.driver.maxResultSize: 10008m|
|spark.driver.memory: 10008m|
|spark.driver.memoryOverhead: 384m|
|spark.driver.port: 7078|
|spark.driver.rpc.io.clientThreads: 5|
|spark.driver.rpc.io.serverThreads: 5|
|spark.driver.rpc.netty.dispatcher.numThreads: 5|
|spark.driver.shuffle.io.clientThreads: 5|
|spark.driver.shuffle.io.serverThreads: 5|
|spark.dynamicAllocation.cachedExecutorIdleTimeout: 600s|
|spark.dynamicAllocation.enabled: false|
|spark.dynamicAllocation.executorAllocationRatio: 1.0|
|spark.dynamicAllocation.executorIdleTimeout: 60s|
|spark.dynamicAllocation.initialExecutors: 1|
|spark.dynamicAllocation.maxExecutors: 2147483647|
|spark.dynamicAllocation.minExecutors: 1|
|spark.dynamicAllocation.schedulerBacklogTimeout: 1s|
|spark.dynamicAllocation.shuffleTracking.enabled: true|
|spark.dynamicAllocation.shuffleTracking.timeout: 600s|
|spark.dynamicAllocation.sustainedSchedulerBacklogTimeout: 1s|
|spark.eventLog.dir: /opt/efs/spark|
|spark.eventLog.enabled: true|
|spark.eventLog.logStageExecutorMetrics: false|
|spark.excludeOnFailure.enabled: true|
|spark.executor.cores: 5|
|spark.executor.extraJavaOptions: -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'|
|spark.executor.id: driver|
|spark.executor.instances: 22|
|spark.executor.logs.rolling.enableCompression: false|
|spark.executor.logs.rolling.maxRetainedFiles: 5|
|spark.executor.logs.rolling.maxSize: 10m|
|spark.executor.logs.rolling.strategy: size|
|spark.executor.memory: 10008m|
|spark.executor.memoryOverhead: 384m|
|spark.executor.processTreeMetrics.enabled: false|
|spark.executor.rpc.io.clientThreads: 5|
|spark.executor.rpc.io.serverThreads: 5|
|spark.executor.rpc.netty.dispatcher.numThreads: 5|
|spark.executor.shuffle.io.clientThreads: 5|
|spark.executor.shuffle.io.serverThreads: 5|
|spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: 2|
|spark.history.fs.driverlog.cleaner.enabled: true|
|spark.history.fs.driverlog.cleaner.maxAge: 2d|
|spark.history.fs.logDirectory: /opt/efs/spark|
|spark.history.ui.port: 4040|
|spark.io.compression.codec: org.apache.spark.io.SnappyCompressionCodec|
|spark.io.compression.snappy.blockSize: 32k|
|spark.jars: local:///opt/spark/examples/xxx.jar,local:///opt/spark/examples/yyy.jar|
|spark.kryo.referenceTracking: false|
|spark.kryo.registrationRequired: false|
|spark.kryo.unsafe: true|
|spark.kryoserializer.buffer: 8m|
|spark.kryoserializer.buffer.max: 1024m|
|spark.kubernetes.allocation.batch.delay: 1s|
|spark.kubernetes.allocation.batch.size: 5|
|spark.kubernetes.allocation.executor.timeout: 600s|
|spark.kubernetes.appKillPodDeletionGracePeriod: 5s|
|spark.kubernetes.authenticate.driver.serviceAccountName: spark|
|spark.kubernetes.configMap.maxSize: 1572864|
|spark.kubernetes.container.image: xxx/xxx:latest|
|spark.kubernetes.container.image.pullPolicy: Always|
|spark.kubernetes.driver.connectionTimeout: 10000|
|spark.kubernetes.driver.limit.cores: 8|
|spark.kubernetes.driver.master: https://asdkadalksjdas.gr7.us-east-1.eks.amazonaws.com:443|
|spark.kubernetes.driver.pod.name: xxx-ddd-rrrr-8be6777b28caacc7-driver|
|spark.kubernetes.driver.request.cores: 5|
|spark.kubernetes.driver.requestTimeout: 10000|
|spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc-mount-d.mount.path: /opt/efs/spark|
|spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc-mount-d.mount.readOnly: false|
|spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc-mount-d.mount.subPath: spark|
|spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc-mount-d.options.claimName: efs-pvc|
|spark.kubernetes.driver.volumes.persistentVolumeClaim.efs-pvc-mount-d.options.storageClass: manual|
|spark.kubernetes.dynamicAllocation.deleteGracePeriod: 5s|
|spark.kubernetes.executor.apiPollingInterval: 60s|
|spark.kubernetes.executor.checkAllContainers: true|
|spark.kubernetes.executor.deleteOnTermination: false|
|spark.kubernetes.executor.eventProcessingInterval: 5s|
|spark.kubernetes.executor.limit.cores: 8|
|spark.kubernetes.executor.missingPodDetectDelta: 30s|
|spark.kubernetes.executor.podNamePrefix: uscb-exec|
|spark.kubernetes.executor.request.cores: 5|
|spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc-mount-e.mount.path: /opt/efs/spark|
|spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc-mount-e.mount.readOnly: false|
|spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc-mount-e.mount.subPath: spark|
|spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc-mount-e.options.claimName: efs-pvc|
|spark.kubernetes.executor.volumes.persistentVolumeClaim.efs-pvc-mount-e.options.storageClass: manual|
|spark.kubernetes.local.dirs.tmpfs: false|
|spark.kubernetes.memoryOverheadFactor: 0.1|
|spark.kubernetes.namespace: default|
|spark.kubernetes.report.interval: 5s|
|spark.kubernetes.resource.type: java|
|spark.kubernetes.submission.connectionTimeout: 10000|
|spark.kubernetes.submission.requestTimeout: 10000|
|spark.kubernetes.submission.waitAppCompletion: true|
|spark.kubernetes.submitInDriver: true|
|spark.local.dir: /tmp|
|spark.locality.wait: 3s|
|spark.locality.wait.node: 3s|
|spark.locality.wait.process: 3s|
|spark.locality.wait.rack: 3s|
|spark.master: k8s://https://NKSLODISNJSKSJSKKLS.gr7.us-east-1.eks.amazonaws.com:443|
|spark.memory.fraction: 0.6|
|spark.memory.offHeap.enabled: false|
|spark.memory.storageFraction: 0.5|
|spark.network.io.preferDirectBufs: true|
|spark.network.maxRemoteBlockSizeFetchToMem: 200m|
|spark.network.timeout: 120s|
|spark.port.maxRetries: 16|
|spark.rdd.compress: false|
|spark.reducer.maxBlocksInFlightPerAddress: 2147483647|
|spark.reducer.maxReqsInFlight: 2147483647|
|spark.reducer.maxSizeInFlight: 48m|
|spark.repl.local.jars: local:///opt/spark/examples/asdasdasd.jar|
|spark.rpc.askTimeout: 120s|
|spark.rpc.io.backLog: 256|
|spark.rpc.io.clientThreads: 5|
|spark.rpc.io.serverThreads: 5|
|spark.rpc.lookupTimeout: 120s|
|spark.rpc.message.maxSize: 128|
|spark.rpc.netty.dispatcher.numThreads: 5|
|spark.rpc.numRetries: 3|
|spark.rpc.retry.wait: 3s|
|spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout: 120s|
|spark.scheduler.listenerbus.eventqueue.appStatus.capacity: 10000|
|spark.scheduler.listenerbus.eventqueue.capacity: 10000|
|spark.scheduler.listenerbus.eventqueue.eventLog.capacity: 10000|
|spark.scheduler.listenerbus.eventqueue.executorManagement.capacity: 10000|
|spark.scheduler.listenerbus.eventqueue.shared.capacity: 10000|
|spark.scheduler.maxRegisteredResourcesWaitingTime: 30s|
|spark.scheduler.minRegisteredResourcesRatio: 0.8|
|spark.scheduler.mode: FIFO|
|spark.scheduler.resource.profileMergeConflicts: false|
|spark.scheduler.revive.interval: 1s|
|spark.serializer: org.apache.spark.serializer.KryoSerializer|
|spark.serializer.objectStreamReset: 100|
|spark.shuffle.accurateBlockThreshold: 104857600|
|spark.shuffle.compress: true|
|spark.shuffle.file.buffer: 128m|
|spark.shuffle.io.backLog: -1|
|spark.shuffle.io.maxRetries: 3|
|spark.shuffle.io.numConnectionsPerPeer: 4|
|spark.shuffle.io.preferDirectBufs: true|
|spark.shuffle.io.retryWait: 5s|
|spark.shuffle.maxChunksBeingTransferred: 9223372036854775807|
|spark.shuffle.registration.maxAttempts: 3|
|spark.shuffle.registration.timeout: 200|
|spark.shuffle.service.enabled: false|
|spark.shuffle.service.index.cache.size: 100m|
|spark.shuffle.service.port: 7737|
|spark.shuffle.sort.bypassMergeThreshold: 200|
|spark.shuffle.spill.compress: true|
|spark.speculation: false|
|spark.speculation.interval: 5s|
|spark.speculation.multiplier: 1.5|
|spark.speculation.quantile: 0.75|
|spark.speculation.task.duration.threshold: 10s|
|spark.sql.adaptive.coalescePartitions.enabled: true|
|spark.sql.adaptive.enabled: true|
|spark.sql.adaptive.fetchShuffleBlocksInBatch: true|
|spark.sql.adaptive.forceApply: false|
|spark.sql.adaptive.localShuffleReader.enabled: true|
|spark.sql.adaptive.logLevel: debug|
|spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin: 0|
|spark.sql.adaptive.skewJoin.enabled: true|
|spark.sql.adaptive.skewJoin.skewedPartitionFactor: 5|
|spark.sql.adaptive.skewJoin.skewedPartitionThresholdInByte: 256MB|
|spark.sql.addPartitionInBatch.size: 100|
|spark.sql.analyzer.failAmbiguousSelfJoin: true|
|spark.sql.analyzer.maxIterations: 100|
|spark.sql.ansi.enabled: false|
|spark.sql.autoBroadcastJoinThreshold: 10MB|
|spark.sql.avro.filterPushdown.enabled: true|
|spark.sql.broadcastExchange.maxThreadThreshold: 128|
|spark.sql.bucketing.coalesceBucketsInJoin.enabled: false|
|spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio: 4|
|spark.sql.cache.serializer: org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer|
|spark.sql.cartesianProductExec.buffer.in.memory.threshold: 4096|
|spark.sql.caseSensitive: false|
|spark.sql.catalogImplementation: in-memory|
|spark.sql.cbo.enabled: false|
|spark.sql.cbo.joinReorder.card.weight: 0|
|spark.sql.cbo.joinReorder.dp.star.filter: false|
|spark.sql.cbo.joinReorder.dp.threshold: 12|
|spark.sql.cbo.joinReorder.enabled: false|
|spark.sql.cbo.planStats.enabled: false|
|spark.sql.cbo.starJoinFTRatio: 0|
|spark.sql.cbo.starSchemaDetection: false|
|spark.sql.codegen.aggregate.fastHashMap.capacityBit: 16|
|spark.sql.codegen.aggregate.map.twolevel.enabled: true|
|spark.sql.codegen.aggregate.map.vectorized.enable: false|
|spark.sql.codegen.aggregate.splitAggregateFunc.enabled: true|
|spark.sql.codegen.cache.maxEntries: 100|
|spark.sql.codegen.comments: false|
|spark.sql.codegen.fallback: true|
|spark.sql.codegen.hugeMethodLimit: 65535|
|spark.sql.codegen.logging.maxLines: 1000|
|spark.sql.codegen.maxFields: 100|
|spark.sql.codegen.methodSplitThreshold: 1024|
|spark.sql.codegen.splitConsumeFuncByOperator: true|
|spark.sql.codegen.useIdInClassName: true|
|spark.sql.codegen.wholeStage: true|
|spark.sql.columnVector.offheap.enabled: false|
|spark.sql.constraintPropagation.enabled: true|
|spark.sql.crossJoin.enabled: true|
|spark.sql.csv.filterPushdown.enabled: true|
|spark.sql.csv.parser.columnPruning.enabled: true|
|spark.sql.datetime.java8API.enabled: false|
|spark.sql.debug: false|
|spark.sql.debug.maxToStringFields: 25|
|spark.sql.decimalOperations.allowPrecisionLoss: true|
|spark.sql.event.truncate.length: 2147483647|
|spark.sql.exchange.reuse: true|
|spark.sql.execution.arrow.enabled: false|
|spark.sql.execution.arrow.fallback.enabled: true|
|spark.sql.execution.arrow.maxRecordsPerBatch: 10000|
|spark.sql.execution.arrow.sparkr.enabled: false|
|spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit: 8|
|spark.sql.execution.fastFailOnFileFormatOutput: false|
|spark.sql.execution.pandas.convertToArrowArraySafely: false|
|spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled: false|
|spark.sql.execution.rangeExchange.sampleSizePerPartition: 100|
|spark.sql.execution.removeRedundantProjects: true|
|spark.sql.execution.removeRedundantSorts: true|
|spark.sql.execution.reuseSubquery: true|
|spark.sql.execution.sortBeforeRepartition: true|
|spark.sql.execution.useObjectHashAggregateExec: true|
|spark.sql.files.ignoreCorruptFiles: false|
|spark.sql.files.ignoreMissingFiles: false|
|spark.sql.files.maxPartitionBytes: 128MB|
|spark.sql.files.maxRecordsPerFile: 0|
|spark.sql.filesourceTableRelationCacheSize: 1000|
|spark.sql.function.concatBinaryAsString: false|
|spark.sql.function.eltOutputAsString: false|
|spark.sql.globalTempDatabase: global_temp|
|spark.sql.groupByAliases: true|
|spark.sql.groupByOrdinal: true|
|spark.sql.hive.advancedPartitionPredicatePushdown.enabled: true|
|spark.sql.hive.convertCTAS: false|
|spark.sql.hive.gatherFastStats: true|
|spark.sql.hive.manageFilesourcePartitions: true|
|spark.sql.hive.metastorePartitionPruning: true|
|spark.sql.hive.metastorePartitionPruningInSetThreshold: 1000|
|spark.sql.hive.verifyPartitionPath: false|
|spark.sql.inMemoryColumnarStorage.batchSize: 10000|
|spark.sql.inMemoryColumnarStorage.compressed: true|
|spark.sql.inMemoryColumnarStorage.enableVectorizedReader: true|
|spark.sql.inMemoryColumnarStorage.partitionPruning: true|
|spark.sql.inMemoryTableScanStatistics.enable: false|
|spark.sql.join.preferSortMergeJoin: true|
|spark.sql.json.filterPushdown.enabled: true|
|spark.sql.jsonGenerator.ignoreNullFields: true|
|spark.sql.legacy.addSingleFileInAddFile: false|
|spark.sql.legacy.allowHashOnMapType: false|
|spark.sql.legacy.allowNegativeScaleOfDecimal: false|
|spark.sql.legacy.allowParameterlessCount: false|
|spark.sql.legacy.allowUntypedScalaUDF: false|
|spark.sql.legacy.bucketedTableScan.outputOrdering: false|
|spark.sql.legacy.castComplexTypesToString.enabled: false|
|spark.sql.legacy.charVarcharAsString: false|
|spark.sql.legacy.createEmptyCollectionUsingStringType: false|
|spark.sql.legacy.createHiveTableByDefault: true|
|spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue: false|
|spark.sql.legacy.doLooseUpcast: false|
|spark.sql.legacy.execution.pandas.groupedMap.assignColumnsByName: true|
|spark.sql.legacy.exponentLiteralAsDecimal.enabled: false|
|spark.sql.legacy.extraOptionsBehavior.enabled: false|
|spark.sql.legacy.followThreeValuedLogicInArrayExists: true|
|spark.sql.legacy.fromDayTimeString.enabled: false|
|spark.sql.legacy.integerGroupingId: false|
|spark.sql.legacy.json.allowEmptyString.enabled: false|
|spark.sql.legacy.keepCommandOutputSchema: false|
|spark.sql.legacy.literal.pickMinimumPrecision: true|
|spark.sql.legacy.notReserveProperties: false|
|spark.sql.legacy.parseNullPartitionSpecAsStringLiteral: false|
|spark.sql.legacy.parser.havingWithoutGroupByAsWhere: false|
|spark.sql.legacy.pathOptionBehavior.enabled: false|
|spark.sql.legacy.sessionInitWithConfigDefaults: false|
|spark.sql.legacy.setCommandRejectsSparkCoreConfs: true|
|spark.sql.legacy.setopsPrecedence.enabled: false|
|spark.sql.legacy.sizeOfNull: true|
|spark.sql.legacy.statisticalAggregate: false|
|spark.sql.legacy.storeAnalyzedPlanForView: false|
|spark.sql.legacy.typeCoercion.datetimeToString.enabled: false|
|spark.sql.legacy.useCurrentConfigsForView: false|
|spark.sql.limit.scaleUpFactor: 4|
|spark.sql.maxMetadataStringLength: 100|
|spark.sql.metadataCacheTTLSeconds: -1|
|spark.sql.objectHashAggregate.sortBased.fallbackThreshold: 128|
|spark.sql.optimizeNullAwareAntiJoin: true|
|spark.sql.optimizer.disableHints: false|
|spark.sql.optimizer.dynamicPartitionPruning.enabled: true|
|spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio: 0|
|spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly: true|
|spark.sql.optimizer.dynamicPartitionPruning.useStats: true|
|spark.sql.optimizer.enableJsonExpressionOptimization: true|
|spark.sql.optimizer.expression.nestedPruning.enabled: true|
|spark.sql.optimizer.inSetConversionThreshold: 10|
|spark.sql.optimizer.inSetSwitchThreshold: 400|
|spark.sql.optimizer.maxIterations: 100|
|spark.sql.optimizer.metadataOnly: false|
|spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources: parquet,orc|
|spark.sql.optimizer.nestedSchemaPruning.enabled: true|
|spark.sql.optimizer.replaceExceptWithFilter: true|
|spark.sql.optimizer.serializer.nestedSchemaPruning.enabled: true|
|spark.sql.orderByOrdinal: true|
|spark.sql.parquet.binaryAsString: false|
|spark.sql.parquet.columnarReaderBatchSize: 4096|
|spark.sql.parquet.compression.codec: snappy|
|spark.sql.parquet.enableVectorizedReader: true|
|spark.sql.parquet.filterPushdown: true|
|spark.sql.parquet.filterPushdown.date: true|
|spark.sql.parquet.filterPushdown.decimal: true|
|spark.sql.parquet.filterPushdown.string.startsWith: true|
|spark.sql.parquet.filterPushdown.timestamp: true|
|spark.sql.parquet.int96AsTimestamp: true|
|spark.sql.parquet.int96TimestampConversion: false|
|spark.sql.parquet.mergeSchema: false|
|spark.sql.parquet.output.committer.class: org.apache.parquet.hadoop.ParquetOutputCommitter|
|spark.sql.parquet.pushdown.inFilterThreshold: 10|
|spark.sql.parquet.recordLevelFilter.enabled: false|
|spark.sql.parquet.respectSummaryFiles: false|
|spark.sql.parquet.writeLegacyFormat: false|
|spark.sql.parser.escapedStringLiterals: false|
|spark.sql.parser.quotedRegexColumnNames: false|
|spark.sql.pivotMaxValues: 10000|
|spark.sql.planChangeLog.level: trace|
|spark.sql.pyspark.jvmStacktrace.enabled: false|
|spark.sql.repl.eagerEval.enabled: false|
|spark.sql.repl.eagerEval.maxNumRows: 20|
|spark.sql.repl.eagerEval.truncate: 20|
|spark.sql.retainGroupColumns: true|
|spark.sql.runSQLOnFiles: true|
|spark.sql.scriptTransformation.exitTimeoutInSeconds: 5s|
|spark.sql.selfJoinAutoResolveAmbiguity: true|
|spark.sql.shuffle.partitions: 200|
|spark.sql.sort.enableRadixSort: true|
|spark.sql.sources.binaryFile.maxLength: 2147483647|
|spark.sql.sources.bucketing.autoBucketedScan.enabled: true|
|spark.sql.sources.bucketing.enabled: true|
|spark.sql.sources.bucketing.maxBuckets: 100000|
|spark.sql.sources.commitProtocolClass: org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol|
|spark.sql.sources.default: parquet|
|spark.sql.sources.fileCompressionFactor: 1|
|spark.sql.sources.ignoreDataLocality: false|
|spark.sql.sources.parallelPartitionDiscovery.parallelism: 10000|
|spark.sql.sources.parallelPartitionDiscovery.threshold: 32|
|spark.sql.sources.partitionColumnTypeInference.enabled: true|
|spark.sql.sources.validatePartitionColumns: true|
|spark.sql.statistics.fallBackToHdfs: false|
|spark.sql.statistics.histogram.enabled: false|
|spark.sql.statistics.histogram.numBins: 254|
|spark.sql.statistics.ndv.maxError: 0|
|spark.sql.statistics.parallelFileListingInStatsComputation.enabled: true|
|spark.sql.statistics.percentile.accuracy: 10000|
|spark.sql.statistics.size.autoUpdate.enabled: false|
|spark.sql.streaming.continuous.epochBacklogQueueSize: 10000|
|spark.sql.streaming.continuous.executorPollIntervalMs: 100|
|spark.sql.streaming.continuous.executorQueueSize: 1024|
|spark.sql.streaming.metricsEnabled: true|
|spark.sql.subexpressionElimination.cache.maxEntries: 100|
|spark.sql.subexpressionElimination.enabled: true|
|spark.sql.subquery.maxThreadThreshold: 16|
|spark.sql.thriftServer.incrementalCollect: false|
|spark.sql.thriftServer.queryTimeout: 20s|
|spark.sql.thriftserver.ui.retainedSessions: 200|
|spark.sql.thriftserver.ui.retainedStatements: 200|
|spark.sql.truncateTable.ignorePermissionAcl.enabled: false|
|spark.sql.ui.explainMode: formatted|
|spark.sql.ui.retainedExecutions: 500|
|spark.sql.variable.substitute: true|
|spark.sql.view.maxNestedViewDepth: 100|
|spark.sql.warehouse.dir: file:/opt/spark/work-dir/spark-warehouse|
|spark.sql.windowExec.buffer.in.memory.threshold: 4096|
|spark.stage.maxConsecutiveAttempts: 4|
|spark.storage.replication.proactive: true|
|spark.submit.deployMode: cluster|
|spark.submit.pyFiles: |
|spark.task.cpus: 1|
|spark.task.maxFailures: 4|
|spark.task.reaper.enabled: true|
|spark.task.reaper.killTimeout: -1|
|spark.task.reaper.pollingInterval: 20s|
|spark.task.reaper.threadDump: true|

 

Any quick help will be greatly appreciated."
Improve logging on ABFS error reporting,13393431,Resolved,Minor,Fixed,04/Aug/21 09:54,19/Aug/21 12:48,3.3.1,"Large 10GB download from abfs failing after 50 minutes, connection reset

Assumptions
* Azure storage/routers etc get bored of long-lived HTTP connections
* ABFS client doesn't recover from socket exceptions

"
S3A to support user-specified content encoding,13395549,Resolved,Minor,Fixed,16/Aug/21 20:05,29/Sep/21 12:44,3.3.1,"User-specified object content-encoding (part of the object metadata) is important for allowing compressed files to be processed in the AWS ecosystem. We should allow the user to specify the content encoding of the files being written. 

metadata can not be changed after a file is written without a rewrite."
abfs & s3a FS instantiate triggers warning about deprecated io.bytes.per.checksum,13403847,Resolved,Minor,Duplicate,28/Sep/21 13:01,05/Oct/22 09:54,3.4.0,"If you don't turn off the deprecation log, you get told off about dfs.bytes-per-checksum
{code}
2021-09-28 15:40:26,551 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
{code}

proposed
* find out where it's used/set
* stop it"
ITestS3ADeleteFilesOneByOne. testBulkRenameAndDelete OOM: Direct buffer memory,13403189,Open,Minor,,24/Sep/21 16:08,,,"on a test setup with bytebuffer, the parallel zero-byte file create phase OOMed

fs.s3a.fast.upload.buffer = ""bytebuffer"" [core-site.xml]
fs.s3a.fast.upload.active.blocks = ""8"" [core-site.xml]
fs.s3a.multipart.size = ""32M"" [core-site.xml]

Root cause: bytebuffer is being allocated on block creation, so every empty file took up 32MB of off-heap storage only for this to be released unused in close()

If this allocation was postponed until the first write(), then empty files wouldn't need any memory allocation. Do the same on-demand creation for byte arrays and filesystem would also have benefits.

this has implications for  HADOOP-17195, which has abfs using a fork of the buffering code 

changing the code there to be on-demand would be a good incentive for s3a to adopt"
JsonSerDeser to collect IOStatistics,13396146,Resolved,Minor,Won't Fix,19/Aug/21 14:07,12/Jul/22 18:21,3.3.1,"json deserializer to build stats on reading costs, which can then be collected too to measure cost of ser/deser and of file IO from the input/output streams, if they provide it.

Allows for committers to report costs better here. "
ABFS: Fix compiler deprecation warning in TextFileBasedIdentityHandler,13397139,Resolved,Minor,Fixed,25/Aug/21 11:36,05/Nov/21 12:56,3.3.1,"TextFileBasedIdentityHandler uses an instance of LineIterator whose closeQuietly method has been deprecated, resulting in compiler warnings during yetus runs. Fix by leveraging a try-with-resources block to avoid the explicit call to closeQuietly by the LineIterator instance.

hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TextFileBasedIdentityHandler.java:192:18:[deprecation] closeQuietly(LineIterator) in LineIterator has been deprecated"
"Document of ""DistCp Guide"" is wrong",13402862,Open,Minor,,23/Sep/21 07:14,,,"In [https://hadoop.apache.org/docs/stable/hadoop-distcp/DistCp.html] ,it says
{quote}Now, consider the following copy operation:
 distcp hdfs://nn1:8020/source/first hdfs://nn1:8020/source/second hdfs://nn2:8020/target

With sources/sizes:
 hdfs://nn1:8020/source/first/1 32
 hdfs://nn1:8020/source/first/2 32
 hdfs://nn1:8020/source/second/10 64
 hdfs://nn1:8020/source/second/20 32

And destination/sizes:
 hdfs://nn2:8020/target/1 32
 hdfs://nn2:8020/target/10 32
 hdfs://nn2:8020/target/20 64

Will effect:
 hdfs://nn2:8020/target/1 32
 hdfs://nn2:8020/target/2 32
 hdfs://nn2:8020/target/10 64
 hdfs://nn2:8020/target/20 32
{quote}
However, since neither _{{-update}}_ or _-overwrite_ is used, _*first*_ and _*second*_ directories will be created"
Add missing RELEASENOTES and CHANGELOG to upstream,13400921,Resolved,Minor,Fixed,14/Sep/21 05:31,20/Oct/21 04:55,,"RELEASENOTES and CHANGELOG of 2.10.1, 3.1.4 and 3.3.0 are missing in trunk."
S3A CSE: minor tuning,13397521,Resolved,Minor,Fixed,26/Aug/21 10:42,05/Oct/21 11:16,3.4.0,"Some minor tuning to the CSE encryption support before backporting to 3.3.x and so shipping this year

* LogExactlyOnce an ""please ignore the warning"" message to a new log (""org.apache.hadoop.fs.s3a.encryption"") which can be set to ERROR if you get bored of the message.

* Extend testing_s3a.md and the SDK upgrade runbook: test CSE always

* change property name of encryption key (maybe: fs.s3a.encryption) and add mapping in S3AFileSystem.addDeprecatedKeys ... docs will need updating too."
BUILDING.txt should not encourage to activate docs profile on building binary artifacts,13402349,Resolved,Minor,Fixed,21/Sep/21 02:55,11/Oct/21 09:00,3.3.1,"BUILDING.txt in trunk([https://github.com/apache/hadoop/blob/4d21655d04d1b488fe37e908d75f833c1aa66b01/BUILDING.txt#L350]) explains how to create binary distribution with documentation and shows the following command line examples.
{noformat}
`$ mvn package -Pdist,native,docs -DskipTests -Dtar`
`$ mvn package -Pdist,native,docs,src -DskipTests -Dtar`
{noformat}
But, they output errors as follows. Class duplication causes this problem.

Results:
{noformat}
[INFO] -------< org.apache.hadoop:hadoop-client-check-test-invariants >--------
[INFO] Building Apache Hadoop Client Packaging Invariants for Test 3.4.0-SNAPSHOT [105/112]
[INFO] --------------------------------[ pom ]---------------------------------

...

[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-banned-dependencies) @ hadoop-client-check-test-invariants ---
[INFO] Adding ignorable dependency: org.apache.hadoop:hadoop-annotations:null
[INFO]   Adding ignore: *
[WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:
Duplicate classes found:

  Found in:
    org.apache.hadoop:hadoop-client-minicluster:jar:3.4.0-SNAPSHOT:compile
    org.apache.hadoop:hadoop-client-runtime:jar:3.4.0-SNAPSHOT:compile
  Duplicate classes:
    org/apache/hadoop/shaded/org/apache/xerces/impl/dv/dtd/NMTOKENDatatypeValidator.class
    org/apache/hadoop/shaded/org/apache/xerces/dom/DOMImplementationListImpl.class

...

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
{noformat}"
cleanupTempFiles in CopyCommitter can work with directWrite option,13400952,Open,Minor,,14/Sep/21 08:43,,3.3.1,"If directWrite is true, then cleanupTempFiles is unnecessary.

 

 "
Maven-eclipse-plugin is no longer needed since Eclipse can import Maven projects by itself.,13402393,Resolved,Minor,Fixed,21/Sep/21 06:41,22/Sep/21 13:11,3.3.1,"BUILDING.txt explains the way to import projects to eclipse. However, when you follow the instruction, you can't import the entire source tree (e.g., hadoop-project directory). In addition to this, maven-eclipse-plugin has already retired. There is a simpler way to import it without maven-eclipse-plugin.

{noformat}
Importing projects to eclipse

When you import the project to eclipse, install hadoop-maven-plugins at first.

  $ cd hadoop-maven-plugins
  $ mvn install

Then, generate eclipse project files.

  $ mvn eclipse:eclipse -DskipTests

At last, import to eclipse by specifying the root directory of the project via
[File] > [Import] > [Existing Projects into Workspace].
{noformat}
"
Allow nested blocks in switch case in checkstyle settings,13399734,Resolved,Minor,Fixed,07/Sep/21 09:22,08/Sep/21 06:15,,Nested blocks in switch case are used in existing code and checkstyle warning are just ignored. It should be allowed in project checkstyle settings.
HTTP Filesystem to qualify paths in open()/getFileStatus(),13397460,Resolved,Minor,Fixed,26/Aug/21 08:06,08/Sep/21 05:31,3.3.1,"HTTP filesystem is inconsistent with other filesystem (eg: S3AFilesystem) in that they don't accept Path which is relative to the URL that the filesystem is instantiated with.

 

As in
{code:java}
FileSystem.get(""http://www.example.com"").open(new Path(""/test"")){code}
will fail with URI not absolute. But it should infact fetch {{http://www.example.com/test}}"
YarnClient Caching Addresses,13390158,Open,Minor,,16/Jul/21 20:13,,,"We have noticed that when the YarnClient is initialized and used, it is not very resilient when dns or /etc/hosts is modified in the following scenario:

Take for instance the following (and reproducable) sequence of events that can occur on a service that instantiates and uses YarnClient. 
  - Yarn has rm HA enabled (*yarn.resourcemanager.ha.enabled* is *true*) and there are two rms (rm1 and rm2).
  - *yarn.client.failover-proxy-provider* is set to *org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider*

1)	rm2 is currently the active rm
2)	/etc/hosts (or dns) is missing host information for rm2
3)	A service is started and it initializes the YarnClient at startup.
4)	At some point in time after YarnClient is done initializing, /etc/hosts is updated and contains host information for rm2
5)	Yarn is queried, for instance calling *yarnclient.getApplications()*
6)	All YarnClient attempts to communicate with rm2 fail with UnknownHostExceptions, even though /etc/hosts now contains host information for it.

"
DNS Query on DNS Registry got connection timed out at times.,13396633,Open,Minor,,23/Aug/21 08:45,,,"DNS query on YARN Registry DNS got connection timed out at times.

 

After checking logs on registry dns, we found the cause that size of UDP Response is greater than Preallocated Byte buffer(4096)

 

We added failsafe case by returning input query with TC flag so that client can know they should retried using TCP protocol.

 

 

We found the problem and patched our cluster on 3.1.2 but it seems trunk still has the same problem.

I attached patch based on the trunk.

 

Thanks!"
Update link of wiki on README,13394820,Resolved,Minor,Not A Problem,12/Aug/21 06:07,12/Aug/21 07:57,,"The [README|https://github.com/apache/hadoop#readme] on [github repo|https://github.com/apache/hadoop] is deprecated now.

Let's update it to [HADOOP2|https://cwiki.apache.org/confluence/display/HADOOP2]"
Update link of PoweredBy wiki page,13393864,Resolved,Minor,Fixed,06/Aug/21 09:55,07/Aug/21 08:28,,"The [PoweredBy wiki page|https://cwiki.apache.org/confluence/display/hadoop/PoweredBy] on [main page|https://hadoop.apache.org/] is not found.

IMHO update it to [here|https://cwiki.apache.org/confluence/display/HADOOP2/PoweredBy]"
Backport HADOOP-17837 to branch-3.2,13393879,Resolved,Minor,Fixed,06/Aug/21 11:32,07/Aug/21 04:22,,
ABFS ExponentialRetryPolicy doesn't pick up configuration values,13391199,Resolved,Minor,Fixed,21/Jul/21 20:46,28/Jul/21 19:23,,"The ABFS driver uses ExponentialRetryPolicy to handle throttling by the ADLS Gen 2 API. The number of retries can already be configured by setting the property fs.azure.io.retry.max.retries. However, none of the other properties on ExponentialRetryPolicy can be set even though configuration properties have already been defined for them. Allow the additional properties (min/max retry wait, default wait) to be configured."
S3 CSV read performance with Spark with Hadoop 3.3.1 is slower than older Hadoop,13387361,Resolved,Minor,Works for Me,02/Jul/21 15:53,28/Jul/21 15:17,3.3.1,"This is issue is continuation to https://issues.apache.org/jira/browse/HADOOP-17755

The input data reported by Spark(Hadoop 3.3.1) was almost double and read runtime also increased (around 20%) compared to Spark(Hadoop 3.2.0) with same exact amount of resource and same configuration. And this is happening with other jobs as well which was not impacted by read fully error as stated above.

*I was having the same exact issue when I was using the workaround  fs.s3a.readahead.range = 1G with Hadoop 3.2.0*

Below is further details :

 
|Hadoop Version|Actual size of the files(in SQL Tab)|Reported size of the file(In Stages)|Time to complete the Stage|fs.s3a.readahead.range|
|Hadoop 3.2.0|29.3 GiB|29.3 GiB|23 min|64K|
|Hadoop 3.3.1|29.3 GiB|*{color:#ff0000}58.7 GiB{color}*|*{color:#ff0000}27 min{color}*|{color:#172b4d}64K{color}|
|Hadoop 3.2.0|29.3 GiB|*{color:#ff0000}58.7 GiB{color}*|*{color:#ff0000}~27 min{color}*|{color:#172b4d}1G{color}|
 * *Shuffle Write* is same (95.9 GiB) for all the above three cases

I was expecting some improvement(or same as 3.2.0) with Hadoop 3.3.1 with read operations, please suggest how to approach this and resolve this.

I have used the default s3a config along with below and also using EKS cluster
{code:java}
spark.hadoop.fs.s3a.committer.magic.enabled: 'true'
spark.hadoop.fs.s3a.committer.name: magic
spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a: org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
spark.hadoop.fs.s3a.downgrade.syncable.exceptions: ""true""{code}
 * I did not use 
{code:java}
spark.hadoop.fs.s3a.experimental.input.fadvise=random{code}

And as already mentioned I have used same Spark, same amount of resources and same config.  Only change is Hadoop 3.2.0 to Hadoop 3.3.1 (Built with Spark using ./dev/make-distribution.sh --name spark-patched --pip -Pkubernetes -Phive -Phive-thriftserver -Dhadoop.version=""3.3.1"")"
[KMS] LoadBalancingKMSClientProvider should implement toString(),13391082,Open,Minor,,21/Jul/21 07:31,,3.3.1,"KMSClientProvider implements toString() but LoadBalancingKMSClientProvider does not inherit from KMSClientProvider, nor does it implement toString(). So running hadoop key command does not show key provider URL:
{noformat}
bin/hadoop key list -metadata
2021-07-21 15:26:43,856 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Listing keys for KeyProvider: org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider@394a2528{noformat}
 "
IOStatistisSource to add new adder methods,13389884,Open,Minor,,15/Jul/21 17:26,,3.3.1,"Based on experience of Manifest committer of MAPREDUCE-7341:

* operation to take a key and a count and a mean entry, min, max and count:
addStatsSample(String key, long count)

* operation to add a mean and sample count to a mean value.
"
mvn verify fails due to duplicate entry in the shaded jar,13337003,Resolved,Blocker,Cannot Reproduce,24/Oct/20 00:10,06/Dec/22 15:57,3.2.2,"Found this when I was chasing a separate shading error with [~smeng].

In trunk:

run mvn verify under hadoop-client-module/
{noformat}

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Client Modules 3.4.0-SNAPSHOT:
[INFO]
[INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  2.607 s]
[INFO] Apache Hadoop Client API ........................... SUCCESS [03:16 min]
[INFO] Apache Hadoop Client Runtime ....................... SUCCESS [01:30 min]
[INFO] Apache Hadoop Client Test Minicluster .............. FAILURE [04:44 min]
[INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED
[INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED
[INFO] Apache Hadoop Client Modules ....................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  09:34 min
[INFO] Finished at: 2020-10-23T16:38:53-07:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.2.1:shade (default) on project hadoop-client-minicluster: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.hadoop.shaded.com.fasterxml.jackson.core.JsonFactory -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit

 {noformat}

This is reproducible in trunk and branch-3.3. However, not reproducible in branch-3.1.
(branch-3.3 has a different error:
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.2.1:shade (default) on project hadoop-client-minicluster: Error creating shaded jar: duplicate entry: META-INF/services/org.apache.hadoop.shaded.javax.ws.rs.ext.MessageBodyReader -> [Help 1])"
S3A NetworkBinding has a runtime class dependency on a third-party shaded class,13338141,Resolved,Blocker,Fixed,30/Oct/20 20:44,03/Feb/21 14:35,3.3.0,"The hadoop-aws library has a dependency on 'com.amazonaws':aws-java-sdk-bundle' which in turn is a fat jar of all AWS SDK libraries and shaded dependencies.

 

This dependency is 181MB.

 

Some applications using the S3AFilesystem may be sensitive to having a large footprint. For example, building an application using Parquet and bundled with Docker.

 

Typically, in prior Hadoop versions, the bundle was replaced by the specific AWS SDK dependencies, dropping the overall footprint.

 

In 3.3 (and maybe prior versions) this strategy does not work because of the following exception: 

{{java.lang.NoClassDefFoundError: com/amazonaws/thirdparty/apache/http/conn/socket/ConnectionSocketFactory}}
{{ at org.apache.hadoop.fs.s3a.S3AUtils.initProtocolSettings(S3AUtils.java:1335)}}
{{ at org.apache.hadoop.fs.s3a.S3AUtils.initConnectionSettings(S3AUtils.java:1290)}}
{{ at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1247)}}
{{ at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:61)}}
{{ at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:644)}}
{{ at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:390)}}
{{ at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3414)}}
{{ at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158)}}
{{ at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3474)}}
{{ at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3442)}}
{{ at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524)}}

 "
Hadoop GPG key is not valid,13339470,Resolved,Blocker,Done,09/Nov/20 09:08,09/Nov/20 16:07,,"Hi,

I build regular hadoop images.

At the moment the KEYS file ([https://dist.apache.org/repos/dist/release/hadoop/common/KEYS]) is invalid.

Error-Message:
{code:java}
gpg: key FC8D04357BB49FF0: public key ""Sammi Chen (CODE SIGNING KEY) <sammichen@apache.org>"" imported
gpg: invalid armor header: mQINBF9U5ZcBEADJS2a8ihhZtN1wXOJfyLZreuHL9HJxRvogQbhrhpFQrKAusdf2\n
gpg: CRC error; 95D523 - 51AC03
gpg: packet(7) with unknown version 103
gpg: read_block: read error: Unknown version in packet
gpg: import from '/tmp/KEYS' failed: Invalid keyring
gpg: Total number processed: 60
gpg:               imported: 60
gpg: no ultimately trusted keys found
{code}
Steps to reproduce:
 * Install Docker
 * Create a Dockerfile with the following Content
 ** 
{code:java}
FROM alpine:3.12
RUN set -ex && \
    /sbin/apk add --no-cache wget gnupg tar && \
    # Install Hadoop
    /usr/bin/wget -nv https://dist.apache.org/repos/dist/release/hadoop/common/KEYS -O /tmp/KEYS && \
    /usr/bin/gpg --import /tmp/KEYS
 {code}

 * Run docker build
 ** 
{code:java}
 docker build -t hadoop-test -f Dockerfile .
{code}

Other users complaining about this error:
 - [https://askubuntu.com/questions/1290190/hadoop-gpg-key-is-not-valid-no-ultimately-trusted-keys-found]
 - [https://stackoverflow.com/questions/64719392/invalid-keyring-are-hadoop-gpg-keys-are-wrong]

I hope for a quick fix, because automatic builds are currently blocked.

If this duplicates another ticket, please close it and link it to the original.

Best Regards
 Reamer"
Don't relocate org.bouncycastle in shaded client jars,13336805,Resolved,Critical,Fixed,22/Oct/20 21:33,11/Nov/20 16:34,3.3.0,"When downstream apps depend on {{hadoop-client-api}}, {{hadoop-client-runtime}} and {{hadoop-client-minicluster}}, it seems the {{MiniYARNCluster}} could have issue because {{org.apache.hadoop.shaded.org.bouncycastle.operator.OperatorCreationException}} is not in any of the above jars. 

{code}
Error:  Caused by: sbt.ForkMain$ForkError: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.org.bouncycastle.operator.OperatorCreationException
Error:  	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
Error:  	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
Error:  	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
Error:  	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
Error:  	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:862)
Error:  	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Error:  	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1296)
Error:  	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:339)
Error:  	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Error:  	at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:353)
Error:  	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:127)
Error:  	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:488)
Error:  	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Error:  	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:109)
Error:  	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:321)
Error:  	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Error:  	at org.apache.spark.deploy.yarn.BaseYarnClusterSuite.beforeAll(BaseYarnClusterSuite.scala:94)
Error:  	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
Error:  	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
Error:  	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
Error:  	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:61)
Error:  	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
Error:  	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
Error:  	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
Error:  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Error:  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Error:  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Error:  	at java.lang.Thread.run(Thread.java:748)
{code}"
NPE when starting MiniYARNCluster from hadoop-client-minicluster,13337005,Resolved,Critical,Fixed,24/Oct/20 00:16,11/Nov/20 07:13,3.3.0,"When starting MiniYARNCluster one could get the following exception:
{code}
  java.lang.NullPointerException:
  at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:72)
  at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
  at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:122)
  at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
  at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceStart(MiniYARNCluster.java:616)
  at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
  at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:122)
  at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceStart(MiniYARNCluster.java:327)
  at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
  at org.apache.spark.deploy.yarn.BaseYarnClusterSuite.beforeAll(BaseYarnClusterSuite.scala:96)
  ...
{code}

Looking into the code, this is because we explicitly exclude resource files under {{hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/resources/TERMINAL}}, and therefore this code in {{WebServer}} fails with NPE:
{code}
    terminalParams.put(""resourceBase"", WebServer.class
        .getClassLoader().getResource(""TERMINAL"").toExternalForm());
{code}

Those who use {{hadoop-minicluster}} may not be affected because they'll also need {{hadoop-yarn-server-nodemanager}} as an extra dependency, which includes the resource files. On the other hand {{hadoop-client-minicluster}} packages both test classes (e.g., {{MiniYARNCluster}}) as well as main classes (e.g., {{ResourceManager}} and {{NodeManager}}) into a single shaded jar. It should include these resource files for testing as well. Otherwise, {{MiniYARNCluster}} is unusable."
WASB : PageBlobOutputStream succeeding hflush even when underlying flush to storage failed ,13335750,Resolved,Critical,Fixed,16/Oct/20 10:33,26/Oct/20 13:32,2.7.0,"In PageBlobOutputStream, write()  APIs will fill the buffer and hflush/hsync/flush call will flush the buffer to underlying storage. Here the Azure calls are handled in another thread 
{code}
private synchronized void flushIOBuffers()  {
    ...
    lastQueuedTask = new WriteRequest(outBuffer.toByteArray());
    ioThreadPool.execute(lastQueuedTask);
  ....
 }
private class WriteRequest implements Runnable {
    private final byte[] dataPayload;
    private final CountDownLatch doneSignal = new CountDownLatch(1);

    public WriteRequest(byte[] dataPayload) {
      this.dataPayload = dataPayload;
    }

    public void waitTillDone() throws InterruptedException {
      doneSignal.await();
    }

    @Override
    public void run() {
      try {
        LOG.debug(""before runInternal()"");
        runInternal();
        LOG.debug(""after runInternal()"");
      } finally {
        doneSignal.countDown();
      }
    }

    private void runInternal() {
      ......
      writePayloadToServer(rawPayload);
      ...........
    }

    private void writePayloadToServer(byte[] rawPayload) {
      ......
      try {
        blob.uploadPages(wrapperStream, currentBlobOffset, rawPayload.length,
            withMD5Checking(), PageBlobOutputStream.this.opContext);
      } catch (IOException ex) {
        lastError = ex;
      } catch (StorageException ex) {
        lastError = new IOException(ex);
      }
      if (lastError != null) {
        LOG.debug(""Caught error in PageBlobOutputStream#writePayloadToServer()"");
      }
    }
  }
{code}
The flushing thread will wait for the other thread to complete the Runnable WriteRequest. Thats fine. But when some exception happened while blob.uploadPages, we just set that to lastError state variable.  This variable is been checked for all subsequent ops like write, flush etc.  But what about the current flush call? that is silently being succeeded.!!  
In standard Azure backed HBase clusters WAL is on page blob. This issue causes a serious issue in HBase and causes data loss! HBase think a WAL write was hflushed and make row write successful. In fact the row was never gone to storage.

Checking the lastError variable at the end of flush op will solve the issue. Then we will throw IOE from this flush() itself."
ABFS: read-ahead error reporting breaks buffer management,13334306,Resolved,Critical,Fixed,07/Oct/20 23:08,14/Oct/20 23:01,3.3.0,"When reads done by readahead buffers failed, the exceptions where dropped and the failure was not getting reported to the calling app. 

Jira HADOOP-16852: Report read-ahead error back

tried to handle the scenario by reporting the error back to calling app. But the commit has introduced a bug which can lead to ReadBuffer being injected into read completed queue twice. "
GCS to support per-bucket configuration,13344113,Open,Major,,03/Dec/20 23:35,,,S3 (HADOOP-13336) and Azure (HADOOP-13972) support setting different configs per bucket. Need the same feature for Google Storage
Use S3 content-range header to update length of an object during reads,13344830,Open,Major,,08/Dec/20 13:39,,3.3.0,"As part of all the openFile work, knowing full length of an object allows for a HEAD to be skipped. But: code knowing only the splits don't know the final length of the file.

If the content-range header is used, then as soon as a single GET is initiated against an object, if the field is returned then we can update the length of the S3A stream to its real/final length

Also: when any input stream fails with an EOF exception, we can distinguish stream-interrupted from ""no, too far"""
[JDK 11] Upgrade dnsjava to remove illegal access warnings,13336272,Resolved,Major,Fixed,20/Oct/20 11:00,24/Jul/21 05:50,3.4.0,"Originally reported by [~kihwal] in https://issues.apache.org/jira/browse/HADOOP-15338?focusedCommentId=17129854&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17129854

When running FsShell commands, there are some warning messages as follows:
{noformat}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.xbill.DNS.ResolverConfig  to method sun.net.dns.ResolverConfiguration.open()
WARNING: Please consider reporting this to the maintainers of org.xbill.DNS.ResolverConfig
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
{noformat}"
Upgrade to jQuery 3.5.1 in hadoop-sls,13334939,Resolved,Major,Fixed,12/Oct/20 07:29,19/Oct/20 12:49,3.3.1,"jQuery is upgraded from 3.3.1 to 3.5.1 at 

hadoop/hadoop-tools/hadoop-sls/src/main/html/js/thirdparty"
ABFS: Implementation for getContentSummary,13345435,Open,Major,,11/Dec/20 06:43,,3.3.0,"Adds implementation for HDFS method getContentSummary, which takes in a Path argument and returns details such as file/directory count and space utilized under that path."
[JDK 15] TestDNS fails by UncheckedIOException,13337399,Resolved,Major,Fixed,27/Oct/20 11:01,30/Jun/21 10:06,3.4.0,"After [JDK-8235783|https://bugs.openjdk.java.net/browse/JDK-8235783], DatagramSocket::connect throws UncheckedIOException if connect fails.
{noformat}
[INFO] Running org.apache.hadoop.net.TestDNS
[ERROR] Tests run: 12, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 0.403 s <<< FAILURE! - in org.apache.hadoop.net.TestDNS
[ERROR] testNullDnsServer(org.apache.hadoop.net.TestDNS)  Time elapsed: 0.134 s  <<< ERROR!
java.io.UncheckedIOException: java.net.SocketException: Unsupported address type
	at java.base/sun.nio.ch.DatagramSocketAdaptor.connect(DatagramSocketAdaptor.java:120)
	at java.base/java.net.DatagramSocket.connect(DatagramSocket.java:341)
{noformat}
Full error log: https://gist.github.com/aajisaka/2a24cb2b110cc3d19f7dec6256db6844"
hadoop-client-integration-tests doesn't work when building with skipShade,13339861,Resolved,Major,Fixed,11/Nov/20 00:04,11/Nov/20 17:40,3.3.1,"Compiling with skipShade:
{code}
mvn clean install -Pdist -DskipShade -DskipTests -Dtar -Danimal.sniffer.skip -Dmaven.javadoc.skip=true
{code}

fails with
{code}
[ERROR] /Users/chao/git/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseMiniCluster.java:[47,37] package org.apache.hadoop.yarn.server does not exist
[ERROR] /Users/chao/git/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseMiniCluster.java:[59,11] cannot find symbol
[ERROR]   symbol:   class MiniYARNCluster
[ERROR]   location: class org.apache.hadoop.example.ITUseMiniCluster
[ERROR] /Users/chao/git/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseMiniCluster.java:[82,23] cannot find symbol
[ERROR]   symbol:   class MiniYARNCluster
[ERROR]   location: class org.apache.hadoop.example.ITUseMiniCluster
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-client-integration-tests
{code}"
LazyPersist Overwrite fails in direct write mode,13337048,Resolved,Major,Fixed,24/Oct/20 17:09,26/Oct/20 16:47,3.3.1,"{noformat}
hadoop fs -put -f -d -l Dockerfile /vol/buck/key/file1{noformat}
fails with {{File exists}} if the file is already there despite of the overwrite flag 

The overwrite flag is missing in create in case of {{LazyPersist}}"
Touch command with -c  option is broken,13335817,Resolved,Major,Fixed,16/Oct/20 17:58,19/Oct/20 05:26,3.2.3,"The touch command has an option -c :
{noformat}
touch
Usage: hadoop fs -touch [-a] [-m] [-t TIMESTAMP] [-c] URI [URI ...]{noformat}
Ref : [https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#touch]

But on using it, it gives error as :
{noformat}
-touch: Illegal option -c
Usage: hadoop fs [generic options]{noformat}
 "
Bump Jetty to the latest version 9.4.35,13339673,Resolved,Major,Fixed,10/Nov/20 07:45,05/Apr/21 01:40,3.2.3,The Hadoop 3 branches are on 9.4.20. We should update to the latest version: 9.4.34
KMS should log full UGI principal,13341670,Resolved,Major,Fixed,19/Nov/20 21:51,04/Dec/20 00:34,3.2.2,"[~daryn] reported that the kms-audit log only logs the short username:
{{OK[op=GENERATE_EEK, key=key1, user=hdfs, accessCount=4206, interval=10427ms]}}

In this example, it's impossible to tell which NN(s) requested EDEKs when they are all lumped together."
KMS ACL: Allow DeleteKey Operation to Invalidate Cache,13335072,Resolved,Major,Fixed,12/Oct/20 19:33,14/Oct/20 08:31,3.4.0,HADOOP-17208 send invalidate cache for key being deleted. The invalidate cache operation itself requires ROLLOVER permission on the key. This ticket is opened to fix the issue caught by TestKMS.testACLs.
Use Yetus before YETUS-994 to enable adding comments to GitHub,13333638,Resolved,Major,Fixed,05/Oct/20 02:36,05/Oct/20 10:17,3.4.0,"After YETUS-994, test-patch updates the commit status instead of adding a comment in GitHub. Use the commit hash before YETUS-994 because of the following reasons:

* The Yetus feature seems to be broken for now (Trying and debugging in https://github.com/apache/hadoop/pull/2348)
* Use the fixed version to avoid surprising the developers
* The latest Yetus version is 0.12.0 but it does not support YETUS-972, which is required to run javadoc with Java 11."
Update the checkstyle config to ban some guava functions,13336425,Resolved,Major,Fixed,21/Oct/20 07:34,22/Oct/20 19:28,3.4.0,"Some guava functions are banned in HADOOP-17111, HADOOP-17099, and HADOOP-17101 via checkstyle, however, the checkstyle configuration does not work after HADOOP-17288 because the package names have been changed.

Originally reported by [~ahussein] in HADOOP-17315."
mvn site commands fails due to MetricsSystemImpl changes,13337097,Resolved,Major,Fixed,25/Oct/20 15:20,29/Oct/20 01:52,3.3.1,"When prepare for branch-3.2.2 release, i found there is one issue while create release. And it also exist in trunk.
command line: mvn install site site:stage -DskipTests -DskipShade -Pdist,src -Preleasedocs,docs
failed log show as the following: 
{quote}[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.6:site (default-site) on project hadoop-common: failed to get report for org.apache.maven.plugins:maven-dependency-plugin: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-common: Compilation failure
[ERROR] /Users/hexiaoqiao/Source/hadoop-common/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java:[298,5] method <T>register(java.lang.String,java.lang.String,T) is already defined in class org.apache.hadoop.metrics2.impl.MetricsSystemImpl{quote}

I am not sure why source code of class MetricsSystemImpl will be changed while building, I try to revert HADOOP-17081 everything seems OK."
Harmonize guava version and shade guava in yarn-csi,13338699,Resolved,Major,Fixed,03/Nov/20 17:33,10/Nov/20 06:20,3.4.0,"yarn-csi defines a separate guava version [https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/pom.xml#L30]. 

 

We should harmonize the guava version (pull it from hadoop-project/pom.xml) and use the shaded guava classes. "
Move Jenkinsfile outside of the root directory,13338773,Resolved,Major,Fixed,04/Nov/20 05:56,01/Feb/21 09:55,3.3.1,"The Jenkinsfile is placed under the project root directory, so when the Jenkinsfile is changed, all the Hadoop unit tests will run and it wastes a lot of time and resources. Let's move the file outside of the root directory."
Replace HTrace with No-Op tracer,13345117,Resolved,Major,Fixed,09/Dec/20 17:41,01/Feb/21 04:44,3.3.2,"Remove HTrace dependency as it is depending on old jackson jars. Use a no-op tracer for now to eliminate potential security issues.

The plan is to move part of the code in [PR#1846|https://github.com/apache/hadoop/pull/1846] out here for faster review."
Upgrade to hadoop-thirdparty-1.1.0,13345164,Resolved,Major,Fixed,10/Dec/20 00:29,19/May/21 06:08,3.3.1,"As of now we are using the snapshot version.(1.1.0-SNAPSHOT)
Make sure to change to the released version before release."
[JDK 16] KerberosUtil#getOidInstance is broken by JEP 396,13345764,Resolved,Major,Fixed,14/Dec/20 01:49,05/Feb/21 07:17,3.3.1,"JEP 396 (Strongly Encapsulate JDK Internals by Default) has been migrated since Java 16 EA Build 28. Calling the internal APIs (except the critical APIs such as sun.misc.Unsafe) are banned by default.
{noformat}
[INFO] Running org.apache.hadoop.security.authentication.server.TestAltKerberosAuthenticationHandler
[ERROR] Tests run: 16, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 17.323 s <<< FAILURE! - in org.apache.hadoop.security.authentication.server.TestAltKerberosAuthenticationHandler
[ERROR] testNonDefaultNonBrowserUserAgentAsNonBrowser(org.apache.hadoop.security.authentication.server.TestAltKerberosAuthenticationHandler)  Time elapsed: 1.272 s  <<< ERROR!
java.lang.IllegalAccessException: class org.apache.hadoop.security.authentication.util.KerberosUtil cannot access class sun.security.jgss.GSSUtil (in module java.security.jgss) because module java.security.jgss does not export sun.security.jgss to unnamed module @48967c8b
	at java.base/jdk.internal.reflect.Reflection.newIllegalAccessException(Reflection.java:385)
	at java.base/java.lang.reflect.AccessibleObject.checkAccess(AccessibleObject.java:687)
	at java.base/java.lang.reflect.Field.checkAccess(Field.java:1096)
	at java.base/java.lang.reflect.Field.get(Field.java:417)
	at org.apache.hadoop.security.authentication.util.KerberosUtil.getOidInstance(KerberosUtil.java:90)
{noformat}"
Update the year to 2021,13347704,Resolved,Major,Fixed,24/Dec/20 07:14,24/Dec/20 13:01,3.2.2,Update the year to 2021.
fs.s3a.buffer.dir to be under Yarn container path on yarn applications,13341393,Resolved,Major,Fixed,18/Nov/20 16:55,22/Feb/22 04:51,3.3.0,"# fs.s3a.buffer.dir defaults to hadoop.tmp.dir which is /tmp or similar
# we use this for storing file blocks during upload
# staging committers use it for all files in a task, which can be a lot more
# a lot of systems don't clean up /tmp until reboot -and if they stay up for a long time then they accrue files written through s3a staging committer from spark containers which fail

Fix: use ${env.LOCAL_DIRS:-${hadoop.tmp.dir}}/s3a as the option so that if env.LOCAL_DIRS is set is used over hadoop.tmp.dir. YARN-deployed apps will use that for the buffer dir. When the app container is destroyed, so is the directory.

"
TestZKFailoverController#testNoZK is flaky,13336783,Open,Major,,22/Oct/20 17:56,,,"TestZKFailoverController#testNoZK failed in https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2397/4/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt
{noformat}
[ERROR] testNoZK(org.apache.hadoop.ha.TestZKFailoverController)  Time elapsed: 0.114 s  <<< ERROR!
java.lang.IllegalStateException
	at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:492)
	at org.apache.hadoop.ha.ActiveStandbyElector.parentZNodeExists(ActiveStandbyElector.java:318)
	at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:230)
	at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:60)
	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:178)
	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:174)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:485)
	at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:174)
	at org.apache.hadoop.ha.TestZKFailoverController.runFC(TestZKFailoverController.java:700)
	at org.apache.hadoop.ha.TestZKFailoverController.testNoZK(TestZKFailoverController.java:135)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:80)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{noformat}"
ABFS: read/cache footer with fs.azure.footer.read.request.size,13338772,Resolved,Major,Fixed,04/Nov/20 05:46,08/Jan/21 10:39,3.4.0,"Optimize read performance for the following scenarios
 # Read small files completely
 Files that are of size smaller than the read buffer size can be considered as small files. In case of such files it would be better to read the full file into the AbfsInputStream buffer.
 # Read last block if the read is for footer
 If the read is for the last 8 bytes, read the full file.
 This will optimize reads for parquet files. [Parquet file format|https://www.ellicium.com/parquet-file-format-structure/]

Both these optimizations will be present under configs as follows
 # fs.azure.read.smallfilescompletely
 # fs.azure.read.optimizefooterread"
ABFS: MsiTokenProvider doesn't retry HTTP 429 from the Instance Metadata Service,13340414,Open,Major,,13/Nov/20 03:16,,3.2.1,"*Summary*
 The instance metadata service has its own guidance for error handling and retry which are different from the Blob store. [https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token#error-handling]

In particular, it responds with HTTP 429 if request rate is too high. Whereas Blob store will respond with HTTP 503. The retry policy used only accounts for the latter as it will retry any status >=500. This can result in job instability when running multiple processes on the same host.

*Environment*
 * Spark talking to an ABFS store

 * Hadoop 3.2.1

 * Running on an Azure VM with user-assigned identity, ABFS configured to use MsiTokenProvider

 * 6 executor processes on each VM

*Example*
 Here's an example error message and stack trace. It's always the same stack trace. This appears in logs a few hundred to low thousands of times a day. It's luckily skating by since the download operation is wrapped in 3 retries.
{noformat}
AADToken: HTTP connection failed for getting token from AzureAD. Http response: 429 null
Content-Type: application/json; charset=utf-8 Content-Length: 90 Request ID:  Proxies: none
First 1K of Body: {""error"":""invalid_request"",""error_description"":""Temporarily throttled, too many requests""}
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:190)
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:506)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:489)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:208)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:473)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:437)
	at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1717)
	at org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:747)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:724)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7(Executor.scala:812)
	at org.apache.spark.executor.Executor.$anonfun$updateDependencies$7$adapted(Executor.scala:803)
	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:792)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:791)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748){noformat}
 CC [~mackrorysd], [~stevel@apache.org]"
S3A AWS Credential provider loading gets confused with isolated classloaders,13339776,Resolved,Major,Duplicate,10/Nov/20 14:54,25/May/21 14:46,3.4.0,"Problem: exception in loading S3A credentials for an FS, ""Class class com.amazonaws.auth.EnvironmentVariableCredentialsProvider does not implement AWSCredentialsProvider""

Location: S3A + Spark dataframes test

Hypothesised cause:

Configuration.getClasses() uses the context classloader, and with the spark isolated CL that's different from the one the s3a FS uses, so it can't load AWS credential providers."
envtoconf broken for .conf and few other formats,13346285,Open,Major,,16/Dec/20 07:49,,,"{{envtoconf}} does not work for some output formats:

 * {{.env}}
 * {{.sh}}
 * {{.cfg}}
 * {{.conf}}

To reproduce:

{code:title=docker run -it --rm -e DUMMY.CONF_key=value apache/hadoop:3}
Traceback (most recent call last):
  File ""/opt/envtoconf.py"", line 117, in <module>
    Simple(sys.argv[1:]).main()
  File ""/opt/envtoconf.py"", line 108, in main
    self.transform()
  File ""/opt/envtoconf.py"", line 96, in transform
    content = transformer_func(content)
  File ""/opt/transformation.py"", line 121, in to_conf
    for key, val in props:
ValueError: too many values to unpack
{code}"
Optimize S3A for maximum performance in directory listings,13343281,Resolved,Major,Fixed,30/Nov/20 10:31,17/May/21 05:39,3.3.0,"Make listing in applications as fast as we can get it especially for query planning.

* All operations used in listing directories for query planning etc to be optimized for their primary use: being passed directories (not files) and so make that faster even at the expense of  more remote IO when handed files or empty directories.
* remove needless calls to S3 wherever possible (e.g. {{getFileStatus(""/"")}}, making bucket existence probes optional)
* Support/enable Asynchronous IO where possible.
 

Review higher level APIs (glob status) and uses on the FsShell and optimize their use by minimising invocations or FS API calls, with bonus goal of reduce/minimize risk of 404 caching.

Work with downstream projects to move to FS APIs which work best in this world -primarily the recursive listing operations and those which return RemoteIterator<FileStatus> -and so make any asynchronous page fetching operations useful. "
hadoop-common to add IOStatistics API,13348453,Resolved,Major,Fixed,30/Dec/20 10:20,31/Dec/20 11:55,3.3.0,"Add the iostatistics API to hadoop-common with

* public interfaces for querying statistics
* serializable snapshot
* logging support

and implementation support for filesystems"
FileSystem.get to support slow-to-instantiate FS clients,13336082,Resolved,Major,Fixed,19/Oct/20 14:31,25/Nov/20 14:55,3.3.0,"A recurrent problem in processes with many worker threads (hive, spark etc) is that calling `FileSystem.get(URI-to-object-store)` triggers the creation and then discard of many FS clients -all but one for the same URL. As well as the direct performance hit, this can exacerbate locking problems and make instantiation a lot slower than it would otherwise be.

This has been observed with the S3A and ABFS connectors.

The ultimate solution here would probably be something more complicated to ensure that only one thread was ever creating a connector for a given URL -the rest would wait for it to be initialized. This would (a) reduce contention & CPU, IO network load, and (b) reduce the time for all but the first thread to resume processing to that of the remaining time in .initialize(). This would also benefit the S3A connector.

We'd need something like

# A (per-user) map of filesystems being created <URI, FileSystem>
# split createFileSystem into two: instantiateFileSystem and initializeFileSystem
# each thread to instantiate the FS, put() it into the new map
# If there was one already, discard the old one and wait for the new one to be ready via a call to Object.wait()
# If there wasn't an entry, call initializeFileSystem) and then, finally, call Object.notifyAll(), and move it from the map of filesystems being initialized to the map of created filesystems

This sounds too straightforward to be that simple; the troublespots are probably related to race conditions moving entries between the two maps and making sure that no thread will block on the FS being initialized while it has already been initialized (and so wait() will block forever).

Rather than seek perfection, it may be safest go for a best-effort optimisation of the #of FS instances created/initialized. That is: its better to maybe create a few more FS instances than needed than it is to block forever.

Something is doable here, it's just not quick-and-dirty. Testing will be ""fun""; probably best to isolate this new logic somewhere where we can simulate slow starts on one thread with many other threads waiting for it.

A simpler option would be to have a lock on the construction process: only one FS can be instantiated per user at a a time.
"
Improve excessive reloading of Configurations,13338896,Resolved,Major,Fixed,04/Nov/20 21:18,12/Nov/20 19:00,,"[~daryn] reported that adding a new resource to a conf forces a complete reload of the conf instead of just loading the new resource. Instantiating a {{SSLFactory}} adds a new resource for the ssl client/server file. Formerly only the KMS client used the SSLFactory but now TLS/RPC uses it too.

The reload is so costly that RM token cancellation falls behind by hours or days. The accumulation of uncancelled tokens in the KMS rose from a few thousand to hundreds of thousands which risks ZK scalability issues causing a KMS outage."
Intermittent failure of S3A tests which make assertions on statistics/IOStatistics,13348624,Resolved,Major,Fixed,31/Dec/20 21:59,12/Jan/21 17:25,3.4.0,"Intermittent failure of ITestHuge* upload tests, when doing parallel test runs.

The count of bytes uploaded through StorageStatistics isn't updated. Maybe the expected counter isn't updated, and somehow in a parallel run with recycled FS instances/set up directory structure this surfaces the way it doesn't in a single test run."
Print the thread parker and lock information in stacks page,13347803,Open,Major,,25/Dec/20 00:32,,3.4.0,"Sometimes, our service stuck because of some lock held by other thread, but we can get nothing from ""stacks"" for ReadWriteLock, and it is widely used in our services, like the fslock, cplock, dirlock of namenode.

Luckily, we can get thread parker from Thread object, it can help us see the thread parker clearly.

 !image-2020-12-25-08-32-32-982.png! "
Upgrade Yetus to 0.13.0,13343685,Resolved,Major,Duplicate,02/Dec/20 04:07,01/Oct/22 19:15,,"After HADOOP-17262 and HADOOP-17297, Hadoop is using a non-release version of Apache Yetus. It should be upgraded to 0.13.0 when released."
Remove S3Guard - no longer needed,13344074,Resolved,Major,Fixed,03/Dec/20 18:10,18/Jan/22 18:05,3.3.0,"With Consistent S3, S3Guard is superfluous. 

stop developing it and wean people off it as soon as they can.

Then we can worry about what to do in the code. It has gradually insinuated its way through the layers, especially things like multi-object delete handling (see HADOOP-17244). Things would be a lot simpler without it


This work is being done in the feature branch HADOOP-17409-remove-s3guard"
AliyunOSS: support ListObjectsV2,13340100,Resolved,Major,Fixed,12/Nov/20 01:49,05/Nov/21 04:47,2.10.1,OSS supports ListObjectsV2([https://help.aliyun.com/document_detail/187544.html?spm=a2c4g.11186623.6.1589.e0623d9fE1b64S)] to optimize versioning bucket list. We should support this feature in AliyunOSS module.
Intermittent S3AInputStream failures: Premature end of Content-Length delimited message body etc,13338162,Resolved,Major,Fixed,30/Oct/20 23:38,18/Dec/20 19:16,3.3.0,"We are seeing the following two kinds of intermittent exceptions when using S3AInputSteam:

1.
{code:java}
Caused by: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 156463674; received: 150001089
at com.amazonaws.thirdparty.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:178)
at com.amazonaws.thirdparty.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:181)
at java.io.DataInputStream.readFully(DataInputStream.java:195)
at java.io.DataInputStream.readFully(DataInputStream.java:169)
at org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:779)
at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:130)
at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:214)
at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:208)
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:63)
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
... 15 more
{code}
2.
{code:java}
Caused by: javax.net.ssl.SSLException: SSL peer shut down incorrectly
at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:596)
at sun.security.ssl.InputRecord.read(InputRecord.java:532)
at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:990)
at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:948)
at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
at com.amazonaws.thirdparty.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
at com.amazonaws.thirdparty.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
at com.amazonaws.thirdparty.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
at com.amazonaws.thirdparty.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:181)
at java.io.DataInputStream.readFully(DataInputStream.java:195)
at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)
at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2361)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2493)
at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)
at cascading.tap.hadoop.io.CombineFileRecordReaderWrapper.next(CombineFileRecordReaderWrapper.java:70)
at org.apache.hadoop.mapred.lib.CombineFileRecordReader.next(CombineFileRecordReader.java:58)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)
... 10 more
{code}
Inspired by
 [https://stackoverflow.com/questions/9952815/s3-java-client-fails-a-lot-with-premature-end-of-content-length-delimited-messa]
 and
 [https://forums.aws.amazon.com/thread.jspa?threadID=83326], we got a solution that has helped us, would like to put the fix to the community version.

The problem is that S3AInputStream had a short-lived S3Object which is used to create the wrappedSteam, and this object got garbage collected at random time, which caused the stream to be closed, thus the symptoms reported.

[https://github.com/aws/aws-sdk-java/blob/1.11.295/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/S3Object.java#L225] is the s3 code that closes the stream when S3 object is garbage collected:

Here is the code in S3AInputStream that creates temporary S3Object and uses it to create the wrappedStream:
{code:java}
   S3Object object = Invoker.once(text, uri,
        () -> client.getObject(request));

    changeTracker.processResponse(object, operation,
        targetPos);
    wrappedStream = object.getObjectContent();
{code}"
Add GCS FS impl reference to core-default.xml,13343360,Resolved,Major,Fixed,30/Nov/20 17:14,07/Jul/21 21:43,,"Akin to current S3 default configuration add GCS configuration, specifically to declare the GCS implementation. [GCS connector|https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage]. Has this not been done since the GCS connector is not part of the hadoop/ASF codebase, or is there any other blocker?"
Fair call queue is defeated by abusive service principals,13338741,Resolved,Major,Fixed,03/Nov/20 23:16,23/Nov/20 23:22,,"[~daryn] reported  that the FCQ prioritizes based on the full kerberos principal (ie. ""user/host@realm"") rather than short name (ie. ""user"") to prevent service principals like the DNs and NMs being de-prioritized since service principals are expected to be well behaved.  Notably the DNs contribute a significant but important load so the intent is not to de-prioritize all DNs because their sum total load is high relative to users.

This has the unfortunate side effect of allowing misbehaving & non-critical service principals to abuse the FCQ. The gstorm/* principals are a prime example.   Each server is spamming opens as fast as possible which ensures that none of the gstorm servers can be de-prioritized because each principal is a fraction of the total load from all principals.

The secondary and more devasting problem is other abusive non-service principals cannot be effectively de-prioritized.  The sum total of all gstorm load prevents other principals from surpassing the priority thresholds.  Principals stay in the highest priority queues which allows the abusive principals to overflow the entire call queue for extended periods of time.  Notably it prevents the FCQ from moderating the heavy create loads from p_gup @ DB which cause significant performance degradation.

Prioritization should be based on short name with configurable exemptions for services like the DN/NM.

[~daryn] suggested a solution that we applied on our clusters."
Upgrade commons-compress to 1.21,13339644,Resolved,Major,Fixed,10/Nov/20 02:29,08/Aug/21 02:37,3.2.1,
Add InetAddress api to ProxyUsers.authorize,13339575,Resolved,Major,Fixed,09/Nov/20 18:04,19/Nov/20 22:23,,"Improve the ProxyUsers implementation by passing the address of the remote peer to avoid resolving the hostname.
Similarly, this requires adding InetAddress api to MachineList."
ABFS does not work with OAuth 2.0: Username and Password,13339323,Open,Major,,07/Nov/20 12:30,,3.3.0,"https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html

I have tried OAuth 2.0 authentication with the username and password written above.
However, it failed with the following exception.

~~~
Exception in thread ""main"" HTTP Error 400; url='https://login.microsoftonline.com/3070a5de-410e-4885-XXXX-XXXXXXXXXXXX/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/3070a5de-410e-4885-XXXX-XXXXXXXXXXXX/oauth2/token failed for getting token from AzureAD.; requestId='187c97a4-82a0-4b36-b764-XXXXXXXXXXXX'; contentType='application/json; charset=utf-8'; response '{""error"":""unauthorized_client"",""error_description"":""AADSTS700016: Application with identifier 'jiro' was not found in the directory '3070a5de-410e-4885-XXXX-XXXXXXXXXXXX'. This can happen if the application has not been installed by the administrator of the tenant or consented to by any user in the tenant. You may have sent your authentication request to the wrong tenant.\r\nTrace ID: 187c97a4-82a0-4b36-b764-a3b8b1c45201\r\nCorrelation ID: 4eb4a71e-2eef-4788-9c8c-24f4c84f6981\r\nTimestamp: 2020-11-07 11:49:21Z"",""error_codes"":[700016],""timestamp"":""2020-11-07 11:49:21Z"",""trace_id"":""187c97a4-82a0-4b36-b764-a3b8b1c45201"",""correlation_id"":""4eb4a71e-2eef-4788-9c8c-24f4c84f6981"",""error_uri"":""https://login.microsoftonline.com/error?code=700016""}'org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error 400; url='https://login.microsoftonline.com/3070a5de-410e-4885-b6cd-95fe759ced2b/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/3070a5de-410e-4885-XXXX-XXXXXXXXXXXX/oauth2/token failed for getting token from AzureAD.; requestId='187c97a4-82a0-4b36-b764-XXXXXXXXXXXX'; contentType='application/json; charset=utf-8'; response '{""error"":""unauthorized_client"",""error_description"":""AADSTS700016: Application with identifier 'jiro' was not found in the directory '3070a5de-410e-4885-XXXX-XXXXXXXXXXXX'. This can happen if the application has not been installed by the administrator of the tenant or consented to by any user in the tenant. You may have sent your authentication request to the wrong tenant.\r\nTrace ID: 187c97a4-82a0-4b36-b764-a3b8b1c45201\r\nCorrelation ID: 4eb4a71e-2eef-4788-9c8c-24f4c84f6981\r\nTimestamp: 2020-11-07 11:49:21Z"",""error_codes"":[700016],""timestamp"":""2020-11-07 11:49:21Z"",""trace_id"":""187c97a4-82a0-4b36-b764-a3b8b1c45201"",""correlation_id"":""4eb4a71e-2eef-4788-9c8c-24f4c84f6981"",""error_uri"":""https://login.microsoftonline.com/error?code=700016""}'
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:215)
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:293)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createDirectory(AzureBlobFileSystemStore.java:445)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.mkdirs(AzureBlobFileSystem.java:409)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2355)
	at com.sample.HelloWorld.main(HelloWorld.java:116)
Caused by: org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error 400; url='https://login.microsoftonline.com/3070a5de-410e-XXXX-XXXXXXXXXXXX/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/3070a5de-410e-4885-XXXX-XXXXXXXXXXXX/oauth2/token failed for getting token from AzureAD.; requestId='187c97a4-82a0-4b36-b764-a3b8b1c45201'; contentType='application/json; charset=utf-8'; response '{""error"":""unauthorized_client"",""error_description"":""AADSTS700016: Application with identifier 'jiro' was not found in the directory '3070a5de-410e-4885-XXXX-XXXXXXXXXXXX'. This can happen if the application has not been installed by the administrator of the tenant or consented to by any user in the tenant. You may have sent your authentication request to the wrong tenant.\r\nTrace ID: 187c97a4-82a0-4b36-b764-a3b8b1c45201\r\nCorrelation ID: 4eb4a71e-2eef-4788-9c8c-24f4c84f6981\r\nTimestamp: 2020-11-07 11:49:21Z"",""error_codes"":[700016],""timestamp"":""2020-11-07 11:49:21Z"",""trace_id"":""187c97a4-82a0-4b36-b764-a3b8b1c45201"",""correlation_id"":""4eb4a71e-2eef-4788-9c8c-24f4c84f6981"",""error_uri"":""https://login.microsoftonline.com/error?code=700016""}'
	at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:394)
	at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:291)
	at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:273)
	at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:96)
	at org.apache.hadoop.fs.azurebfs.oauth2.UserPasswordTokenProvider.refreshToken(UserPasswordTokenProvider.java:54)
	at org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:670)
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:168)
	... 6 more
~~~

The cause of the error seems to be that UserPasswordTokenProvider is calling getTokenUsingClientCreds() for the service principal.

https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth-ropc

I checked the API specifications of Azure and fixed the cause of this error.

After this, I plan to create a Pull Request.

Best regards,
Shin"
Increase docker memory limit in Jenkins,13346619,Resolved,Major,Fixed,17/Dec/20 16:26,12/Jan/21 06:35,,"Yetus keeps failing with OOM.

 
{code:bash}
unable to create new native thread

java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957)
	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
	at org.apache.maven.surefire.booter.ForkedBooter.launchLastDitchDaemonShutdownThread(ForkedBooter.java:369)
	at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:333)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:145)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
 

This jira to increase the memory limit from 20g to 22g.

*Note: This is only a workaround to get things more productive. If this change reduces the frequency of the OOM failure, there must be a follow-up profile the runtime to figure out which components are causing the docker to run out of memory.*

CC: [~aajisaka], [~elgoiri], [~weichiu], [~ebadger], [~tasanuma], [~iwasakims], [~ayushtkn], [~inigoiri]"
Creating a token identifier should not do kerberos name resolution,13338668,Resolved,Major,Fixed,03/Nov/20 15:28,05/Nov/20 22:17,2.10.1,"This problem was found and fixed internally for us by [~daryn].

Creating a token identifier tries to do auth_to_local short username translation. The authentication process creates a blank token identifier for deserializing the wire format. Attempting to resolve an empty username is useless work.

Discovered the issue during fair call queue backoff testing. The readers are unnecessary slowed down by this bug.

"
Remote exception messages should not include the exception class,13342219,Resolved,Major,Fixed,23/Nov/20 21:26,03/Dec/20 19:59,,"HADOOP-9844 added a change that caused some remote SASL exceptions to redundantly include the exception class causing the client to see ""{{Class: Class: message}}"" from an unwrapped RemoteException."
Doing hadoop ls on Har file triggers too many RPC calls,13339238,Resolved,Major,Fixed,06/Nov/20 20:12,16/Nov/20 14:59,,"[~daryn] has noticed that Invoking hadoop ls on HAR is taking too much of time.

The har system has multiple deficiencies that significantly impacted performance:

# Parsing the master index references ranges within the archive index. Each range required re-opening the hdfs input stream and seeking to the same location where it previously stopped.
# Listing a har stats the archive index for every ""directory"". The per-call cache used a unique key for each stat, rendering the cache useless and significantly increasing memory pressure.
# Determining the children of a directory scans the entire archive contents and filters out children. The cached metadata already stores the exact child list.
# Globbing a har's contents resulted in unnecessary stats for every leaf path.

 "
TestLdapGroupsMapping failing -string mismatch in exception validation,13338474,Resolved,Major,Fixed,02/Nov/20 18:12,07/Nov/20 04:14,3.3.1,"Looks like a change in the exception strings is breaking the validation code
{code}
[ERROR]   TestLdapGroupsMapping.testLdapReadTimeout:447  Expected to find 'LDAP response read timed out, timeout used:4000ms' but got unexpected exception: javax.naming.NamingException: LDAP response read timed out, timeout used: 4000 ms.; remaining name ''
	at com.sun.jndi.ldap.LdapRequest.getReplyBer(LdapRequest.java:129)
{code}"
Javadoc warnings and errors are ignored in the precommit jobs,13335758,Resolved,Major,Fixed,16/Oct/20 11:43,19/Oct/20 02:16,2.10.0,"After changing the JDK from OracleJDK to the OpenJDK packaged in Ubuntu, javadoc warnings and errors are not shown in the precommit Jenkins jobs.

In Ubuntu/Debian, doclint is not enabled by default:
 - [https://bugs.launchpad.net/ubuntu/+source/openjdk-8/+bug/1898116]
 - [https://github.com/Debian/openjdk-8/blob/master/debian/patches/disable-doclint-by-default.patch]"
Fix the error of TestDynamometerInfra,13340117,Resolved,Major,Fixed,12/Nov/20 04:48,07/May/21 05:10,,"https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/321/artifact/out/patch-unit-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt
{noformat}
[INFO] Running org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.43 s <<< FAILURE! - in org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
[ERROR] org.apache.hadoop.tools.dynamometer.TestDynamometerInfra  Time elapsed: 0.43 s  <<< ERROR!
java.io.FileNotFoundException: http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1896)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1498)
	at org.apache.commons.io.FileUtils.copyURLToFile(FileUtils.java:1506)
	at org.apache.hadoop.tools.dynamometer.DynoInfraUtils.fetchHadoopTarball(DynoInfraUtils.java:151)
	at org.apache.hadoop.tools.dynamometer.TestDynamometerInfra.setupClass(TestDynamometerInfra.java:176)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}"
Downgrade guava version in trunk,13346812,Resolved,Major,Won't Fix,18/Dec/20 15:44,19/Apr/21 05:46,,"See details the Jira HADOOP-17439  comments.
h1.  

 "
No shade guava in trunk,13346794,Open,Major,,18/Dec/20 14:02,,,!image-2020-12-18-22-01-45-424.png!
ITestCustomSigner fails with gcs s3 compatible endpoint. ,13335396,Resolved,Major,Fixed,14/Oct/20 11:41,02/Nov/20 07:35,3.3.0,CC [~sseth] [~stevel@apache.org]
enable s3a magic committer by default,13343822,Resolved,Major,Duplicate,02/Dec/20 16:54,04/Feb/21 13:45,3.3.0,"Now that AWS S3 is consistent, we can safely enable the magic committer everywhere. Change the setting."
Magic committer files don't have the count of bytes written collected by spark,13344592,Resolved,Major,Fixed,07/Dec/20 14:28,26/Jan/21 19:47,3.2.0,"The spark statistics tracking doesn't correctly assess the size of the uploaded files as it only calls getFileStatus on the zero byte objects -not the yet-to-manifest files. Which, given they don't exist yet, isn't easy to do.

Solution: 
* Add getXAttr and listXAttr API calls to S3AFileSystem
* Return all S3 object headers as XAttr attributes prefixed ""header."" That's custom and standard (e.g header.Content-Length).

The setXAttr call isn't implemented, so for correctness the FS doesn't
declare its support for the API in hasPathCapability().

The magic commit file write sets the custom header 
set the length of the data final data in the header
x-hadoop-s3a-magic-data-length in the marker file.

A matching patch in Spark will look for the XAttr
""header.x-hadoop-s3a-magic-data-length"" when the file
being probed for output data is zero byte long. 
As a result, the job tracking statistics will report the
bytes written but yet to be manifest.
"
ABFS: Allow Random Reads to be of Buffer Size,13330755,Resolved,Major,Fixed,03/Oct/20 22:31,22/Jan/21 11:09,3.3.0,"ADLS Gen2/ABFS driver is optimized to read only the bytes that are requested for when the read pattern is random. 

It was observed in some spark jobs that though the reads are random, the next read doesn't skip by a lot and can be served by the earlier read if read was done in buffer size. As a result the job triggered a higher count of read calls/higher IOPS, resulting in higher IOPS throttling and hence resulted in higher job runtime.

When these jobs were run against Gen1 which always reads in buffer size , the jobs fared well. 

This Jira attempts to get a Gen1 customer migrating to Gen2 get the same overall i/o pattern as gen1 and the same perf characteristics.

*+Stats from Customer Job:+*

 
|*Customer Job*|*Gen 1 timing*|*Gen 2 Without patch*|*Gen2 with patch and RAH=0*|
|Job1|2 h 47 m|3 h 45 m|2 h 27 mins|
|Job2|2 h 17 m|3 h 24 m|2 h 39 mins|
|Job3|3 h 16 m|4 h 29 m|3 h 21 mins|
|Job4|1 h 59 m|3 h 12 m|2 h 28 mins|

 

*+Stats from Internal TPCDs runs+* 

[Total number of TPCDs queries per suite run = 80  

Full suite repeat run count per config = 3]
| |*Gen1*|Gen2 Without patch|*Gen2 With patch and RAH=0*
*(Gen2 in Gen1 config)*|*Gen2 With patch and RAH=2*|
|%Run Duration|100|140|213|70-90|
|%Read IOPS|100|106|98|110-115|

 

*Without patch = default Jar with random read logic

*With patch=Modified Jar with change to always read buffer size

*RAH=ReadAheadQueueDepth

 "
ABFS: Piggyback flush on Append calls for short writes,13343624,Resolved,Major,Fixed,01/Dec/20 17:51,22/Jan/21 11:12,3.3.0,"When Hflush or Hsync APIs are called, a call is made to store backend to commit the data that was appended. 

If the data size written by Hadoop app is small, i.e. data size :
 * before any of HFlush/HSync call is made or

 * between 2 HFlush/Hsync API calls

is less than write buffer size, 2 separate calls, one for append and another for flush is made,

Apps that do such small writes eventually end up with almost similar number of calls for flush and append.

This PR enables Flush to be piggybacked onto append call for such short write scenarios.

 

NOTE: The changes is guarded over a config, and is disabled by default until relevant supported changes is made available on all store production clusters.

New Config added: fs.azure.write.enableappendwithflush"
ABFS: Delete Idempotency handling can lead to NPE,13344002,Resolved,Major,Fixed,03/Dec/20 11:59,22/Jan/21 11:11,3.3.0,"Delete idempotency code returns success with a dummy success HttpOperation. the calling code that checks continuation token throws NPE as the dummy success instance does not have any response headers.

In case of non-HNS account, server coulf return continuation token.  Dummy success response code is modified to not fail while accessing response headers.

 "
ABFS: Release Elastic ByteBuffer pool memory at outputStream close,13344551,Resolved,Major,Fixed,07/Dec/20 11:50,22/Jan/21 11:10,3.3.0,Each AbfsOutputStream holds on to an instance of elastic bytebuffer pool. This instance needs to be released so that the memory can be given back to JVM's available memory pool. 
Update the hadoop version in TestDynamometerInfra,13347160,Resolved,Major,Duplicate,21/Dec/20 15:25,21/Jan/21 05:22,,
Optimize NetworkTopology while sorting of block locations,13344042,Resolved,Major,Fixed,03/Dec/20 15:29,08/Jan/21 20:03,,"In {{NetworkTopology}}, I noticed that there are some hanging fruits to improve the performance.

Inside {{sortByDistance}}, collections.shuffle is performed on the list before calling {{secondarySort}}.

{code:java}
Collections.shuffle(list, r);
if (secondarySort != null) {
  secondarySort.accept(list);
}
{code}

However, in different call sites, {{collections.shuffle}} is passed as the secondarySort to {{sortByDistance}}. This means that the shuffle is executed twice on each list.
Also, logic wise, it is useless to shuffle before applying a tie breaker which might make the shuffle work obsolete.

In addition, [~daryn] reported that:
* topology is unnecessarily locking/unlocking to calculate the distance for every node
* shuffling uses a seeded Random, instead of ThreadLocalRandom, which is heavily synchronized
"
S3AInputStream to be resilient to faiures in abort(); translate AWS Exceptions,13336044,Resolved,Major,Duplicate,19/Oct/20 11:01,06/Jan/21 16:07,3.2.1,"Stack overflow issue complaining about ConnectionClosedException during S3AInputStream close(), seems triggered by an EOF exception in abort. That is: we are trying to close the stream and it is failing because the stream is closed. oops.

https://stackoverflow.com/questions/64412010/pyspark-org-apache-http-connectionclosedexception-premature-end-of-content-leng

Looking @ the stack, we aren't translating AWS exceptions in abort() to IOEs, which may be a factor."
Update Jetty hadoop dependency,13347124,Resolved,Major,Duplicate,21/Dec/20 11:12,04/Jan/21 19:41,3.2.1,"Vulnerability fixes needed for Jetty hadoop dependency library

The jetty jars where CVEs are found are ,

================ =====

Jetty [version 9.4.20.v20190813 ]

jetty-server-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ]
================ =====

Jetty-http [version 9.4.20.v20190813 ]

jetty-http-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ]"
Jetty 9.4.20 can't generate resourceBase with NPE,13348358,Resolved,Major,Duplicate,29/Dec/20 22:40,04/Jan/21 19:40,,"While I was looking into TestDistributedShell logs, I noticed the following {{Warning}}


{code:bash}
2020-12-29 16:22:26,379 INFO  [Time-limited test] handler.ContextHandler (ContextHandler.java:doStart(824)) - Started o.e.j.s.ServletContextHandler@75389179{logs,/logs,file:///hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target/log,AVAILABLE}
2020-12-29 16:22:26,380 INFO  [Time-limited test] handler.ContextHandler (ContextHandler.java:doStart(824)) - Started o.e.j.s.ServletContextHandler@116ed75c{static,/static,jar:file:~/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.4.0-SNAPSHOT/hadoop-yarn-common-3.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2020-12-29 16:22:26,390 WARN  [Time-limited test] webapp.WebInfConfiguration (WebInfConfiguration.java:getCanonicalNameForWebAppTmpDir(794)) - Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
2020-12-29 16:22:26,469 INFO  [Time-limited test] util.TypeUtil (TypeUtil.java:<clinit>(201)) - JVM Runtime does not support Modules
{code}

For OS X, it looks like {{webAppContext.setBaseResource}} and accessing the sources from a jar file will cause {{file.resource.toURI().getPath()}} to return {{null}} for {{jar:-urls}}

I checked that changing the jetty-version  from {{9.4.20.v20190813}} to something above {{9.4.21}} (aka., 9.4.23.v20191118) fixes the warning.

[~inigoiri], [~aajisaka], [~weichiu], [~ayushtkn]
Do you guys think we should consider upgrading Jetty to the [latest versions of 9.4.x|https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-webapp] like 9.4.35?

"
lz4 sources missing for native Visual Studio project,13343144,Resolved,Major,Fixed,29/Nov/20 04:44,30/Nov/20 06:54,3.3.0,lz4 sources are missing for the *native.vcxproj* Visual Studio project for Windows. This is causing compilation failure on Windows.
Skip license check on lz4 code files,13341688,Resolved,Major,Fixed,20/Nov/20 00:33,20/Nov/20 14:15,,"There are some files against the asflicense check which will make _This commit cannot be built_ when building,  this is caused by the following two files:
{noformat}
hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/lz4/lz4.c  
hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/lz4/lz4.h{noformat}
The two do not have an Apache license header, maybe we should skip check that to let the buildings go.

 "
ABFS: Set default ListMaxResults to max server limit,13345009,Resolved,Major,Fixed,09/Dec/20 07:33,21/Dec/20 06:50,3.3.0,"{{Changing the default value of maximum size of }}{{results to be}}{{ returned by ListStatus from 500 to 5000, since the maximum number of items supported by a listStatus server call is 5000.}}"
Remove Hadoop 2.9.2 from the download page,13346277,Resolved,Major,Fixed,16/Dec/20 07:14,18/Dec/20 01:53,,"2.9.x is EoL: https://cwiki.apache.org/confluence/display/HADOOP/EOL+%28End-of-life%29+Release+Branches
Let's remove 2.9.2 from https://hadoop.apache.org/releases.html"
"When `fs.s3a.connection.ssl.enabled=true`,   Error when visit S3A with AKSK",13344467,Resolved,Major,Duplicate,07/Dec/20 02:23,14/Dec/20 12:22,,"When we update hadoop version from hadoop-3.2.1 to hadoop-3.3.0, Use AKSK access s3a with ssl enabled, then this error happen
{code:java}
<property>   
    <name>ipc.client.connection.maxidletime</name> 
   <value>20000</value> 
</property> 
<property>
    <name>fs.s3a.secret.key</name> 
    <value></value> 
</property> 
<property>   
    <name>fs.s3a.access.key</name> 
    <value></value>
</property> 
<property> 
    <name>fs.s3a.aws.credentials.provider</name> 
    <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
</property>
{code}
!image-2020-12-07-10-25-51-908.png!"
Skipping network I/O in S3A getFileStatus(/) breaks some tests,13342792,Resolved,Major,Fixed,26/Nov/20 09:11,26/Nov/20 20:58,3.3.1,"[*ERROR*]   *ITestS3ABucketExistence.testNoBucketProbing:65->expectUnknownStore:102 Expected a org.apache.hadoop.fs.s3a.UnknownStoreException to be thrown, but got the result: : S3AFileStatus\{path=s3a://random-bucket-57a85110-c715-4db2-a049-e43999d7e51b/; isDirectory=true; modification_time=0; access_time=0; owner=mthakur; group=mthakur; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=UNKNOWN eTag=null versionId=null*

[*ERROR*]   *ITestS3AInconsistency.testGetFileStatus:114->Assert.fail:88 getFileStatus should fail due to delayed visibility.*

 

*ITestMarkerTool.testRunWrongBucket:227->AbstractMarkerToolTest.runToFailure:276 » UnknownStore*"
Reduce UGI overhead in token ops ,13344887,Resolved,Major,Later,08/Dec/20 17:46,11/Dec/20 21:39,,"{{DelegationTokenIdentifier}} has a {{ugiCache}} but  AbstractDelegationTokenManager calls a static method {{getRemoteUser()}} which would bypass the cache.

Performance analysis of the KMS revealed the RPC server layer is creating many and redundant UGI instances.. UGIs are not cheap to instantiate, require synchronization, and waste memory. Reducing instantiations will improve the performance of the ipc readers.



 

 "
Reduce synchronization in the token secret manager,13344884,Resolved,Major,Later,08/Dec/20 17:35,11/Dec/20 21:38,,"[~daryn] reported that Reducing synchronization in the ZK secret manager is complicated by excessive and unnecessary global synchronization in the AbstractDelegationTokenSecretManager.  All RPC services, not just the KMS, will benefit from the reduced synchronization.

 "
Add reload option to RefreshCallQueue,13345458,Patch Available,Major,,11/Dec/20 08:22,,,"Currently the can use the dfsadmin command of ""refreshCallQueue"" to refresh all configs of FairCallQueue, but it will cause call queue spike during the refresh on NameNode.

In  [HADOOP-17421|https://issues.apache.org/jira/projects/HADOOP/issues/HADOOP-17421?filter=allopenissues], we added some configurations to specify queues for some static users, with which we can have fine-grained control on the behavior of different users manually, this feature will require us to refresh call queue of NameNodes constantly, which will sure cause bad impacts to NameNode's performance.

This ticket was to propose adding a reload option for the command of refreshCallQueue, which will only trigger the _callQueueManager_ to reload something, instead of reconstructing new scheduler and new queues.

The basic design is :
 # Add a method of ""reload"" in the interface of _Scheduler._
 # Add different _RefreshCallQueueTypes_ to trigger different refresh operations.
 # Add the argument in DFSAdmin for *reload* option."
Specify user's queue via configuration in FairCallQueue ,13345008,Patch Available,Major,,09/Dec/20 07:26,,,"The feature of FairCallQueue helps a lot in maintaining a fair and good service in a multi-tenant cluster, each user is assigned to queues with different priority to reach this goal. But in production, we met some problems that the automatic assignment won't fit, the problems are as follows:
 # We have a service account that would send more NN requests, for some reasons, we would like to keep this user and allow this user to keep this volume of operations. When we deployed FairCallQueue, this service user would be treated as a bad user and assigned to a lower queue, causing some slowness on the service account.
 # We are having more Flink jobs writing checkpoints to our NN, and the checkpoint operations have a characteristic that they would have a periodically high cost on the NN with an interval of several minutes. FairCallQueue (with cost-based enabled) doesn't have good control of this kind of operations because when this kind of operations starts, the cost in the decay window of this user is quite low, so the user will be assigned to queue 0, after some windows, when the users' high cost has got the attention and assigned to a lower queue, the user's operations are already finished. 

For problem 1, we noticed that there is already an option mentioned in HADOOP-17165, but in our case, the service account isn't that important that we'd allow it to always be assigned to queue 0. 

To solve these problems, we'd like to raise a solution by specifying the queue for some static users via config. The basic design is as follows:
 * Specify the static users in config for each queue.
 * Load the mapping from the config while initializing the callqueue.
 * Check the configured queue for each user when assigning the queue.
 * The cost time of the static users would not be count in our decay calculation to mitigate the impacts on other normal users' costs.

 "
Replace HTrace with NoOp tracer,13341443,Resolved,Major,Duplicate,18/Nov/20 22:01,10/Dec/20 19:10,,"HADOOP-17171 raised the concern that the deprecated htrace binaries has a few CVEs in its dependency jackson-databind. Not that HADOOP-15566 may not be merged any time soon. We can replace the existing htrace impl with a noop tracer (dummy).

This could be realized by reusing some code in HADOOP-15566's PR."
Javadoc failure in hadoop-aws (branch-3.3),13345236,Resolved,Major,Duplicate,10/Dec/20 09:29,10/Dec/20 09:33,,"In branch-3.3, javadoc is failing in hadoop-aws module.
https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2522/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-aws.txt
{noformat}
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2522/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ListingOperationCallbacks.java:81: error: unexpected text
[ERROR] * {@link this.getMaxKeys()}.
[ERROR] ^
{noformat}
Found in https://github.com/apache/hadoop/pull/2522#issuecomment-741996431"
"ERROR conf.Configuration: error parsing conf core-site.xml com.ctc.wstx.exc.WstxParsingException: Illegal processing instruction target (""xml""); xml (case insensitive) is reserved by the specs.",13345113,Open,Major,,09/Dec/20 16:54,,2.9.2,"ERROR conf.Configuration: error parsing conf core-site.xml
com.ctc.wstx.exc.WstxParsingException: Illegal processing instruction target (""xml""); xml (case insensitive) is reserved by the specs."
GCS to support per-bucket configuration,13343290,Resolved,Major,Duplicate,30/Nov/20 11:26,03/Dec/20 23:36,,S3 (HADOOP-13336) and Azure (HADOOP-13972) support setting different configs per bucket. Need the same feature for Google Storage
can't change the fix version without reopening the bug,13344100,Resolved,Major,Fixed,03/Dec/20 20:44,03/Dec/20 20:57,,
ABFS: SAS Test updates for version and permission update,13342719,Resolved,Major,Fixed,25/Nov/20 22:02,03/Dec/20 14:38,3.3.0,"This Jira will track the below 2 updates to SAS test code:
 # Upgrading the SAS version in Service SAS generator (test code)
 # Updating the permission in Delegation SAS to ""op"" from ""p"" for ACL operation as identities added as suoid/saoid added by tests are not owners of test path (Again test code).
 [Relevant public documentation: https://docs.microsoft.com/en-us/rest/api/storageservices/create-user-delegation-sas#specify-a-signed-object-id-for-a-security-principal-preview|https://docs.microsoft.com/en-us/rest/api/storageservices/create-user-delegation-sas#specify-a-signed-object-id-for-a-security-principal-preview]"
ABFS: Logs should redact SAS signature,13335994,Resolved,Major,Fixed,19/Oct/20 04:15,25/Nov/20 14:52,3.3.0,Signature part of the SAS should be redacted for security purposes.
WASB: Test failures,13336847,Resolved,Major,Fixed,23/Oct/20 04:49,23/Nov/20 17:27,3.3.0,"WASB tests are failing in Apache trunk resulting in Yetus run failures for PRs.

 
||Reason||Tests||
|Failed junit tests|hadoop.fs.azure.TestNativeAzureFileSystemMocked|
| |hadoop.fs.azure.TestNativeAzureFileSystemConcurrency|
| |hadoop.fs.azure.TestWasbFsck|
| |hadoop.fs.azure.TestNativeAzureFileSystemOperationsMocked|
| |hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck|
| |hadoop.fs.azure.TestNativeAzureFileSystemContractMocked|
| |hadoop.fs.azure.TestOutOfBandAzureBlobOperations|
| |hadoop.fs.azure.TestBlobMetadata|

Many PRs are hit by this. Test report link from one of the PRs:
[https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2368/5/testReport/]

 "
ABFS: testRenameFileOverExistingFile Fails after Contract test update,13342697,Resolved,Major,Fixed,25/Nov/20 19:51,26/Nov/20 10:17,3.3.1,"Post updates to rename on existing file test, ABFS contract test is having failure.

Updates were made in the AbstractContractTest class in https://issues.apache.org/jira/browse/HADOOP-17365.

To align to test expectation, ABFS tests need config ""fs.contract.rename-returns-false-if-dest-exists"" set to true. 

 "
ITestS3ADeleteCost.testDirMarkersFileCreation failure,13341380,Resolved,Major,Fixed,18/Nov/20 15:51,26/Nov/20 17:29,3.4.0,"Test runs after apply of recent patches HADOOP-17318 and HADOOP-17244 are showing a failure in ITestS3ADeleteCost.testDirMarkersFileCreation

Either one of the patches is failing and wasn't picked up, or the combined changes are conflicting in some way. Not backporting HADOOP-17318 to branch-3.3 until this is addressed."
ITestS3AContractRename failing against stricter tests,13340290,Resolved,Major,Fixed,12/Nov/20 15:59,16/Nov/20 11:26,3.3.1,HADOOP-17365 tightened the contract test for rename over file; S3A is failing.
[JDK 11] mvn package -Pdocs fails,13342308,Resolved,Major,Fixed,24/Nov/20 09:02,26/Nov/20 02:35,,"""mvn package -Pdocs -DskipTests -DskipShade"" fails with Java 11.
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc (default) on project hadoop-common: An error has occurred in Javadoc report generation: 
[ERROR] Exit code: 1 - javadoc: warning - You have specified the HTML version as HTML 4.01 by using the -html4 option.
[ERROR] The default is currently HTML5 and the support for HTML 4.01 will be removed
[ERROR] in a future release. To suppress this warning, please ensure that any HTML constructs
[ERROR] in your comments are valid in HTML5, and remove the -html4 option.
[ERROR] /Users/aajisaka/git/hadoop/hadoop-common-project/hadoop-common/target/generated-sources/java/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.java:3467: error: cannot find symbol
[ERROR]       com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                          ^
(snip)
{noformat}
The goal ""javadoc"" should be ""javadoc-no-fork""."
AbfsInputStream#seek throws EOFException when seek past the end of stream,13341172,Open,Major,,17/Nov/20 23:13,,,"Currently AbfsInputStream#seek has below check:
{code}
public synchronized void seek(long n) throws IOException {
...
if (n > contentLength) {
 throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);
}
...
}
{code}
So if a the seek is called with a position past the end of the stream, a EOFException gets thrown. However, it appears to me this behavior is different from some other inputstream impls, such as \{{S3AInputStream}} and \{{LocalFSFileInputStream}}, where seek passing the end is allowed, only that a later read call will return -1 indicating EOF. I think it would be better to have AbfsInputStream do the same thing."
AbstractS3ATokenIdentifier to issue date in UTC,13341572,Resolved,Major,Fixed,19/Nov/20 12:17,20/Nov/20 11:00,,"Follow on to HADOOP-17379: I think we should go to UTC and Time.now()/System.currentTimeMillis()

All uses of the field in hadoop-* seem to assume UTC
"
AbstractS3ATokenIdentifier to set issue date == now,13340829,Resolved,Major,Fixed,16/Nov/20 12:42,17/Nov/20 14:58,3.3.0,The AbstractS3ATokenIdentifier DT identifier doesn't set the issue date to the current time (unlike ABFS); this confuses spark (SPARK-33440)
AbstractS3ATokenIdentifier to provide issue date == created && max date == expiry time,13341203,Open,Major,,18/Nov/20 04:22,,3.3.0,"As we've seen the behavior from HADOOP-17379, AbstractS3ATokenIdentifier doesn't guarantee providing correct ""issue date"" and ""max date"" before HADOOP-17379, and even after, ""max date"" is not a thing we can make a guarantee only with AbstractS3ATokenIdentifier, as there's no information.

In the meanwhile, I realized there're two methods in AbstractS3ATokenIdentifier, getCreated() and getExpiryTime(), which feel like the same purpose with getIssueDate() and getMaxDate() in AbstractDelegationTokenIdentifier.

If we think these pairs are doing the same, we can just override AbstractS3ATokenIdentifier.getIssueDate() to call getCreated() && AbstractS3ATokenIdentifier.getMaxDate() to call getExpiryTime()."
Ability to specify separate compression settings when intermediate and final output use the same codec,13341154,Open,Major,,17/Nov/20 21:10,,,"The ZStandard codec may become a codec that users will want to use for both intermediate data and for final output data yet specify different compression levels for those use cases.

It would be nice if there was a way we could create a ""meta codec"" like IntermediateCodec that used conf prefix techniques, like Oozie does with oozie.launcher for the Oozie launcher configs, to create a custom config namespace of sorts for setting arbitrary codec settings specific to the intermediate codec separate from the final output codec even if the same underlying codec is used for both.

However Codecs don't allow a configuration to be passed when obtaining a codec stream, and I think we would have to bypass the CodecPool entirely to be able to pass a custom conf to an arbitrary Codec.

Another approach is to skip trying to generalize the solution and specifically focus on ZStandard. It would be easy to create a wrapper codec around the existing ZStandardCompressor and ZStandardDecompressor which take the relevant parameters directly in their constructors.

"
Update apache/hadoop:3 to 3.3.0 release,13336491,Resolved,Major,Done,21/Oct/20 14:08,17/Nov/20 08:16,,{{apache/hadoop:3}} docker image should be updated to the [Hadoop 3.3.0 release|https://hadoop.apache.org/release/3.3.0.html].
java.lang.NoClassDefFoundError: org/apache/hadoop/tracing/SpanReceiverHost,13340427,Resolved,Major,Invalid,13/Nov/20 04:21,13/Nov/20 05:42,3.0.0,"Hi. I need help.

now Im trying to migration Cloudera flume to Apache flume. 

(which means no use XXX _chd5.16 any more)

during the test, when i stored data in HDFS i faced this problem below.

java.lang.NoClassDefFoundError: org/apache/hadoop/tracing/SpanReceiverHostjava.lang.NoClassDefFoundError: org/apache/hadoop/tracing/SpanReceiverHost at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:634) at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619) at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2816) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:98) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2853) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2835) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:387) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296) at com.poscoict.posframe.bdp.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:238) at com.poscoict.posframe.bdp.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:230) at com.poscoict.posframe.bdp.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:675) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924) at org.apache.flume.auth.UGIExecutor.execute(UGIExecutor.java:46) at com.poscoict.posframe.bdp.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:672) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tracing.SpanReceiverHost at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) 

 

And i don't know fixed it .

please help me. 

thank you!"
Zookeeper secret manager attempts to reuse token sequence numbers,13339607,In Progress,Major,,09/Nov/20 22:24,,,"[~daryn] reported that the ZK delegation token secret manager uses a {{SharedCounter}} to synchronize increments of a monotonically increasing sequence number for new tokens. Yet the KMS logs occasionally, depending on load, contains an odd error indicating collisions: 


{code:bash}
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_137547444
{code}


ZKDTSM does a CAS get and set of the sequence number. Rather than return the value it set, it returns the current value which may have already been incremented by another KMS."
RawLocalFileSystem's lastModifiedTime() loses milli seconds in JDK < 10.b09,13335556,Reopened,Major,,15/Oct/20 06:38,,,"RawLocalFileSystem's FileStatus uses {{File.lastModified()}} api from JDK.

This api looses milliseconds due to JDK bug.

[https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8177809]

This bug fixed in JDK 10 b09 onwards and still exists in JDK 8 which is still being used in many productions.

Apparently, {{Files.getLastModifiedTime()}} from java's nio package returns correct time.

Use {{Files.getLastModifiedTime()}} instead of {{File.lastModified}} as workaround. "
TestLdapGroupsMapping is failing in trunk,13339216,Resolved,Major,Duplicate,06/Nov/20 17:34,07/Nov/20 00:57,," 

[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/317/testReport/junit/org.apache.hadoop.security/TestLdapGroupsMapping/testLdapReadTimeout/]

 

[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/317/testReport/junit/org.apache.hadoop.security/TestLdapGroupsMapping/testLdapConnectionTimeout/]

 

The tests are failing in open-jdk due to change in exception message(One space character). The tests passes in oracle java(for me)

 
{noformat}
 Expected to find 'LDAP response read timed out, timeout used:3000ms' but got unexpected exception: javax.naming.NamingException: LDAP response read timed out, timeout used: 3000 ms.{noformat}
 

 

 

 

 "
RPC FairCallQueue for special users,13338805,Resolved,Major,Won't Fix,04/Nov/20 10:06,06/Nov/20 16:22,,"In HADOOP-15016, the idea was first raised to support special users by assigning each special user an independent queue with a share. The design was intended for the user to better control the RPC schedule, but there is also a risk that users may add a lot of items in the config of special-users, causing a lot of queues in the RPCScheduler.

This ticket records some ideas to mitigate the risks while solving the special-user problem based on HADOOP-15016.

0. The current implementation is as follows, all users will be treated equally, _multiplexer_ will decide the call count in each queue.

!Implement 0.png!

1. The first idea is to amplify the weight of super-users and resue the initial queues. This idea is easy to implement, but ordinary users and special users would be affected by each other, and it would be difficult for the _multiplexer_ to guarantee the traffic of super-suers.
!Implement 1.png!

2. The second idea is to set up one independent queue for all special users with a config controlling the weight of all special-users. One concern for this idea is that the scheduler between super-users' calls may not be fair.
!Implement 2.png!
3. The third idea is to also use priority queues for special-users based on idea 2, ensuring the fair handling of all super-users. Another benefit of this idea is we can use the queues to implement cost-based calculation.

!Implement 3.png!

I think Idea 3 should be a good balance of complexity and useability."
MetricsRecordFiltered error,13337574,Open,Major,,28/Oct/20 05:51,,3.2.1," Got sink exception,when set  datanode.sink.ganglia.metric.filter.exclude=metricssystem in hadoop-metrics2.properties ,

java.lang.ClassCastException: org.apache.hadoop.metrics2.impl.MetricsRecordFiltered$1 cannot be cast to java.util.Collection
 at org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30.putMetrics(GangliaSink30.java:165)
 at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)
 at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
 at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
 at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:135)
 at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:89)


//////////////////////////////////////////////////
This case can show the exception
	public static void main(String[] args) {
		 List<AbstractMetric> metricsd=new LinkedList<AbstractMetric>();		 
		 MetricsInfo info=MsInfo.ProcessName;
		 long timestamp=System.currentTimeMillis();
         List<MetricsTag> tags=new LinkedList<>();		 
		 org.apache.hadoop.metrics2.impl.MetricsRecordImpl recordimp = new MetricsRecordImpl(info, timestamp, tags, metricsd);
		 MetricsFilter filter=new RegexFilter();         
		 MetricsRecordFiltered  recordfilter=new MetricsRecordFiltered(recordimp,filter);		
		 SubsetConfiguration conf=new SubsetConfiguration(new PropertyListConfiguration(),""test"");
		 conf.addProperty(AbstractGangliaSink.SUPPORT_SPARSE_METRICS_PROPERTY, true);
		 GangliaSink30  ganliasink=new GangliaSink30();
		 ganliasink.init(conf);		 
		 ganliasink.putMetrics(recordfilter);
		
	}

///////////////////////////////////////////////////////////////
The root cause is:
 Gets a Iterable object in  MetricsRecordFiltered.java:
 @Override public Iterable<AbstractMetric> metrics() {
    return new Iterable<AbstractMetric>() {
      final Iterator<AbstractMetric> it = delegate.metrics().iterator();
      @Override public Iterator<AbstractMetric> iterator() {
        return new AbstractIterator<AbstractMetric>() {
          @Override public AbstractMetric computeNext() {
            while (it.hasNext()) {
              AbstractMetric next = it.next();
              if (filter.accepts(next.name())) {
                return next;
              }
            }
            return (AbstractMetric)endOfData();
          }
        };
      }
    };
  }

but convert to Collection in GangliaSink30.java line 164
        Collection<AbstractMetric> metrics = (Collection<AbstractMetric>) record
            .metrics();

"
"Backport HADOOP-16005-""NativeAzureFileSystem does not support setXAttr"" and HADOOP-16785. ""Improve wasb and abfs resilience on double close() calls. followup to abfs close() fix."" to branch-2.10",13337764,Resolved,Major,Fixed,28/Oct/20 23:32,03/Nov/20 20:02,2.10.1,"Backport:

 HADOOP-16005-""NativeAzureFileSystem does not support setXAttr"" to branch-2.10

HADOOP-16785. ""Improve wasb and abfs resilience on double close() calls. followup to abfs close() fix.""

from branch-3.2 to branch-2.10"
Work with externally managed user credentials,13338705,Open,Major,,03/Nov/20 18:09,,,"We don't have a good test coverage for externally managed user credentials. It's not clear how someone could ingest kerberos credentials or delegation tokens that are externally managed. It's not clear what services/file system implementation supports them.

File this Jira to track all relevant fixes/support/test/doc
 #  what is supported.
 #  test coverage
 #  document for best practices (how to do it right) "
"Backport HADOOP-16005-""NativeAzureFileSystem does not support setXAttr"" to branch-3.2",13337294,Resolved,Major,Fixed,26/Oct/20 22:00,28/Oct/20 22:40,3.2.1,"Backport:

 HADOOP-16005-""NativeAzureFileSystem does not support setXAttr"" to branch-3.2"
Reduce port collisions in JUnits,13336148,Patch Available,Major,,19/Oct/20 21:27,,,"Several Junit tests passes a non-zero port number to {{ServerSocketUtil.getPort()}}.
I noticed that those non-zero values are shared between test methods. Based, on the implementation of {{ServerSocketUtil.getPort()}}, the port is checked then, the socket is closed immediately.
Two concurrent tests can have the same port number, but one of them fails with binding exceptions.

Passing a zero port number reduces the probability of returning the same port numbers in two different tests, because the {{ServerSocketUtil.getPort()}} will call a random port number.

An example of that is as described in HDFS-15618. {{testRead()}} between {{TestBlockTokenWithDFSStriped}} and {{TestBlockTokenWithDFS}} where port 19870 is used by the two test cases."
Improve job submitter framework path handling,13336523,Resolved,Major,Invalid,21/Oct/20 17:03,21/Oct/20 20:07,,"[~daryn] pointed out that {{HttpServer2#bindForPortRange(}}) handles IOE then check whether the exception is instance of {{BindException}}.

This Jira improves the handling of the Exception"
Upgrade JUnit to 4.13.1,13336194,Open,Major,,20/Oct/20 05:19,,,"JUnit < 4.13.1 are vulnerable to CVE-2020-15250. The severity is low but it is worth upgrading.
https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"
start-build-env.sh hangs on Mac when the uid is too large,13335749,Open,Major,,16/Oct/20 10:29,,,"If the uid is too large, Docker hangs and consumes all the disk space in: 
{noformat:title=start-build-env.sh}
RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME}
{noformat}
Reference: https://github.com/docker/for-mac/issues/2038"
 S3A to always probe S3 in S3A getFileStatus on non-auth paths,13330557,Resolved,Major,Fixed,02/Oct/20 13:12,08/Oct/20 14:40,3.3.1,"an incidental part of HDP-13230 was a fix to innerGetFileStatus, wherein after a HEAD request we would update the DDB record, so resetting it's TTL

Applications which did remote updates of buckets without going through s3guard are now triggering failures in applications in the cluster when they go to open the file"
FileSystem.DirListingIterator.next() call should return NoSuchElementException,13333854,Resolved,Major,Fixed,06/Oct/20 07:22,07/Oct/20 13:03,,"FileSystem.DirListingIterator.next() call should return NoSuchElementException rather than IllegalStateException

 

Stacktrace for new test failure:

 
{code:java}
java.lang.IllegalStateException: No more items in iteratorjava.lang.IllegalStateException: No more items in iterator at com.google.common.base.Preconditions.checkState(Preconditions.java:507) at org.apache.hadoop.fs.FileSystem$DirListingIterator.next(FileSystem.java:2232) at org.apache.hadoop.fs.FileSystem$DirListingIterator.next(FileSystem.java:2205) at org.apache.hadoop.fs.contract.ContractTestUtils.iteratorToListThroughNextCallsAlone(ContractTestUtils.java:1495) at org.apache.hadoop.fs.contract.AbstractContractGetFileStatusTest.testListStatusIteratorFile(AbstractContractGetFileStatusTest.java:366)
{code}
 

CC [~stevel@apache.org]"
DistCp should fail fast if the configuration is not valid,13333697,Open,Major,,05/Oct/20 11:27,,,"If the number of Map task is more than 200 and using dynamic option, DistCp always fails by DynamicInputFormat#validateNumChunksUsing. This failure is after building the src/dst paths, which takes more than 10 minutes if the directory is large (i.e. DistCp fails by configuration error after more than 10 minutes).

I think DistCp should validate the configuration before building the src/dst paths and fail if the configuration is not valid."
Upgrade commons-codec to 1.15,13338509,Resolved,Minor,Fixed,02/Nov/20 22:19,04/Nov/20 17:13,3.3.3,"This issue aims to upgrade commons-codec to 1.15 to bring the latest bug fixes.

- https://commons.apache.org/proper/commons-codec/changes-report.html#a1.15"
Bump up snappy-java to 1.1.8.2,13345141,Resolved,Minor,Fixed,09/Dec/20 20:47,10/Dec/20 04:14,3.3.1,"1.1.8.2 includes:

* Support Apple Silicon (M1, Mac-aarch64)
* Fixed the pure-java Snappy fallback logic when no native library for your platform is found."
Bump up snappy-java to 1.1.8.1,13339625,Resolved,Minor,Fixed,09/Nov/20 23:40,10/Nov/20 06:01,3.4.0,The native libraries provided by snappy-java does not work on some distros on aarch64 and ppc64le. Upgrading snappy-java should fix this.
Netty library causing issue while running spark on yarn in PSuedodistributed mode,13348064,Patch Available,Minor,,28/Dec/20 05:28,,3.2.1,"While running in psuedodistributed mode,there is different version of netty-all library in hdfs (4.0.52), this is fine till you try to run spark on it. The spark 3.0.1 comes with netty-all 4.1.47.

Now both the libraries get loaded in classpath and thus spark submit fails with method error. "
hadoop-cloud-storage transient dependencies need review,13339558,Resolved,Minor,Duplicate,09/Nov/20 16:54,03/Mar/23 18:15,3.4.0,"A review of the hadoop cloud storage dependencies shows that things are creeping in there

{code}
[INFO] |  +- org.apache.hadoop:hadoop-cloud-storage:jar:3.1.4:compile
[INFO] |  |  +- (org.apache.hadoop:hadoop-annotations:jar:3.1.4:compile - omitted for duplicate)
[INFO] |  |  +- org.apache.hadoop:hadoop-aliyun:jar:3.1.4:compile
[INFO] |  |  |  \- com.aliyun.oss:aliyun-sdk-oss:jar:3.4.1:compile
[INFO] |  |  |     +- org.jdom:jdom:jar:1.1:compile
[INFO] |  |  |     +- org.codehaus.jettison:jettison:jar:1.1:compile
[INFO] |  |  |     |  \- stax:stax-api:jar:1.0.1:compile
[INFO] |  |  |     +- com.aliyun:aliyun-java-sdk-core:jar:3.4.0:compile
[INFO] |  |  |     +- com.aliyun:aliyun-java-sdk-ram:jar:3.0.0:compile
[INFO] |  |  |     +- com.aliyun:aliyun-java-sdk-sts:jar:3.0.0:compile
[INFO] |  |  |     \- com.aliyun:aliyun-java-sdk-ecs:jar:4.2.0:compile
[INFO] |  |  +- (org.apache.hadoop:hadoop-aws:jar:3.1.4:compile - omitted for duplicate)
[INFO] |  |  +- (org.apache.hadoop:hadoop-azure:jar:3.1.4:compile - omitted for duplicate)
[INFO] |  |  +- org.apache.hadoop:hadoop-azure-datalake:jar:3.1.4:compile
[INFO] |  |  |  \- com.microsoft.azure:azure-data-lake-store-sdk:jar:2.2.7:compile
[INFO] |  |  |     \- (org.slf4j:slf4j-api:jar:1.7.21:compile - omitted for conflict with 1.7.30)
{code}

Need to review and cut things which come in hadoop-common (slf4j, maybe some of the allyun stuff)"
ITestS3AContractSeek.teardown closes FS before superclass does its cleanup,13340907,Resolved,Minor,Duplicate,16/Nov/20 19:53,08/Aug/22 10:43,3.3.0,"ITestS3AContractSeek.teardown closes the FS, but because it does it before calling super.teardown, the superclass doesn't get the opportunity to delete the test dirs.

Proposed: change the order. "
Upgrade aws-java-sdk to 1.11.901,13338695,Resolved,Minor,Fixed,03/Nov/20 17:17,23/Nov/20 14:10,3.3.1,Upgrade AWS SDK to most recent version
Move dedicated pre-logging statements into existing logging guards,13330678,Open,Minor,,03/Oct/20 13:27,,,"I find some cases where some pre-processing statements dedicated to logging calls are not guarded by existing logging guards. Most of them are easy to fix. And the performance and maintainability of these logging calls can be improved to some extend. So I create a PR to fix them.

These issues are detected by a static analysis tool wrote by myself. This tool can extract all the dedicated statements for each debug-logging calls (i.e., the results of these statements are only used by debug-logging calls). Because I realize that debug logs will incur overhead in production, such as string concatenation and method calls in the parameters of logging calls as well as pre-processing statements. And I want to perform a systematic evaluation for the overhead of debugging logging calls in production."
S3A ITestPartialRenamesDeletes.testRenameDirFailsInDelete failure: missing directory marker,13343369,Resolved,Minor,Won't Fix,30/Nov/20 18:23,05/Jan/22 16:05,3.3.1,"Seemingly transient failure of the test ITestPartialRenamesDeletes with the latest HADOOP-17244 changes in: an expected directory marker was not found.

Test run was (unintentionally) sequential, markers=delete, s3guard on
{code}
-Dmarkers=delete -Ds3guard -Ddynamo -Dscale 
{code}

Hasn't come back since.

The bucket's retention policy was authoritative, but no dirs were declared as such"
Log the remote address for authentication success,13339097,Resolved,Minor,Fixed,06/Nov/20 00:29,16/Nov/20 22:28,,"IPC server logs ""Authentication successful for USER"". 
Unlike the hdfs audit log that includes the address with every request, the kms audit log is 10s aggregate per-user op counts.  Including the remote address would make debugging much easier."
s3a listing operation will fail in async prefetch if fs closed,13337708,Resolved,Minor,Cannot Reproduce,28/Oct/20 16:53,17/May/21 05:38,3.4.0,"The async prefetch logic in the S3A listing code gets into trouble if the FS closed and there was an async listing in progress. 

In this situation we should think about recognising and converting into some FS-is-closed exception"
ADLFS: Update SDK version from 2.3.6 to 2.3.9,13347488,Resolved,Minor,Fixed,23/Dec/20 05:58,06/Jan/21 14:37,3.4.0,Update SDK version from 2.3.6 to 2.3.9
TestNativeIO fails in SELinux security context,13344988,Open,Minor,,09/Dec/20 05:26,,3.3.0,"As per [https://www.gnu.org/software/coreutils/manual/html_node/What-information-is-listed.html] ""GNU ls uses a ‘.’ character to indicate a file with a security context, but no other alternate access method. The affects the parsing of the standard Unix file permissions format in TestNativeIO resulting in test failure."
TestLdapGroupsMapping test failures in JDK 11,13344990,Open,Minor,,09/Dec/20 05:31,,,"Some of the tests in TestLdapGroupsMapping rely on validating the precise text of a NamingException which has changed in JDK11. The message is substantially the same, but not identical, so tests fails. Specifically: 

testLdapConnectionTimeout

and 

testLdapReadTimeout"
Update Hadoop Documentation with a new AWS Credential Provider used with EKS,13346523,Open,Minor,,17/Dec/20 08:22,,,
Use shaded guava in ClientCache.java,13336191,Resolved,Minor,Fixed,20/Oct/20 04:55,20/Oct/20 14:25,,"After HADOOP-17288, we should use shaded guava."
Skipping network I/O in S3A getFileStatus(/) breaks ITestAssumeRole,13345900,Resolved,Minor,Fixed,14/Dec/20 17:22,19/Jan/21 18:00,3.3.0,"Test failure in ITestAssumeRole.testAssumeRoleRestrictedPolicyFS if the test bucket is unguarded. I've been playing with my bucket settings so this probably didn't surface before. 

test arguments -Dparallel-tests -DtestsThreadCount=4 -Dmarkers=keep  -Dfs.s3a.directory.marker.audit=true"
Improve S3A upload statistics collection from ProgressEvent callbacks,13346160,Open,Minor,,15/Dec/20 17:19,,3.4.0,"Collection of S3A upload stats from ProgressEvent callbacks can be improved

Two similar but different implementations of listeners
* org.apache.hadoop.fs.s3a.S3ABlockOutputStream.BlockUploadProgress
* org.apache.hadoop.fs.s3a.ProgressableProgressListener. Used on simple PUT calls.

Both call back into S3A FS to incrementWriteOperations; BlockUploadProgress also updates S3AInstrumentation/IOStatistics.

* I'm not 100% confident that BlockUploadProgress is updating things (especially gauges of pending bytes) at the right time
* or that completion is being handled
* And the other interface doesn't update S3AInstrumentation; numbers are lost.
* And there's no incremental updating during {{CommitOperations.uploadFileToPendingCommit()}}, which doesn't call Progressable.progress() other than on every block.
* or in MultipartUploader 

Proposed: 
* a single Progress listener which updates BlockOutputStreamStatistics, used by all interfaces.
* WriteOperations to help set this up for callers; 
* And it's uploadPart API to take a Progressable (or the progress listener to use for uploading that part)
* Multipart upload API to also add a progressable...would help for distcp-like applications.

+Itests to verify that the gauges come out right. At the end of each operation, the #of bytes pending upload == 0; that of bytes uploaded == the original size

"
Contract test for renaming over existing file is too lenient,13339510,Resolved,Minor,Fixed,09/Nov/20 11:39,11/Nov/20 21:21,,"{{AbstractContractRenameTest#testRenameFileOverExistingFile}} is too lenient in its assertions.

* {{FileAlreadyExistsException}} is accepted regardless of ""rename overwrites"" and ""rename returns false if exists"" contract options.  I think it should be accepted only if both of those options are false.
* ""rename returns false if exists"" option is ignored if the file is not overwritten by the implementation.

Also, I think the ""rename returns false if exists"" option is incorrectly inverted in the test, which it can get away with because the checks are loose.

(Found this while looking at a change in Ozone FS implementation from throwing exception to returning false.  The contract test unexpectedly passed without changing {{contract.xml}}.)"
 Restore ability to set Text to empty byte array,13345481,Resolved,Minor,Fixed,11/Dec/20 10:17,05/Jan/21 21:16,,"In org.apache.hadoop.io.Text:clear() method, the comments show that we can free the bytes by call set(new byte[0]), but it's not going to work now. Maybe we can follow this comments.

 

 
{code:java}
// org.apache.hadoop.io.Text 

/**
 * Clear the string to empty.
 *
 * <em>Note</em>: For performance reasons, this call does not clear the
 * underlying byte array that is retrievable via {@link #getBytes()}.
 * In order to free the byte-array memory, call {@link #set(byte[])}
 * with an empty byte array (For example, <code>new byte[0]</code>).
 */
public void clear() {
  length = 0;
  textLength = -1;
}
{code}
 

 "
[Hadoop-Tools]S3A MultiObjectDeleteException after uploading a file,13339009,Resolved,Minor,Cannot Reproduce,05/Nov/20 12:00,06/Jan/21 15:55,2.10.0,"Hello,

 

I am using org.apache.hadoop.fs.s3a.S3AFileSystem as implementation for S3 related operation.

When I upload a file onto a path, it returns an error:
{code:java}
20/11/05 11:49:13 ERROR s3a.S3AFileSystem: Partial failure of delete, 1 errors20/11/05 11:49:13 ERROR s3a.S3AFileSystem: Partial failure of delete, 1 errorscom.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted (Service: null; Status Code: 200; Error Code: null; Request ID: 767BEC034D0B5B8A; S3 Extended Request ID: JImfJY9hCl/QvninqT9aO+jrkmyRpRcceAg7t1lO936RfOg7izIom76RtpH+5rLqvmBFRx/++g8=; Proxy: null), S3 Extended Request ID: JImfJY9hCl/QvninqT9aO+jrkmyRpRcceAg7t1lO936RfOg7izIom76RtpH+5rLqvmBFRx/++g8= at com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2287) at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1137) at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1389) at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2304) at org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:2270) at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.writeSuccessful(S3AFileSystem.java:2768) at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:371) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:74) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:108) at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:69) at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:128) at org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:488) at org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:410) at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:342) at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:277) at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:262) at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:327) at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:299) at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:257) at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:281) at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:265) at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:228) at org.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:285) at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119) at org.apache.hadoop.fs.shell.Command.run(Command.java:175) at org.apache.hadoop.fs.FsShell.run(FsShell.java:317) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) at org.apache.hadoop.fs.FsShell.main(FsShell.java:380)20/11/05 11:49:13 ERROR s3a.S3AFileSystem: bv/: ""AccessDenied"" - Access Denied
{code}
The problem is that Hadoop tries to create fake directories to map with S3 prefix and it cleans them after the operation. The cleaning is done from the parent folder until the root folder.

If we don't give the corresponding permission for some path, it will encounter this problem:

[https://github.com/apache/hadoop/blob/rel/release-2.10.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L2296-L2301]

 

During uploading, I don't see any ""fake"" directories are created. Why should we clean them if it is not really created ?

It is the same for the other operations like rename or mkdir where the ""deleteUnnecessaryFakeDirectories"" method is called.

Maybe the solution is to check the deleting permission before it calls the deleteObjects method.

 

To reproduce the problem:
 # With a bucket named my_bucket, we have the path s3://my_bucket/a/b/c inside
 # The corresponding user has only permission on the path b and sub-path inside.
 # We do the command ""hdfs dfs -mkdir s3://my_bucket/a/b/c/d"""
"Exception ""Server has invalid Kerberos principal"" when try to connect using reverseDNS hostname by IP resolving",13347880,Open,Minor,,25/Dec/20 12:38,,3.1.3,"I try to create filesystem object to operate with remote kerberised cluster but there is an exception because of fail verification NameNode server principal with principal from config

The reason of exception:
In org.apache.hadoop.security.SaslRpcClient#getServerPrincipal there is verifying process of NameNode server principal and principal which gets from config. In config principal keeps in format nn/_HOST@EXAMPLE.COM where _HOST is a placeholder for real NameNode host
Then there is replacing of _HOST placeholder with real host name of name node. And real host name gets as result of InetAddress.getCanonicalHostName() method (look /Users/a16689075/.m2/repository/org/apache/hadoop/hadoop-common/3.1.3/hadoop-common-3.1.3-sources.jar!/org/apache/hadoop/security/SecurityUtil.java:211)

But if there is some reverse DNS in infrastructure where I run this code, reverse DNS resolves host by IP and return host with dot at the end. So InetAddress.getCanonicalHostName() returns real host of NameNode but with dot at the end: nn/ex1.example.com. (the same is actual when you execute for example nslookup ip-addr in command line)
So principal from config after placeholder replacement looks like nn/ex1.example.com.@EXAMPLE.COM (where hostname keeps dot at the end)

Then there is checking if nameNode server pricipal is equals with principal from config. And of course they are different because of dot exists at the end of hostname in config principal

At the same time we can look at constructor sun.security.krb5.PrincipalName#PrincipalName(java.lang.String, int, java.lang.String where we can see similar logic for processing InetAddress.getCanonicalHostName();
If InetAddress.getCanonicalHostName() received host with dot at the end, there it cuts this dot and keeps only hostname without dot

 

Cluster has Kerberos Auth and configs with HA mode for HDFS with 2 namenodes

My code to reproduce this issue is:

...

HdfsConfiguration conf = new HdfsConfiguration(false);
 conf.addResource(hdfsSiteXmlInputStream);
 conf.addResource(coreSiteXmlInputStream)));

Path hdfsKeytabPath = Paths.get(""./hdfs.keytab"");


 PrincipalName hdfsPrincipalName = KerberosUtils.getPrincipalFromKeytab(hdfsKeytabPath.toFile());
 UserGroupInformation.setConfiguration(conf);
 UserGroupInformation.loginUserFromKeytab(hdfsPrincipalName.getName(), hdfsKeytabPath.toAbsolutePath().toString());

fileSystem = FileSystem.get(conf);

 

Exception:

java.lang.IllegalArgumentException: Server has invalid Kerberos principal: nn/ex1.example.com@EXAMPLE.COM, expecting: nn/ex1.example.com.@EXAMPLE.COM
at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:337)
at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:234)
at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:160)
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:627)
at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:421)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:814)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:810)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:810)
... 73 common frames omitted

 "
"s3a getFileStatus(""/"") to skip IO",13336785,Resolved,Minor,Fixed,22/Oct/20 18:24,24/Nov/20 11:25,3.3.0,"Calls to ""hadoop fs ls s3a://something/"" add a full getFileStatus() sequence (HEAD, LIST, LIST) because FsShell Ls calls getFileStatus on every input. 

We should just build a root status entry immediately. There's one consequence: if a user has disabled the s3a existence probe, there will be no checks for the bucket existence. But that can be viewed as a ""well, that's what happens"" behaviour"
Generally support PEM keystores,13344923,Open,Minor,,08/Dec/20 20:44,,,"Several Apache projects have added some minimal PEM support.  Usually this is presented as part of supporting generated certificates for a trust store.  By adding a PEM Java KeyStore to Hadoop common, we could provide this support for many clients.

Example:
{code:java}
<property>
  <name>ssl.server.truststore.type</name>
  <value>PEM</value>
</property>{code}"
S3A committer to support concurrent jobs with same app attempt ID & dest dir,13336304,Resolved,Minor,Fixed,20/Oct/20 14:10,26/Nov/20 17:29,3.3.0,"Reported failure of magic committer block uploads as pending upload ID is unknown. Likely cause: it's been aborted by another job

# Make it possible to turn off cleanup of pending uploads in magic committer
# log more about uploads being deleted in committers
# and upload ID in the S3aBlockOutputStream errors

There are other concurrency issues when you look close, see SPARK-33230

* magic committer uses app attempt ID as path under __magic; if there are duplicate then they will conflict
* staging committer local temp dir uses app attempt id

Fix will be to have a job UUID which for spark will be picked up from the SPARK-33230 changes, (option to self-generate in job setup for hadoop 3.3.1+ older spark builds); fall back to app-attempt *unless that fallback has been disabled*

MR: configure to use app attempt ID
Spark: configure to fail job setup if app attempt ID is the source of a job uuid"
Add Flink to CallerContext LimitedPrivate scope,13342534,Resolved,Minor,Not A Problem,25/Nov/20 06:35,25/Nov/20 06:45,3.3.0,"A lots of Flink applications run on Hadoop. Spark will invoke Hadoop caller context APIs to set up its caller contexts in HDFS/Yarn, so Hadoop should add Spark as one of the users in the LimitedPrivate scope."
Improve MiniDFSCluster by a adding a default constructor,13341254,Resolved,Minor,Fixed,18/Nov/20 08:27,18/Nov/20 08:28,,
hadoop component start with failed to init Ganglia31,13338836,Open,Minor,,04/Nov/20 14:02,,,"When start namenode, datanode or releated components, there exists an Warning :

Error creating sink 'ganglia'
 org.apache.hadoop.metrics2.impl.MetricsConfigException: Error creating plugin: org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31

if there exists configuration, like : *.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40。

 

Maybe, this is a bug about common-configuration2.  

 

The default conversion handler used DisabledListDelimiterHandler.INSTANCE as EXTRACTOR, while the comma list delimiter handler is expected."
CosNInputStream should throw exception when filesystem is closed,13337583,Open,Minor,,28/Oct/20 06:37,,3.3.0,"Thread hangs when other thread call filesystem.close(). log as below

971  [main] INFO  org.apache.hadoop.fs.cosn.CosNFileSystem  - Open the file: [cosn://bigdata-test

1075 [main] ERROR org.apache.hadoop.util.BlockingThreadPoolExecutorService  - Could not submit task to executor java.util.concurrent.ThreadPoolExecutor

the reason of thread hangs is :
 # filesystem is cached when multiple thread using FileSystem#get method for creating instance
 # CosNInputStream#read method is not check CosNFileSystem is closed
 # CosNInputStream is using multiple thread(CosNFileReadTask) for read and main thread will await when readbuffer status is init
 # CosNFileSystem#close method will shutdown boundedIOThreadPool . so CosNInputStream#CosNFileReadTask will not notify main thread and cause thread hangs

 

 "
Backslash in username causes build failure in the environment started by start-build-env.sh.,13333674,Resolved,Minor,Fixed,05/Oct/20 09:03,20/Oct/20 01:12,,"If a username includes a backslash, `mvn clean install` fails in an environment started by start-build-env.sh.

Here is my result in Amazon WorkSpaces.

 
{code:java}
CORPbtkuramototkr@b8e750b1e386:/home/CORP\btkuramototkr/hadoop/hadoop-build-to
ols$ mvn clean install
/usr/bin/mvn: 1: cd: can't cd to /home/CORtkuramototkr/hadoop/hadoop-build-tools/..
[INFO] Scanning for projects...
[INFO] 
[INFO] ----------------< org.apache.hadoop:hadoop-build-tools >----------------
[INFO] Building Apache Hadoop Build Tools 3.4.0-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hadoop-build-tools ---
[INFO] 
[INFO] --- maven-resources-plugin:3.0.1:copy-resources (copy-resources) @ hadoop-build-tools ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1.074 s
[INFO] Finished at: 2020-10-05T02:51:53Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-resources-plugin:3.0.1:copy-resources (copy-resources) on project hadoop-build-tools: Cannot create resource output directory: /home/CORP/btkuramototkr/hadoop/hadoop-build-tools/target/extra-resources -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}
 

This problem can be solved by adding an option to change the path to maven's local repository in the container so that users can remove backslashes from their username.

 "
Fix typos existance to existence,13330650,Resolved,Trivial,Fixed,03/Oct/20 03:09,06/Oct/20 01:12,3.4.0,existance -> existence
Improve BUILDING.TXT file to clarify instructions for new users,13345598,Resolved,Trivial,Won't Fix,11/Dec/20 21:12,13/May/21 12:08,2.10.1,"In the BUILDING.TXT file on the apache/hadoop git repo, the instructions for Linux/macOS users needs an addition to show how to set a shell script as executable using sudo and then running the script. This should help avoid potential confusion for users who are unfamiliar with using executable shell scripts in the Linux/macOS terminal."
S3A marker tool mixes up -min and -max,13337489,Resolved,Trivial,Fixed,27/Oct/20 18:12,03/Dec/20 11:47,3.3.1,"HADOOP-17227 manages to get -min and -max mixed up through the call chain,. 

{code}
hadoop s3guard markers -audit -max 2000  s3a://stevel-london/
{code}
leads to
{code}
2020-10-27 18:11:44,434 [main] DEBUG s3guard.S3GuardTool (S3GuardTool.java:main(2154)) - Exception raised
46: Marker count 0 out of range [2000 - 0]
	at org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanResult.finish(MarkerTool.java:489)
	at org.apache.hadoop.fs.s3a.tools.MarkerTool.run(MarkerTool.java:318)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:505)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:2134)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.main(S3GuardTool.java:2146)
2020-10-27 18:11:44,436 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 46: 46: Marker count 0 out of range [2000 - 0]
{code}

Trivial fix."
S3A: IAMCredentialsProvider throttling results in AWS auth failures,13579827,Open,Blocker,,20/May/24 10:01,,3.4.0,"Tests report throttling errors in IAM being remapped to noauth and failure

Again, impala tests, but with multiple processes on same host. this means that HADOOP-18945 isn't sufficient as even if it ensures a singleton instance for a process
* it doesn't if there are many test buckets (fixable)
* it doesn't work across processes (not fixable)

we may be able to 
* use a singleton across all filesystem instances
* once we know how throttling is reported, handle it through retries + error/stats collection


"
EC: Fix calculation errors caused by special index order,13578943,Resolved,Critical,Fixed,11/May/24 10:16,19/Aug/24 04:47,,"I found that if the erasedIndexes distribution is such that the parity index is in front of the data index, ec will produce wrong results when decoding.

In fact, HDFS-15186 has described this problem, but does not fundamentally solve it.

The reason is that the code assumes that erasedIndexes is preceded by the data index and followed by parity index. If there is a parity index placed in front of the data index, a calculation error will occur."
TestStagingCommitter.testJobCommitFailure failing ,13580170,Resolved,Critical,Fixed,22/May/24 21:49,28/May/24 17:37,,"{code:java}
[INFO] 
[ERROR] Failures: 
[ERROR]   TestStagingCommitter.testJobCommitFailure:662 [Committed objects compared to deleted paths org.apache.hadoop.fs.s3a.commit.staging.StagingTestBase$ClientResults@1b4ab85{ requests=12, uploads=12, parts=12, tagsByUpload=12, commits=5, aborts=7, deletes=0}] 
Expecting:
  <[""s3a://bucket-name/output/path/r_0_0_0e1f4790-4d3f-4abb-ba98-2b39ec8b7566"",
    ""s3a://bucket-name/output/path/r_0_0_92306fea-0219-4ba5-a2b6-091d95547c11"",
    ""s3a://bucket-name/output/path/r_1_1_016c4a25-a1f7-4e01-918e-e24a32c7525f"",
    ""s3a://bucket-name/output/path/r_0_0_b2698dab-5870-4bdb-98ab-0ef5832eca45"",
    ""s3a://bucket-name/output/path/r_1_1_600b7e65-a7ff-4d07-b763-c4339a9164ad""]>
to contain exactly in any order:
  <[]>
but the following elements were unexpected:
  <[""s3a://bucket-name/output/path/r_0_0_0e1f4790-4d3f-4abb-ba98-2b39ec8b7566"",
    ""s3a://bucket-name/output/path/r_0_0_92306fea-0219-4ba5-a2b6-091d95547c11"",
    ""s3a://bucket-name/output/path/r_1_1_016c4a25-a1f7-4e01-918e-e24a32c7525f"",
    ""s3a://bucket-name/output/path/r_0_0_b2698dab-5870-4bdb-98ab-0ef5832eca45"",{code}"
[JDK23] org.apache.hadoop.security.UserGroupInformation use of Subject needs to move to replacement APIs,13584145,Open,Major,,28/Jun/24 08:02,,3.5.0,"`javax.security.auth.Subject.getSubject` and `Subject.doAs` were deprecated for removal in JDK 17. The replacement APIs are `Subject.current` and `callAs`. See [JEP 411]([https://openjdk.org/jeps/411]) for background.

The `Subject.getSubject` API has been ""degraded"" in JDK 23 to throw `UnsupportedOperationException` if not running with the option to allow a SecurityManager. In a future JDK release, the `Subject.getSubject` API will be degraded further to throw`UnsupportedOperationException` unconditionally.

[renaissance/issues/439]([https://github.com/renaissance-benchmarks/renaissance/issues/439]) is a failure with a Spark benchmark due to the code in `org.apache.hadoop.security.UserGroupInformation` using the deprecated `Subject.getSubject` method. The maintainers of this code need to migrate this code to the replacement APIs to ensure that this code will continue to work once the security manager feature is removed."
ABFS: Support FNS Accounts over BlobEndpoint,13579416,Open,Major,,15/May/24 19:44,,3.4.0,"As a pre-requisite to deprecating WASB Driver, ABFS Driver will need to match FNS account support as intended by WASB driver. This will provide an official migrating means for customers still using the legacy driver to ABFS Driver. 

 

Parent Jira for WASB deprecation: [HADOOP-19178] WASB Driver Deprecation and eventual removal - ASF JIRA (apache.org)"
Do not hard code security providers.,13576102,Resolved,Major,Fixed,16/Apr/24 19:34,14/May/24 18:28,,"In order to support different security providers in different clusters, we should not hard code a provider in our code."
Improve ABFS metric integration with iOStatistics,13580278,Open,Major,,23/May/24 14:13,,,"Followup to HADOOP-18325 covering the outstanding comments of

https://github.com/apache/hadoop/pull/6314/files

"
S3A prefetching to support Vector IO,13574667,Resolved,Major,Won't Fix,04/Apr/24 16:42,20/Feb/25 17:23,3.4.0,"Add explicit support for vector IO in s3a prefetching stream.

* if a range is in 1+ cached block, it SHALL be read from cache and returned
* if a range is not in cache : TBD
* If a range is partially in cache: TBD

these are the same decisions that abfs has to make: should the client fetch/cache block or just do one or more GET requests

A big issue is: does caching of data fetched in a range request make any sense at all? Or more specifically: does fetching the blocks in which range requests are found make sense

Simply going to the store is a lot simpler"
ABFS: [FnsOverBlob] Response Handling of Blob Endpoint APIs and Metadata APIs,13583033,Resolved,Major,Fixed,18/Jun/24 10:28,26/Dec/24 04:57,3.4.0,"Blob Endpoint APIs has a different format for response than DFS Endpoint APIs.
There are some behavioral differences as well that need to be handled at client side."
Upgrade aws sdk v2 to 2.25.53,13581806,Resolved,Major,Fixed,06/Jun/24 14:12,08/Jul/24 09:26,3.4.1,Upgrade aws sdk v2 to 2.25.53
ABFS: [FnsOverBlob] Making AbfsClient Abstract for supporting both DFS and Blob Endpoint,13580508,Resolved,Major,Fixed,27/May/24 05:59,20/Aug/24 17:07,3.4.0,"Azure Services support two different set of APIs.
Blob: [https://learn.microsoft.com/en-us/rest/api/storageservices/blob-service-rest-api] 
DFS: [https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/operation-groups] 

As per the plan in HADOOP-19179, this task enables ABFS Driver to work with both set of APIs as per the requirement.

Scope of this task is to refactor the ABfsClient so that ABFSStore can choose to interact with the client it wants based on the endpoint configured by user.

The blob endpoint support will remain ""Unsupported"" until the whole code is checked-in and well tested."
Upgrade commons-cli to 1.6.0.,13574574,In Progress,Major,,04/Apr/24 08:55,,3.4.1,"commons-cli can be upgraded to 1.6.0, I will try to upgrade."
Include FileStatus when opening a file from FileSystem,13582038,Resolved,Major,Duplicate,08/Jun/24 11:07,10/Jun/24 14:07,3.4.0,"The FileSystem abstract class prevents that if you have information about the FileStatus of a file, you use it to open that file, which means that in the implementations of the open method, they have to request the FileStatus of the same file again, making unnecessary requests.

A very clear example is seen in today's latest version of the parquet-hadoop implementation, where:

https://github.com/apache/parquet-java/blob/apache-parquet-1.14.0/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopInputFile.java

Although to create the implementation you had to consult the file to know its FileStatus, when opening it only the path is included, since the FileSystem implementation is the only thing it allows you to do. This implies that the implementation will surely, in its open function, verify that the file exists or what information the file has and perform the same operation again to collect the FileStatus.

 

This would simply be resolved by taking the latest current version:

 

[https://github.com/apache/hadoop/blob/release-3.4.0-RC3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java]

and including the following:

 

  public FSDataInputStream open(FileStatus f) throws IOException {
        return this.open(f.getPath(), this.getConf().getInt(""io.file.buffer.size"", 4096));
    }

 

This would imply that it is backward compatible with all current Filesystems, but since it is in the implementation it could be used when this information is already known.

 

 

 

 "
S3A: Support AWS KMS Encryption Context,13581854,Resolved,Major,Fixed,06/Jun/24 23:33,06/Dec/24 15:12,3.4.0,"S3A properties allow users to choose the AWS KMS key ({_}fs.s3a.encryption.key{_}) and S3 encryption algorithm to be used (f{_}s.s3a.encryption.algorithm{_}). In addition to the AWS KMS Key, an encryption context can be used as non-secret data that adds additional integrity and authenticity to check the encrypted data. However, there is no option to specify the [AWS KMS Encryption Context|https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context] in S3A.

In AWS SDK v2 the encryption context in S3 requests is set by the parameter [ssekmsEncryptionContext.|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/CreateMultipartUploadRequest.Builder.html#ssekmsEncryptionContext(java.lang.String)] It receives a base64-encoded UTF-8 string holding JSON with the encryption context key-value pairs. The value of this parameter could be set by the user in a new property {_}*fs.s3a.encryption.context*{_}, and be stored in the [EncryptionSecrets|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java] to later be used when setting the encryption parameters in [RequestFactoryImpl|https://github.com/apache/hadoop/blob/f92a8ab8ae54f11946412904973eb60404dee7ff/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java]."
S3A: handle alternative forms of connection failure,13579090,Open,Major,,13/May/24 13:15,,3.3.6,"We've had reports of network connection failures surfacing deeper in the stack where we don't convert to AWSApiCallTimeoutException so they aren't retried properly (retire connection and repeat)


{code}
Unable to execute HTTP request: Broken pipe (Write failed)
{code}


{code}
 Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout
{code}

note, this is v1 sdk but the 400 error is treated as fail-fast in all our versions and I don't think we do the same for the broken pipe. that one is going to be trickier to handle as unless that is coming from the http/tls libraries ""broken pipe"" may not be in the newer builds. We'd have to look for the string in the SDKs to see what causes it and go from there

"
S3A Xattr headers need hdfs-compatible prefix,13579368,Open,Major,,15/May/24 13:02,,3.3.6,"x3a xattr list needs a prefix compatible with hdfs or existing code which tries to copy attributes between stores can break

we need a prefix of {user/trusted/security/system/raw}.

now, problem: currently xattrs are used by the magic committer to propagate file size progress; renaming the prefix will break existing code. But as it's read only we could modify spark to look for both old and new values.

{code}

org.apache.hadoop.HadoopIllegalArgumentException: An XAttr name must be prefixed with user/trusted/security/system/raw, followed by a '.'
	at org.apache.hadoop.hdfs.XAttrHelper.buildXAttr(XAttrHelper.java:77) 
	at org.apache.hadoop.hdfs.DFSClient.setXAttr(DFSClient.java:2835) 
	at org.apache.hadoop.hdfs.DistributedFileSystem$59.doCall(DistributedFileSystem.java:3106) 
	at org.apache.hadoop.hdfs.DistributedFileSystem$59.doCall(DistributedFileSystem.java:3102) 
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.setXAttr(DistributedFileSystem.java:3115) 
	at org.apache.hadoop.fs.FileSystem.setXAttr(FileSystem.java:3097)

{code}
"
Test ITestAbfsRestOperationException#testAuthFailException is broken. ,13576073,Resolved,Major,Fixed,16/Apr/24 17:47,29/Apr/24 16:52,,"{code:java}
intercept(Exception.class,
        () -> {
          fs.getFileStatus(new Path(""/""));
        }); {code}
Intercept shouldn't be used as there are assertions in catch statements. 

 

CC [~stevel@apache.org]  [~anujmodi2021] [~asrani_anmol] "
WASB Driver Deprecation and eventual removal,13579414,Resolved,Major,Fixed,15/May/24 19:37,07/Jun/24 13:31,3.4.0,"*WASB Driver*

WASB driver was developed to support FNS (FlatNameSpace) Azure Storage accounts. FNS accounts do not honor File-Folder syntax. HDFS Folder operations hence are mimicked at client side by WASB driver and certain folder operations like Rename and Delete can lead to lot of IOPs with client-side enumeration and orchestration of rename/delete operation blob by blob. It was not ideal for other APIs too as initial checks for path is a file or folder needs to be done over multiple metadata calls. These led to a degraded performance.

To provide better service to Analytics customers, Microsoft released ADLS Gen2 which are HNS (Hierarchical Namespace) , i.e File-Folder aware store. ABFS driver was designed to overcome the inherent deficiencies of WASB and customers were informed to migrate to ABFS driver.

*Customers who still use the legacy WASB driver and the challenges they face* 

Some of our customers have not migrated to the ABFS driver yet and continue to use the legacy WASB driver with FNS accounts.  

These customers face the following challenges: 
 * They cannot leverage the optimizations and benefits of the ABFS driver.
 * They need to deal with the compatibility issues should the files and folders were modified with the legacy WASB driver and the ABFS driver concurrently in a phased transition situation.
 * There are differences for supported features for FNS and HNS over ABFS Driver
 * In certain cases, they must perform a significant amount of re-work on their workloads to migrate to the ABFS driver, which is available only on HNS enabled accounts in a fully tested and supported scenario.

*Deprecation plans for WASB*

We are introducing a new feature that will enable the ABFS driver to support FNS accounts (over BlobEndpoint) using the ABFS scheme. This feature will enable customers to use the ABFS driver to interact with data stored in GPv2 (General Purpose v2) storage accounts. 

With this feature, the customers who still use the legacy WASB driver will be able to migrate to the ABFS driver without much re-work on their workloads. They will however need to change the URIs from the WASB scheme to the ABFS scheme. 

Once ABFS driver has built FNS support capability to migrate WASB customers, WASB driver will be declared deprecated in OSS documentation and marked for removal in next major release. This will remove any ambiguity for new customer onboards as there will be only one Microsoft driver for Azure Storage and migrating customers will get SLA bound support for driver and service, which was not guaranteed over WASB.

 We anticipate that this feature will serve as a stepping stone for customers to move to HNS enabled accounts with the ABFS driver, which is our recommended stack for big data analytics on ADLS Gen2. 

*Any Impact for* *existing customers who are using ADLS Gen2 (HNS enabled account) with ABFS driver* *?*

This feature does not impact the existing customers who are using ADLS Gen2 (HNS enabled account) with ABFS driver.

They do not need to make any changes to their workloads or configurations. They will still enjoy the benefits of HNS, such as atomic operations, fine-grained access control, scalability, and performance. 

*Official recommendation*

Microsoft continues to recommend all Big Data and Analytics customers to use Azure Data Lake Gen2 (ADLS Gen2) using the ABFS driver and will continue to optimize this scenario in future, we believe that this new option will help all those customers to transition to a supported scenario immediately, while they plan to ultimately move to ADLS Gen2 (HNS enabled account).

 *New Authentication options that a WASB to ABFS Driver migrating customer will get*

Below auth types that WASB provides will continue to work on the new FNS over ABFS Driver over configuration that accepts these SAS types (similar to WASB)
 * SharedKey
 * Account SAS
 * Service/Container SAS

Below authentication types that were not supported by WASB driver but supported by ABFS driver will continue to be available for new FNS over ABFS Driver
 * OAuth 2.0 Client Credentials
 * OAuth 2.0: Refresh Token
 * Azure Managed Identity
 * Custom OAuth 2.0 Token Provider

ABFS Driver SAS Token Provider plugin present today for UserDelegation SAS and Directly SAS will continue to work only for HNS accounts."
ABFS: Fixing logic to determine HNS nature of account to avoid extra getAcl() calls,13583227,Resolved,Major,Fixed,19/Jun/24 16:56,17/Jul/24 15:07,3.4.0,"ABFS driver needs to know the type of account being used. It relies on the user to inform the account type using the config `fs.azure.account.hns.enabled`.
If not configured, driver makes a getAcl call to determine the account type.

Expectation is getAcl() will fail with 400 Bad Request if made on the FNS Account.
For any other case including 200, 404 it will indicate account is HNS.

Today, when determining this, the logic only checks status code to be either 200 or 400. In case of 404, nothing is inferred, and this leads to more getAcl again and again until 200 or 400 comes.

Fix is to update the logic such that if getAcl() fails with 400, it is FNS account. For all other cases it will be an HNS account. In case of throttling, if all retries are exhausted, FS init itself will fail.

This is also to fix a test case failing on trunk. {{testGetAclCallOnHnsConfigAbsence(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemInitAndCreate)}}"
Support configurable SASL mechanism,13576089,Resolved,Major,Fixed,16/Apr/24 18:23,29/Apr/24 17:03,,"Currently, the SASL mechanism is hard coded to DIGEST-MD5.  As mentioned in HADOOP-14811, DIGEST-MD5 is known to be insecure; see [rfc6331|https://datatracker.ietf.org/doc/html/rfc6331].

In this JIRA, we will make the SASL mechanism configurable.  The default mechanism will still be DIGEST-MD5 in order to maintain compatibility."
upgrade bouncy castle to 1.78.1 due to CVEs,13576484,Open,Major,,19/Apr/24 09:10,,3.3.6,"[https://www.bouncycastle.org/releasenotes.html#r1rv78]

There is a v1.78.1 release but no notes for it yet.

For v1.78
h3. 2.1.5 Security Advisories.

Release 1.78 deals with the following CVEs:
 * CVE-2024-29857 - Importing an EC certificate with specially crafted F2m parameters can cause high CPU usage during parameter evaluation.
 * CVE-2024-30171 - Possible timing based leakage in RSA based handshakes due to exception processing eliminated.
 * CVE-2024-30172 - Crafted signature and public key can be used to trigger an infinite loop in the Ed25519 verification code.
 * CVE-2024-301XX - When endpoint identification is enabled and an SSL socket is not created with an explicit hostname (as happens with HttpsURLConnection), hostname verification could be performed against a DNS-resolved IP address. This has been fixed."
Explore dropping protobuf 2.5.0 from the distro,13578476,Resolved,Major,Fixed,07/May/24 17:32,24/Sep/24 15:29,,"explore if protobuf-2.5.0 can be dropped from distro, it is a transitive dependency from HBase, but HBase doesn't use it in the code.

Check if it is the only one pulling it into the distro & will something break if we exclude that, if none lets get rid of it"
Hadoop CLI MiniCluster is broken,13578108,Resolved,Major,Fixed,03/May/24 20:52,21/Sep/24 15:57,,"Documentation is also broken & it doesn't work either

(https://apache.github.io/hadoop/hadoop-project-dist/hadoop-common/CLIMiniCluster.html)

*Fails with:*
{noformat}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/mockito/stubbing/Answer
	at org.apache.hadoop.hdfs.MiniDFSCluster.isNameNodeUp(MiniDFSCluster.java:2666)
	at org.apache.hadoop.hdfs.MiniDFSCluster.isClusterUp(MiniDFSCluster.java:2680)
	at org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1510)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:989)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:588)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:530)
	at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.start(MiniHadoopClusterManager.java:160)
	at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.run(MiniHadoopClusterManager.java:132)
	at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.main(MiniHadoopClusterManager.java:320)
Caused by: java.lang.ClassNotFoundException: org.mockito.stubbing.Answer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 9 more{noformat}
{*}Command executed:{*}
{noformat}
bin/mapred minicluster -format{noformat}
*Documentation Issues:*
{noformat}
bin/mapred minicluster -rmport RM_PORT -jhsport JHS_PORT{noformat}

Without -format option it doesn't work the first time telling Namenode isn't formatted, So, this should be corrected.


{noformat}
2024-05-04 00:35:52,933 WARN namenode.FSNamesystem: Encountered exception loading fsimage
java.io.IOException: NameNode is not formatted.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:253)
{noformat}

This isn't required either:

{noformat}
NOTE: You will need protoc 2.5.0 installed.
{noformat}
"
Update and optimize hadoop-runner,13583862,Resolved,Major,Implemented,26/Jun/24 06:42,16/Sep/24 06:42,,{{hadoop-runner:latest}} is 5 years old and could use some improvements.
Update solr from 8.11.2 to 8.11.3 to address CVE-2023-50298,13575957,Open,Major,,16/Apr/24 05:36,,,Update solr from 8.11.2 to 8.11.3 to address CVE-2023-50298
S3A: Support external id in assume role,13582050,Resolved,Major,Fixed,08/Jun/24 13:53,10/Sep/24 16:15,3.4.0,"Extend IAM role suport with external IDs which can be set in fs.s3a.assumed.role.external.id
Support external id in AssumedRoleCredentialProvider.

 

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html"
Upgrade commons-io to 2.16.1,13574204,Resolved,Major,Fixed,02/Apr/24 00:53,16/Aug/24 18:44,3.4.1,"commons-io can be upgraded from 2.14.0 to 2.16.0, try to upgrade."
Upgrade kafka to 3.4.0,13580056,Resolved,Major,Duplicate,22/May/24 06:28,23/May/24 14:37,3.4.0,Upgrade kafka to 3.4.0 to resolve CVE-2023-25194
hadoop-common still exports logback as a transitive dependency,13576227,Resolved,Major,Fixed,17/Apr/24 15:07,16/Aug/24 12:55,3.4.0,"Even though HADOOP-19084 set out to stop it, somehow ZK's declaration of a logback dependency is still contaminating the hadoop-common dependency graph, so causing problems downstream.

"
S3A initialization/close slower than with v1 SDK,13582717,Resolved,Major,Fixed,14/Jun/24 16:23,05/Jul/24 15:42,3.4.0,"Hive QE have observed slowdown in LLAP queries due to time to create and close s3a filesystems instances. A key aspect of that is they keep closing the fs instances (HIVE-27884), but looking at the profiles, the reason things seem to have regressed is

* two s3 clients are being created (sync and async)
* these seem to take a lot of time scanning the classpath for ""global interceptors"", which is at least an O(jars) operation; #of index entries in the zip files may factor too.

Proposed:
* create async client on demand when the transfer manager is invoked
* look at why passwords are being scanned for if InstanceProfileCredentialsProvider is in use...that seems slow too

SDK wishes
* SDK maybe allow us to turn off that scan for interceptors?

attaching screenshots of the profile. storediag snippet:
{code}

[001]  fs.s3a.access.key = (unset)
[002]  fs.s3a.secret.key = (unset)
[003]  fs.s3a.session.token = (unset)
[004]  fs.s3a.server-side-encryption-algorithm = (unset)
[005]  fs.s3a.server-side-encryption.key = (unset)
[006]  fs.s3a.encryption.algorithm = (unset)
[007]  fs.s3a.encryption.key = (unset)
[008]  fs.s3a.aws.credentials.provider = ""com.amazonaws.auth.InstanceProfileCredentialsProvider"" [core-site.xml]

{code}
"
"S3A: option ""fs.s3a.performance.flags"" to take list of performance flags",13577979,Resolved,Major,Fixed,02/May/24 17:33,29/Jul/24 15:06,3.4.1,"HADOOP-19072 shows we want to add more optimisations than that of HADOOP-18930.

* Extending the new optimisations to the existing option is brittle
* Adding explicit options for each feature gets complext fast.

Proposed
* A new class S3APerformanceFlags keeps all the flags
* it build this from a string[] of values, which can be extracted from getConf(),
* and it can also support a ""*"" option to mean ""everything""
* this class can also be handed off to hasPathCapability() and do the right thing.

Proposed optimisations
* create file (we will hook up HADOOP-18930)
* mkdir (HADOOP-19072)
* delete (probe for parent path)
* rename (probe for source path)

We could think of more, with different names, later.
The goal is make it possible to strip out every HTTP request we do for safety/posix compliance, so applications have the option of turning off what they don't need."
AliyunOSS: Support vectored read API,13584011,In Progress,Major,,27/Jun/24 06:16,,3.2.4,
[ABFS]: No GetPathStatus call for opening AbfsInputStream,13574450,Open,Major,,03/Apr/24 12:42,,,Read API gives contentLen and etag of the path. This information would be used in future calls on that inputStream. Prior information of eTag is of not much importance.
s3a: TestS3AAWSCredentialsProvider and TestS3AInputStreamRetry really slow,13583949,Resolved,Major,Fixed,26/Jun/24 18:24,02/Jul/24 10:36,3.4.0,"Not noticed this before, but the unit tests TestS3AAWSCredentialsProvider and TestS3AInputStreamRetry are so slow they will be hurting over all test performance times: no integration tests will start until these are all complete.


{code}

mvn test -T 1C -Dparallel-tests

...
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.877 s - in org.apache.hadoop.fs.s3a.TestS3AInputStreamRetry
...
[INFO] Tests run: 25, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 90.038 s - in org.apache.hadoop.fs.s3a.TestS3AAWSCredentialsProvider
{code}

The PR cuts total execution time of a 10 thread test run from 3 minutes to 2:30"
Add test to find unshaded dependencies in the aws sdk,13581779,Resolved,Major,Fixed,06/Jun/24 10:46,24/Jun/24 11:24,3.4.0,Write a test to assess the aws sdk for unshaded artefacts on the class path which might cause deployment failures. 
VectorIO regression: empty ranges are now rejected,13582448,Resolved,Major,Fixed,12/Jun/24 18:33,21/Jun/24 14:06,3.4.1,"The validation now rejects a readvectored with an empty range, whereas before it was a no-op

Proposed fix, return the empty list; add test

"
Hadoop release contains a 530MB bundle-2.23.19.jar,13582872,Resolved,Major,Duplicate,17/Jun/24 11:24,21/Jun/24 02:25,,"The size of Hadoop binary release (v3.4.0) is 1.7 GB.
{code:java}
hadoop-3.4.0$du -h -d 1
$du -h -d 1 .
2.0M	./bin
260K	./libexec
 72K	./include
212K	./sbin
184K	./etc
232K	./licenses-binary
316M	./lib
1.4G	./share
1.7G	.
{code}
A large component is bundle-2.23.19.jar, which is [AWS Java SDK :: Bundle|https://mvnrepository.com/artifact/software.amazon.awssdk/bundle/2.23.19]
{code:java}
hadoop-3.4.0$ls -lh share/hadoop/tools/lib/bundle-2.23.19.jar  
-rw-r--r--@ 1 szetszwo  staff   530M Mar  4 15:41 share/hadoop/tools/lib/bundle-2.23.19.jar
{code}
We should revisit if such a large jar is really needed to be included in the release."
WrappedIO BulkDelete API to raise IOEs as UncheckedIOExceptions,13582397,Resolved,Major,Fixed,12/Jun/24 10:29,20/Jun/24 10:26,3.4.1,"
It's easier to invoke methods through reflection through parquet/iceberg DynMethods if the invoked method raises unchecked exceptions, because it doesn't then rewrape the raised exception in a generic RuntimeException

Catching the IOEs and wrapping as UncheckedIOEs makes it much easier to unwrap IOEs after the invocation"
[ABFS]Prevent ABFS initialization for non-hierarchical-namespace account if Customer-provided-key configs given.,13574261,Resolved,Major,Fixed,02/Apr/24 10:00,11/Jun/24 18:16,3.4.0,"Store doesn't flow in the namespace information to the client. 

In https://github.com/apache/hadoop/pull/6221, getIsNamespaceEnabled is added in client methods which checks if namespace information is there or not, and if not there, it will make getAcl call and set the field. Once the field is set, it would be used in future getIsNamespaceEnabled method calls for a given AbfsClient.

Since, CPK both global and encryptionContext are only for hns account, the fix that is proposed is that we would fail fs init if its non-hns account and cpk config is given."
Upgrade protobuf version to 3.25.3,13578028,Resolved,Major,Fixed,03/May/24 06:07,21/May/24 19:15,,
Reduce the number of headObject when opening a file with the s3 file system,13582042,Resolved,Major,Duplicate,08/Jun/24 12:20,10/Jun/24 13:32,3.3.6,"In the implementation of the S3 filesystem, of the hadoop aws package, if you use it with spark, every time you open a file for anything you will have to send two Head Objects, since to open the file, you will first look to see if this file exists, executing a HeadObject, and then when opening it, the implementation, both of sdk1 and sdk2, forces you to make a head object again. This is not the fault of the implementation of this class (S3AFileSystem), but of the abstract FileSystem class of the Hadoop core, since it does not allow the FileStatus to be passed but only allows the use of Path.

If the FileSystem implementation is changed, it could be used to not have to request that HeadObject again."
SecretManager should not hardcode HMAC algorithm,13581965,Open,Major,,07/Jun/24 14:34,,,"{code}
//SecretManager.java
private static final String DEFAULT_HMAC_ALGORITHM = ""HmacSHA1"";
{code}
See https://github.com/apache/hadoop/blob/10df59e4210206508da648d5676f1c7d423b0353/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java#L114"
Create orphan commit for website deployment,13581629,Resolved,Major,Fixed,05/Jun/24 09:33,05/Jun/24 14:27,,
Change loglevel to ERROR/WARNING so that it would easy to identify the problem without ignoring,13580355,Open,Major,,24/May/24 10:58,,,"On the new Host with Java version 11, the DN was not able to communicate with the NN. We enabled DEBUG logging for the DN and the below message was logged under DEBUG level.

DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:hdfs/av3l704p.bigdata.it.internal@PRODUCTION.LOCAL (auth:KERBEROS) cause:javax.security.sasl.SaslExcept
ion: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Receive timed out)]

Without a DEBUG level logging, this was shown up as a WARNING as below

WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for hdfs/av3l704p.bigdata.it.internal@PRODUCTION.LOCAL to avl2785p.bigdata.it.internal/172.24.178.32:8022
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Receive timed out)]

A considerable amount of time was spent troubleshooting this issue as this exception was moved to a DEBUG level which was difficult to track in the logs.

Can we have such critical WARNINGs shown up at the WARN/ERROR level so that it's not missed when we enable DEBUG level logging for datanodes?"
Batch APIs for delete,13581486,Resolved,Major,Duplicate,04/Jun/24 07:50,04/Jun/24 13:49,,"Add batch APIs with for delete to allow better performance for object stores:

{{boolean[] delete(Path[] paths);}}

The API should have a default implementation that delegates to the singular delete. Implementations can override to provide better performance."
ZooKeeper based state stores use different ZK address configs,13576891,Resolved,Major,Fixed,23/Apr/24 10:02,29/May/24 12:46,,"Currently, the Zookeeper-based state stores of RM, YARN Federation, and HDFS Federation use the same ZK address config `{{{}hadoop.zk.address`{}}}. But in our production environment, we hope that different services can use different ZKs to avoid mutual influence.

This jira adds separate ZK address configs for each service."
Upgrade Kafka Clients due to CVEs,13578868,Resolved,Major,Duplicate,10/May/24 11:32,23/May/24 14:38,,"Upgrade Kafka Clients due to CVEs

CVE-2023-25194:- Affected versions of this package are vulnerable to Deserialization of Untrusted Data when there are gadgets in the {{{}classpath{}}}. The server will connect to the attacker's LDAP server and deserialize the LDAP response, which the attacker can use to execute java deserialization gadget chains on the Kafka connect server.
CVSS Score:- 8.8(High)
[https://nvd.nist.gov/vuln/detail/CVE-2023-25194] 

CVE-2021-38153

CVE-2018-17196

Insufficient Entropy

[https://security.snyk.io/package/maven/org.apache.kafka:kafka-clients] 

Upgrade Kafka-Clients to 3.4.0 or higher."
Upgrade org.apache.derby:derby to 10.17.1.0,13579102,In Progress,Major,,13/May/24 15:01,,3.4.1,Upgrade org.apache.derby:derby to 10.17.1.0.
TestS3ACachingBlockManager fails intermittently in Yetus,13579399,Open,Major,,15/May/24 16:54,,3.4.0,"{code:java}
[ERROR] org.apache.hadoop.fs.s3a.prefetch.TestS3ACachingBlockManager.testCachingOfGet -- Time elapsed: 60.45 s <<< ERROR!
java.lang.IllegalStateException: waitForCaching: expected: 1, actual: 0, read errors: 0, caching errors: 1
	at org.apache.hadoop.fs.s3a.prefetch.TestS3ACachingBlockManager.waitForCaching(TestS3ACachingBlockManager.java:465)
	at org.apache.hadoop.fs.s3a.prefetch.TestS3ACachingBlockManager.testCachingOfGetHelper(TestS3ACachingBlockManager.java:435)
	at org.apache.hadoop.fs.s3a.prefetch.TestS3ACachingBlockManager.testCachingOfGet(TestS3ACachingBlockManager.java:398)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR] org.apache.hadoop.fs.s3a.prefetch.TestS3ACachingBlockManager.testCachingFailureOfGet
[ERROR]   Run 1: TestS3ACachingBlockManager.testCachingFailureOfGet:405->testCachingOfGetHelper:435->waitForCaching:465 IllegalState waitForCaching: expected: 1, actual: 0, read errors: 0, caching errors: 1
[ERROR]   Run 2: TestS3ACachingBlockManager.testCachingFailureOfGet:405->testCachingOfGetHelper:435->waitForCaching:465 IllegalState waitForCaching: expected: 1, actual: 0, read errors: 0, caching errors: 1
[ERROR]   Run 3: TestS3ACachingBlockManager.testCachingFailureOfGet:405->testCachingOfGetHelper:435->waitForCaching:465 IllegalState waitForCaching: expected: 1, actual: 0, read errors: 0, caching errors: 1 {code}
Discovered in [https://github.com/apache/hadoop/pull/6646#issuecomment-2111558054] "
Fixes compilation issues on Mac,13579051,Resolved,Major,Fixed,13/May/24 10:30,14/May/24 03:04,,"When I build hadoop-common native in Mac OS, I found this error:
{code:java}
/xxxxx/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c:114:50: error: function-like macro '__GLIBC_PREREQ' is not defined
#if defined(__sun) || defined(__GLIBC_PREREQ) && __GLIBC_PREREQ(2, 32) {code}
The reason is that Mac OS does not support glibc. And C conditional compilation requires validation of all expressions."
Tez and hive jobs fail due to google's protobuf 2.5.0 in classpath,13579181,Open,Major,,14/May/24 07:07,,,"There are two issues here:

*1. We are running tez 0.10.3 which uses hadoop 3.3.6 version. Tez has protobuf version 3.21.1*

Below is the exception we get. This is due to protobuf-2.5.0 in our hadoop classpath
{code:java}
java.lang.IllegalAccessError: class org.apache.tez.dag.api.records.DAGProtos$ConfigurationProto tried to access private field com.google.protobuf.AbstractMessage.memoizedSize (org.apache.tez.dag.api.records.DAGProtos$ConfigurationProto and com.google.protobuf.AbstractMessage are in unnamed module of loader 'app')
at org.apache.tez.dag.api.records.DAGProtos$ConfigurationProto.getSerializedSize(DAGProtos.java:21636)
at com.google.protobuf.AbstractMessageLite.writeTo(AbstractMessageLite.java:75)
at org.apache.tez.common.TezUtils.writeConfInPB(TezUtils.java:170)
at org.apache.tez.common.TezUtils.createByteStringFromConf(TezUtils.java:83)
at org.apache.tez.common.TezUtils.createUserPayloadFromConf(TezUtils.java:101)
at org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:436)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
at org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2600)
at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
at org.apache.tez.dag.app.DAGAppMaster.initAndStartAppMaster(DAGAppMaster.java:2597)
at org.apache.tez.dag.app.DAGAppMaster.main(DAGAppMaster.java:2384)
2024-04-18 16:27:54,741 [INFO] [shutdown-hook-0] |app.DAGAppMaster|: DAGAppMasterShutdownHook invoked
2024-04-18 16:27:54,743 [INFO] [shutdown-hook-0] |service.AbstractService|: Service org.apache.tez.dag.app.DAGAppMaster failed in state STOPPED
java.lang.NullPointerException: Cannot invoke ""org.apache.tez.dag.app.rm.TaskSchedulerManager.initiateStop()"" because ""this.taskSchedulerManager"" is null
at org.apache.tez.dag.app.DAGAppMaster.initiateStop(DAGAppMaster.java:2111)
at org.apache.tez.dag.app.DAGAppMaster.serviceStop(DAGAppMaster.java:2126)
at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
at org.apache.tez.dag.app.DAGAppMaster$DAGAppMasterShutdownHook.run(DAGAppMaster.java:2432)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-18 16:27:54,744 [WARN] [Thread-2] |util.ShutdownHookManager|: ShutdownHook 'DAGAppMasterShutdownHook' failed, java.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke ""org.apache.tez.dag.app.rm.TaskSchedulerManager.initiateStop()"" because ""this.taskSchedulerManager"" is null
java.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke ""org.apache.tez.dag.app.rm.TaskSchedulerManager.initiateStop()"" because ""this.taskSchedulerManager"" is null
at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)
at org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:124)
at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)
Caused by: java.lang.NullPointerException: Cannot invoke ""org.apache.tez.dag.app.rm.TaskSchedulerManager.initiateStop()"" because ""this.taskSchedulerManager"" is null
at org.apache.tez.dag.app.DAGAppMaster.initiateStop(DAGAppMaster.java:2111)
at org.apache.tez.dag.app.DAGAppMaster.serviceStop(DAGAppMaster.java:2126)
at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
at org.apache.tez.dag.app.DAGAppMaster$DAGAppMasterShutdownHook.run(DAGAppMaster.java:2432)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
at java.base/java.lang.Thread.run(Thread.java:840){code}
*2. Run Hive having protobuf 3.24.4 with hadoop 3.3.6*

Containers fail with below exception :

 
{code:java}
2024-04-20 13:23:28,008 [INFO] [Dispatcher thread
{Central}
] |container.AMContainerImpl|: Container container_e02_1713455139547_0111_01_000004 exited with diagnostics set to Container failed, exitCode=-1000. [2024-04-20 13:23:27.799]com/google/protobuf/ServiceException java.lang.NoClassDefFoundError: com/google/protobuf/ServiceException at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert(PBHelperClient.java:807) at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlockProto(PBHelperClient.java:680) at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlocks(PBHelperClient.java:985) at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert(PBHelperClient.java:837) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:337) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) at jdk.proxy2/jdk.proxy2.$Proxy16.getBlockLocations(Unknown Source) at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:900) at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:889) at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:878) at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1046) at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:343) at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:339) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:356) at org.apache.hadoop.fs.FileSystem.lambda$openFileWithOptions$0(FileSystem.java:4776) at org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52) at org.apache.hadoop.fs.FileSystem.openFileWithOptions(FileSystem.java:4774) at org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4913) at org.apache.hadoop.yarn.util.FSDownload.unpack(FSDownload.java:342) at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:314) at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:292) at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:72) at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:425) at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:422) at java.base/java.security.AccessController.doPrivileged(AccessController.java:712) at java.base/javax.security.auth.Subject.doAs(Subject.java:439) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:422) at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:255) at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:248) at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:236) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840) Caused by: java.lang.ClassNotFoundException: com.google.protobuf.ServiceException at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188
{code}
 

This exception is thrown by Nodemanager when it tries to localize hive jars for the container."
Hadoop: Upgrade @shore/bootstrap 3.3.5-shore.76,13579036,Open,Major,,13/May/24 09:21,,,Upgrade @shore/bootstrap 3.3.5-shore.76 to stable version
hadoop-auth should not depend on kerb-simplekdc,13577924,Resolved,Major,Fixed,02/May/24 10:24,03/May/24 19:09,3.4.0,"HADOOP-16179 attempted to remove dependency on {{kerb-simplekdc}} from {{hadoop-common}}.  However, {{hadoop-auth}} still has a compile-scope dependency on the same, and {{hadoop-common}} proper depends on {{hadoop-auth}}.  So {{kerb-simplekdc}} is still a transitive dependency of {{hadoop-common}}.

{code}
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli) @ hadoop-common ---
[INFO] org.apache.hadoop:hadoop-common:jar:3.5.0-SNAPSHOT
...
[INFO] +- org.apache.hadoop:hadoop-auth:jar:3.5.0-SNAPSHOT:compile
...
[INFO] |  \- org.apache.kerby:kerb-simplekdc:jar:2.0.3:compile
{code}"
Add LzoCodec implementation based on aircompressor,13577987,Open,Major,,02/May/24 19:50,,,"I remember due to license issue, Hadoop doesn't contain built-in LzoCodec. Users can choose to build and install Lzo codec like hadoop-lzo manually. Some implement LzoCodec based on other open source implementations like aircompressor. But it is somehow inconvenience to maintain it separately.

I'm wondering if we can add LzoCodec implementation based on aircompressor into Hadoop as default LzoCodec."
S3A: Support ByteBufferPositionedReadable through vector IO,13577241,Open,Major,,25/Apr/24 17:55,,3.4.0,"Make it easy for any stream with vector io to support {{ByteBufferPositionedReadable}}

Specifically, {{ByteBufferPositionedReadable.readFully()}}

is exactly a single range read so is easy to read.

the simpler read() call which can return less isn't part of the vector API.
Proposed: invoke the readFully() but convert an EOFException to -1 "
Software Architecture Document,13574874,Open,Major,,06/Apr/24 21:25,,,"We (GitHub @lkhorasandzhian & @vacherkasskiy) have prepared features for documentation. This attached Software Architecture Document is very useful for new contributors and developers to get acquainted with enormous system in a short time. Currently it's only in Russian, but if you're interested in such files we can translate it in English.
There are no changes in code, only adding new documentation files."
Update ISA-L to 2.31.0 in the build image,13575830,Open,Major,,15/Apr/24 08:36,,,Intel ISA-L has several improvements in version 2.31.0. Let's update ISA-L in our build image to this version.
[ABFS] Filesystem contract tests to use methodPath for robust parallel test runs,13576932,Open,Major,,23/Apr/24 14:32,,3.4.0,"hadoop-azure supports parallel test runs, but unlike hadoop-aws, the azure ones are parallelised across methods in the same test suites.

this can fail badly where contract tests have hard coded filenames and assume that they can use this across all test cases. Shows up when you are testing on a store with reduced IO capacity triggering retries and making some test cases slower

Fix: hadoop-common contract tests to use methodPath() names"
ABFS: Implement ThreadLocal for ObjectMapper in AzureHttpOperation via config option with static shared instance as an alternative.,13576066,Open,Major,,16/Apr/24 17:10,,3.4.0,"While doing internal tests on Hive TPCDS queries we have seen many instances of ObjectMapper have been created in an Application Master thus sharing a thread local object mapper instances will improve the performance.  

 

CC [~stevel@apache.org]  [~harshit.gupta] "
Remove Jcache 1.0-alpha,13574200,Resolved,Major,Fixed,02/Apr/24 00:01,05/Apr/24 14:16,3.4.1,"In YARN Federation, we use JCache. The version of JCache has not been maintained for a long time. We directly use ECache instead of JCache in YARN-11663, so we can remove JCache."
Update VectorIO default values consistently,13574498,Resolved,Major,Fixed,03/Apr/24 16:23,04/Apr/24 15:06,3.4.1,
CSE-KMS S3A: Support for InstructionFile to store ECEK meta info,13574431,Open,Major,,03/Apr/24 11:12,,,"{*}Task{*}: Support for InstructionFile to store ECEK meta info 

*Current implementation/Context:*  

Hadoop-aws supports CSE-KMS. During CSE, key encryption info needs to be kept somewhere. AWS SDK supports two ways:
 # *S3 Object's metadata* : Current integration in haddop-aws only supports this approach.
 ## But S3 metadata has limitation of 2 KB size.
 ## Also, metadata can not be updated independently. It would be complete object read/write operation even if we only need to change the metadata.  
 # *Instruction file approach:* It's a small file containing meta-info in the same bucket at the same location. This approach needs one extra trip to S3 Read/Write operation but could be useful if business needs frequent metadata changes.

*Use case:* to implement KMS RE-ENCRYPT, where only CEK(DEK) needs to be encrypted with new key material. Here instruction file approach could be useful.

Plus there could be many other use cases based on different business needs.

*My analysis:* I tried to enable this by setting *CryptoStorageMode.InstructionFile* in 

CryptoConfigurationV2 while building AmazonS3EncryptionClientV2Builder. 

Note: ObjectMetadata is the default value.

{*}Result{*}: Write operation worked but read failed due to missing instruction file.

*RCA:* On debugging, I found following:

On put request, say myfile.txt : 
 * First , S3AFileSystem writes the file to S3 like *myfile.txt_COPYING_*
 * Second, it writes the corresponding instruction file as  *myfile.txt_COPYING_.instruction*
 * Third, it calls rename.
 ** Rename here means copy the file bytes to *myfile.txt and*
 ** *delete the* *myfile.txt_COPYING*
 * Here problem occurs, 
 ** AmazonS3EncryptionClientV2 class, after deleting any file it looks for corresponding instruction file and if found it deletes that one also. As a result, it deletes *myfile.txt_COPYING_.instruction* as well.

Related  Code:

com.amazonaws.services.s3.AmazonS3EncryptionClientV2.deleteObject() // part of aws sdk bundle

*Possible solution:* S3AFileSystem (part of hadoop-aws) needs to be updated to first rename the instruction file , then the original file. This way deletion of instruction file can be avoided.

It also requires config changes to take Objemetadata/InstructionFile as config parameter.

Let's discuss if we have any better solution and can be incorporated.

Once we agree on one common solution, I can work on implementation part.

 "
ITestS3ACommitterFactory failing,13581175,Resolved,Minor,Fixed,31/May/24 13:22,10/Sep/24 18:26,3.4.0,"we've had ITestS3ACommitterFactory failing for a while, where it looks like changed committer settings aren't being picked up.

{code}
ERROR] ITestS3ACommitterFactory.testEverything:115->testInvalidFileBinding:165 Expected a org.apache.hadoop.fs.s3a.commit.PathCommitException to be thrown, but got the result: : FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl
{code}

I've spent some time looking at it and it is happening because the test sets the fileystem ref for the local test fs, and not that of the filesystem created by the committer, which is where the option is picked up.

i've tried to parameterize it but things are still playing up and I'm not sure how hard to try to fix.


"
"[ABFS, S3A] Add IORateLimiter api to hadoop common",13574495,Open,Minor,,03/Apr/24 15:53,,3.4.0,"Create a rate limiter API in hadoop common which code (initially, manifest committer, bulk delete).. can request iO capacity for a specific operation.

this can be exported by filesystems so support shared rate limiting across all threads

pulled from HADOOP-19093 PR"
testUpdateDeepDirectoryStructureToRemote intermittent failures,13584169,Open,Minor,,28/Jun/24 10:47,,,"Test testUpdateDeepDirectoryStructureToRemote intermittently fails. Following is an instance in ABFS test runs:

 
{code:java}
[ERROR] testUpdateDeepDirectoryStructureToRemote(org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractDistCp) Time elapsed: 2.951 s <<< FAILURE!
java.lang.AssertionError: Files Copied value 2 above maximum 1
at org.junit.Assert.fail(Assert.java:89)
at org.junit.Assert.assertTrue(Assert.java:42)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.assertCounterInRange(AbstractContractDistCpTest.java:294)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.distCpUpdateDeepDirectoryStructure(AbstractContractDistCpTest.java:334)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.testUpdateDeepDirectoryStructureToRemote(AbstractContractDistCpTest.java:259) {code}
 

There is one JIRA in Apache-Ozone which was raised in S3 test run:
https://issues.apache.org/jira/browse/HDDS-10616

 
{code:java}
org.apache.hadoop.fs.contract.s3a.ITestS3AContractDistCp
testUpdateDeepDirectoryStructureToRemote(org.apache.hadoop.fs.contract.s3a.ITestS3AContractDistCp) Time elapsed: 2.375 s <<< FAILURE!
java.lang.AssertionError: Files Copied value 2 above maximum 1
at org.junit.Assert.fail(Assert.java:89)
at org.junit.Assert.assertTrue(Assert.java:42)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.assertCounterInRange(AbstractContractDistCpTest.java:294)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.distCpUpdateDeepDirectoryStructure(AbstractContractDistCpTest.java:334)
at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.testUpdateDeepDirectoryStructureToRemote(AbstractContractDistCpTest.java:259){code}"
Log level is WARN when fail to load native hadoop libs,13581628,Resolved,Minor,Fixed,05/Jun/24 09:29,14/Jun/24 18:09,3.3.6,
Bulk delete api doesn't take the path to delete as the base path,13581829,Resolved,Minor,Fixed,06/Jun/24 16:58,11/Jun/24 20:26,3.4.1,"If you use the path of the file you intend to delete as the base path, you get an error. This is because the validation requires the list to be of children, but the base path itself should be valid."
TestHarFileSystem and TestFilterFileSystem failing after bulk delete API added,13580592,Resolved,Minor,Fixed,27/May/24 15:49,04/Jun/24 13:51,3.4.1,"oh, we need to update a couple of tests so they know not to worry about the new interface/method. The details are in the javadocs of FileSystem.

Interesting these snuck through yetus, though they fail in PRs based atop #6726

{code}
[ERROR] Failures: 
[ERROR] org.apache.hadoop.fs.TestFilterFileSystem.testFilterFileSystem
[ERROR]   Run 1: TestFilterFileSystem.testFilterFileSystem:181 1 methods were not overridden correctly - see log
[ERROR]   Run 2: TestFilterFileSystem.testFilterFileSystem:181 1 methods were not overridden correctly - see log
[ERROR]   Run 3: TestFilterFileSystem.testFilterFileSystem:181 1 methods were not overridden correctly - see log
[INFO] 
[ERROR] org.apache.hadoop.fs.TestHarFileSystem.testInheritedMethodsImplemented
[ERROR]   Run 1: TestHarFileSystem.testInheritedMethodsImplemented:402 1 methods were not overridden correctly - see log
[ERROR]   Run 2: TestHarFileSystem.testInheritedMethodsImplemented:402 1 methods were not overridden correctly - see log
[ERROR]   Run 3: TestHarFileSystem.testInheritedMethodsImplemented:402 1 methods were not overridden correctly - see log

{code}
"
Skip ITestS3AEncryptionWithDefaultS3Settings.testEncryptionFileAttributes when bucket not encrypted with sse-kms,13581225,Resolved,Minor,Fixed,31/May/24 22:03,03/Jun/24 17:09,3.4.1,"[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 4, Time elapsed: 12.80 s <<< FAILURE! -- in org.apache.hadoop.fs.s3a.ITestS3AEncryptionWithDefaultS3Settings
[ERROR] org.apache.hadoop.fs.s3a.ITestS3AEncryptionWithDefaultS3Settings.testEncryptionFileAttributes -- Time elapsed: 5.065 s <<< FAILURE!
org.junit.ComparisonFailure: [Server side encryption algorithm must match] expected:<""[aws:kms]""> but was:<""[AES256]"">
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at org.apache.hadoop.fs.s3a.EncryptionTestUtils.validateEncryptionFileAttributes(EncryptionTestUtils.java:138)
        at org.apache.hadoop.fs.s3a.ITestS3AEncryptionWithDefaultS3Settings.testEncryptionFileAttributes(ITestS3AEncryptionWithDefaultS3Settings.java:112)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
 "
Change of Codec configuration does not work,13578246,Resolved,Minor,Fixed,06/May/24 08:50,17/May/24 02:36,,"In one of my projects, I need to dynamically adjust compression level for different files. 
However, I found that in most cases the new compression level does not take effect as expected, the old compression level continues to be used.

Here is the relevant code snippet:
ZStandardCodec zStandardCodec = new ZStandardCodec();
zStandardCodec.setConf(conf);
conf.set(""io.compression.codec.zstd.level"", ""5""); // level may change dynamically
conf.set(""io.compression.codec.zstd"", zStandardCodec.getClass().getName());
writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(sequenceFilePath),
                                SequenceFile.Writer.keyClass(LongWritable.class),
                                SequenceFile.Writer.valueClass(BytesWritable.class),
                                SequenceFile.Writer.compression(CompressionType.BLOCK));

The reason is SequenceFile.Writer.init() method will call CodecPool.getCompressor(codec, null) to get a compressor. 
If the compressor is a reused instance, the conf is not applied because it is passed as null:
public static Compressor getCompressor(CompressionCodec codec, Configuration conf) {
Compressor compressor = borrow(compressorPool, codec.getCompressorType());
if (compressor == null)

{ compressor = codec.createCompressor(); LOG.info(""Got brand-new compressor [""+codec.getDefaultExtension()+""]""); }

else {
compressor.reinit(conf);   //conf is null here
......

 

Please also refer to my unit test to reproduce the bug. 
To address this bug, I modified the code to ensure that the configuration is read back from the codec when a compressor is reused."
Upgrade aws-java-sdk to 1.12.720,13579099,Resolved,Minor,Fixed,13/May/24 14:10,16/May/24 14:00,3.3.6,"Update to the latest AWS SDK, to stop anyone worrying about the ion library CVE https://nvd.nist.gov/vuln/detail/CVE-2024-21634

This isn't exposed in the s3a client, but may be used downstream. 

on v2 sdk releases, the v1 sdk is only used during builds; 3.3.x it is shipped"
update s3a committer docs,13579213,Open,Minor,,14/May/24 10:16,,3.4.0,"Update s3a committer docs

* declare that magic committer is stable and make it the recommended one
* show how to use new command ""mapred successfile"" to print the success file."
[DOC] Drop Migrating from Apache Hadoop 1.x to Apache Hadoop 2.x,13578489,Open,Minor,,07/May/24 18:35,,,"Reading the docs, found this page, which is pretty irrelevant in current context or upcoming 3.x releases, can explore dropping it

https://apache.github.io/hadoop/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html"
noaa-cors-pds bucket access with global endpoint fails,13575546,Resolved,Minor,Fixed,11/Apr/24 19:13,30/Apr/24 12:46,3.4.0,"All tests accessing noaa-cors-pds use us-east-1 region, as configured at bucket level. If global endpoint is configured (e.g. us-west-2), they fail to access to bucket.

 

Sample error:
{code:java}
org.apache.hadoop.fs.s3a.AWSRedirectException: Received permanent redirect response to region [us-east-1].  This likely indicates that the S3 region configured in fs.s3a.endpoint.region does not match the AWS region containing the bucket.: null (Service: S3, Status Code: 301, Request ID: PMRWMQC9S91CNEJR, Extended Request ID: 6Xrg9thLiZXffBM9rbSCRgBqwTxdLAzm6OzWk9qYJz1kGex3TVfdiMtqJ+G4vaYCyjkqL8cteKI/NuPBQu5A0Q==)
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:253)
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:155)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4041)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3947)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$26(S3AFileSystem.java:3924)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2716)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2735)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3922)
    at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)
    at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)
    at org.apache.hadoop.fs.Globber.glob(Globber.java:202)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$globStatus$35(S3AFileSystem.java:4956)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2716)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2735)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:4949)
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:281)
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:445)
    at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:311)
    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:328)
    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:201)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1677)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1674)
 {code}
{code:java}
Caused by: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 301, Request ID: PMRWMQC9S91CNEJR, Extended Request ID: 6Xrg9thLiZXffBM9rbSCRgBqwTxdLAzm6OzWk9qYJz1kGex3TVfdiMtqJ+G4vaYCyjkqL8cteKI/NuPBQu5A0Q==)
    at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)
    at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)
    at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)
    at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)
    at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:93)
    at software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:279)
    ...
    ...
    ...
    at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)
    at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:6319)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2901)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2889)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2869)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4019)
 {code}"
Fix hadoop-aws document for fs.s3a.committer.abort.pending.uploads,13577428,Resolved,Minor,Fixed,27/Apr/24 12:37,29/Apr/24 14:52,3.3.6,The description about `fs.s3a.committer.abort.pending.uploads` in the _Concurrent Jobs writing to the same destination_ is not all correct.
Fix TestZKSignerSecretProvider failing unit test,13576880,Open,Minor,,23/Apr/24 07:14,,3.4.0,"* {{TestZKSignerSecretProvider and }}{{{}TestRandomSignerSecretProvider{}}}{{{{}} unit test o{}}}ccasional failure
 * The reason was that the MockZKSignerSecretProvider class rollSecret method is {{synchronized}}
 * {{{}s{}}}ometimes verify (secretProvider, timeout (timeout). AtLeastOnce ()). RollSecret () method first in RolloverSignerSecretProvider scheduler thread lock, this results in a timeout

 "
Invalid GPG commands in Download page,13584255,Resolved,Trivial,Fixed,29/Jun/24 18:47,01/Jul/24 12:31,,"Instructions in [Download page|https://hadoop.apache.org/releases.html] shows GPG commands with {{--}} converted to &ndash;, which makes the commands invalid.

{code}
gpg –import KEYS
gpg –verify hadoop-X.Y.Z-src.tar.gz.asc
{code}"
Switch the baseurl for Centos 8,13431770,Resolved,Blocker,Fixed,03/Mar/22 17:23,05/Mar/22 01:42,3.4.0,"Centos 8 has reached its End-of-Life and thus its packages are no longer accessible from  mirror.centos.org. We need to switch the *baseurl* to vault.centos.org where the packages are archived.

Please see https://www.centos.org/centos-linux-eol/ for more details."
Disable S3A auditing by default.,13424407,Resolved,Blocker,Fixed,24/Jan/22 13:32,24/Jan/22 15:07,3.3.2,"See HADOOP-18091. S3A auditing leaks memory through ThreadLocal references

* Adds a new option fs.s3a.audit.enabled to controls whether or not auditing
is enabled. This is false by default.

* When false, the S3A auditing manager is NoopAuditManagerS3A,
which was formerly only used for unit tests and
during filsystem initialization.

* When true, ActiveAuditManagerS3A is used for managing auditing,
allowing auditing events to be reported.

* updates documentation and tests.
"
Update the year to 2022,13420488,Resolved,Blocker,Duplicate,04/Jan/22 02:07,04/Jan/22 02:13,,
Distcp: Sync moves filtered file to home directory rather than deleting,13425237,Resolved,Critical,Fixed,27/Jan/22 18:21,10/Feb/22 20:59,3.3.2,"Distcp sync with snapshot, if the file being copied is renamed to a path which is in the exclusion filter, tries to delete the file.

But instead of deleting, it moves the file to home directory

 "
Implement paging during S3 multi object delete.,13426374,Resolved,Critical,Fixed,03/Feb/22 10:08,11/Mar/22 10:56,3.3.1," 

{*}Error{*}:

Rename operation fails during multi object delete of size more than 1000. We see an exception during multi object delete of more than 1000 keys in one go during rename operation.

 
{noformat}
org.apache.hadoop.fs.s3a.AWSBadRequestException: rename  com.amazonaws.services.s3.model.AmazonS3Exception
: The XML you provided was not well-formed or did not validate against our published schema (Service: Amazon S3; Status Code: 400; Error Code: MalformedXML; Request ID: XZ8PGAQHP0FGHPYS; S3 Extended Request ID: vTG8c+koukzQ8yMRGd9BvWfmRwkCZ3fAs/EOiAV5S9E
JjLqFTNCgDOKokuus5W600Z5iOa/iQBI=; Proxy: null), S3 Extended Request ID: vTG8c+koukzQ8yMRGd9BvWfmRwkCZ3fAs/EOiAV5S9EJjLqFTNCgDOKokuus5W600Z5iOa/iQBI=:MalformedXML: The XML you provided was not well-formed or did not validate against our published schema 
(Service: Amazon S3; Status Code: 400; Error Code: MalformedXML; Request ID: XZ8PGAQHP0FGHPYS; S3 Extended Request ID: vTG8c+koukzQ8yMRGd9BvWfmRwkCZ3fAs/EOiAV5S9EJjLqFTNCgDOKokuus5W600Z5iOa/iQBI=; Proxy: null)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:247)
        at org.apache.hadoop.fs.s3a.s3guard.RenameTracker.convertToIOException(RenameTracker.java:267)
        at org.apache.hadoop.fs.s3a.s3guard.RenameTracker.deleteFailed(RenameTracker.java:198)
        at org.apache.hadoop.fs.s3a.impl.RenameOperation.removeSourceObjects(RenameOperation.java:706)
        at org.apache.hadoop.fs.s3a.impl.RenameOperation.completeActiveCopiesAndDeleteSources(RenameOperation.java:274)
        at org.apache.hadoop.fs.s3a.impl.RenameOperation.recursiveDirectoryRename(RenameOperation.java:484)
        at org.apache.hadoop.fs.s3a.impl.RenameOperation.execute(RenameOperation.java:312)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1912)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1759)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2250)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1757)
        at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1605)
        at org.apache.hadoop.fs.TrashPolicyDefault.moveToTrash(TrashPolicyDefault.java:186)
        at org.apache.hadoop.fs.Trash.moveToTrash(Trash.java:110){noformat}
 

{*}Solution{*}:

So implementing paging of requests to reduce the number of keys in a single request. Page size can be configured

using ""fs.s3a.bulk.delete.page.size"""
Setup Jenkins nightly CI for Windows 10,13429542,Resolved,Critical,Fixed,20/Feb/22 15:22,03/May/23 17:15,3.4.0,Need to run the Jenkins Nightly CI for Windows 10 environment so that we catch any breaking changes for Hadoop on the Windows 10 platform. Need to get Yetus to run on Windows 10 with against the Hadoop codebase.
Add Dockerfile for Windows 10,13429541,Resolved,Critical,Fixed,20/Feb/22 15:21,12/Oct/22 16:31,3.4.0,Need to write a Dockerfile for Windows 10 that creates a Docker image for building Hadoop on the Windows 10 platform.
toString method of RpcCall throws IllegalArgumentException,13430474,Resolved,Critical,Won't Fix,24/Feb/22 16:00,24/Aug/22 10:30,,"We have observed breaking tests such as TestApplicationACLs. We have located the root cause, which is HADOOP-18082. It seems that there is a concurrency issue within ProtobufRpcEngine2. When using a debugger, the missing fields are there, hence the suspicion of concurrency problem. The stack trace:
{noformat}
java.lang.IllegalArgumentException
    at java.nio.Buffer.position(Buffer.java:244)
    at org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper.readFrom(RpcWritable.java:131)
    at org.apache.hadoop.ipc.RpcWritable$Buffer.getValue(RpcWritable.java:232)
    at org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest.getRequestHeader(ProtobufRpcEngine2.java:645)
    at org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest.toString(ProtobufRpcEngine2.java:663)
    at java.lang.String.valueOf(String.java:3425)
    at java.lang.StringBuilder.append(StringBuilder.java:516)
    at org.apache.hadoop.ipc.Server$RpcCall.toString(Server.java:1328)
    at java.lang.String.valueOf(String.java:3425)
    at java.lang.StringBuilder.append(StringBuilder.java:516)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3097){noformat}"
Hadoop auth does not handle HTTP Headers in a case-insensitive way,13427877,Resolved,Critical,Fixed,10/Feb/22 20:33,20/May/22 08:52,,"According to [RFC-2616|https://www.ietf.org/rfc/rfc2616.txt] HTTP Headers are case-insensitive. There are proxies / load balancers (e.g.: newer versions of HA-proxy) which deliberately make some of the HTTP headers lower-case results in an authentication / authorization failure inside the Hadoop codebase.

I've created a small patch (I'm from Cloudera):  [^hadoop-auth-headers.patch]. This resolves our authentication issue. Can someone please have a look at this?"
Jenkins jobs intermittently failing in the end,13434827,Open,Critical,,21/Mar/22 07:05,,,"The PR results are not completed sometime:

{noformat}
13:52:57  Recording test results
13:53:18  Remote call on hadoop10 failed
[Pipeline] echo
13:53:18  junit processing: java.io.IOException: Remote call on hadoop10 failed
{noformat}
Ref:
https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4081/8/consoleFull

Daily Builds also face similar issues:

{noformat}
java.io.IOException: Pipe closed after 0 cycles
	at org.apache.sshd.common.channel.ChannelPipedInputStream.read(ChannelPipedInputStream.java:126)
	at org.apache.sshd.common.channel.ChannelPipedInputStream.read(ChannelPipedInputStream.java:105)
	at hudson.remoting.FlightRecorderInputStream.read(FlightRecorderInputStream.java:93)
	at hudson.remoting.ChunkedInputStream.readHeader(ChunkedInputStream.java:74)
	at hudson.remoting.ChunkedInputStream.readUntilBreak(ChunkedInputStream.java:104)
	at hudson.remoting.ChunkedCommandTransport.readBlock(ChunkedCommandTransport.java:39)
	at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read(AbstractSynchronousByteArrayCommandTransport.java:34)
	at hudson.remoting.SynchronousCommandTransport$ReaderThread.run(SynchronousCommandTransport.java:61)
Caused: java.io.IOException: Backing channel 'hadoop19' is disconnected.
{noformat}
Ref:
https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/810/console

https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/807/console
"
Exclude log4j2 dependency from hadoop-huaweicloud module,13424270,Resolved,Critical,Duplicate,24/Jan/22 05:46,24/Jan/22 06:09,,"[https://github.com/apache/hadoop/pull/3906#issuecomment-1018401121]

The following log4j2 dependencies must be excluded.
{code:java}
[INFO] \- org.apache.hadoop:hadoop-huaweicloud:jar:3.4.0-SNAPSHOT:compile
[INFO]    \- com.huaweicloud:esdk-obs-java:jar:3.20.4.2:compile
[INFO]       +- com.jamesmurty.utils:java-xmlbuilder:jar:1.2:compile
[INFO]       +- com.squareup.okhttp3:okhttp:jar:3.14.2:compile
[INFO]       +- org.apache.logging.log4j:log4j-core:jar:2.12.0:compile
[INFO]       \- org.apache.logging.log4j:log4j-api:jar:2.12.0:compile {code}"
S3A Authentication to support WebIdentity,13432165,Open,Major,,05/Mar/22 13:21,,2.10.2,"We are using the latest version of [delta-sharing|https://github.com/delta-io/delta-sharing] which takes advantage of [hadoop-aws|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html] (S3A) connector in [Hadoop release version 2.10.1|https://github.com/apache/hadoop/tree/rel/release-2.10.1] to mount an AWS S3 File System. In our particular setup, all services are operated in Amazon Elastic Kubernetes Service (EKS) and need to comply to the AWS security concept [IAM roles for service accounts|https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html] (IRSA).

As [Delta sharing S3 connection|https://github.com/delta-io/delta-sharing#s3] doesn't offer any corresponding support, we patched hadoop-aws-2.10.1 to address this need via a new credentials provider class org.apache.hadoop.fs.s3a.OIDCTokenCredentialsProvider. We also upgraded dependency aws-java-sdk-bundle to its latest version 1.12.167 as [AWS WebIdentityTokenCredentialsProvider class|https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/WebIdentityTokenCredentialsProvider.html%E2%80%A6] was not yet available in original version 1.11.271.

We believe that other delta-sharing users could benefit from this short-term contribution. Then sooner or later, delta-sharing owners will have to upgrade their project to a more recent version of hadoop-aws that is probably more widely used. The effort to promote this change is probably low.

Additional note: AWS WebIdentityTokenCredentialsProvider class is directly supported by Spark applications submitted with configuration properties `spark.hadoop.fs.s3a.aws.credentials.provider`and `spark.kubernetes.authenticate.submission.oauthToken` ([doc|https://spark.apache.org/docs/latest/running-on-kubernetes.html#spark-properties]). So bringing this support to Hadoop will primarily be interesting for non-Spark users."
Implement a variant of ElasticByteBufferPool which uses weak references for garbage collection.,13425980,Resolved,Major,Fixed,01/Feb/22 13:52,16/Jun/22 20:12,,"Currently in hadoop codebase, we have two classes which implements byte buffers pooling.

One is ElasticByteBufferPool which doesn't use weak references and thus could cause memory leaks in production environment. 

Other is DirectBufferPool which uses weak references but doesn't support caller's preference for either on-heap or off-heap buffers. 

 

The idea is to create an improved version of ElasticByteBufferPool by subclassing it ( as it is marked as public and stable and used widely in hdfs ) with essential functionalities required for effective buffer pooling. This is important for the parent Vectored IO work."
S3A: Upgrade AWS SDK to V2,13421427,Resolved,Major,Fixed,08/Jan/22 00:22,11/Sep/23 13:38,3.3.1,"This task tracks upgrading Hadoop's AWS connector S3A from AWS SDK for Java V1 to AWS SDK for Java V2.

Original use case:
{quote}We would like to access s3 with AWS SSO, which is supported in software.amazon.awssdk:sdk-core:2.*.

In particular, from [https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html], when to set 'fs.s3a.aws.credentials.provider', it must be ""com.amazonaws.auth.AWSCredentialsProvider"". We would like to support ""software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider"" which supports AWS SSO, so users only need to authenticate once.
{quote}"
High performance vectored read API in Hadoop,13425972,Resolved,Major,Fixed,01/Feb/22 13:13,09/Jan/23 18:47,3.3.4,"Add support for multiple ranged vectored read api in PositionedReadable. The default iterates through the ranges to read each synchronously, but the intent is that FSDataInputStream subclasses can make more efficient readers especially object stores implementation."
Replace log4j 1.x with reload4j,13423884,Resolved,Major,Fixed,21/Jan/22 02:10,18/Apr/22 14:17,,"As proposed in the dev mailing list (https://lists.apache.org/thread/fdzkv80mzkf3w74z9120l0k0rc3v7kqk) let's replace log4j 1 with reload4j in the maintenance releases (i.e. 3.3.x, 3.2.x and 2.10.x)"
Upgrade jackson to 2.13.2 and jackson-databind to 2.13.2.2. CVE-2020-36518,13435952,Resolved,Major,Fixed,26/Mar/22 12:03,11/Apr/22 05:58,3.3.2,"CVE-2020-36518
https://github.com/FasterXML/jackson-databind/issues/2816"
UserGroupInformation shouldn't log exception ,13429789,Open,Major,,21/Feb/22 21:14,,,"In {{UserGroupInformation.doAs}}, we currently create a new {{Exception}} and log it in {{LOG.debug}}. This doesn't look necessary:
{code}
      if (LOG.isDebugEnabled()) {
        LOG.debug(""PrivilegedAction [as: {}][action: {}]"", this, action,
            new Exception());
      }
{code}"
Produce Windows binaries of Hadoop,13429553,Resolved,Major,Fixed,20/Feb/22 18:31,09/Apr/24 17:17,3.4.0,We currently only provide Linux libraries and binaries. We need to provide the same for Windows. We need to port the [create-release script|https://github.com/apache/hadoop/blob/5f9932acc4fa2b36a3005e587637c53f2da1618d/dev-support/bin/create-release] to run on Windows and produce the Windows binaries.
S3a copyFromLocalOperation doesn't support single file,13435694,Open,Major,,25/Mar/22 02:07,,3.3.2,"Spark job uses aws s3 as fileSystem and calls 
{code:java}
fs.copyFromLocalFile(delSrc, overwrite, src, dest) 

delSrc = false
overwrite = true
src = ""/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar""
dest = ""s3a://spark/spark-upload-a703d8e7-8dd2-4e29-beca-b4df2fedefbd/spark-examples_2.12-3.4.0-SNAPSHOT.jar""{code}
Then throw a PathIOException, message is as follow
{code:java}
Exception in thread ""main"" org.apache.spark.SparkException: Uploading file /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar failed...        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:332)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:277)        
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)        
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)        
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)        
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)        
at scala.collection.TraversableLike.map(TraversableLike.scala:286)        
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)        
at scala.collection.AbstractTraversable.map(Traversable.scala:108)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:275)        
at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatureStep.scala:187)       
at scala.collection.immutable.List.foreach(List.scala:431)        
at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:178)
at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$5(KubernetesDriverBuilder.scala:86)        at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)        
at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)        
at scala.collection.immutable.List.foldLeft(List.scala:91)        
at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:84)        
at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:104)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:248)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:242) 
at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2738)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:242)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:214)        
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)        
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)        
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)        
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)        
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)        
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)        
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: org.apache.spark.SparkException: Error uploading file spark-examples_2.12-3.4.0-SNAPSHOT.jar        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:355)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:328)        
... 30 more
Caused by: org.apache.hadoop.fs.PathIOException: `Cannot get relative path for URI:file:///Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar': Input/output error 
at apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:365)        
at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:226)        
at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:170)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$25(S3AFileSystem.java:3920)        
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3913)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:352)        
... 31 more {code}
I add some logs
{code:java}
22/03/25 09:33:24 INFO KubernetesUtils: Uploading file: /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to dest: s3a://spark/spark-upload-a703d8e7-8dd2-4e29-beca-b4df2fedefbd/spark-examples_2.12-3.4.0-SNAPSHOT.jar...
22/03/25 09:33:24 INFO S3AFileSystem: Copying local file from /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to s3a://spark/spark-upload-a703d8e7-8dd2-4e29-beca-b4df2fedefbd/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/25 09:33:24 INFO CopyFromLocalOperation: Copying local file from /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to s3a://spark/spark-upload-a703d8e7-8dd2-4e29-beca-b4df2fedefbd/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/25 09:33:24 INFO CopyFromLocalOperation: execute#CopyFromLocalOperation, sourceFile is /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/25 09:33:24 INFO CopyFromLocalOperation: uploadSourceFromFS#CopyFromLocalOperation, localFile 1: path is LocatedFileStatus{path=file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar; isDirectory=false; length=1567474; replication=1; blocksize=33554432; modification_time=1647874074000; access_time=1647874074000; owner=hengzhen.sq; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
22/03/25 09:33:24 INFO CopyFromLocalOperation: getFinalPath#CopyFromLocalOperation, src is file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar, source is /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar {code}
It looks like copyFromLocalOperation doesn't support single file."
Über-JIRA: abfs phase III: Hadoop 3.4.0 features & fixes,13421344,Resolved,Major,Done,07/Jan/22 14:09,27/Feb/24 11:10,3.3.2,ABFS related changes for hadoop 3.3.3
ABFS: Skip testEtagConsistencyAcrossRename for Non-HNS accounts,13422467,Open,Major,,13/Jan/22 09:32,,3.3.2,"The rename operation is not supported for non-HNS accounts. Hence, tests verifying matching etag for file before and after rename should not be run against non-HNS accounts."
ABFS: Lease operations,13430356,Open,Major,,24/Feb/22 07:08,,3.3.1,
FileNotFoundException in abfs mkdirs() call,13422486,Open,Major,,13/Jan/22 10:23,,3.3.1,"seen in production: calling mkdirs in FileOutputCommitter setupJob is triggering an FNFE

{code}
 java.io.FileNotFoundException: Operation failed: ""The specified path does not exist."", 404, PUT, https://bcket.dfs.core.windows.net/table1/_temporary/0?resource=directory&timeout=90, PathNotFound, ""The specified path does not exist.""
 	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1131)
 	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.mkdirs(AzureBlobFileSystem.java:445)
 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2347)
{code}

I suspect what is happening is that while this job is setting up, a previous job is doing cleanup/abort on the same path

assuming that abfs mkdirs is like the posix one -nonatomic, as it goes up/down the chain of parent dirs, something else gets in the way.

if so, this is something which can be handled in the client -when we get an FNFE we could warn and retry.

in the manifest committer each job will have a unique id under _temporary and there will be the option to skip deleting the temp dir entirely, for better coexistence of active jobs.
"
ABFS: Toggle Store Mkdirs request overwrite parameter with default value,13423038,Open,Major,,17/Jan/22 12:23,,3.3.2," mkdirs config is set to overwrite=false as default as the related backend deployment is completed.

PR :- [ABFS: Toggle Store Mkdirs request overwrite parameter by anmolanmol1234 · Pull Request #3830 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/3830]"
Utility to identify git commit / Jira fixVersion discrepancies for RC preparation,13428288,Resolved,Major,Fixed,14/Feb/22 09:42,18/Apr/22 14:18,2.10.2,"As part of RC preparation,  we need to identify all git commits that landed on release branch, however their corresponding Jira is either not resolved yet or does not contain expected fixVersions. Only when we have git commits and corresponding Jiras with expected fixVersion resolved, we get all such Jiras included in auto-generated CHANGES.md as per Yetus changelog generator.

Proposal of this Jira is to provide such script that can be useful for all upcoming RC preparations and list down all Jiras where we need manual intervention. This utility script should use Jira API to retrieve individual fields and use git log to loop through commit history.

The script should identify these issues:
 # commit is reverted as per commit message
 # commit does not contain Jira number format (e.g. HADOOP-XXXX / HDFS-XXXX etc) in message
 # Jira does not have expected fixVersion
 # Jira has expected fixVersion, but it is not yet resolved
 # Jira has release corresponding fixVersion and is resolved, but no corresponding commit yet found

It can take inputs as:
 # First commit hash to start excluding commits from history
 # Fix Version
 # JIRA Project Name
 # Path of project's working dir
 # Jira server url"
Update the year to 2022,13420194,Resolved,Major,Fixed,01/Jan/22 11:27,04/Jan/22 02:43,2.10.2,Update the year to 2022
"Certificate doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]",13434074,Resolved,Major,Fixed,16/Mar/22 10:37,21/Jun/22 19:17,3.3.1,"h2. If you see this error message when trying to use s3a:// or gs:// URLs, look for copies of cos_api-bundle.jar on your classpath and remove them.

Libraries which include shaded apache httpclient libraries (hadoop-client-runtime.jar, aws-java-sdk-bundle.jar, gcs-connector-shaded.jar, cos_api-bundle.jar) all load and use the unshaded resource mozilla/public-suffix-list.txt. If an out of date version of this is found on the classpath first, attempts to negotiate TLS connections may fail with the error ""Certificate doesn't match any of the subject alternative names"". 

In a hadoop installation, you can use the findclass tool to track down where the public-suffix-list.txt is coming from.

{code}
hadoop org.apache.hadoop.util.FindClass locate mozilla/public-suffix-list.txt
{code}

So far, the cos_api-bundle-5.6.19.jar appears to be the source of this problem.

----

h2. bug report


Trying to run any job after bumping our Spark version (which is now using Hadoop 3.3.1), lead us to the current exception while reading files on s3:
{code:java}
org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://<bucket>/<path>.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for <bucket.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]: Unable to execute HTTP request: Certificate for <bucket> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com] at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:208) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3351) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185) at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4277) at {code}
 
{code:java}
Caused by: javax.net.ssl.SSLPeerUnverifiedException: Certificate for <bucket.s3.amazonaws.com> doesn't match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:507)
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:437)
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384)
		at com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
		at com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)
		at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)
		at com.amazonaws.http.conn.$Proxy16.connect(Unknown Source)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
		at com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
		at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
		at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
		at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
		at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1333)
		at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)  {code}
We found similar problems in the following tickets but:
 - https://issues.apache.org/jira/browse/HADOOP-17017 (we don't use `.` in our bucket names)
 - [https://github.com/aws/aws-sdk-java-v2/issues/1786] (we tried to override it by using `httpclient:4.5.10` or `httpclient:4.5.8`, with no effect).

We couldn't test it using the native `openssl` configuration due to our setup, so we would like to stick with the java ssl implementation, if possible.

 "
Change scope of inner classes in InodeTree to make them accessible outside package,13425839,Resolved,Major,Fixed,31/Jan/22 18:31,12/Jul/22 10:45,3.2.4,
Change URI[] in INodeLink to String[] to reduce memory footprint of ViewFileSystem,13429181,Resolved,Major,Fixed,17/Feb/22 18:37,18/Mar/22 00:30,2.10.2,"Around 40k instances of INodeLink each of which is taking memory ranging from 1680bytes to 1160bytes. Multiplying 40k with 1160bytes will give us approximate 45mb.

With changing from URI to String in INodeLink the memory consumed by each of the INodeLink objects has reduced from ~1160 bytes to ~320 bytes. Overall size becomes (40k X 320) 12mb"
Bump aliyun-sdk-oss to 3.13.2 and jdom2 to 2.0.6.1,13425878,Resolved,Major,Fixed,01/Feb/22 00:08,03/Feb/22 23:52,3.4.0,"The current aliyun-sdk-oss 3.13.0 is affected by [CVE-2021-33813|https://github.com/advisories/GHSA-2363-cqg2-863c] due to jdom 2.0.6. maven-shade-plugin is also affected by the CVE. 

Bumping aliyun-sdk-oss to 3.13.2 and jdom2 to 2.0.6.1 will resolve this issue
{code:java}
[INFO] +- org.apache.maven.plugins:maven-shade-plugin:jar:3.2.1:provided
[INFO] |  +- org.apache.maven.shared:maven-artifact-transfer:jar:0.10.0:provided
[INFO] |  +- org.jdom:jdom2:jar:2.0.6:provided
......
[INFO] +- com.aliyun.oss:aliyun-sdk-oss:jar:3.13.1:compile
[INFO] |  +- org.jdom:jdom2:jar:2.0.6:compile
{code}
 "
Add metrics to track delegation token secret manager operations,13434986,Resolved,Major,Fixed,21/Mar/22 20:50,11/May/22 15:29,3.3.5,"New metrics to track operations that store, update and remove delegation tokens in implementations of AbstractDelegationTokenSecretManager. This will help evaluate the impact of using different secret managers and add optimizations."
outputstream.md typo issue,13428956,Resolved,Major,Fixed,17/Feb/22 02:53,02/Mar/22 10:28,3.4.0,"There is a typo issue in outputstream.md on the branch – trunk

!image-2022-02-17-10-53-01-704.png!"
s3a prefetching to use SemaphoredDelegatingExecutor for submitting work,13436428,Resolved,Major,Fixed,29/Mar/22 12:49,16/Sep/22 19:56,3.4.0,"Use SemaphoredDelegatingExecutor for each to stream to submit work, if possible, for better fairness in processes with many streams.

this also takes a DurationTrackerFactory to count how long was spent in the queue, something we would want to know

if cherrypicking, follow with HADOOP-18466 and HADOOP-18455"
Support touch command for directory,13436449,Resolved,Major,Fixed,29/Mar/22 14:26,07/Apr/22 08:30,3.4.0,Currently hadoop fs -touch command cannot update the mtime and the atime of directory. The feature would be useful when we check whether the filesystem is ready to write or not without creating any file.
Collect IOStatistics during S3A prefetching ,13436474,Resolved,Major,Fixed,29/Mar/22 16:13,26/Jul/22 11:05,3.4.0,"There is a lot more happening in reads, so there's a lot more data to collect and publish in IO stats for us to view in a summary at the end of processes as well as get from the stream while it is active.

Some useful ones would seem to be:

counters
 * is in memory. using 0 or 1 here lets aggregation reports count total #of memory cached files.
 * prefetching operations executed
 * errors during prefetching

gauges
 * number of blocks in cache
 * total size of blocks
 * active prefetches
+ active memory used

duration tracking count/min/max/ave
 * time to fetch a block
 * time queued before the actual fetch begins
 * time a reader is blocked waiting for a block fetch to complete

and some info on cache use itself
 * number of blocks discarded unread
 * number of prefetched blocks later used
 * number of backward seeks to a prefetched block
 * number of forward seeks to a prefetched block

the key ones I care about are
 # memory consumption
 # can we determine if cache is working (reads with cache hit) and when it is not (misses, wasted prefetches)
 # time blocked on executors

The stats need to be accessible on a stream even when closed, and aggregated into the FS. once we get per-thread stats contexts we can publish there too and collect in worker threads for reporting in task commits"
"getTrashRoot/s in ViewFileSystem should return viewFS path, not targetFS path",13430788,Resolved,Major,Fixed,26/Feb/22 01:24,14/Mar/22 18:33,2.10.2,"It is probably incorrect that we return a targetFS path from getTrashRoot() in ViewFileSystem, as that path will be used later on by ViewFileSystem in other operations, such as rename. ViewFileSystem is assuming the path that it receives is a viewFS path, but not a target FS path. For example, rename() in ViewFileSystem will call getUriPath() for src/dst path, which will remove the scheme/authority and then try to resolve the path-only component. It thus sometimes leads to incorrect path resolution, as we are doing the path resolution again on a targetFS path. 

 

On the other hand, it is not always trivial/feasible to determine the correct viewFS path for a given trash root in targetFS path. 

Example:

Assume we have a mount point for /user/foo -> abfs:/containerA

User foo calls getTrashRoot(""/a/b/c"") and ""/a/b/c"" does not match any mount point. We fall back to the fallback hdfs, which by default returns hdfs://localhost/user/foo/.Trash. In this case, it is incorrect to return the trash root as viewfs:/user/foo, as it will be resolved to the abfs mount point, instead of the fallback hdfs.

  "
getDelegationTokens in ViewFs should also fetch the token from the fallback FS,13435173,Resolved,Major,Fixed,22/Mar/22 17:50,31/Mar/22 22:21,2.10.2,"getDelegationTokens in ViewFs does not include the delegationToken from the fallback FS, while it should. "
ABFS: Add changes for expect hundred continue header with append requests,13430917,Resolved,Major,Fixed,28/Feb/22 05:56,28/Mar/23 15:32,3.3.5," Heavy load from a Hadoop cluster lead to high resource utilization at FE nodes. Investigations from the server side indicate payload buffering at Http.Sys as the cause. Payload of requests that eventually fail due to throttling limits are also getting buffered, as its triggered before FE could start request processing.

Approach: Client sends Append Http request with Expect header, but holds back on payload transmission until server replies back with HTTP 100. We add this header for all append requests so as to reduce.

We made several workload runs with and without hundred continue enabled and the overall observation is that :-
 # The ratio of TCP SYN packet count with and without expect hundred continue enabled is 0.32 : 3 on average.
 #  The ingress into the machine at TCP level is almost 3 times lesser with hundred continue enabled which implies a lot of bandwidth save."
ProfileOutputServlet unable to proceed due to NPE,13421784,Resolved,Major,Fixed,10/Jan/22 18:25,12/Jan/22 08:20,3.4.0,"ProfileOutputServlet context doesn't have Hadoop configs available and hence async profiler redirection to output servlet is failing to identify if admin access is allowed:
{code:java}
HTTP ERROR 500 java.lang.NullPointerException
URI:    /prof-output-hadoop/async-prof-pid-98613-cpu-2.html
STATUS:    500
MESSAGE:    java.lang.NullPointerException
SERVLET:    org.apache.hadoop.http.ProfileOutputServlet-58c34bb3
CAUSED BY:    java.lang.NullPointerException
Caused by:
java.lang.NullPointerException
    at org.apache.hadoop.http.HttpServer2.isInstrumentationAccessAllowed(HttpServer2.java:1619)
    at org.apache.hadoop.http.ProfileOutputServlet.doGet(ProfileOutputServlet.java:51)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
    at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.eclipse.jetty.server.Server.handle(Server.java:516)
    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400)
    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645)
    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392)
    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:748){code}"
Basic verification for the release candidate vote,13425529,Resolved,Major,Fixed,29/Jan/22 06:56,07/Feb/22 01:39,3.4.0,"We should provide script for the basic sanity of Hadoop release candidates. It should include:
 * Signature
 * Checksum
 * Rat check
 * Build from src
 * Build tarball from src

 

Although we can include unit test as well, but overall unit test run is going to be significantly higher, and precommit Jenkins builds provide better view of UT sanity."
ViewFileSystem: Add Support for Localized Trash Root,13426039,Resolved,Major,Fixed,01/Feb/22 19:08,11/Feb/22 01:24,3.4.0,"getTrashRoot() in ViewFileSystem calls getTrashRoot() from underlying filesystem, to return the trash root. Most of the time, we get a trash root in user home dir. This can lead to problems when an application wants to delete a file in a mounted point using moveToTrash() in TrashPolicyDefault, because we can not rename across multiple filesystems/hdfs namenodes. 

 

We propose the following extension to getTrashRoot/getTrashRoots in ViewFileSystem: add a flag to return a localized trash root for ViewFileSystem. A localized trash root is a trash root which starts from the root of a mount point (e.g., /mountpointRoot/.Trash/\{user}). 

* If CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is not set to true, or
* when the path p is in a snapshot or an encryption zone, return
* the default trash root in user home dir.
*
* when CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is set to true,
* 1) if path p is mounted from the same targetFS as user home dir,
* return a trash root in user home dir.
* 2) else, return a trash root in the mounted targetFS
*"
Upgrade maven enforcer plugin and relevant dependencies,13429387,Resolved,Major,Fixed,18/Feb/22 18:59,08/Mar/22 08:28,3.4.0,"Maven enforcer plugin's latest version 3.0.0 has some noticeable improvements (e.g. MENFORCER-350, MENFORCER-388, MENFORCER-353) and fixes for us to incorporate. Besides, some of the relevant enforcer dependencies (e.g. extra enforcer rules and restrict import enforcer) too have good improvements.

We should upgrade maven enforcer plugin and the relevant dependencies."
Update junit 5 version due to build issues,13428575,Resolved,Major,Fixed,15/Feb/22 13:12,17/Feb/22 05:06,,"{code:java}
Feb 11, 2022 11:31:43 AM org.junit.platform.launcher.core.DefaultLauncher handleThrowable WARNING: TestEngine with ID 'junit-vintage' failed to discover tests org.junit.platform.commons.JUnitException: Failed to parse version of junit:junit: 4.13.2 at org.junit.vintage.engine.JUnit4VersionCheck.parseVersion(JUnit4VersionCheck.java:54) {code}
[https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3980/1/artifact/out/patch-unit-root.txt]

seems like junit.vintage.version=5.5.1 is incompatible with junit.version=4.13.2

see 2nd answer on [https://stackoverflow.com/questions/59900637/error-testengine-with-id-junit-vintage-failed-to-discover-tests-with-spring]

my plan is to upgrade junit.vintage.version and junit.jupiter.version to 5.8.2

 "
Allow configuration of zookeeper server principal,13430312,Resolved,Major,Fixed,24/Feb/22 00:17,24/Feb/22 23:27,,"Allow configuration of zookeeper server principal.
This would allow the Router to specify the principal."
Boost S3A Stream Read Performance,13436140,Open,Major,,28/Mar/22 11:28,,3.3.2,"calibrate S3A input stream performance against recent applications/data formats and improve where necessary.

HADOOP-18028 is a key part of this, but there are other issues/opertunities

# we could add machine parsable trace-level logging in FSDataInputStream to collect stats on how stream apis are invoked, so collect data from real apps; analyze
# implement those APIs which some apps use (ByteBufferPositionedReadable), not so much for direct implementation as to get better information from the app as its read plan
# the `normal` mode doesn't switch from sequential on forward seeks. Is that always appropriate?
# choose different buffering options when doing whole file IO vs sequential vs random"
Netty version 3.10.6 to be upgraded to handle CVE-2021-43797,13427954,Resolved,Major,Duplicate,11/Feb/22 08:03,12/Jun/23 21:51,3.1.1,"Netty version 3.10.6 to be upgraded to handle CVE-2021-43797, even Netty-4 upgraded to handle this issue.

Please feedback"
Upgrade Netty to 4.1.77.Final,13422138,Resolved,Major,Fixed,12/Jan/22 07:37,27/Jul/22 18:25,3.3.3,"h4. Netty version - 4.1.71 has fix some CVEs.

CVE-2019-20444,
CVE-2019-20445
CVE-2022-24823

Upgrade to latest version.

"
json smart 1.3.2 still appears in Trivy scan of build ,13431024,Open,Major,,28/Feb/22 15:42,,3.3.2,"when building 3.3.2 Hadoop is still failing CVE scans showing the following error. We are unable to use Hadoop with this CVE showing

 
""VulnerabilityID"": ""CVE-2021-31684"", 
""PkgName"": ""net.minidev:json-smart"", 
""PkgPath"": ""...lib/org.apache.hadoop.hadoop-client-runtime-3.3.2.jar"", 
""InstalledVersion"": ""1.3.2"", 
""FixedVersion"": ""2.4.5, 1.3.3"",

 

more specifically

 
""VulnerabilityID"": ""CVE-2021-31684"", 
""PkgName"": ""net.minidev:json-smart"", 
""PkgPath"": "".../lib/com.nimbusds.nimbus-jose-jwt-9.8.1.jar"", 
""InstalledVersion"": ""1.3.2"", 
""FixedVersion"": ""2.4.5, 1.3.3"", 
 "
Add configs to configure minSeekForVectorReads and maxReadSizeForVectorReads,13425977,Resolved,Major,Fixed,01/Feb/22 13:39,29/Apr/22 23:05,,
Vectored IO support for large S3 files. ,13425995,Resolved,Major,Fixed,01/Feb/22 14:37,16/Jun/22 20:12,,This effort would mostly be adding more tests for large files under scale tests and see if any new issue surfaces. 
Handle memory fragmentation in S3 Vectored IO implementation.,13425986,Resolved,Major,Fixed,01/Feb/22 14:02,20/Jun/22 22:21,,"As we have implemented merging of ranges in the S3AInputStream implementation of vectored IO api, it can lead to memory fragmentation. Let me explain by example.

 

Suppose client requests for 3 ranges. 

0-500, 700-1000 and 1200-1500.

Now because of merging, all the above ranges will get merged into one and we will allocate a big byte buffer of 0-1500 size but return sliced byte buffers for the desired ranges.

Now once the client is done reading all the ranges, it will only be able to free the memory for requested ranges and memory of the gaps will never be released for eg here (500-700 and 1000-1200).

 "
Über-jira: S3A Hadoop 3.3.5 features,13420882,Resolved,Major,Fixed,05/Jan/22 15:32,21/Dec/22 12:18,3.3.2,"Everything for s3a connector in hadoop 3.3.5.

followed up by HADOOP-18477"
tune logging of prefetch problems,13436420,Open,Major,,29/Mar/22 12:31,,3.4.0,"we need to think what we want to do here.

* we would want errors to be counted and included in stream iostats
* but we've seen abfs apps flooded with stack traces when a transient network error breaks every prefetch.
* logging through LogExactlyOnce would highlight a problem, but on a long lived process, not show enough, unless the cache manager had a per instance value.

# PrefetchTask needs to be non static and update the CachingBlockManager when there is a problem; it can LogExactlyOnce
# stream statistics need to include cache values (hits, misses, prefech failures, how long futures were queued before execution...)
"
Verify FileUtils.unTar() handling of missing .tar files: Fixes CVE-2022-25168,13429758,Resolved,Major,Fixed,21/Feb/22 17:02,04/Apr/22 12:03,2.10.1,"add a test to verify FileUtils.unTar() of a non .gz fails meaningfully if file isn't present; fix if not.

test both the unix and windows paths.

This patch contains the fix (and tests to verify it) for CVE-2022-25168

[mitre CVE|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25168]

 hadoop branches without YARN-2185 are at risk in yarn downloads; those with the patch in are not


h2. Announcement
{code}
Severity: important

Versions affected:

2.0.0 to 2.10.1, 3.0.0-alpha to 3.2.3, 3.3.0 to 3.3.2

Description:

Apache Hadoop's FileUtil.unTar(File, File) API does not escape the
input file name before being passed to the shell. An attacker can
inject arbitrary commands.

This is only used in Hadoop 3.3
InMemoryAliasMap.completeBootstrapTransfer, which is only ever run by
a local user.

It has been used in Hadoop 2.x for yarn localization, which does
enable remote code execution.

It is used in Apache Spark, from the SQL command ADD ARCHIVE. As the
ADD ARCHIVE command adds new binaries to the classpath, being able to
execute shell scripts does not confer new permissions to the caller.

SPARK-38305. ""Check existence of file before untarring/zipping"", which
is included in 3.3.0, 3.1.4, 3.2.2, prevents shell commands being
executed, regardless of which version of the hadoop libraries are in
use.


Mitigation:

Users should upgrade to Apache Hadoop 2.10.2, 3.2.4, 3.3.3 or upper
(including HADOOP-18136).

Credit:

Apache Hadoop would like to thank Kostya Kortchinsky for reporting this issue

{code}
"
S3A auditing leaks memory through ThreadLocal references,13424167,Resolved,Major,Fixed,22/Jan/22 15:34,10/Feb/22 12:35,3.3.2,"{{ActiveAuditManagerS3A}} uses thread locals to map to active audit spans, which (because they are wrapped) include back reference to the audit manager instance and the config it was created with.

these *do not* get cleaned up when the FS instance is closed.

if you have a long lived process creating and destroying many FS instances, then memory gets used up.

This fix moves off threadlocal into a map of weak references. while a strong reference is held` (for example in the s3a entry point method) then the references will always resolve. but if those are released then when a GC is triggered these weak references will not be retained, so not use up memory other than entries in the the ha!sh map. the map is held by the s3a auditing integration, so when the fs is closed, everything is freed up.

h3. backport/cherrypicking

if this is backported, followup with HADOOP-18456
"
S3 SDK Upgrade causes AccessPoint ARN endpoint mistranslation,13423328,Resolved,Major,Fixed,18/Jan/22 16:41,04/Feb/22 16:23,3.3.5,"Since upgrading the [SDK to 1.12.132|https://github.com/apache/hadoop/pull/3864] the access point endpoint translation was broken.

Correct endpoints should start with ""s3-accesspoint."", after SDK upgrade they start with ""s3.accesspoint-"" which messes up tests + region detection by the SDK."
hadoop-azure support for the Manifest Committer of MAPREDUCE-7341,13434142,Resolved,Major,Fixed,16/Mar/22 14:55,17/Mar/22 11:52,3.3.5,"Follow-on patch to MAPREDUCE-7341: abfs support and tests

* resilient rename
* tests for job commit through the manifest committer."
TemporaryAWSCredentialsProvider has no credentials,13422031,Resolved,Major,Cannot Reproduce,11/Jan/22 18:10,18/Jan/22 10:04,3.3.1,"Not quite sure how to phrase this bugreport but I'll give it a try..
We are using a SparkSession to access parquet files on AWS/S3

it is ok, if there is only one  s3a URL supplied
it used to be ok if there is a bunch of s3a URLs - that's broken siince hadoop:3.3.1

 

 

I've attached a sample script - yet it relys on spark+hadoop installed "
CVE-2021-0341 in okhttp@2.7.5 detected in hdfs-client  ,13420913,Resolved,Major,Duplicate,05/Jan/22 19:47,22/Apr/22 09:57,3.3.1,"Our static vulnerability scanner (Fortify On Demand) detected [NVD - CVE-2021-0341 (nist.gov)|https://nvd.nist.gov/vuln/detail/CVE-2021-0341#VulnChangeHistorySection] in our application. We traced the vulnerability to a transitive dependency coming from hadoop-hdfs-client, which depends on okhttp@2.7.5 ([hadoop/pom.xml at trunk · apache/hadoop (github.com)|https://github.com/apache/hadoop/blob/trunk/hadoop-project/pom.xml#L137]). To resolve this issue, okhttp should be upgraded to 4.9.2+ (ref: [CVE-2021-0341 · Issue #6724 · square/okhttp (github.com)|https://github.com/square/okhttp/issues/6724])."
Remove use of scala jar twitter util-core with java futures in S3A prefetching stream,13436153,Resolved,Major,Fixed,28/Mar/22 12:34,04/Apr/22 14:46,3.4.0,This jar will cause trouble for scala projects like spark that use different scala versions from the scala 2.11 used in the twitter util-core
Exclude com/jcraft/jsch classes from being shaded/relocated,13424131,Open,Major,,22/Jan/22 09:17,,3.3.1,"Spark 3.2.0 transitively introduces hadoop-client-api and hadoop-client-runtime dependencies.

When we create a SFTPFileSystem instance (org.apache.hadoop.fs.sftp.SFTPFileSystem) it tries to load the relocated classes from _com.jcraft.jsch_ package.

The filesystem instance creation fails with error:
{code:java}
java.lang.ClassNotFoundException: org.apache.hadoop.shaded.com.jcraft.jsch.SftpException
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357) {code}

Excluding client from transitive load of spark and directly using hadoop-common/hadoop-client is the way its working for us."
Upgrade AWS SDK to 1.12.132,13420895,Resolved,Major,Fixed,05/Jan/22 17:08,18/Jan/22 12:23,3.3.2,"With this update, the versions of key shaded dependencies are

  jackson    2.12.3
  httpclient 4.5.13"
S3File to store reference to active S3Object in a field.,13436203,Resolved,Major,Not A Problem,28/Mar/22 16:06,28/Jul/22 12:37,3.4.0,HADOOP-17338 showed us how recent {{S3Object.finalize()}} can call stream.close() and so close an active stream if a GC happens during a read. replicate the same fix here.
Partial/Incomplete groups list can be returned in LDAP groups lookup,13421662,Resolved,Major,Fixed,10/Jan/22 10:30,14/Jul/22 12:54,,"Hello,

The  
{code:java}
Set<String> doGetGroups(String user, int goUpHierarchy) {code}
method in

[https://github.com/apache/hadoop/blob/b27732c69b114f24358992a5a4d170bc94e2ceaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/LdapGroupsMapping.java#L476]

Looks like having an issue if in the middle of the loop a *NamingException* is caught:

The groups variable is not reset in the catch clause and therefore the fallback lookup cannot be executed (when goUpHierarchy==0 at least):
||
{code:java}
if (groups.isEmpty() || goUpHierarchy > 0) {        
    groups = lookupGroup(result, c, goUpHierarchy);
}
{code}
 

Consequence is that only a partial list of groups is returned, which is not correct.

Following options could be used as solution:
 * Reset the group to an empty list in the catch clause, to trigger the fallback query.
 * Add an option flag to enable ignoring groups with Naming Exception (since they are not groups most probably)

Independently, would any issue also occur (and therefore full list cannot be returned) in the first lookup as well as in the fallback query, the method should/could(with option flag) throw an Exception, because in some scenario accuracy is important."
hadoop-yarn-ui has a number of insecure dependencies,13434707,Open,Major,,20/Mar/22 01:00,,,"Many of these are rates as critical or high risk vulnerabilities. This list is the tip of the iceberg.

Examples found by dependabot
* https://github.com/advisories/GHSA-35jh-r3h4-6jhm (lodash-es)
* https://github.com/advisories/GHSA-p6mc-m468-83gw (lodash)
* https://github.com/advisories/GHSA-pc58-wgmc-hfjr (mout)
* https://github.com/advisories/GHSA-4rq4-32rv-6wp6 (shelljs)
* https://github.com/advisories/GHSA-5955-9wpr-37jh (tar)
* https://github.com/advisories/GHSA-765h-qjxv-5f44 (handlebars)
* https://github.com/advisories/GHSA-f2jv-r9rf-7988 (handlebars)
* https://github.com/advisories/GHSA-xfhh-g9f5-x4m4 (socket.io-parser)
* https://github.com/advisories/GHSA-72mh-269x-7mh5 (xmlhttprequest-ssl)
* https://github.com/advisories/GHSA-g78m-2chm-r7qv (websocket-extensions)

may need to upgrade ember to allow some of these NPMs to be updated"
`org.wildfly.openssl` should not be shaded by Hadoop build,13434079,Resolved,Major,Fixed,16/Mar/22 10:50,18/Apr/22 15:31,3.3.1,"`org.wildfly.openssl` is a runtime library and its references are being shaded on Hadoop, breaking the integration with other frameworks like Spark, whenever the ""fs.s3a.ssl.channel.mode"" is set to ""openssl"". The error produced in this situation is:
{code:java}
Suppressed: java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/org/wildfly/openssl/OpenSSLProvider{code}
Whenever it tries to be instantiated from the `DelegatingSSLSocketFactory`. Spark tries to add it to its classpath without the shade, thus creating this issue.

Dependencies which are not on ""compile"" scope should probably not be shaded to avoid this kind of integration issues.

 "
"The CallerContext should not use "":"" as the separator.",13432107,Open,Major,,05/Mar/22 00:10,,,"Since the goal of having fields in the CallerContext is to support adding ip addresses, we need to pick something that is compatible with both ip4 and ip6. "":"" fails that test, since every ip6 address uses it extensively."
Fix failure of create-release script due to releasedocmaker changes in branch-2.10,13432773,Resolved,Major,Fixed,09/Mar/22 05:31,09/Mar/22 15:36,2.10.1,The file name generated by releasedocmaker of Yetus was changed from CHANGES.md to CHANGELOG.md. dev-support scripts should be updated along with the change.
fix bugs when looking up record from upstream DNS servers.,13423879,Open,Major,,21/Jan/22 01:16,,3.1.2,"When query A record which is chained by CNAME, YARN Registry DNS Server does not properly respond. Some CNAME records are missing.

For example, ""repo.maven.apache.org"" is chaned as follows:

repo.maven.apache.org.	21317	IN	CNAME	repo.apache.maven.org.
repo.apache.maven.org.	20114	IN	CNAME	maven.map.fastly.net.
maven.map.fastly.net.	7	IN	A	199.232.192.215
maven.map.fastly.net.	7	IN	A	199.232.196.215

If ask A record for ""repo.maven.apache.org"" using ""dig"" or ""nslookup"", YARN Registry DNS Server will give answers similar to this:
(10.1.2.3, 10.8.8.8 IP is virtual)


{code}
$ nslookup repo.maven.apache.org 10.1.2.3
Server:		10.1.2.3
Address:	10.1.2.3#53

Non-authoritative answer:
repo.maven.apache.org	canonical name = repo.apache.maven.org.
Name:	maven.map.fastly.net
Address: 151.101.196.215
** server can't find repo.apache.maven.org: NXDOMAIN
{code}

The reason why you can see ""NXDOMAIN"", ""nslookup"" will query ""A"" & ""AAAA"" records.
If there is no answer from other dns server, ""answers == null"" but YARN Registry DNS Server has a bug. There is no null handling.


{code:java}
    // Forward lookup to primary DNS servers
    Record[] answers = getRecords(name, type);
    try {
      for (Record r : answers) {
        if (!response.findRecord(r)) {
          if (r.getType() == Type.SOA) {
            response.addRecord(r, Section.AUTHORITY);
          } else {
            response.addRecord(r, Section.ANSWER);
          }
        }
        if (r.getType() == Type.CNAME) {
          Name cname = r.getName();
          if (iterations < 6) {
            remoteLookup(response, cname, type, iterations + 1);
          }
        }
      }
    } catch (NullPointerException e) {
      return Rcode.NXDOMAIN;
    } catch (Throwable e) {
      return Rcode.SERVFAIL;
    }
    return Rcode.NOERROR;
{code}





It should be like this:

{code}
nslookup repo.maven.apache.org 10.8.8.8
Server:		10.8.8.8
Address:	10.8.8.8#53

Non-authoritative answer:
repo.maven.apache.org	canonical name = repo.apache.maven.org.
repo.apache.maven.org	canonical name = maven.map.fastly.net.
Name:	maven.map.fastly.net
Address: 151.101.196.215
{code}

I will make a pull request at https://github.com/apache/hadoop soon.

"
test failures with prefetching s3a input stream,13435890,Resolved,Major,Fixed,25/Mar/22 20:30,05/May/22 10:20,3.4.0,identify and fix all test regressions from the prefetching s3a input stream
document use and architecture design of prefetching s3a input stream,13435893,Resolved,Major,Fixed,25/Mar/22 20:41,26/Apr/22 17:37,3.4.0,"Document S3PrefetchingInputStream for users  (including any new failure modes in troubleshooting) and the architecture for maintainers

there's some markdown in hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/read/README.md already"
ViewFileSystem fails on determining owning group when primary group doesn't exist for user,13427932,Open,Major,,11/Feb/22 06:14,,,ViewFileSystem should not fail on determining owning group when primary group doesn't exist for user
Ensure that default permissions of directories under internal ViewFS directories are the same as directories on target filesystems,13426029,Resolved,Major,Fixed,01/Feb/22 18:02,18/Apr/22 14:08,,"* Ensure that default permissions of directories under internal ViewFS directories are the same as directories on target filesystems
 * Add new unit test"
Fileutil's unzip method causes unzipped files to lose their original permissions,13430850,Resolved,Major,Fixed,27/Feb/22 07:41,30/Mar/22 11:56,2.7.2,"When Spark decompresses the zip file, if the original file has the executable permission, but the unzip method of FileUtil is invoked, the decompressed file loses the executable permission, we should save the original permission"
s3a prefetching stream to move off twitter FuturePool,13435892,Resolved,Major,Duplicate,25/Mar/22 20:36,28/Mar/22 14:32,3.4.0,"This has to be a blocker for the merge I'm afraid: move off twitter's util lib and its future pool

it's not just another jar, its a full scala runtime. and as we know, that's a very brittle runtime. for existing scala code. that's their problem...we don't want to get involved in this

{code}
[INFO] +- com.twitter:util-core_2.11:jar:21.2.0:compile
[INFO] |  +- org.scala-lang:scala-library:jar:2.11.12:compile
[INFO] |  +- com.twitter:util-function_2.11:jar:21.2.0:compile
[INFO] |  +- org.scala-lang.modules:scala-collection-compat_2.11:jar:2.1.2:compile
[INFO] |  +- org.scala-lang:scala-reflect:jar:2.11.12:compile
[INFO] |  \- org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.2:compile

{code}
"
IBM Java detected while running on OpenJDK class library,13435758,Open,Major,,25/Mar/22 09:15,,3.3.2,"In our project we are using hadoop-client library and everything works fine while running inside containers with official OpenJDK base image.

But for optimisation purposes we also use ibm-semeru-runtimes base images ([https://www.ibm.com/support/pages/semeru-runtimes-release-notes] ). To be specific we use *open-17.0.1_12-jre* version of this image and encountered the following problem.

Our application is throwing an exception while using hadoop-client to upload files:
{code:java}
failure to login: javax.security.auth.login.LoginException: No LoginModule found for com.ibm.security.auth.module.JAASLoginModule{code}
 

After a little investigation I found that this login module is selected by Hadoop only when it detects that it is being run on IBM Java (see [https://github.com/apache/hadoop/blob/672e380c4f6ffcb0a6fee6d8263166e16b4323c2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L434] )
{code:java}
  private static String getOSLoginModuleName() {
    if (IBM_JAVA) {
      return ""com.ibm.security.auth.module.JAASLoginModule"";
    } else {
      return windows ? ""com.sun.security.auth.module.NTLoginModule""
        : ""com.sun.security.auth.module.UnixLoginModule"";
    }
  } {code}
 

and IBM Java is detected base on *java.vendor* system property value (see [https://github.com/apache/hadoop/blob/672e380c4f6ffcb0a6fee6d8263166e16b4323c2/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/PlatformName.java#L50] )
{code:java}
  /**
   * The java vendor name used in this platform.
   */
  public static final String JAVA_VENDOR_NAME = System.getProperty(""java.vendor"");

  /**
   * A public static variable to indicate the current java vendor is
   * IBM java or not.
   */
  public static final boolean IBM_JAVA = JAVA_VENDOR_NAME.contains(""IBM"");  {code}
 

I checked inside the ibm-semeru-runtimes based version of our docker container and the *java.vendor* system property is set to the following value:
{code:java}
java.vendor: IBM Corporation {code}
but, as the documentation for IBM Semeru runtimes images says, it contains OpenJDK class libraries with Eclipse OpenJ9 JVM. I confirmed it by running {*}java -version{*}:

 
{code:java}
openjdk version ""17.0.1"" 2021-10-19
IBM Semeru Runtime Open Edition 17.0.1.0 (build 17.0.1+12)
Eclipse OpenJ9 VM 17.0.1.0 (build openj9-0.29.1, JRE 17 Linux amd64-64-Bit Compressed References 20211207_75 (JIT enabled, AOT enabled)
OpenJ9   - 7d055dfcb
OMR      - e30892e2b
JCL      - fc67fbe50a0 based on jdk-17.0.1+12) {code}
therefore there is no {{com.ibm.security.auth.module.JAASLoginModule}} class present.

 

Therefore I would like to ask if there is any other way of detecting IBM java instead of checking the *java.vendor* system property, that would be more accurate and allow using hadoop-client on OpenJDK provided by IBM?

I tried to think about something to suggest - the first thing that came to my mind was to  check if one of the classes from IBM packages actually exists by trying to load it but I don't know the other usages of the *IBM_JAVA* variable in hadoop-client, to be sure that it's a good idea for you.

 "
NameNode Access Time Precision,13435563,Resolved,Major,Invalid,24/Mar/22 12:21,24/Mar/22 18:23,,
Collect disk I/Os time on the node,13435381,Open,Major,,23/Mar/22 14:37,,,"Collect disk I/Os time on SysInfo.

Add IoTimeTracker, calculate a coefficient for each configured mounted disk to describe disk load in periods."
hadoop-common enhancements for the Manifest Committer of MAPREDUCE-7341,13434140,Resolved,Major,Fixed,16/Mar/22 14:47,17/Mar/22 11:51,3.3.5,"Make the necessary changes to hadoop-common to support the manifest committer of 
MAPREDUCE-7341j

* new stats names in StoreStatisticNames (for joint use with s3a committers)
* improvements to IOStatistics
* s3a committer task pool to move over as ThreadPool

this does not break s3a committer as they will only adopt these changes in HADOOP-17833"
Backport HADOOP-13055 into branch-2.10,13428687,Resolved,Major,Fixed,16/Feb/22 03:04,16/Mar/22 00:07,2.10.0,HADOOP-13055 introduce linkMergeSlash and linkFallback for ViewFileSystem. Would be good to backport it to branch-2.10
fail to upload logfile to s3 by flume using hadoop-tools,13431976,Open,Major,,04/Mar/22 11:39,,3.0.0,fail to upload logfile to s3 by flume using hadoop-tools
Fix ITestAuditManagerDisabled after S3A audit logging was enabled in HADOOP-18091,13431631,Resolved,Major,Fixed,03/Mar/22 07:01,03/Mar/22 18:57,3.3.5,"In HADOOP-18094, we disabled audit logging by default and introduced an IT which verifies that by default the audit logging is disabled. After HADOOP-18091, we switched the audit logging back on, but that test remained, which now fails since by default we have the audit logging enabled now.

*StackTrace:*
{noformat}
java.lang.AssertionError: 
Expecting:
  <Service ActiveAuditManagerS3A in state ActiveAuditManagerS3A: STARTED, auditor=LoggingAuditor{ID='38e14ec9-3439-4b8f-a2f6-993514a2ee4a', headerEnabled=true, rejectOutOfSpan=false}}>
to be an instance of:
  <org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A>
but was instance of:
  <org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A>    at org.apache.hadoop.fs.s3a.audit.ITestAuditManagerDisabled.testAuditorDisabled(ITestAuditManagerDisabled.java:60)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.lang.Thread.run(Thread.java:748){noformat}"
ABFS: Fix failure caused by listFiles() in ITestAbfsRestOperationException,13421665,Resolved,Major,Fixed,10/Jan/22 10:52,01/Mar/22 13:51,3.3.2,"testAbfsRestOperationExceptionFormat in ITestAbfsRestOperationException fails due to the wrong exception format of the FileNotFound exception. The test invokes the Filesystem method listFiles(), and the exception thrown is found to be of the GetPathStatus format instead of ListStatus (difference in number of error fields in response).

The Filesystem implementation of listFiles() calls listLocatedStatus(), which then makes a listStatus call. A recent check-in that added implementation for listLocatedStatus() in ABFS driver led to a GetFileStatus request before ListStatus api are invoked, leading to the aberrant FNF exception format. The fix eliminates the GetPathStatus request before ListStatus is called."
ITestAbfsRestOperationException failing,13430997,Resolved,Major,Duplicate,28/Feb/22 13:51,28/Feb/22 14:43,3.4.0,
ABFS: Set driver global timeout for ITestAzureBlobFileSystemBasics,13420978,Resolved,Major,Fixed,06/Jan/22 04:05,23/Feb/22 21:31,3.3.1,"Unlike all other ABFS driver tests that have a timeout of 15min, ITestAzureBlobFileSystemBasics times out after 30s due to the global timeout inherited from FileSystemContractBaseTest. Setting a 15min timeout for the test will ensure sufficient time to allow retries and avoid transient failures.

Example failure:
testListOnFolderWithNoChildren(org.apache.hadoop.fs.azurebfs.contract.ITestAzureBlobFileSystemBasics)  Time elapsed: 34.655 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds"
S3A to support exponential backoff when throttled,13429398,Resolved,Major,Not A Problem,18/Feb/22 21:08,21/Feb/22 07:11,,"S3 API has limits which we can exceed when using a large number of writers/readers/or listers. We should add randomized-exponential back-off to the s3 client when it encounters:

 

com.amazonaws.services.s3.model.AmazonS3Exception: Please reduce your request rate. (Service: Amazon S3; Status Code: 503; Error Code: SlowDown; 

 

 "
hadoop-client-runtime latest version 3.3.1 has security issues,13429194,Resolved,Major,Not A Problem,17/Feb/22 19:23,18/Feb/22 00:19,,"hadoop-client-runtime latest version 3.3.1 ([Maven Repository: org.apache.hadoop » hadoop-client-runtime » 3.3.1 (mvnrepository.com)|https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client-runtime/3.3.1]) has many security issues.
Beside the ones list in maven repo,  it's dependency:

""org.eclipse.jetty_jetty-io"" (9.4.40.v20210413) has [CVE-2021-34429|https://nvd.nist.gov/vuln/detail/CVE-2021-34429] and [CVE-2021-28169|https://nvd.nist.gov/vuln/detail/CVE-2021-28169]

""com.fasterxml.jackson.core_jackson-databind"" (2.10.5.1) has [PRISMA-2021-0213.|https://github.com/FasterXML/jackson-databind/issues/3328]



Need to upgrade to higher version.

 "
AbstractJavaKeyStoreProvider: need a way to read credential store password from Configuration,13420867,Resolved,Major,Invalid,05/Jan/22 15:22,10/Jan/22 10:28,,"Codepath in focus is [this|https://github.com/apache/hadoop/blob/c3006be516ce7d4f970e24e7407b401318ceec3c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java#L316]
{code}
      password = ProviderUtils.locatePassword(CREDENTIAL_PASSWORD_ENV_VAR,
          conf.get(CREDENTIAL_PASSWORD_FILE_KEY));
{code}

Since HIVE-14822, we can use custom keystore that Hiveserver2 propagates to jobs/tasks of different execution engines (mr, tez, spark).
We're able to pass any ""jceks:"" url, but not a password, e.g. on this codepath:
{code}
Caused by: java.security.UnrecoverableKeyException: Password verification failed
	at com.sun.crypto.provider.JceKeyStore.engineLoad(JceKeyStore.java:879) ~[sunjce_provider.jar:1.8.0_232]
	at java.security.KeyStore.load(KeyStore.java:1445) ~[?:1.8.0_232]
	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.locateKeystore(AbstractJavaKeyStoreProvider.java:326) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:86) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.KeyStoreProvider.<init>(KeyStoreProvider.java:49) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:42) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:35) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:68) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:73) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:2409) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:2347) ~[hadoop-common-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getPasswordString(AbfsConfiguration.java:295) ~[hadoop-azure-3.1.1.7.1.7.0-551.jar:?]
	at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getTokenProvider(AbfsConfiguration.java:525) ~[hadoop-azure-3.1.1.7.1.7.0-551.jar:?]
{code}

Even there is a chance of reading a text file, it's not secure, we need to try reading a Configuration property first and if it's null, we can go to the environment variable.
Hacking the System.getenv() is only possible with reflection, doesn't look so good."
Extract method to check two uris are from same hosts,13425879,Open,Major,,01/Feb/22 00:09,,,Extracted the logic out to a new method. Ran existing unit tests. Added a new test
is there any plan to fix the vulnerabilities in hadoop-common,13426019,Open,Major,,01/Feb/22 17:10,,3.3.1,"Hi all, I use a library that is using hadoop-commons as dependency in quite an old version.

anyway I was trying to upgrate it to the latest version and found that still there, there are some problems in hadoop commons.

I can see them even in maven 

[https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.3.1]

 

[CVE-2022-23305|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23305]
[CVE-2022-23302|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23302]
[CVE-2021-4104|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-4104]
[CVE-2021-36374|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-36374]
[CVE-2021-36090|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-36090]
[CVE-2021-35516|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-35516]
[CVE-2021-34429|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-34429]
[CVE-2021-22569|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22569]
[CVE-2020-15522|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15522]

 

Anyway I'm definitely not an expert on this but is there plans to fix this vulnerabilities? 

Or is this library not to be used anymore and we need to migrate to something else?

Thanks for any feedback "
Change default permissions of internal directories in ViewFileSystem to 777,13426315,Open,Major,,03/Feb/22 04:18,,,"Checking write permissions of a path that is an internal directory could fail. Currently, ViewFS sets the permissions of internal directories to 555. Changing this to 777 can solve this issue."
Upgrade bundled Tomcat to 8.5.75,13425711,Resolved,Major,Fixed,31/Jan/22 07:43,01/Feb/22 05:28,2.10.1,Let's upgrade to the latest 8.5.x version.
S3Object must be closed/ref set to null in S3AInputStream.closeStream(),13421741,Resolved,Major,Invalid,10/Jan/22 15:13,25/Jan/22 06:20,3.3.2,
Remove org.checkerframework.dataflow from hadoop-shaded-guava artifact (GNU GPLv2 license),13423692,Resolved,Major,Not A Problem,20/Jan/22 09:09,20/Jan/22 12:55,,"Please refer to TEZ-4378 for further details:
{code}
 jar tf ./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/app/WEB-INF/lib/hadoop-shaded-guava-1.1.1.jar | grep ""dataflow""

org/apache/hadoop/thirdparty/org/checkerframework/dataflow/
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/Deterministic.class
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/Pure$Kind.class
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/Pure.class
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/SideEffectFree.class
org/apache/hadoop/thirdparty/org/checkerframework/dataflow/qual/TerminatesExecution.class
{code}

 I can see that checker-qual LICENSE.txt was removed in the scope of HADOOP-17648, but it has nothing to do with the license itself, only for [resolving a shading error|https://github.com/apache/hadoop-thirdparty/pull/9#issuecomment-822398949]

my understanding is that in the current way an Apache licensed package (guava shaded jar) will contain a GPLv2 licensed software, which makes it a subject of GPLv2, also triggers license violations in security tools (like BlackDuck)"
libhdfspp authentication failed,13420976,Open,Major,,06/Jan/22 03:51,,3.3.0,"I build the example lihdfspp client code in [https://github.com/apache/hadoop/blob/rel/release-3.3.0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/examples/cc/cat/cat.cc]

 

When running, the following error happened. While I can successfully connect to the same cluster using example code from C API from [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/LibHdfs.html]

 

[WARN  ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810402694976][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/namenode_tracker.cc:50]    Nameservice declares more than two nodes.  Some won't be used.

[ERROR ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810357286656][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/rpc_engine.cc:191]    RpcEngine::AsyncRpcCommsError called; status=""AuthenticationFailed"" conn=0x13ab6e0 reqs=1

[WARN  ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810357286656][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/rpc_engine.cc:202]    RpcEngine::RpcCommsError called; status=""AuthenticationFailed"" conn=0x13ab6e0 reqs=1

[WARN  ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810357286656][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/rpc_connection_impl.h:387]    Network error during RPC read: Bad file descriptor

[ERROR ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810357286656][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/rpc_engine.cc:191]    RpcEngine::AsyncRpcCommsError called; status=""Bad file descriptor"" conn=0x13ab6e0 reqs=0

[WARN  ][RPC           ][Thu Jan  6 11:47:12 2022][Thread id = 139810357286656][/tmp/orc/build/libhdfspp_ep-prefix/src/libhdfspp_ep/lib/rpc/rpc_engine.cc:202]    RpcEngine::RpcCommsError called; status=""Bad file descriptor"" conn=0x13ab6e0 reqs=0

Could not connect to jssz-bigdata-proxy-ns1:. AuthenticationFailed"
remove filtering of directory markers in s3a RenameOperation,13426394,Open,Minor,,03/Feb/22 11:22,,3.4.0,"S3A RenameOperation tries to filter out renaming intermediate directory markers so as to avoid retaining them when copying from an authoritiative dir to a non-auth one. we did this to support installations where dir marker retention was only enabled for auth dirs

* nobody, AFAIK, does this..it is all or nothing
* all recent hadoop releases, including earlier branches, is now marker aware
* the extras complexity makes debugging rename problems harder.

let's just cut it out."
S3A connector to improve support for all AWS partitions,13424465,Open,Minor,,24/Jan/22 16:52,,3.3.2,"There are some minor issues in using the S3A connector's more advanced features in china

see https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html

Specifically, that ""arn:aws:"" prefix we use for all arns needs to be configurable so that aws-cn can be used instead.

This means finding where we create and use these in production code (dynamically creating IAM role policies) and in tests, and making it configurable.  

proposed
* add an option {{fs.s3a.aws.partition}}, default aws.
* new StoreContext methods to query this, and create the arn for the current bucket (string concat or from the bucket's ARN if created with an AP ARN)
* docs

I remember ABFS had a problem with oauth endpoints, that was a lot more serious.

Can't think of real tests for this, other than verifying that if you create an invalid partition ""aws-mars"" some things break.

someone needs to run all our existing tests in china, including those with IAM roles and SSE-KMS."
s3a prefetching stream to support unbuffer(),13436219,In Progress,Minor,,28/Mar/22 17:31,,3.4.0,"Apache Impala uses unbuffer() to free up all client side resources held by a stream, so allowing it to have a map of available (path -> stream) objects, retained across queries.

This saves on having to reopen the files, with the cost of HEAD checks etc. S3AInputStream just closes its http connection. here there is a lot more state to discard, but all memory and file storage must be freed.

until this done, ITestS3AContractUnbuffer must skip when the prefetch stream is used.

its notable that the other tests don't fail, even though the stream doesn't implement the interface; the graceful degradation handles that. it should fail if the test xml resource says the stream does it, but that the stream capabilities say it doesn't."
Fix KMS Accept Queue Size default value to 500,13427474,Resolved,Minor,Fixed,09/Feb/22 11:51,14/Sep/22 16:53,3.3.1,"From HADOOP-15638,`hadoop.http.socket.backlog.size` was set as 500 by default ,we can change code default value to keep consistent."
Remove unused import AbstractJavaKeyStoreProvider in Shell class,13420204,Resolved,Minor,Fixed,01/Jan/22 14:41,04/Jan/22 02:30,3.4.0,"In Shell, there are some invalid imports.
For example:
 !image-2022-01-01-22-40-50-604.png! 

Among them, AbstractJavaKeyStoreProvider does not seem to be referenced anywhere."
Add an option to preserve root directory permissions,13427406,Resolved,Minor,Fixed,09/Feb/22 08:22,18/Feb/22 11:13,3.4.0,"As mentioned in https://issues.apache.org/jira/browse/HADOOP-15211

 

If *-update* or *-overwrite* is being passed when *distcp* used, the root directory will be skipped in two occasions (CopyListing#doBuildListing & CopyCommitter#preserveFileAttributesForDirectories), which will ignore root directory's attributes.

 

We face the same issue when distcp huge data between clusters and it takes too much effort to update root directories attributes manually.

 

From the earlier ticket it's obvious why this behaviour is there, but sometime we need to enforce root directory update hence I will add a new option for distcp to enable someone (who understands the need of this and know what they are doing) to enforce the update of root directory's attributes (permissions, ownership, ...)

 

It should be simple one, something like this
{code:java}
$ hadoop distcp -p -update -updateRootDirectoryAttributes /a/b/c /a/b/d {code}
This behaviour is optional and will be *false* by default. (it should not affect existing *distcp* users)."
s3a audit logs to publish range start/end of GET requests in audit header,13436217,Resolved,Minor,Fixed,28/Mar/22 17:17,31/Jul/23 16:39,3.3.2,"we don't get the range of ranged get requests in s3 server logs, because the AWS s3 log doesn't record that information. we can see it's a partial get from the 206 response, but the length of data retrieved is lost.

LoggingAuditor.beforeExecution() would need to recognise a ranged GET and determine the extra key-val pairs for range start and end (rs & re?)

we might need to modify {{HttpReferrerAuditHeader.buildHttpReferrer()}} to take a map of <string, string> so it can dynamically create a header for each request; currently that is not in there."
S3PrefetchingInputStream to support status probes when closed,13436469,Resolved,Minor,Fixed,29/Mar/22 15:58,19/Oct/22 13:38,3.4.0,"S3PrefetchingInputStream is a bit over aggressive on raising exceptions/downgrading responses after a stream is closed

* MUST: getPos() to return last read location, or a least 0 (maybe we should add this to filesystem spec)
* MUST: getIOStatistics(). critical for collecting stats in processes
* MUST: seekToNewSource() (it's a no op anyway)
* MAY S3AInputStreamStatistics and getStreamStatistics() though that is only in used in testing...ioStatistics have given us a stable stats api. we may want to tag the method @VisibleForTesting to discourage use."
Change scope of getRootFallbackLink for InodeTree to make them accessible from outside package,13435655,Resolved,Minor,Fixed,24/Mar/22 20:22,23/May/22 05:10,2.10.2,"Sometimes, we need to access rootFallBackLink in InodeTree from another package. One such case is we extend from ViewFileSystem but want to put the new filesystem in org.apache.hadoop.fs package, instead of org.apache.hadoop.fs.viewfs package. As a result, we need make them public, similar as what we did for getMountPoints() in HADOOP-18100. "
Test coverage for Async profiler servlets,13423933,Resolved,Minor,Fixed,21/Jan/22 08:01,26/Jan/22 03:52,3.4.0,"As discussed in HADOOP-18077, we should provide sufficient test coverage to discover any potential regression in async profiler servlets: ProfileServlet and ProfileOutputServlet."
Add debug log when RPC#Reader gets a Call,13422977,Reopened,Minor,,17/Jan/22 07:58,,2.9.2,"Now there is an important question. That is, we only know when a Call is actually executed by the Handler. By logging, we cannot know when the Call came in.
If I log some information from the moment the Call enters the inside of the RPC, it will help us to know more about the Call.
The records here should be in the form of logs, and the priority of the logs should not be too high, debug is the best."
Increase precommit job timeout from 24 hr to 30 hr,13430359,Patch Available,Minor,,24/Feb/22 07:14,,,"As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here|https://github.com/apache/hadoop/pull/4000] where more than 5 builds timed out after 24 hr). We should increase it to 30 hr."
"Address JavaDoc warnings in classes like MarkerTool, S3ObjectAttributes, etc.",13432619,Resolved,Minor,Fixed,08/Mar/22 11:40,20/Oct/22 16:47,3.3.2,"{noformat}
home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:856: warning: empty <p> tag
[ERROR]    * <p></p>
[ERROR]         ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:150: warning: empty <p> tag
[ERROR]    * <p></p>
[ERROR]         ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:964: warning: no @param for source
[ERROR]     public ScanArgsBuilder withSourceFS(final FileSystem source) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:964: warning: no @return
[ERROR]     public ScanArgsBuilder withSourceFS(final FileSystem source) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:970: warning: no @param for p
[ERROR]     public ScanArgsBuilder withPath(final Path p) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:970: warning: no @return
[ERROR]     public ScanArgsBuilder withPath(final Path p) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:976: warning: no @param for d
[ERROR]     public ScanArgsBuilder withDoPurge(final boolean d) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:976: warning: no @return
[ERROR]     public ScanArgsBuilder withDoPurge(final boolean d) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:982: warning: no @param for min
[ERROR]     public ScanArgsBuilder withMinMarkerCount(final int min) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:982: warning: no @return
[ERROR]     public ScanArgsBuilder withMinMarkerCount(final int min) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:988: warning: no @param for max
[ERROR]     public ScanArgsBuilder withMaxMarkerCount(final int max) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:988: warning: no @return
[ERROR]     public ScanArgsBuilder withMaxMarkerCount(final int max) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:994: warning: no @param for l
[ERROR]     public ScanArgsBuilder withLimit(final int l) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:994: warning: no @return
[ERROR]     public ScanArgsBuilder withLimit(final int l) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:1000: warning: no @param for b
[ERROR]     public ScanArgsBuilder withNonAuth(final boolean b) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:1000: warning: no @return
[ERROR]     public ScanArgsBuilder withNonAuth(final boolean b) {
[ERROR]                            ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:479: warning: no @return
[ERROR]     public int getExitCode() {
[ERROR]                ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:484: warning: no @return
[ERROR]     public DirMarkerTracker getTracker() {
[ERROR]                             ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/tools/MarkerTool.java:489: warning: no @return
[ERROR]     public MarkerPurgeSummary getPurgeSummary() {
[ERROR]                               ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ObjectAttributes.java:69: warning: no description for @param
[ERROR]    * @param path
[ERROR]      ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ObjectAttributes.java:73: warning: no description for @param
[ERROR]    * @param len
[ERROR]      ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java:84: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java:86: warning: empty <p> tag
[ERROR]      * <p></p>
[ERROR]           ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java:72: warning: empty <p> tag
[ERROR]      * <p></p>
[ERROR]           ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirectoryPolicy.java:79: warning: empty <p> tag
[ERROR]      * <p></p>
[ERROR]           ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:131: warning: empty <p> tag
[ERROR]    * <p></p>
[ERROR]         ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:226: warning: empty <p> tag
[ERROR]    * <p></p>
[ERROR]         ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:37: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:41: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:45: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:50: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DirMarkerTracker.java:109: warning: empty <p> tag
[ERROR]    * <p></p>
[ERROR]         ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/OperationCallbacks.java:129: warning: no @param for destKey
[ERROR]   CopyResult copyFile(String srcKey,
[ERROR]              ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java:56: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java:60: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java:65: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java:74: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java:77: warning: empty <p> tag
[ERROR]  * <p></p>
[ERROR]       ^
[ERROR] 
[ERROR] Command line was: /usr/lib/jvm/java-8-openjdk-amd64/jre/../bin/javadoc @options @packages
[ERROR] 
[ERROR] Refer to the generated Javadoc files in '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4045@2/ubuntu-focal/src/hadoop-tools/hadoop-aws/target/site/apidocs' dir.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException{noformat}"
The FSDownload verifyAndCopy method doesn't support S3,13431307,Resolved,Minor,Invalid,01/Mar/22 15:50,17/Apr/23 19:19,,"The modification time comparison in FSDownload's verifyAndCopy method fails for S3, which prohibits distributed cache files being loaded from S3. This change allows S3 to be supported via a config change, that would replace the IO Exception with a warning log entry."
Verify FileUtils.unTar() handling of missing .tgz files.,13429759,Resolved,Minor,Duplicate,21/Feb/22 17:03,04/Apr/22 12:03,3.3.1,"add a test to verify FileUtils.unTar() of a non .gz fails meaningfully if file isn't present; fix if not.

test both the unix and windows paths.

"
 move org.apache.hadoop.fs.common package into hadoop-common module,13436189,Resolved,Minor,Fixed,28/Mar/22 14:49,17/Aug/22 08:54,3.4.0,"move org.apache.hadoop.fs.common package from hadoop-aws, along with any tests, into hadoop-common jar and the+ package org.apache.hadoop.fs.impl

(except for any bits we find are broadly useful in applications to use any new APIs, in which case somewhere more public, such as  o.a.h.util.functional for the futures work)

we can and should pick on new package and move the classes there, even while they are in hadoop-aws. why so? lets us add checkstyle/findbugs rules with the final classnames"
Convert s3a prefetching to use JavaDoc for fields and enums,13436437,Resolved,Minor,Fixed,29/Mar/22 13:15,17/Aug/22 08:52,3.4.0,There's lots of good comments for fields and enum values in the current code. Let's expose these to your IDE with JavaDoc instead.
ITestMarkerTool.testRunLimitedLandsatAudit failing due to most of bucket content purged,13435127,Resolved,Minor,Fixed,22/Mar/22 13:43,03/May/22 13:56,3.3.5,"{{ITestMarkerTool.testRunLimitedLandsatAudit}} is failing -a scan which was meant to stop after the first page of results is finishing because there aren't so many objects there.
 
first visible sign of the landsat-pds cleanup

now we have requester pays, we could do this against another store with stability promises, e.g common crawl.

This test failure is fixed in branch-3.3 and trunk: don't worry about the test failing in earlier releases"
Backport HADOOP-12760 to 2.10,13434242,Open,Minor,,17/Mar/22 02:02,,2.10.0,
S3AFileSystem cannot find a fake dir on minio,13432745,Resolved,Minor,Duplicate,09/Mar/22 04:06,10/Mar/22 13:55,3.2.2,"I am using spark and delta lake for reading and writing datasets. After I upgraded the hadoop version, I found that writing files reported an error, indicating that the folder does not exist.

{code:java}
    java.io.FileNotFoundException: No such file or directory: s3a://table_cache/t5ffe773c987844cbba0aafd/_delta_log
        at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3356)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)
        at org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:120)
        at org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.exists(S3SingleDriverLogStore.scala:156)
        at org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.write(S3SingleDriverLogStore.scala:174)
        at org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommit(OptimisticTransaction.scala:742)
...
{code}
When this folder has been created, the error is still reported.

The related commit is 49f8ae96, It delete some code that dealing with Fake directory.


{code:java}
// Either a normal file was not found or the probe was skipped.
      // because the key ended in ""/"" or it was not in the set of probes.
      // Look for the dir marker
      if (probes.contains(StatusProbeEnum.DirMarker)) {
        String newKey = maybeAddTrailingSlash(key);
        try {
          ObjectMetadata meta = getObjectMetadata(newKey);

          if (objectRepresentsDirectory(newKey, meta.getContentLength())) {
            LOG.debug(""Found file (with /): fake directory"");
            return new S3AFileStatus(Tristate.TRUE, path, username);
          } else {
            LOG.warn(""Found file (with /): real file? should not happen: {}"",
                key);

            return new S3AFileStatus(meta.getContentLength(),
                    dateToLong(meta.getLastModified()),
                    path,
                    getDefaultBlockSize(path),
                    username,
                    meta.getETag(),
                    meta.getVersionId());
          }
        } catch (AmazonServiceException e) {
          if (e.getStatusCode() != SC_404 || isUnknownBucket(e)) {
            throw translateException(""getFileStatus"", newKey, e);
          }
        } catch (AmazonClientException e) {
          throw translateException(""getFileStatus"", newKey, e);
        }
      }
    }
{code}


"
[WASB] Retry not getting implemented when using wasb scheme in hadoop-azure 2.7.4 ,13434101,Open,Minor,,16/Mar/22 12:02,,2.7.4,"I am using prestodb to read data from blob.
Presto is using  hadoop-azure-2.7.4 jar.

I'm using *wasb* scheme to query the data on blob. I'm afraid for some reason the hadoop-azure library is not retrying when getting IO exception.

Attaching the stack trace below,
{code:java}
com.facebook.presto.spi.PrestoException: Error reading from wasb://oemdpv3prd-v1@oemdpv3prd.blob.core.windows.net/data/pipelines/hudi/kafka/telemetrics_v2/dp.hmi.quectel.bms.data.packet.v2/dt=2022-01-15/e576abc3-942a-434d-be02-6899798258eb-0_5-13327-290407_20220115211203.parquet at position 65924529
    at com.facebook.presto.hive.parquet.HdfsParquetDataSource.readInternal(HdfsParquetDataSource.java:66)
    at com.facebook.presto.parquet.AbstractParquetDataSource.readFully(AbstractParquetDataSource.java:60)
    at com.facebook.presto.parquet.AbstractParquetDataSource.readFully(AbstractParquetDataSource.java:51)
    at com.facebook.presto.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:247)
    at com.facebook.presto.parquet.reader.ParquetReader.readColumnChunk(ParquetReader.java:330)
    at com.facebook.presto.parquet.reader.ParquetReader.readBlock(ParquetReader.java:313)
    at com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:182)
    at com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:160)
    at com.facebook.presto.common.block.LazyBlock.assureLoaded(LazyBlock.java:291)
    at com.facebook.presto.common.block.LazyBlock.getLoadedBlock(LazyBlock.java:282)
    at com.facebook.presto.operator.ScanFilterAndProjectOperator$RecordingLazyBlockLoader.load(ScanFilterAndProjectOperator.java:314)
    at com.facebook.presto.operator.ScanFilterAndProjectOperator$RecordingLazyBlockLoader.load(ScanFilterAndProjectOperator.java:300)
    at com.facebook.presto.common.block.LazyBlock.assureLoaded(LazyBlock.java:291)
    at com.facebook.presto.common.block.LazyBlock.getLoadedBlock(LazyBlock.java:282)
    at com.facebook.presto.operator.project.InputPageProjection.project(InputPageProjection.java:69)
    at com.facebook.presto.operator.project.PageProjectionWithOutputs.project(PageProjectionWithOutputs.java:56)
    at com.facebook.presto.operator.project.PageProcessor$ProjectSelectedPositions.processBatch(PageProcessor.java:323)
    at com.facebook.presto.operator.project.PageProcessor$ProjectSelectedPositions.process(PageProcessor.java:197)
    at com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)
    at com.facebook.presto.operator.WorkProcessorUtils$YieldingIterator.computeNext(WorkProcessorUtils.java:79)
    at com.facebook.presto.operator.WorkProcessorUtils$YieldingIterator.computeNext(WorkProcessorUtils.java:65)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:141)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:136)
    at com.facebook.presto.operator.project.MergingPageOutput.getOutput(MergingPageOutput.java:113)
    at com.facebook.presto.operator.ScanFilterAndProjectOperator.processPageSource(ScanFilterAndProjectOperator.java:295)
    at com.facebook.presto.operator.ScanFilterAndProjectOperator.getOutput(ScanFilterAndProjectOperator.java:242)
    at com.facebook.presto.operator.Driver.processInternal(Driver.java:418)
    at com.facebook.presto.operator.Driver.lambda$processFor$9(Driver.java:301)
    at com.facebook.presto.operator.Driver.tryWithLock(Driver.java:722)
    at com.facebook.presto.operator.Driver.processFor(Driver.java:294)
    at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1077)
    at com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:162)
    at com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:599)
    at com.facebook.presto.$gen.Presto_0_256_2ed7f73____20220112_133056_1.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: undefined
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:598)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.BlobInputStream.dispatchRead(BlobInputStream.java:264)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.BlobInputStream.readInternal(BlobInputStream.java:448)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.BlobInputStream.read(BlobInputStream.java:420)
    at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
    at java.io.DataInputStream.read(DataInputStream.java:149)
    at org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream.read(NativeAzureFileSystem.java:735)
    at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:65)
    at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:78)
    at org.apache.hadoop.fs.BufferedFSInputStream.readFully(BufferedFSInputStream.java:113)
    at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:107)
    at com.facebook.presto.hive.parquet.HdfsParquetDataSource.readInternal(HdfsParquetDataSource.java:58)
    ... 36 more
Caused by: com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.StorageException: Blob hash mismatch (integrity check failed), Expected value is gRfj04hQvdOqOaAdqo+lGw==, retrieved 4V4j3H2KabAsssDyGnJN9w==.
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.CloudBlob$10.postProcessResponse(CloudBlob.java:1714)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.CloudBlob$10.postProcessResponse(CloudBlob.java:1613)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:148)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.CloudBlob.downloadRangeInternal(CloudBlob.java:1468)
    at com.facebook.presto.hadoop.$internal.com.microsoft.azure.storage.blob.BlobInputStream.dispatchRead(BlobInputStream.java:255)
    ... 47 more{code}
Please suggest a way to get the retry implemented.


I want the retry to be implemented if there is an IO exception while calling *read* method below, 
{code:java}
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream.read(NativeAzureFileSystem.java:735){code}"
ViewFileSystem#getUri should return a URI with a empty path component,13427851,Open,Minor,,10/Feb/22 18:00,,,"The goal here is that the semantics of {{FileSystem#getURI()}} remains the same as before when using ViewFileSystem. 

We should assume that the URI returned by {{FileSystem#getURI()}} doesn't have a Path component (i.e., path component is null). DistributedFileSystem follows this pattern. However, ViewFileSystem add ""/"" as the path component in the URI returned."
Change scope of InodeTree and inner fields of ViewFileSystem ,13428108,Open,Minor,,11/Feb/22 23:28,,,Change scope of InodeTree and inner fields of ViewFileSystem to make them accessible from outside package.
GCS connector in core-default.xml file missing,13427168,Open,Minor,,08/Feb/22 11:19,,3.2.2," 

{{<property>
    <name>fs.gs.impl</name>
    <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
    <description>The FileSystem for gs: (GCS) uris.</description>
</property>
<property>
    <name>fs.AbstractFileSystem.gs.impl</name>
    <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>
    <description>
     The AbstractFileSystem for gs: (GCS) uris. Only necessary for use with Hadoop 2.
    </description>
</property>}}

{{}}

{{these config are missing from core-default.xml which is present inside hadoop-common-3.2.2.jar file, which is causing the ssl enabled solr to not communicate with GCS.}}

{{}}

{{By adding these class in the jar file , we will be able to access the gcs directory as the connection between the connector and HDFS will be established.}}

{{}}

{{}}

{{}}"
"StagingCommitter getFinalKey method can add an extra / if getS3KeyPrefix returns """"",13425282,Open,Minor,,28/Jan/22 00:00,,3.3.1,"I am trying to test staging committer against an on prem object store using spark terasort and ran into this issue. All my initiate MPU were failing with S3 error key not found. This object store doesn't support virtual host style request, so I had path style enabled. After adding some extra debug and building hadoop-aws locally, I found that staging committer was always adding a '/' prefix to my key.  

 

So instead of part part-r-00000-4ead11c8-bc20-4dee-9753-1b1f1ae4e578 I would end up with /part-r-00000-4ead11c8-bc20-4dee-9753-1b1f1ae4e578. I traced it to getFinalKey in StagingCommitter.java which had the following code

 
 *      return getS3KeyPrefix(context) + ""/""
-          + Paths.addUUID(relative, getUUID());

If getS3KeyPrefix(context) is """", then we end up with /part-r... as the key. 

 

I made the following change locally and was able to resolve the issue

 

 

---

diff --git a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java
index 59114f7ab73..6d76cf2d419 100644
--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java
+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java
@@ -365,11 +365,16 @@ public Path getTempTaskAttemptPath(TaskAttemptContext context) {
    * @return the S3 key where the file will be uploaded
    */
   protected String getFinalKey(String relative, JobContext context) {
+    StringBuilder sb = new StringBuilder();
+    final String pfx = getS3KeyPrefix(context);
+    if (!pfx.isEmpty()) {
+        sb.append(pfx).append('/');
+    }
+
     if (uniqueFilenames) {
-      return getS3KeyPrefix(context) + ""/""
-          + Paths.addUUID(relative, getUUID());
+        return sb.append(Paths.addUUID(relative, getUUID())).toString(); 
     } else {
-      return getS3KeyPrefix(context) + ""/"" + relative;
+        return sb.append(relative).toString();
     }
   }"
ExecutorHelper.logThrowableFromAfterExecute() is too noisy. ,13420762,Resolved,Minor,Fixed,05/Jan/22 09:16,06/Jan/22 09:58,3.3.1,"{code:java}
if (t == null && r instanceof Future<?> && ((Future<?>) r).isDone()) {
  try {
    ((Future<?>) r).get();
  } catch (ExecutionException ee) {
    LOG.warn(
        ""Execution exception when running task in "" + Thread.currentThread()
            .getName());
    t = ee.getCause();
  } catch (InterruptedException ie) {
    LOG.warn(""Thread ("" + Thread.currentThread() + "") interrupted: "", ie);
    Thread.currentThread().interrupt();
  } catch (Throwable throwable) {
    t = throwable;
  }
}

if (t != null) {
  LOG.warn(""Caught exception in thread "" + Thread
      .currentThread().getName() + "": "", t);
} {code}
We should downgrade the logging here from warn to debug.

 

CC [~stevel@apache.org]  [~mehakmeetSingh] "
ABFS: Add testfilePath while verifying test contents are read correctly,13423243,Resolved,Minor,Fixed,18/Jan/22 10:51,19/Jan/22 10:19,3.3.2,Add testfilePath while verifying test contents are read correctly for any file.
Better exception handling for testFileStatusOnMountLink() in ViewFsBaseTest.java,13424273,Resolved,Trivial,Fixed,24/Jan/22 06:02,25/Jan/22 14:10,2.10.2,"The following exception handling code section in testFileStatusOnMountLink() 

 
{code:java}
    try {
      fcView.getFileStatus(new Path(""/danglingLink""));
      Assert.fail(""Excepted a not found exception here"");
    } catch ( FileNotFoundException e) {
      // as excepted
    }{code}
can be replaced with a single in the @Test 
{code:java}
@Test(expected = FileNotFoundException.class){code}"
Update default value of hadoop.ssl.enabled.protocols in the EncryptedShuffle doc,13430339,Resolved,Trivial,Duplicate,24/Feb/22 05:57,24/May/22 20:13,2.8.4,"The default value of hadoop.ssl.enabled.protocols was updated several times but our doc was never updated.

https://hadoop.apache.org/docs/r3.1.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html
"
Refactor tests in TestFileUtil,13432485,Resolved,Trivial,Fixed,07/Mar/22 17:02,18/Apr/22 14:16,3.4.0,"We need to ensure that we check the results of file operations whenever we invoke *mkdir*, *deleteFile* etc. and assert them right there before proceeding on. Also, we need to ensure that some of the relevant FileSystem APIs don't return null."
Change scope of fixFileStatus,13427892,Open,Trivial,,11/Feb/22 00:00,,,"* Change scope of fixFileStatus
* Fix path in fileStatus for internal directories"
Documentation Syntax Error Fix > AWS Assumed Roles,13426545,Resolved,Trivial,Fixed,04/Feb/22 04:14,09/Feb/22 10:51,,"Configuration property ""fs.s3a.assumed.role.credentials.provider"" was mis-documented without the ""f"" in front as ""s.s3a.assumed.role.credentials.provider""

PR already exists and will update the title to reflect this ticket.
https://github.com/apache/hadoop/pull/3949"
EvaluatingStatisticsMap::entrySet may not need parallelstream,13426556,Open,Trivial,,04/Feb/22 05:46,,,"When large number of S3AInputStreams are opened, it ends up showing up in profile, as internally parallelstream makes use of fork and join. If it is not mandatory, it can be refactored to get rid of parallelstream. Here is the relevant profile output for ref.

  !Screenshot 2022-02-04 at 11.10.39 AM.png|width=632,height=429!

 "
replace URL.toString() with URL.toURI().toString(),13420639,Resolved,Trivial,Invalid,04/Jan/22 18:26,04/Jan/22 19:36,3.3.1,"Some parts of the code, mostly logging and exception messages, call URL.toString().
They should use URL.toURI().toString() to avoid network IO.

This is straightforward to do in the explicit and implicit invocations in our code. It will not detect/fix the case where a URL instance is passed as an argument to logger methods. That will have to be out of scope
"
Add some Abortable.abort() interface for streams etc which can be terminated,13289626,Resolved,Blocker,Fixed,04/Mar/20 20:22,27/Feb/21 10:56,3.3.0,"Some IO we want to be able to abort rather than close cleanly, especially if the inner stream is an HTTP connection which itself supports some abort() method. For example: uploads to an object where we want to cancel the upload without close() making an incomplete write visible.

Proposed: Add a generic interface which things like streams can implement
{code}
AbortableIO {
  public void abortIO() throws IOE;
}
{code}

+do for s3a output stream. I wouldn't do this a passthrough on FSDataOutputStream because we need to consider what expectations callers have of an operation being ""aborted""
"
Update dependency in branch-3.1,13290978,Resolved,Blocker,Won't Fix,11/Mar/20 04:11,13/Dec/21 01:55,3.1.4,"Jackson-databind 2.9.10.3 --> 2.10.3
Zookeeper 3.4.13 --> 3.4.14
hbase-client 1.2.6 --> 1.2.6.1
aws-java-sdk-bundle 1.11.271 --> 1.11.563? (this is the version used by trunk)"
Increase entropy to improve cryptographic randomness on precommit Linux VMs,13280162,In Progress,Blocker,,17/Jan/20 15:05,,,"I was investigating a JUnit test (MAPREDUCE-7079 :TestMRIntermediateDataEncryption is failing in precommit builds) that was consistently hanging on Linux VMs and failing Mapreduce pre-builds.
I found that the test hangs slows or hangs indefinitely whenever Java reads the random file.

I explored two different ways to get that test case to work properly on my local Linux VM running rel7:
# To install ""haveged"" and ""rng-tools"" on the virtual machine running Rel7. Then, start rngd service {{sudo service rngd start}} . This will fix the problem for all the components on the image including java, native and any other component.
# Change java configuration to load urandom
{code:bash}
sudo vim $JAVA_HOME/jre/lib/security/java.security
## Change the line “securerandom.source=file:/dev/random” to read: securerandom.source=file:/dev/./urandom
{code}

The first solution is better because this will fix the problem for everything that requires SSL/TLS or other services that depend upon encryption.

Since the precommit build runs on Docker, then it would be best to mount {{/dev/urandom}} from the host as {{/dev/random}} into the container:

{code:java}
docker run -v /dev/urandom:/dev/random
{code}

For Yetus, we need to add the mount to the {{DOCKER_EXTRAARGS}} as follows:

{code:java}
DOCKER_EXTRAARGS+=(""-v"" ""/dev/urandom:/dev/random"")
{code}

 ..."
"Bug in widely-used helper function caused valid configuration value to fail on multiple tests, causing build failure",13282822,Patch Available,Blocker,,02/Feb/20 07:48,,3.2.1,"{code:java}
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths1
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryUnknownLength
org.apache.hadoop.io.file.tfile.TestTFileLzoCodecsStreams#testOneEntryMixedLengths1
org.apache.hadoop.io.file.tfile.TestTFileLzoCodecsStreams#testOneEntryUnknownLength{code}
 

4 actively-used tests above call the helper function `TestTFileStreams#writeRecords()` to write key-value pairs (kv pairs), then call `TestTFileByteArrays#readRecords()` to assert the key and the value part (v) of these kv pairs matched with what they wrote. All v of kv pairs are hardcode strings with a length of 6.

 

`readRecords()` uses `org.apache.hadoop.io.file.tfile.TFile.Reader.Scanner.Entry#getValueLength()` to get full length of the v of these kv pairs.  But `getValueLength()` can only get v's full length when it is less than the value of configuration parameter `tfile.io.chunk.size`, otherwise `readRecords()` will throw an exception. So, *when `tfile.io.chunk.size` is configured/set to a value less than 6, these 4 tests failed because of the exception from `readRecords()`, even 6 is a valid value for `tfile.io.chunk.size`.*

The definition of `tfile.io.chunk.size` is ""Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown())"". 

*Fixes*

`readRecords()` should call `org.apache.hadoop.io.file.tfile.TFile.Reader.Scanner.Entry#getValue(byte[])` instead, which returns the correct full length of the `value` part despite whether the value's length is larger than `tfile.io.chunk.size`."
Update Netty to 4.1.46Final,13289655,Resolved,Blocker,Duplicate,04/Mar/20 23:20,10/Mar/20 14:10,,"https://nvd.nist.gov/vuln/detail/CVE-2019-20444
{quote}
HttpObjectDecoder.java in Netty before 4.1.44 allows an HTTP header that lacks a colon, which might be interpreted as a separate header with an incorrect syntax, or might be interpreted as an ""invalid fold.""
{quote}
"
Upgrade jackson-databind to 2.9.10.3,13288139,Resolved,Blocker,Fixed,27/Feb/20 19:49,29/Feb/20 01:08,,"New [RCE|https://nvd.nist.gov/vuln/detail/CVE-2020-8840] found in jackson-databind 2.0.0 through 2.9.10.2.

Patched in 2.9.10.3. [Looks critical|https://github.com/jas502n/CVE-2020-8840/blob/master/Poc.java#L13].

After HADOOP-16882 get in we should backport this to those lower-version branches ASAP."
"Update jackson-databind to 2.9.10.2 in branch-3.1, branch-2.10",13287505,Resolved,Blocker,Fixed,25/Feb/20 13:57,29/Feb/20 01:00,,"We updated jackson-databind multiple times but those changes only made into trunk and branch-3.2.

Unless the dependency update is backward incompatible (which is not in this case), we should update them in all active branches"
Upgrade jackson-databind to 2.9.10.2,13279500,Resolved,Blocker,Fixed,15/Jan/20 06:20,15/Jan/20 10:56,,"CVE-2019-20330 is reported and fixed in jackson-databind 2.9.10.2.
https://nvd.nist.gov/vuln/detail/CVE-2019-20330"
Hadoop resource manager jdk 8 dependency,13277398,Resolved,Blocker,Not A Bug,03/Jan/20 16:35,03/Jan/20 16:49,2.10.0,"When I am trying to create a single cluster Hadoop environment, I cannot load the Node Manager or Resource Manager. From the log I get the error:

Caused by: java.lang.ClassNotFoundException: javax.activation.DataSourceCaused by: java.lang.ClassNotFoundException: javax.activation.DataSource at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:602) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)

... 83 more

I gather this is from a dependency on javax.activation from JDK8. The only online solution I could find involved changing these two lines in the yarn-env.sh file in the usr/local/Cellar/hadoop/\{version}/libexec/etc/hadoop folder:

export YARN_RESOURCEMANAGER_OPTS=""--add-modules=ALL-SYSTEM""

export YARN_NODEMANAGER_OPTS=""--add-modules=ALL-SYSTEM""

This did not work and I continue to get the same error in the log when trying to run the YARN manager. Can anyone advise as to how to get around this dependency?"
pylint fails in the build environment,13295051,Resolved,Critical,Fixed,30/Mar/20 17:01,04/Apr/20 08:00,,"Hadoop-side of YETUS-960
{noformat}
$ ./start-build-env.sh
(snip)
aajisaka@21a025f08d49:~/hadoop$ pylint
Traceback (most recent call last):
  File ""/usr/local/bin/pylint"", line 11, in <module>
    sys.exit(run_pylint())
  File ""/usr/local/lib/python2.7/dist-packages/pylint/__init__.py"", line 15, in run_pylint
    from pylint.lint import Run
  File ""/usr/local/lib/python2.7/dist-packages/pylint/lint.py"", line 67, in <module>
    from pylint import checkers
  File ""/usr/local/lib/python2.7/dist-packages/pylint/checkers/__init__.py"", line 45, in <module>
    from pylint.config import OptionsProviderMixIn
  File ""/usr/local/lib/python2.7/dist-packages/pylint/config.py"", line 49, in <module>
    import configparser
ImportError: No module named configparser
{noformat}"
Possible inconsistent state of AbstractDelegationTokenSecretManager,13280684,Patch Available,Major,,21/Jan/20 09:12,,3.3.0,"[AbstractDelegationTokenSecretManager.updateCurrentKey|https://github.com/apache/hadoop/blob/581072a8f04f7568d3560f105fd1988d3acc9e54/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java#L360] increments the current key id and creates the new delegation key in two distinct synchronized blocks.

This means that other threads can see the class in an *inconsistent state, where the key for the current key id doesn't exist (yet)*.

For example the following method sometimes returns null when the token remover thread is between the two synchronized blocks:
{noformat}
@Override
public DelegationKey getCurrentKey() {
  return getDelegationKey(getCurrentKeyId());
}{noformat}
 

Also it is possible that updateCurrentKey is called from multiple threads at the same time so *distinct keys can be generated with the same key id*.

 

This issue is suspected to be the cause of the intermittent failure of  [TestLlapSignerImpl.testSigning|https://github.com/apache/hive/blob/3c0705eaf5121c7b61f2dbe9db9545c3926f26f1/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java#L195] - HIVE-22621."
Use spotbugs-maven-plugin instead of findbugs-maven-plugin,13286148,Resolved,Major,Fixed,19/Feb/20 04:48,29/Mar/21 00:45,3.3.1,findbugs-maven-plugin is no longer maintained. Use spotbugs-maven-plugin instead.
[JDK13] Support HTML5 Javadoc,13292627,Resolved,Major,Fixed,19/Mar/20 06:09,04/Sep/24 10:51,3.4.0,"javadoc -html4 option has been removed since Java 13.
https://bugs.openjdk.java.net/browse/JDK-8215578"
Unable to build 3.1.3 on Win 10 - missing native header files.  Error (compile-ms-native-dll) on project hadoop-common,13280457,Open,Major,,20/Jan/20 02:32,,3.1.3,"Maven build Error (compile-ms-native-dll) on project hadoop-common

mvn package -Pdist,native -DskipTests -Dmaven.javadoc.skip=true

Missing numerous native C file headers, such as:

org_apache_hadoop_io_compress_lz4_Lz4Compressor.h

org_apache_hadoop_io_compress_lz4_Lz4Decompressor.h

org_apache_hadoop_io_nativeio_NativeIO.h

org_apache_hadoop_io_nativeio_NativeIO_POSIX.h

org_apache_hadoop_security_JniBasedUnixGroupsMapping.h

org_apache_hadoop_util_NativeCrc32.h

 

 "
Provide source artifacts for hadoop-client-api,13280728,Resolved,Major,Fixed,21/Jan/20 10:30,13/May/21 12:42,3.2.3,"h5. Improvement request
The third-party libraries shading hadoop-client-api (& hadoop-client-runtime) artifacts are super useful.
 
Having uber source jar for hadoop-client-api (maybe even hadoop-client-runtime) would be great for downstream development & debugging purposes.

Are there any obstacles or objections against providing fat jar with all the hadoop client api as well ?

h5. Dev links
- *maven-shaded-plugin* and its *shadeSourcesContent* attribute
- https://maven.apache.org/plugins/maven-shade-plugin/shade-mojo.html#shadeSourcesContent

h2. Update April 2024: this has been reverted.

It turns out that it complicates debugging. If you want the source when debugging, the best way is just to check out the hadoop release you are working with and point your IDE at it."
ABFS: Have all external dependent module execution tracked with DurationInfo,13277629,Open,Major,,06/Jan/20 05:50,,,"To be able to break down the perf impact of the external module executions within ABFS Driver, add execution time computation using DurationInfo in all the relative places. 

 "
ABFS: Fix for OutofMemoryException from AbfsOutputStream,13284725,Open,Major,,12/Feb/20 05:37,,3.4.0,"Currently in environments where memory is restricted, current max concurrent request count logic will trigger a large number of buffers needed for the execution to be blocked leading to out Of Memory exceptions. "
ABFS: Test failure ITestAzureBlobFileSystemRandomRead.testRandomReadPerformance,13290463,Resolved,Major,Fixed,09/Mar/20 05:37,14/Oct/20 22:54,3.3.1,"Ref: https://issues.apache.org/jira/browse/HADOOP-16890

The following test fails randomly. This test compares the perf between Non HNS account against WASB.
ITestAzureBlobFileSystemRandomRead.testRandomReadPerformance"
Java 11 compile support,13278095,Open,Major,,08/Jan/20 08:46,,,"Split from HADOOP-15338.
Now Hadoop must be compiled with Java 8. This issue is to support compiling Hadoop with Java 11."
Zookeeper Delegation Token Manager fetch sequence number by batch,13281562,Resolved,Major,Fixed,25/Jan/20 22:26,02/Jun/20 19:11,3.3.1,"Currently in ZKDelegationTokenSecretManager.java the seq number is incremented by 1 each time there is a request for creating new token. This will need to send traffic to Zookeeper server. With multiple managers running, there is data contention going on. Also, since the current logic of incrementing is using tryAndSet which is optimistic concurrency control without locking. This data contention is having performance degradation when the secret manager are under volume of traffic.

The change here is to fetching this seq number by batch instead of 1, which will reduce the traffic sent to ZK and make many operations inside ZK secret manager's memory.

After putting this into production we saw huge improvement to the RPC processing latency of get delegationtoken calls. Also, since ZK takes less traffic in this way. Other write calls, like renew and cancel delegation tokens are benefiting from this change.

 

 "
Add hadoop.http.idle_timeout.ms to core-default.xml,13287559,Resolved,Major,Fixed,25/Feb/20 17:42,01/May/20 09:07,3.0.4,"HADOOP-15696 made the http server connection idle time configurable  (hadoop.http.idle_timeout.ms).

This configuration key is added to kms-default.xml and httpfs-default.xml but we missed it in core-default.xml. We should add it there because NNs/JNs/DNs also use it too."
"Add -S option in ""Count"" command to show only Snapshot Counts",13293360,Resolved,Major,Fixed,23/Mar/20 17:58,06/Apr/20 02:05,3.3.1,
[JDK11] Support JDK11 in the precommit job,13287926,Resolved,Major,Fixed,27/Feb/20 05:35,19/Jun/20 04:30,3.4.0,Install openjdk-11 in the Dockerfile and use Yetus multijdk plugin to run precommit job in both jdk8 and jdk11.
Prune Jackson 1 from the codebase and restrict it's usage for future,13289681,Resolved,Major,Fixed,05/Mar/20 01:18,20/Dec/21 07:02,3.4.0,The jackson 1 code has silently creeped into the Hadoop codebase again. We should prune them out.
FileUtil.copy() to throw IOException if the source and destination are the same,13287228,Reopened,Major,,24/Feb/20 14:52,,3.3.0,"We encountered an error during a test in our QE when the file destination and source path were the same. This happened during an ADLS test, and there were no meaningful error messages, so it was hard to find the root cause of the failure.
The error we saw was that file size has changed during the copy operation. The new file creation in the destination - which is the same as the source - creates a file and sets the file length to zero. After this, getting the source file will fail because the sile size changed during the operation.

I propose a solution to at least log in error level in the {{FileUtil}} if the source and destination of the copy operation is the same, so debugging issues like this will be easier in the future.

Update: This has been reverted as it caused problems. When rolling back from a branch

* revert MAPREDUCE-7303
* apply HADOOP-17936"
add initial S3A layering + async init,13284148,Open,Major,,09/Feb/20 15:38,,3.3.0,"Split the S3A code into layers

* S3AFileSystem
* S3AStore + Impl
* RawS3A + Impl

S3AFS will create the others and start in order: DelegationTokens, RawS3A, S3AStore, Metastore

this will involve wrapping all access of DTs, s3client, Metastore to block until that layer is complete, or raise an exception if instantiation of it/predecessor failed.

New layers will all be subclasses of Service, split into Interface and Impl, so we can manage the init/start/stop lifecycle with existing code"
Extend Hadoop S3a access from single endpoint to multiple endpoints,13295198,Open,Major,,31/Mar/20 08:53,,3.1.3,"The client API of Hadoop aws can only support a single endpoint to access. However, there are multiple endpoints in object storage (such as ceph), and therefore the storage resources could not be fully used. To address the issue, we create a new Implementation of S3AFileSystem, which support multi-endpoint access. After the optimization, system performance will increase significantly.

Usage:
 1.Ensure hadoop-aws API availiable.
 2.Copy hadoop-aws-3.1.3.jar and aws-java-sdk-bundle-1.11.271.jar to directory share/hadoop/common/lib in hadoop (hadoop-aws-3.1.3.jar and aws-java-sdk-bundle-1.11.271.jar are normally located at directory share/hadoop/tools/lib).
 3.In file etc/hadoop/hadoop-env.sh, add the following:
 export HADOOP_CLASSPATH=/(hadoop root directory)/share/hadoop/common/lib/hadoop-aws-3.1.3.jar:/(hadoop root directory)/share/hadoop/common/lib/hadoop-aws-3.1.3.jar:$HADOOP_CLASSPATH
 4.Edit configuration file ""core-site.xml"" and set properties below:
 <property>
 <name>fs.s3a.s3.client.factory.impl</name>
 <value>org.apache.hadoop.fs.s3a.MultiAddrS3ClientFactory</value>
 </property>
 <property>
 <name>fs.s3a.endpoint</name>
 <value>[http://addr1:port1,http://addr2:port2|http://addr1:port1%2Chttp//addr2:port2],...</value>
 </property>
 5.Optional configuration in ""core-site.xml"":
 <property>
 <name>fs.s3a.S3ClientSelector.class</name>
 <value>org.apache.hadoop.fs.s3a.RandomS3ClientSelector</value>
 </property>
 This configuration is used to set the s3a service selection policy. The default value is org.apache.hadoop.fs.s3a.RandomS3ClientSelector, which is a completely random selector. The configuration can be set to org.apache.hadoop.fs.s3a.PathS3ClientSelector, which is a selector according to the file path."
Dependency update for Hadoop 2.10,13290983,Resolved,Major,Fixed,11/Mar/20 05:12,12/Sep/20 10:57,,"A number of dependencies can be updated.

nimbus-jose-jwt
jetty
netty
zookeeper
hbase-common
jackson-databind

and many more. They should be updated in the 2.10.1 release."
Add Public IOStatistics API,13281797,Resolved,Major,Fixed,27/Jan/20 20:34,14/Jan/21 14:07,3.3.0,"Applications like to collect the statistics which specific operations take, by collecting exactly those operations done during the execution of FS API calls by their individual worker threads, and returning these to their job driver

* S3A has a statistics API for some streams, but it's a non-standard one; Impala &c can't use it
* FileSystem storage statistics are public, but as they aren't cross-thread, they don't aggregate properly

Proposed

# A new IOStatistics interface to serve up statistics
# S3A to implement
# other stores to follow
# Pass-through from the usual wrapper classes (FS data input/output streams)

It's hard to think about how best to offer an API for operation context stats, and how to actually implement.

ThreadLocal isn't enough because the helper threads need to update on the thread local value of the instigator

My Initial PoC doesn't address that issue, but it shows what I'm thinking of"
KMS delegation tokens are memory expensive,13286603,Resolved,Major,Cannot Reproduce,21/Feb/20 01:13,23/Jan/23 16:37,,"We recently saw a number of users reporting high memory consumption in KMS.

Part of the reason being HADOOP-14445. Without that, the number of kms delegation tokens that zookeeper stores is proportional to the number of KMS servers.

There are two problems:
(1) it exceeds zookeeper jute buffer length and operations fail.
(2) KMS uses more heap memory to store KMS DTs.

But even with HADOOP-14445, KMS DTs are still expensive. Looking at a heap dump from KMS, the majority of the heap is occupied by znode and KMS DT objects. With the growing number of encrypted clusters and use cases, this is increasingly a problem our users encounter."
Add dockerfile for ARM builds,13278410,Resolved,Major,Fixed,09/Jan/20 09:05,13/Jan/20 05:12,,"Similar to x86 docker image in {{dev-support/docker/Dockerfile}},
add one more Dockerfile to support aarch64 builds.

And support all scripts (createrelease, start-build-env.sh, etc ) to make use of it in ARM platform."
Large DeleteObject requests are their own Thundering Herd,13280819,Resolved,Major,Fixed,21/Jan/20 18:41,13/Feb/20 20:07,3.2.1,"Currently AWS S3 throttling is initially handled in the AWS SDK, only reaching the S3 client code after it has given up.

This means we don't always directly observe when throttling is taking place.

Proposed:

* disable throttling retries in the AWS client library
* add a quantile for the S3 throttle events, as DDB has
* isolate counters of s3 and DDB throttle events to classify issues better

Because we are taking over the AWS retries, we will need to expand the initial delay en retries and the number of retries we should support before giving up.

Also: should we log throttling events? It could be useful but there is a risk of logs overloading especially if many threads in the same process were triggering the problem.

Proposed: log at debug.

Note: if S3 bucket logging is enabled then throttling events will be recorded as 503 responses in the logs. If the hadoop version contains the audit logging of HADOOP-17511, this can be used to identify operations/jobs/users which are triggering problems."
Update jackson-databind to 2.10.3 to relieve us from the endless CVE patches,13289590,Resolved,Major,Fixed,04/Mar/20 17:37,07/Mar/20 03:21,,"Jackson-databind 2.10 should relieve us from the endless CVE patches according to https://medium.com/@cowtowncoder/jackson-2-10-features-cd880674d8a2

Not sure if this is an easy update, but i think we should do this in the Hadoop 3.3.0 and before removing jackson-databind entirely."
Über-jira: S3A Hadoop 3.3.1 features,13281794,Resolved,Major,Fixed,27/Jan/20 20:09,13/May/21 04:25,3.3.0,"Über-jira: S3A features/fixes for Hadoop 3.4

As usual, this will clutter up with everything which hasn't gone in: don't interpret presence on this list as a commitment to implement.

And for anyone wanting to add patches

MUST
# reviews via github PRs
# *no declaration of AWS S3 endpoint (or other S3 impl) -no review*

SHOULD
# have a setup for testing SSE-KMS, DDB/S3Guard
# including an assumed role we can use for AssumedRole Delegation Tokens

If you are going near those bits of code, they uprate from SHOULD to MUST."
S3A reverts KMS encryption to the bucket's default KMS key in rename/copy,13278092,Resolved,Major,Fixed,08/Jan/20 08:40,21/Apr/20 09:18,3.2.1,"When using (bucket-level) S3 Default Encryption with SSE-KMS and a CMK, all files uploaded via the HDFS {{FileSystem}} {{s3a://}} scheme receive the wrong encryption key, always falling back to the region-specific AWS-managed KMS key for S3, instead of retaining the custom CMK."
NetUtils.createSocketAddr was not handling IPV6 scoped address,13288107,Resolved,Major,Duplicate,27/Feb/20 17:36,01/Oct/21 06:01,,
ABFS: Send error back to client for Read Ahead request failure,13284632,Resolved,Major,Fixed,11/Feb/20 17:49,28/May/20 03:58,3.3.1,"Issue seen by a customer:

The failed requests we were seeing in the AbfsClient logging actually never made it out over the wire. We have found that there’s an issue with ADLS passthrough and the 8 read ahead threads that ADLSv2 spawns in ReadBufferManager.java. We depend on thread local storage in order to get the right JWT token and those threads do not have the right information in their thread local storage. Thus, when they pick up a task from the read ahead queue they fail by throwing an AzureCredentialNotFoundException exception in AbfsRestOperation.executeHttpOperation() where it calls client.getAccessToken(). This exception is silently swallowed by the read ahead threads in ReadBufferWorker.run(). As a result, every read ahead attempt results in a failed executeHttpOperation(), but still calls AbfsClientThrottlingIntercept.updateMetrics() and contributes to throttling (despite not making it out over the wire). After the read aheads fail, the main task thread performs the read with the right thread local storage information and succeeds, but first sleeps for up to 10 seconds due to the throttling."
Upgrade to Apache ZooKeeper 3.5.8,13286195,Resolved,Major,Duplicate,19/Feb/20 09:44,03/Jun/21 12:40,,"Apache ZooKeeper 3.5.7 has been released, which contains some important fixes including third party CVE, possible split brain and data loss in some very rare but plausible scenarios etc.
the release has been tested by the curator team to be compatible with 4.2.0"
S3A mkdirs to indicate which parent path element refers to a file,13290034,Resolved,Major,Duplicate,06/Mar/20 10:52,26/May/21 15:49,3.2.1,If there is a file somewhere up the path you're trying to create a directory in S3. S3A's mkdirs() Will fail was an error -but it does not indicate which path element is at fault.
ABFS: hadoop-dist fails to add wildfly in class path for hadoop-azure,13284726,Resolved,Major,Fixed,12/Feb/20 05:43,14/Apr/20 20:43,3.3.0,
Very large files can be truncated when written through S3AFileSystem,13289055,Resolved,Major,Fixed,02/Mar/20 22:16,20/May/20 12:45,3.2.1,"If a written file size exceeds 10,000 * {{fs.s3a.multipart.size}}, a corrupt truncation of the S3 object will occur as the maximum number of parts in a multipart upload is 10,000 as [specified|https://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html] by the S3 API, and there is an apparent bug where this failure is not fatal allowing the multipart upload operation to be marked as successfully completed without being fully complete."
S3Guard fsck: Add option to remove orphaned entries,13284828,Resolved,Major,Fixed,12/Feb/20 14:25,18/Mar/20 11:52,3.3.0,
ABFS driver enhancement - Allow customizable translation from AAD SPNs and security groups to Linux user and group,13285160,Resolved,Major,Fixed,13/Feb/20 18:50,17/Aug/20 03:37,,"ABFS driver does not support the translation of AAD Service principal (SPI) to Linux identities causing metadata operation failure. Hadoop MapReduce client [[JobSubmissionFiles|https://github.com/apache/hadoop/blob/d842dfffa53c8b565f3d65af44ccd7e1cc706733/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmissionFiles.java#L138]] expects the file owner permission to be the Linux identity, but the underlying ABFS driver returns the AAD Object identity. Hence need ABFS driver enhancement."
Shade & Update guava version to 29.0-jre,13291656,Resolved,Major,Done,13/Mar/20 22:38,19/Apr/21 05:43,3.4.0,"I'd like to propose
(1) shade guava into hadoop-thirdparty.
(2) migrate the hadoop main repo to use the shaded guava.
(3) update guava to 28.x in Hadoop 3.4 (Hadoop 3.3.0 is on guava 27) --> this is a moving target. There's probably going to be a new version of guava by the time Hadoop 3.4 is released."
[OpenTelemetry] Update doc for tracing,13287800,Open,Major,,26/Feb/20 17:24,,,"We should remove this doc https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Tracing.html
and replace it with the OT usage in Hadoop."
s3guard can't init table if caller doesn't have tag permissions,13292048,Resolved,Major,Won't Fix,16/Mar/20 20:10,27/Feb/21 11:10,,"If the user doesn't have the permissions to tag a DDB table creation will fail.


Caused by  HADOOP-16520; we downgrade the failure on other inits, HADOOP-16653, but not on actual creation.


We could downgrade creation the same way"
ABFS: Optimize HttpRequest retry triggers,13284729,Resolved,Major,Fixed,12/Feb/20 05:55,23/Apr/20 13:56,3.3.1,"Currently retry logic gets triggered when access token fetch fails even with irrecoverable errors. Causing a large wait time for the request failure to be reported. 

 

Retry logic needs to be optimized to identify such access token fetch failures and fail fast."
KerberosAuthentication does not disconnect HttpURLConnection leading to CLOSE_WAIT cnxns,13287465,Resolved,Major,Fixed,25/Feb/20 10:45,03/Dec/20 20:06,3.3.0,"PseudoAuthenticator and KerberosAuthentication does not disconnect HttpURLConnection leading to lot of CLOSE_WAIT connections. YARN-8414 issue is observed due to this.

"
s3a mkdir path/ can add 404 to S3 load balancers,13279610,Resolved,Major,Not A Problem,15/Jan/20 14:53,03/Dec/20 11:44,3.3.0,"Not seen in the wild, but inferred from a code review

mkdirs creates an empty dir marker -but it looks for one first

proposed
 * only look for file marker and LIST; don't worry about an empty dir marker
 * always PUT one there

Saves a HEAD on every mkdir too"
s3a mkdirs() to not check dest for a dir marker,13287276,Resolved,Major,Duplicate,24/Feb/20 17:48,03/Dec/20 11:45,3.2.1,"S3A innerMkdirs() calls getFileStatus() to probe dest path for being a file or dir, then goes to reject/no-op.


The HEAD path + / in that code may add a 404 to the S3 load balancers, so subsequent probes for the path fail. 

Proposed: only look for file then LIST underneath

if no entry found: probe for parent being a dir (LIST; HEAD + /), if true create the marker entry. If not, start the walk (or should we then check?)

This increases the cost of mkdir on an existing empty dir marker; reduces it on a non-empty dir. Creates dir markers above dir markers to avoid those cached 404s.
"
Report on S3A cached 404 recovery better,13285318,Resolved,Major,Not A Problem,14/Feb/20 12:42,03/Dec/20 11:43,3.3.0,"A big hadoop -fs copyFromLocal is showing that 404 cacheing is still happening. 

{code}
20/02/13 01:02:18 WARN s3a.S3AFileSystem: Failed to find file s3a://dilbert/dogbert/queries_split_1/catberg.q._COPYING_. Either it is not yet visible, or it has been deleted.
0/02/13 01:02:18 WARN s3a.S3AFileSystem: Failed to find file s3a://dilbert/dogbert/queries_split_1/catberg.q._COPYING_. Either it is not yet visible, or it has been deleted.
{noformat}

We are recovering (good) but it's (a) got the people running this code worried and (b) shouldn't be happening.


Proposed

* error message to -> to a wiki link to a (new) doc on the topic.
* retried clause to increment counter & if count >1 report on #of attempts and duration
* S3A FS.deleteOnExit to avoid all checks
* and review the copyFromLocal to make sure no other probes are happening'"
ABFS: Add unbuffer support to AbfsInputStream,13284878,Resolved,Major,Fixed,12/Feb/20 17:11,06/Apr/20 02:41,,Added unbuffer support to {{AbfsInputStream}} so that apps can cache ABFS file handles.
Performance improvement when distcp files in large dir with -direct option,13286171,Open,Major,,19/Feb/20 08:15,,3.2.1,"We use distcp with -direct option to copy a file between two large directories. We found it costed a few minutes. If we launch too much distcp jobs at the same time, NameNode  performance degradation is serious.

hadoop -direct -skipcrccheck -update -prbugaxt -i -numListstatusThreads 1 hdfs://cluster1:8020/source/100.log  hdfs://cluster2:8020/target/100.jpg
|| ||Dir path||Count||
||Source dir||  hdfs://cluster1:8020/source/ ||100k+ files||
||Target dir||hdfs://cluster2:8020/target/ ||100k+  files||

 

Check code in CopyCommitter.java, we find in function

deleteAttemptTempFiles() has a code targetFS.globStatus(new Path(targetWorkPath, "".distcp.tmp."" + jobId.replaceAll(""job"",""attempt"") + ""*"")); 

It will waste a lot of time when distcp between two large dirs. When we use distcp with -direct option,  it will direct write to the target file without generate a  '.distcp.tmp'  temp file. So, i think this code need add a judgment before call function deleteAttemptTempFiles, if distcp with -direct option, do nothing , directly return .  

 "
S3A creating folder level delete markers,13294199,Resolved,Major,Duplicate,26/Mar/20 13:24,26/Mar/20 20:56,2.8.3,"Using S3A URL scheme while writing out data from Spark to S3 is creating many folder level delete markers.

Writing the same with S3 URL scheme, does not create any delete markers at all.

 

Spark - 2.4.4

Hadoop - 3.2.1

EMR version - 6.0.0

Write Mode - Append
{code:scala}
[hadoop@ip-192-0-161-212 ~]$ spark-shell
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
20/03/27 07:37:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark context Web UI available at http://ip-192-0-161-212.ec2.internal:4040
Spark context available as 'sc' (master = yarn, app id = application_1585294390030_0003).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val df = spark.sql(""select 1 as a"")
df: org.apache.spark.sql.DataFrame = [a: int]

scala> df.write.mode(org.apache.spark.sql.SaveMode.Append).save(""s3://my-bucket/tmp/vijayant/test/s3/"")
                                                                                
scala> df.write.mode(org.apache.spark.sql.SaveMode.Append).save(""s3a://my-bucket/tmp/vijayant/test/s3a/"")
                                                                                
scala> 
{code}
Getting delete markers from `s3` write
{code:bash}
aws s3api list-object-versions --bucket my-bucket --prefix tmp/vijayant/test/s3/
{
    ""Versions"": [
        {
            ""LastModified"": ""2020-03-27T07:38:17.000Z"", 
            ""VersionId"": ""V06OzeE7j221Tq7keSGj8bveCYyJFIcf"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3/_SUCCESS"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:38:16.000Z"", 
            ""VersionId"": ""dLYtHDugLhFIdw2YHLFmoFOxXkm.21Wo"", 
            ""ETag"": ""\""26e70a1e26c709e3e8498acd49cfaaa3-1\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3/part-00000-9d9a8925-f119-415d-b547-b742396e2ca7-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 384
        }
    ]
} 
{code}
Getting delete markers from `s3a` write
{code:bash}
aws s3api list-object-versions --bucket my-bucket --prefix tmp/vijayant/test/s3a/

{
    ""DeleteMarkers"": [
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""NJWRZMcb_eYYwCJh_isX4H1Ox6W362Wb"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""F0h0mLcVVwkMtcHxd95Hj7BACL4Up_Q9"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": "".sBcE6cXeggekOnSgZ4n7pyCDHnsLERK"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""nzm39jiUPC4H0ZaS.5Shp0FYPnR8wNf9"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""BPM65R1HkZngPDYtDL3zPZYPw_G_m9Ic"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:08.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""LJt8_MVDOiD4UdgUqEMycxjvtinJlTNt"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""RqunJTn8Od0PgFR4yu44PX4kL54k6EDv"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""4vY8cnqUI5VJAk3VfEt_VD_KEczo3bmY"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:08.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""ln47YYy.yiE.k70cvqvfgYCEQoYFnKQW"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""5Bsrt7s1caM90mzGNgk0MsTU9q8UjTTA"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""pN3HzDfnmqIqrMwAL2jqKEBkvoHZALor"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""wg91poO1KXReXxvsZHzZXrHR1IgIX8t2"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""cv5Noykq3sMilQqJXAH3E.N7qAWnIBx7"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""6xzt9SxlCUJaOLD8krkE3yXfQU14rErX"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""wGmJAo7x_gkLWAiHzxPGdPMVSus7Wcp1"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }
    ], 
    ""Versions"": [
        {
            ""LastModified"": ""2020-03-27T07:39:11.000Z"", 
            ""VersionId"": ""2py_ZXKl7yh6fwhzksAx8Os1BriDJCBb"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_SUCCESS"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:08.000Z"", 
            ""VersionId"": ""lDqTnLCqDYtjrOiY.V7E6AKTRQLKrqUT"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:10.000Z"", 
            ""VersionId"": ""g.rGoTDdmrGrNjrLchvwz3jMmGePkgiD"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:09.000Z"", 
            ""VersionId"": "".ZCpY2UW4hRlbLL87dFUJRuk021Hyq8p"", 
            ""ETag"": ""\""3def7238a0858c17c62d7045290175cf\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 384
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:10.000Z"", 
            ""VersionId"": ""JSNjTDHSQqe9zSAV93bc6TXPuqA.vDJE"", 
            ""ETag"": ""\""3def7238a0858c17c62d7045290175cf\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 384
        }
    ]
}

{code}
This in turn makes listing objects slow and we have even noticed timeouts due to too many delete markers."
Test failure in TestAbfsOutputStream because of HADOOP-16910,13293473,Resolved,Major,Duplicate,24/Mar/20 08:19,27/Aug/20 14:18,3.2.1,"Failure in TestAbfsOutputStream.java due to not passing FileSystem.Statistics in AbfsOutputstream.

job:

- Passing Statistics through AbfsOutputStream.
- Closing streams in the test"
MetricsConfig incorrectly loads the configuration whose value is String list in the properties file,13291888,Resolved,Major,Fixed,16/Mar/20 07:05,24/Aug/20 13:07,3.1.3,"[HADOOP-15549|https://jira.apache.org/jira/browse/HADOOP-15549] modified loadFirst function in MetricsConfig, and forget to set the DelimiterHandler, which caused that when loading the properties file, if the configured value is a String List, the value will not be loaded as a String Array, but just a String. For example, if we set
{code:java}
*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40
{code}
in hadoop-metrics2.properties. If we use conf.getStringArray(""*.sink.ganglia.dmax"") to get the value list, we will get an array with single element, the content of which is ""jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40"", which wil cause an error during loadGangliaConf. loadGangliaConf will assume that the value of jvm.metrics.threadsBlocked is ""70, jvm.metrics.memHeapUsedM"", which will cause an error ""java.lang.NumberFormatException: For input string: ""70,jvm.metrics.memHeapUsedM"".
"
Multiple Tests failure in hadoop-azure componnt,13293541,Resolved,Major,Cannot Reproduce,24/Mar/20 13:41,17/Aug/20 03:45,3.3.0,"I am seeing many test failures in hadoop azure in trunk. Posting some samples:

[*ERROR*] *Failures:* 

[*ERROR*]   *ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:100->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>*

[*ERROR*]   *ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:53->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>*

[*ERROR*]   *ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:63->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>*

[*ERROR*]   *ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferMultipleReads:111->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<128> but was:<-1>*"
[JDK11] Support JavaDoc,13285270,Resolved,Major,Duplicate,14/Feb/20 07:53,03/Aug/20 01:54,,"This issue is to run {{mvn javadoc:javadoc}} successfully in Apache Hadoop with Java 11.
Now there are many errors."
job commit failure in S3A MR magic committer test,13278752,Resolved,Major,Fixed,10/Jan/20 16:28,30/Jun/20 09:54,3.3.0,"failure in 
{code}
ITestS3ACommitterMRJob.test_200_execute:304->Assert.fail:88 Job job_1578669113137_0003 failed in state FAILED with cause Job commit failed: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@6e894de2 rejected from org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor@225eed53[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
{code}

Stack implies thread pool rejected it, but toString says ""Terminated"". Race condition?

*update 2020-04-22*: it's caused when a task is aborted in the AM -the threadpool is disposed of, and while that is shutting down in one thread, task commit is initiated using the same thread pool. When the task committer's destroy operation times out, it kills all the active uploads.

"
ABFS: Backport HADOOP-16730,13288218,Resolved,Major,Won't Fix,28/Feb/20 07:52,13/Jul/20 04:24,,Backport commit to support SAS Azure Storage access through ABFS Driver tracked under HADOOP-16730 to branch-2.
ABFS: Support for OAuth  v2.0 endpoints,13290416,Resolved,Major,Fixed,08/Mar/20 18:52,24/Jun/20 07:16,,Driver should upport v2.0 auth endpoints
ABFS: update abfs.md to include config keys for identity transformation,13281111,Resolved,Major,Fixed,23/Jan/20 04:01,24/Jan/20 04:40,,Update the abfs.md to include the config keys for identity transformation.
ABFS Streams to update FileSystem.Statistics counters on IO.,13289981,Resolved,Major,Fixed,06/Mar/20 07:50,31/Mar/20 12:49,3.2.1,"Abfs FileSystem Counters are not populated and hence not shown on the console side.

purpose:
 * Passing Statistics in AbfsOutputStream and populating FileSystem Counter(Write_ops) there.
 * Populating Read_ops in AbfsInputStream.
 * Showing Bytes_written, Bytes_read, Write_ops and Read_ops on the console while using ABFS."
Adding Output Stream Counters in ABFS,13290431,Resolved,Major,Fixed,08/Mar/20 23:07,23/Apr/20 13:56,3.2.1,"AbfsOutputStream does not have any counters that can be populated or referred to when needed for finding bottlenecks in that area.



purpose:
 * Create an interface and Implementation class for all the AbfsOutputStream counters.
 * populate the counters in AbfsOutputStream in appropriate places.
 * Override the toString() to see counters in logs."
Use Yetus 0.12.0 in GitHub PR,13294406,Resolved,Major,Fixed,27/Mar/20 08:58,21/Apr/20 12:18,,"HADOOP-16054 wants to upgrade the ubuntu version of the docker image from 16.04 to 18.04. However, ubuntu 18.04 brings maven 3.6.0 by default and the pre-commit jobs fail to add comments to GitHub and JIRA. The issue was fixed by YETUS-957 and upgrading the Yetus version to 0.12.0-SNAPSHOT (or 0.12.0, if released) will fix the problem.

How to upgrade Yetus version in the pre-commit jobs:
* GitHub PR (hadoop-multibranch): Upgrade Jenkinsfile
* JIRA (PreCommit-XXXX-Build): Manually update the config in builds.apache.org"
Improve wasb and abfs resilience on double close() calls,13277342,Resolved,Major,Fixed,03/Jan/20 10:37,08/Jan/20 12:15,3.2.1,"# if you call write() after the NativeAzureFsOutputStream is closed it throws an NPE ... which isn't always caught by closeQuietly code. It needs to raise an IOE
# abfs close ops can trigger failures in try-with-resources use"
AliyunOSS: getFileStatus throws FileNotFoundException in versioning bucket,13283356,Resolved,Major,Fixed,05/Feb/20 02:13,09/Mar/20 05:07,2.10.0,"When hadoop lists object in versioning bucket with many delete marker in it, OSS will return
{code:java}
<?xml version=""1.0"" encoding=""UTF-8""?>
<ListBucketResult>
  <Name>select-us-east-1</Name>
  <Prefix>test/hadoop/file/</Prefix>
  <Marker></Marker>
  <MaxKeys>100</MaxKeys>
  <Delimiter>/</Delimiter>
  <IsTruncated>true</IsTruncated>
  <NextMarker>test/hadoop/file/sub2</NextMarker>
</ListBucketResult>
{code}
It sets *IsTruncated* to true and without *ObjectSummaries* or *CommonPrefixes*, and will throw FileNotFoundException
{code:java}
// code placeholder
java.io.FileNotFoundException: oss://select-us-east-1/test/hadoop/file: No such file or directory!java.io.FileNotFoundException: oss://select-us-east-1/test/hadoop/file: No such file or directory!
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem.getFileStatus(AliyunOSSFileSystem.java:281) at org.apache.hadoop.fs.aliyun.oss.TestAliyunOSSFileSystemContract.testGetFileStatusInVersioningBucket{code}
 
{code:java}
// code placeholder
ObjectListing listing = store.listObjects(key, 1, null, false);
if (CollectionUtils.isNotEmpty(listing.getObjectSummaries()) ||
    CollectionUtils.isNotEmpty(listing.getCommonPrefixes())) {
  return new OSSFileStatus(0, true, 1, 0, 0, qualifiedPath, username);
} else {
  throw new FileNotFoundException(path + "": No such file or directory!"");
}
{code}
 In this case, we should call listObjects until *IsTruncated* is false or *ObjectSummaries* is not empty or *CommonPrefixes* is not empty."
 Upgrade findbugs-maven-plugin to 3.0.5 to fix mvn findbugs:findbugs failure,13286147,Resolved,Major,Fixed,19/Feb/20 04:46,21/Feb/20 03:26,,"mvn findbugs:findbugs is failing:
{noformat}
[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs (default-cli) on project hadoop-project: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]
 {noformat}

We have to update the version of findbugs-maven-plugin."
S3A FS deleteOnExit to skip the exists check,13286740,Resolved,Major,Duplicate,21/Feb/20 11:51,08/Apr/20 17:46,3.2.1,"S3A FS deleteOnExiit is getting that 404 in because it looks for file.exists() before adding. it should just queue for a delete

proposeAlso, processDeleteOnExit() to skip those checks too; just call delete(). "
add experimental optimization of s3a directory marker handling,13283718,Resolved,Major,Duplicate,06/Feb/20 14:43,07/Apr/20 13:09,3.2.1,"Add support for doing ""less"" about directory markers. Make this an experimental option for now, to see what breaks.

Rationale
* reduce throttling from deletes
* reduce tombstone markers in versioned buckets"
fs.s3a.authoritative.path should support multiple FS URIs,13293783,Resolved,Major,Fixed,25/Mar/20 10:58,31/Mar/20 14:52,3.3.0,"(This may have worked until we did recent changes to s3a path qualification)

fs.s3a.authoritative.path was intended to support not just a list of paths, but a list of URIs, so that you could explicitly list those paths which were authoritative in specific buckets. 

Parsing of the list in an FS instance must therefore discard all which are of different URI authorities"
Allow IPv6 in hadoop_config.sh,13294220,Open,Major,,26/Mar/20 15:16,,,"Hi!

I am currently using CDH 5.x and moving to BigTop 1.4 / 1.5 (so jumping from 2.6 to 2.8.5/2.10) but the following bit of hadoop_config.sh is always there:

 # Disable ipv6 as it can cause issues
 HADOOP_OPTS=""$HADOOP_OPTS -Djava.net.preferIPv4Stack=true""

Is there any reason to keep it? I tried to override it via daemon-specific environment variables, but in other daemons like Hive is not that easy (see [https://phabricator.wikimedia.org/T240255#5728881] for an example). 

IPv6 should be safe to use (IIRC hdfs was the main concern at the time but it was fixed HADOOP-11890).

Thanks in advance,

Luca

 

 "
Failure in Abfs tests,13293538,Resolved,Major,Duplicate,24/Mar/20 13:31,26/Mar/20 07:27,3.2.1,"Failures in Abfs tests in hadoop trunk.

Stack traces(Some of them) :


{code:java}
// code placeholder[ERROR] Failures: 
[ERROR]   ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:100->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>
[ERROR]   ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferAfterRead:53->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>
[ERROR]   ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferBeforeRead:63->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>
[ERROR]   ITestAbfsContractUnbuffer>AbstractContractUnbufferTest.testUnbufferMultipleReads:111->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<128> but was:<-1>
[ERROR]   ITestAbfsContractUnbuffer.testUnbufferOnClosedFile:67->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 failed to read expected number of bytes from stream expected:<1024> but was:<-1>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testCreateFileOverExistingFileNoOverwrite:94->AbstractContractCreateTest.testCreateFileOverExistingFileNoOverwrite:76->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateFileOverExistingFileNoOverwrite-builder status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateFileOverExistingFileNoOverwrite-builder; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055630000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF531D7B237""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testCreateFileUnderFile:337->AbstractContractCreateTest.expectCreateUnderFileFails:402->AbstractContractCreateTest.createFile:436->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateFileUnderFile status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateFileUnderFile; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055613000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF52773EA8C""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testCreateNewFile:66->AbstractContractCreateTest.testCreateNewFile:59->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateNewFile status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateNewFile; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055638000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF536C8E3ED""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testCreateUnderFileSubdir:354->AbstractContractCreateTest.expectCreateUnderFileFails:402->AbstractContractCreateTest.createFile:436->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateUnderFileSubdir status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testCreateUnderFileSubdir; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055640000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF537FFDC55""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testFileStatusBlocksizeNonEmptyFile:281->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testFileStatusBlocksizeNonEmptyFile status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testFileStatusBlocksizeNonEmptyFile; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055632000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF533034150""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testMkdirUnderFile:366->AbstractContractCreateTest.expectMkdirsUnderFileFails:419->AbstractContractCreateTest.createFile:436->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testMkdirUnderFile status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testMkdirUnderFile; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055615000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF528A44690""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testMkdirUnderFileSubdir:376->AbstractContractCreateTest.expectMkdirsUnderFileFails:419->AbstractContractCreateTest.createFile:436->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testMkdirUnderFileSubdir status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testMkdirUnderFileSubdir; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055617000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF529D2813D""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractCreate>AbstractContractCreateTest.testOverwriteExistingFile:119->AbstractContractCreateTest.testOverwriteExistingFile:103->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testOverwriteExistingFile-builder status: VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-4/test/testOverwriteExistingFile-builder; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055636000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF5356D59CC""'} expected:<256> but was:<0>
[ERROR]   ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.largeFilesToRemote:452->AbstractContractDistCpTest.largeFiles:536->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 wrong length VersionedFileStatus{VersionedFileStatus{path=abfs://mehakmeet-abfs@mehakmeetdata.dfs.core.windows.net/fork-1/test/ITestAbfsFileSystemContractDistCp/largeFilesToRemote/remote/outputDir/inputDir/file1; isDirectory=false; length=0; replication=1; blocksize=268435456; modification_time=1585055616000; access_time=0; owner=mehakmeet.singh; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}; version='""0x8D7CFF52928F203""'} expected:<2097152> but was:<0>
[ERROR]   ITestAbfsFileSystemContractDistCp>AbstractContractDistCpTest.testDeepDirectoryStructureFromRemote:458->AbstractContractDistCpTest.distCpDeepDirectoryStructure:499->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 wrong length DeprecatedRawLocalFileStatus{path=file:/Users/mehakmeet.singh/hadoopfork/hadoop/hadoop-tools/hadoop-azure/target/test-dir/1/ITestAbfsFileSystemContractDistCp/testDeepDirectoryStructureFromRemote/local/outputDir/inputDir/file1; isDirectory=false; length=0; replication=1; blocksize=33554432; modification_time=1585055683000; access_time=1585055684000; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} expected:<100> but was:<0>
{code}"
Emit per priority RPC queue time and processing time from DecayRpcScheduler,13290250,Resolved,Major,Fixed,07/Mar/20 03:58,25/Mar/20 17:24,,"At ipc Server level we have the overall rpc queue time and processing time for the whole CallQueueManager. In the case of using FairCallQueue, it will be great to know the per queue/priority level rpc queue time since many times we want to keep certain queues to meet some queue time SLA for customers."
Make non-HA proxy providers pluggable,13293670,Resolved,Major,Fixed,24/Mar/20 23:38,25/Mar/20 15:07,,We need to make non-ha proxy providers plugable to handle custom refresh code to address bugs like this: https://issues.apache.org/jira/browse/HADOOP-16543
ITestAzureBlobFileSystemCheckAccess failing,13280984,Resolved,Major,Fixed,22/Jan/20 15:01,06/Feb/20 20:27,3.3.0,"Tests added in HADOOP-16455 are failing.

java.lang.IllegalArgumentException: The value of property fs.azure.account.oauth2.client.id must not be null

Looks to me like there are new configuration options which are undocumented


# these need documentation in testing markdown file
# tests MUST downgrade to skip if not set"
"ARM Compile Scripts only work for AArch64, not AArch32",13292883,Open,Major,,20/Mar/20 10:02,,,The dockerfile added in HADOOP-16797 only works for AArch32. The detection itself only recognizes 64-bit ARM architectures as well.
Update hadoop-thirdparty dependency version to 1.0.0,13292552,Resolved,Major,Fixed,18/Mar/20 19:53,20/Mar/20 09:48,,"Now hadoop-thirdparty 1.0.0 is released, its time to upgrade to released version in hadoop"
Encryption zone file copy failure leaks temp file ._COPYING_ and wrapped stream,13287554,Resolved,Major,Fixed,25/Feb/20 17:27,13/Mar/20 19:27,3.3.0,Copy file into encryption on  trunk with HADOOP-16490 caused a leaking temp file _COPYING_ left and potential wrapped stream unclosed. This ticked is opened to track the fix for it. 
[thirdparty] Revisit LICENSEs and NOTICEs,13288388,Resolved,Major,Fixed,28/Feb/20 20:07,11/Mar/20 18:50,,"LICENSE.txt and NOTICE.txt have many entries which are unrelated to thirdparty,
Revisit and cleanup such entries."
[thirdparty] Handle release package related issues,13291018,Resolved,Major,Fixed,11/Mar/20 08:49,11/Mar/20 18:52,,"Handle following comments from [~elek] in 1.0.0-RC0 voting mail thread here[[https://lists.apache.org/thread.html/r1f2e8325ecef239f0d713c683a16336e2a22431a9f6bfbde3c763816%40%3Ccommon-dev.hadoop.apache.org%3E]]
{quote}3. Yetus seems to be included in the source package. I am not sure if
 it's intentional but I would remove the patchprocess directory from the
 tar file.

7. Minor nit: I would suggest to use only the filename in the sha512
 files (instead of having the /build/source/target prefix). It would help
 to use `sha512 -c` command to validate the checksum.
{quote}
Also, update available artifacts in docs."
"Upgrade Netty version to 4.1.45.Final to handle CVE-2019-20444,CVE-2019-16869",13286152,Resolved,Major,Fixed,19/Feb/20 05:03,10/Mar/20 01:22,3.3.0,"[CVE-2019-20444 |[https://rnd-vulncenter.huawei.com/vuln/toViewOfficialDetail?cveId=CVE-2019-20444]]

[CVE-2019-16869|[https://rnd-vulncenter.huawei.com/vuln/toViewOfficialDetail?cveId=CVE-2019-16869]]

We should upgrade the netty dependency to 4.1.45.Final version"
Declare 2.8.x release line EOL,13287282,Resolved,Major,Done,24/Feb/20 18:24,09/Mar/20 19:10,2.8.6,"The Hadoop community discussed the 2.8.x release line EOL plan:
https://s.apache.org/hadoop2.8eol

The first 2.8 release, 2.8.0 was released on 03/27/2017
The last 2.8 release, 2.8.5, was released on 09/15/2018

Per the [Hadoop EOL policy|https://cwiki.apache.org/confluence/display/HADOOP/EOL+%28End-of-life%29+Release+Branches] 2.8.5 was released nearly 18 months ago and with no volunteer taking up the RM for 2.8.6, it is time to start the EOL process.

I am going to update the wiki and post an announcement to the user/general mailing list declaring 2.8 EOL."
Batch listing of multiple directories to be an unstable interface,13288890,Resolved,Major,Fixed,02/Mar/20 14:11,09/Mar/20 14:52,3.3.0,"HDFS-13616 added a new API for batch listing of multiple directories, but
it isn't yet ready for tagging as stable & doesn't suit object stores.


* the new API is pulled into a new interface marked unstable;
* new classes (PartialListing) also tagged unstable.
* Define a new path capability.

HDFS will implement, but not filter/HarFS; it is an HDFS exclusive implementation for now.
"
Add OpenTracing in S3A Cloud Connector,13289344,Open,Major,,03/Mar/20 21:54,,3.3.0,
update jackon-databind version,13287181,Resolved,Major,Duplicate,24/Feb/20 10:28,04/Mar/20 17:28,,"according to [CVE-2020-8840|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8840], maybe we should update  jackson-databind to 2.9.10.3 or 2.10.x?*"
ABFS test failures when running with a storage account without a hierarchical namespace,13289569,Open,Major,,04/Mar/20 16:01,,3.3.0,"Some tests are failing when running on a storage account without hierarchical namespace: 

{noformat}
[ERROR] Failures:
[ERROR]   ITestAzureBlobFileSystemAuthorization.testSetOwnerUnauthorized:212->runTest:274 Expected an exception of type class org.apache.hadoop.fs.azurebfs.contracts.exceptions.SASTokenProviderException
[ERROR]   ITestAzureBlobFileSystemAuthorization.testSetPermissionUnauthorized:217->runTest:274 Expected an exception of type class org.apache.hadoop.fs.azurebfs.contracts.exceptions.SASTokenProviderException
[ERROR]   ITestGetNameSpaceEnabled.testXNSAccount:51->Assert.assertTrue:41->Assert.fail:88 Expecting getIsNamespaceEnabled() return true
[ERROR] Errors:
[ERROR]   ITestAzureBlobFileSystemAuthorization.testGetAclStatusAuthorized:247->runTest:278->executeOp:298 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testGetAclStatusUnauthorized:252->runTest:274->lambda$runTest$4:275->executeOp:298 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testModifyAclEntriesUnauthorized:222->runTest:274->lambda$runTest$4:275->executeOp:335 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testRemoveAclEntriesUnauthorized:227->runTest:274->lambda$runTest$4:275->executeOp:331 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testRemoveAclUnauthorized:237->runTest:274->lambda$runTest$4:275->executeOp:339 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testRemoveDefaultAclUnauthorized:232->runTest:274->lambda$runTest$4:275->executeOp:342 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemAuthorization.testSetAclUnauthorized:242->runTest:274->lambda$runTest$4:275->executeOp:304 ? UnsupportedOperation
[ERROR]   ITestAzureBlobFileSystemCheckAccess.testWhenCheckAccessConfigIsOff:125->setupTestDirectoryAndUserAccess:313->modifyAcl:304 ? UnsupportedOperation
{noformat}

This should be fixed, so these tests will be skipped or will fail with a more meaningful error."
Announce user-zh mailing list,13288341,Resolved,Major,Done,28/Feb/20 16:49,28/Feb/20 22:50,,"A user-zh mailing list for Mandarin-speaking Hadoop users is set up now. Let's make it public:

(1) Add the mailing list to https://hadoop.apache.org/mailing_lists.html
(2) Announce the ML at the user@, general@
(3) Send notification to the ASF slack channel.
"
TestIPC#testProxyUserBinding failure,13288402,Open,Major,,28/Feb/20 21:51,,3.1.3,"mvn clean install -Pdist -Pnative -Dtar -DskipTests in hadoop-common fails due to TestIPC failure, specifically testProxyUserBinding. Ive tested separately and the different components seem to work just fine, so Im not sure why its failing."
Encryption zone file copy failure leaks temp file ._COPYING_ and wrapped stream,13287553,Patch Available,Major,,25/Feb/20 17:25,,,Copy file into encryption on  trunk with HADOOP-16490 caused a leaking temp file _COPYING_ left and potential wrapped stream unclosed. This ticked is opened to track the fix for it. 
[thirdparty] Add shaded JaegerTracer,13285964,Resolved,Major,Fixed,18/Feb/20 12:03,21/Feb/20 15:31,,"Add artifact {{hadoop-shaded-jaeger}} to {{hadoop-thirdparty}} for OpenTracing work in HADOOP-15566.

CC [~weichiu]"
S3Guard: add support for other MetadataStores,13286492,Open,Major,,20/Feb/20 14:27,,3.2.1,"Hi all,

 

Are there any plans to add other MetadataStore implementations for S3Guard? DynamoDB costs are too high when the read capacity/write capacity are high.

 

Maybe a Postgres/MySQL implementation is simple enough to implement and offer strong consistency.

Another idea is to implement a Cassandra/Scylla MetadataStore(for better write scalability), but we should pay attention to consistency.

 

Any thoughts?"
ABFS: Test code with Delegation SAS generation logic,13285482,Resolved,Major,Duplicate,15/Feb/20 06:19,20/Feb/20 06:30,3.2.1,Add sample delegation SAS token generation code in test framework for reference for any authorizer adopters of SAS authentication.
ipc.Server readAndProcess threw NullPointerException,13286101,Resolved,Major,Fixed,18/Feb/20 21:34,19/Feb/20 05:55,,"{code}
2020-01-18 10:19:02,109 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client xx.xx.xx.xx threw exception [java.lang.NullPointerException]
java.lang.NullPointerException
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1676)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:935)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:791)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:762)
{code}"
InstrumentedLock should log lock queue time,13282234,Resolved,Major,Fixed,29/Jan/20 16:53,18/Feb/20 22:46,3.3.0,"Within the Datanode, we often see contention around the FsDatasetImpl lock. This can be for various reasons, eg the DN is under IO load, and other bugs, eg HDFS-15131.

When DN slow downs happen, it is very difficult to debug what is causing it, as there are few messages in the logs which indicate what is happening.

In my experience, the presence of this log is informative:

{code}
2019-11-27 09:00:49,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Took 21540ms to process 1 commands from NN
{code}

This comes from the datanode heartbeat thread, and it is logged when the commands returned from the heartbeat cannot be enqueued, and usually this is because the thread cannot get the FsDatasetImpl lock due to contention with other slow threads.

HDFS-14997 moved the command processing to an async thread, and hence this useful message will disappear.

InstrumentedLock introduced a feature to the datanode, where it will log a stack trace if any thread holds the lock for over 300ms. However this will not catch a scenario where 10 threads each hold a lock for 200ms, leading to the next in the queue having waited over 2 seconds.

I believe it would be useful to extend InstrumentedLock to log if a thread has to wait for over some threshold. That way, we could be able to catch scenarios like the heartbeat thread shows us, but in a much more obvious way, provided lock contention is involved."
Trunk is broken on OS X,13285216,Resolved,Major,Invalid,13/Feb/20 23:43,17/Feb/20 21:19,,"Trunk does not compiler on Mac OS X with Java ""1.8.0_242"".


{code:bash}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-maven-plugins: Compilation failure
[ERROR] /home/ahussein/hadoop/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocRunner.java:[16,39] package com.fasterxml.jackson.core.type does not exist
{code}
"
Backport HADOOP-16152 to branch-2.10,13285835,Open,Major,,17/Feb/20 18:11,,,We should make the effort to migrate from Jetty 9.3 to 9.4 in branch-2.10.
Support getting thread info from thread group for JvmMetrics to improve the performance,13284495,Resolved,Major,Fixed,11/Feb/20 04:54,14/Feb/20 07:19,2.10.1,"Recently we found jmx request taken almost 5s+ to be done when there were 1w+ threads in a stressed datanode process, meanwhile other http requests were blocked and some disk operations were affected (we can see many ""Slow manageWriterOsCache"" messages in DN log, and these messages were hard to be seen again after we stopped sending jxm requests)

The excessive time is spent in getting thread info via ThreadMXBean inside which ThreadImpl#getThreadInfo native method is called, the time complexity of ThreadImpl#getThreadInfo is O(n*n) according to [JDK-8185005|https://bugs.openjdk.java.net/browse/JDK-8185005] and it holds global thread lock and prevents creation or termination of threads.

To improve this, I propose to support getting thread info from thread group which will improve a lot by default, also support using original approach when ""-Dhadoop.metrics.jvm.use-thread-mxbean=true"" is configured in the startup command.

An example of performance tests between these two approaches is as follows:
{noformat}
#Threads=100, ThreadMXBean=382372 ns, ThreadGroup=72046 ns, ratio: 5
#Threads=200, ThreadMXBean=776619 ns, ThreadGroup=83875 ns, ratio: 9
#Threads=500, ThreadMXBean=3392954 ns, ThreadGroup=216269 ns, ratio: 15
#Threads=1000, ThreadMXBean=9475768 ns, ThreadGroup=220447 ns, ratio: 42
#Threads=2000, ThreadMXBean=53833729 ns, ThreadGroup=579608 ns, ratio: 92
#Threads=3000, ThreadMXBean=196829971 ns, ThreadGroup=1157670 ns, ratio: 170
{noformat}"
Prune -tombstones to remove children entries,13285120,Resolved,Major,Invalid,13/Feb/20 16:04,13/Feb/20 17:17,3.3.0,"When you prune a directory in S3Guard the children entries are not pruned with the tombstoned parent (directory).
I propose the solution to remove all children entries (the whole hierarchy) to avoid orphaned entries once the parent (directory) tombstone is removed.
"
Support for `fs.s3a.endpoint.region`,13283239,Resolved,Major,Works for Me,04/Feb/20 13:02,04/Feb/20 13:28,,"Currently it is not possible to connect S3 Compatible services like MinIO, Ceph, etc (running with a custom region) to Spark with s3a connector. For example, if MinIO is running on a Server with
 * IP Address: 192.168.0.100
 * Region: ap-southeast-1

The s3a connector can't be configured to use the region `ap-southeast-1`. 

It would be great to have a configuration field like `fs.s3a.endpoint.region`. This will be very helpful for users deploying Private Cloud and who intend to use S3 like services on premises."
ITestAbfsClient.testContinuationTokenHavingEqualSign failing,13283572,Resolved,Major,Fixed,05/Feb/20 20:55,06/Feb/20 20:29,3.3.0,"Testcase testContinuationTokenHavingEqualSign is failing as request that was expected to fail is passing.

There is change in the queryparam validation in ContinuationToken at server end wihch has resulted in this behaviour. 

Server request trace:

2020-02-05 16:59:17,001 DEBUG [JUnit-testContinuationTokenHavingEqualSign]: services.AbfsClient (AbfsRestOperation.java:executeHttpOperation(263)) - HttpRequest: 200,,cid=87c3ebea-def7-4fdd-a21a-a56c63a59387,rid=0931c565-201f-004c-1317-dcdd90000000,sent=0,recv=0,GET,[https://snvijayaabfsns.dfs.core.windows.net/abfs-testcontainer-85bb9523-fccd-45f3-ae6d-37622d8231e5?upn=false&resource=filesystem&maxResults=500&directory=/&continuation=%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D%3D&timeout=90&recursive=true]

 

Disabling the test until the server fix is in and deployed on all regions.

 "
S3Guard testing doc: Add required parameters for S3Guard testing in IDE,13281963,Resolved,Major,Fixed,28/Jan/20 15:29,06/Feb/20 14:13,3.2.1,"When I tried running the S3 guard tests using my IDE, they were getting skipped saying s3 guard is not enabled. To enable S3 guard test you have to configure the following property in auth-keys.xml

<property>
 <name>fs.s3a.s3guard.test.enabled</name>
 <value>true</value>
</property>

Once you have done this tests will still get skipped saying ""Test only applies when DynamoDB is used for S3Guard Store""

To configure dynamo db you have  to set this property in auth-keys.xml

<property>
 <name>fs.s3a.s3guard.test.implementation</name>
 <value>dynamo</value>
</property>

 "
Spark-SQL test running on Windows: hadoop chgrp warnings	,13283180,Open,Major,,04/Feb/20 07:28,,2.6.5,"Running SparkSQL local embedded unit tests on Win10, using winutils.

Got warnings about 'hadoop chgrp'.

See environment info.
{code:bash}
-chgrp: 'TEST\Domain users' does not match expected pattern for group
Usage: hadoop fs [generic options] -chgrp [-R] GROUP PATH...
-chgrp: 'TEST\Domain users' does not match expected pattern for group
Usage: hadoop fs [generic options] -chgrp [-R] GROUP PATH...
-chgrp: 'TEST\Domain users' does not match expected pattern for group
Usage: hadoop fs [generic options] -chgrp [-R] GROUP PATH...
{code}
Related info on SO: [https://stackoverflow.com/questions/48605907/error-in-pyspark-when-insert-data-in-hive]

hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShellPermissions.java:210

The problem is: backslash character isn't included to allowedChars, see attached HadoopGroupTest.java

Original issue in Spark: https://issues.apache.org/jira/browse/SPARK-30701"
Boundary condition to check if there is nothing to truncate in StringExpr functions,13281803,Resolved,Major,Abandoned,27/Jan/20 21:27,27/Jan/20 21:28,,"Add a boundary condition to check if there is nothing to truncate in StringExpr functions

truncate()

rightTrimAndTruncate()"
TestHarFileSystem.testInheritedMethodsImplemented broken,13281397,Resolved,Major,Fixed,24/Jan/20 14:30,24/Jan/20 18:50,3.3.0,"caused by HADOOP-16759.

I'm surprised this didn't surface earlier -or embarrassed that if it did, somehow I missed it. Will fix.

 

will also review the checksum FS to make sure it's gone through there too. 

 

Given I was using the IDE to refactor, it should have all been automatic"
Let s3 clients configure request timeout,13277809,Resolved,Major,Fixed,07/Jan/20 01:04,24/Jan/20 13:44,3.3.0,"S3 does not guarantee latency. Every once in a while a request may straggle and drive latency up for the greater procedure. In these cases, simply timing-out the individual request is beneficial so that the client application can retry. The retry tends to complete faster than the original straggling request most of the time. Others experienced this issue too: [https://arxiv.org/pdf/1911.11727.pdf] .

S3 configuration already provides timeout facility via `ClientConfiguration#setTimeout`. Exposing this configuration is beneficial for latency sensitive applications. S3 client configuration is shared with DynamoDB client which is also affected from unreliable worst case latency.

 

 "
[thirdparty] port HADOOP-16754 (Fix docker failed to build yetus/hadoop) to thirdparty Dockerfile,13280910,Resolved,Major,Fixed,22/Jan/20 07:24,22/Jan/20 10:11,,port HADOOP-16754 to avoid Docker build failure
[thirdparty] ChangeLog and ReleaseNote are not packaged by createrelease script,13280703,Resolved,Major,Fixed,21/Jan/20 09:51,22/Jan/20 07:21,thirdparty-1.0.0,createrelease script is not packaging CHANGELOGS and RELEASENOTES during generation of site package for hadoop-thirdparty module.
[pb-upgrade] Use 'o.a.h.thirdparty.protobuf' shaded prefix instead of 'protobuf_3_7',13280715,Resolved,Major,Fixed,21/Jan/20 10:07,21/Jan/20 17:40,,"As per discussion  [here|https://github.com/apache/hadoop/pull/1635#issuecomment-576247014], versioned package name may make upgrade of library to a non-trivial task. package name needs to be updated in all usages in all modules. 

So common package name is preferred."
"Please remove or minimize need for native libraries ""winutils"" to run Spark locally on Windows.",13280634,Resolved,Major,Duplicate,21/Jan/20 01:23,21/Jan/20 14:33,3.1.3,"Please remove the need for building and using custom native libraries ""winutils"" to run Spark on Windows systems just to run Spark & similar.

Reference: [https://github.com/steveloughran/winutils]

If native libraries are needed for performance benefits then please try to load the native libraries and then if not found then provide and use a functional equivalent Java implementation such that Spark will build and run applications using only Java. "
Stripping Submarine site from Hadoop site,13280086,Resolved,Major,Fixed,17/Jan/20 09:11,21/Jan/20 02:17,,"Now that Submarine is getting out of Hadoop and has its own repo, it's time to stripe the Submarine site from Hadoop site."
Enable Filesystem caching to optionally include URI Path,13279663,Resolved,Major,Duplicate,15/Jan/20 19:44,17/Jan/20 12:20,3.3.0,Implementing AWSCredentialsProviders that dynamically retrieve STS tokens based on the URI being accessed fail if Filesystem caching is enabled and the job accesses more than one URI Path within the same bucket.
"Unable to build 3.1.3 winutils and libwinutils native x64 using VisualStudio 2019: winutils.sln, Platform Toolset v142, SDK v10.0, C++ LangStd: Default, and other default settings",13280437,Open,Major,,19/Jan/20 23:01,,3.1.3,"libwinutils.c  line 40 gives several build errors:

C2065  'L': undeclared identifier

E0065 expected a ';'

E0020 identifier ""L"" is undefined

C2099 initializer is not a constant

C2143 syntax error: missing ';' before 'string'

 

 

 

  "
Javadoc errors are always ignored in the precommit jobs,13279494,Open,Major,,15/Jan/20 04:21,,,"There are many javadoc errors reported and fixed. This is mostly because the precommit jobs cannot capture the errors. There is an issue (YETUS-554) in Yetus but there is something to do in Hadoop-side.

Examples:
* MAPREDUCE-7256
* HADOOP-15904
* HADOOP-16390
* HADOOP-16654
* HADOOP-16681
* YARN-8939
* YARN-9812
* and many more"
wasb tests can't work with https only wasb storage account,13277395,Open,Major,,03/Jan/20 16:32,,3.3.0,"If you create a wasb storage account which is https only, the wasb tests all fail to instantiate the FS.

Either we fix the tests to support wasbs:// schemas in place of the hard-coded wasb ones, or we just add a switch to say ""all wasb URLs are HTTPS""

happy either way"
Checksum FS #hsync does not sync to disk,13277347,Resolved,Major,Won't Fix,03/Jan/20 11:02,03/Jan/20 14:54,,"Recently I created a SequenceFile Writer and was surprised to see that the file size was not changing despite hsync being called after each append.

After some fiddling around, I noticed that if write checksums are disabled then the hsync is effective.

I checked the documentation and could not find any mention of the fact that hsync could be ineffective when write checksums are on.

The Writer was using the LocalFilesystem."
Update the year to 2020,13277068,Resolved,Major,Fixed,01/Jan/20 02:52,02/Jan/20 16:30,,Update the year to 2020
Add com.amazonaws.auth.profile.ProfileCredentialsProvider to hadoop-aws docs,13293061,Resolved,Minor,Fixed,21/Mar/20 02:24,17/Jul/20 11:21,3.2.1,"There is a very, very useful S3A authentication method that is not currently documented: {{com.amazonaws.auth.profile.ProfileCredentialsProvider}}

This provider lets you source your AWS credentials from a shared credentials file, typically stored under {{~/.aws/credentials}}, using a [named profile|https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html]. All you need is to set the {{AWS_PROFILE}} environment variable, and the provider will get the appropriate credentials for you.

I discovered this from my coworkers, but cannot find it in the docs for hadoop-aws. I'd expect to see it at least mentioned in [this section|https://hadoop.apache.org/docs/r2.9.2/hadoop-aws/tools/hadoop-aws/index.html#S3A_Authentication_methods]. It should probably be added to the docs for every minor release that supports it, which I'd guess includes 2.8 on up.

(This provider should probably also be added to the default list of credential provider classes, but we can address that in another ticket. I can say that at least in 2.9.2, it's not in the default list.)

(This is not to be confused with {{com.amazonaws.auth.InstanceProfileCredentialsProvider}}, which serves a completely different purpose.)"
Add AWS S3 Transfer acceleration support,13278164,Open,Minor,,08/Jan/20 13:48,,2.8.5,It would be great to be able to use [S3 Transfer acceleration |[https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html]]especially when reading data from multiple locations around the globe as there is a significant performance improvement be routing traffic over the AWS backbone.
AWS AssumedRoleCredentialProvider needs ExternalId add,13279657,Open,Minor,,15/Jan/20 18:58,,3.2.1,"AWS has added a security feature to the assume-role function in the form of the ""ExternalId"" key in the AWS Java SDK {{STSAssumeRoleSessionCredentialsProvider.Builder}} class.  To support this security feature, the hadoop aws {{AssumedRoleCredentialProvider}} needs a patch to include this value from the configuration as well as an added Constant to the {{org.apache.hadoop.fs.s3a.Constants}} file.

The ExternalId is not a required security feature, it is an augmentation of the current assume role configuration. 

Proposed: 
 * Get the assume-role ExternalId token from the configuration for the configuration key {{fs.s3a.assumed.role.externalid}}
 * Use the configured ExternalId value in the {{STSAssumeRoleSessionCredentialsProvider.Builder}}   

e.g.

{{if (StringUtils.isNotEmpty(externalId)) {}}
 {{    builder.withExternalId(externalId); // include the token for cross-account assume role}}
 {{}}}

 Tests:
 * +Unit test+ which verifies the ExternalId state value of the {{AssumedRoleCredentialProvider}} is consistent with the configured value - either empty or populated
 * Question: not sure about how to write the +integration test+ for this feature.  We have an account configured for this use-case that verifies this feature but I don't have much context on the Hadoop project AWS S3 integration tests, perhaps a pointer could help.

 

 "
ITestCustomSigner uses absolute paths off the bucket root rather than fork-relative,13293877,Open,Minor,,25/Mar/20 17:06,,3.3.0,"ITestCustomSigner.testCustomSignerAndInitializer is creating a a couple of paths (/customsignerpath1, /customsignerpath2) rather than in a specific fork.

the paths are uniquely named enough to be low risk, but they should be moved to being fork-relative"
S3A DT marshalling to  include nested error text in wrapped message,13283423,Open,Minor,,05/Feb/20 09:48,,3.3.0,"When you get an unmarshalling error with S3A DTs, the thrown exception wraps the cause, but doesn't include it's text in the message, so on a long trace the cause can get lost"
abfs can't access storage account if soft delete is enabled,13283422,Open,Minor,,05/Feb/20 09:44,,3.1.3,"Facing the issue in which if soft delete is enabled on storage account.

Hadoop fs -ls command fails with 
{noformat}
 Operation failed: ""This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint."", 409, HEAD, https://<account_name>.[dfs.core.windows.net/test-container-1//?upn=false&action=getAccessControl&timeout=90|http://dfs.core.windows.net/test-container-1//?upn=false&action=getAccessControl&timeout=90]
{noformat}
Trying to access storage account by issuing below command :
{noformat}
 hadoop fs -Dfs.azure.account.auth.type.<account_name>.[dfs.core.windows.net|http://dfs.core.windows.net/]=OAuth -Dfs.azure.account.oauth.provider.type.<account_name>.[dfs.core.windows.net|http://dfs.core.windows.net/]=org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider -ls [abfs://test-container-1]@<account_name>.[dfs.core.windows.net/|http://dfs.core.windows.net/]
{noformat}"
Upgrade spotbugs to 4.0.6,13285885,Resolved,Minor,Fixed,18/Feb/20 04:53,17/Jul/20 11:06,3.4.0,"[https://github.com/spotbugs/spotbugs/releases]

spotbugs 4.0.0 is now released. 

 

We can upgrade spotbugs' version to:

1. 3.1.12  (conservative option)

2. 4.0.0 (which might includes incompatible changes, according to the migration guide: [https://spotbugs.readthedocs.io/en/stable/migration.html])

 

Step by step approach is also acceptable."
ABFS: Support infinite lease dirs,13295042,Resolved,Minor,Fixed,30/Mar/20 16:23,20/Apr/21 21:36,3.3.1,This would allow some directories to be configured as single writer directories. The ABFS driver would obtain a lease when creating or opening a file for writing and would automatically renew the lease and release the lease when closing the file.
FS/FC createFile() builder to add a lazy-check option for async probes of the store,13283428,Open,Minor,,05/Feb/20 10:04,,3.3.0,"For object stores, file creation can triggers checks for: path being to a file, path not being a directory, parent path existing.

These are all done before an output stream is returned -even though nothing may be manifest in the remote store until the screen is flashed or closed.

If we supported lazy checks we could do those probes against the store while the application was actually generating data to write.

Changing the behaviour of open() would be too fundamental. What we can do is add a new option to the createFile() builder which stores can support. When set, the caller is declaring they are happy with late reporting of failures -with any problems surfacing during a write/flush/close. It is notable with S3A permissions issues only surface later on anyway.
"
Update ADLS client credential creation docs,13288283,Open,Minor,,28/Feb/20 13:06,,3.3.0,"Azure portal has moved things around, renamed concepts, and the ADLS config generation instructions don't match the UI any more

Update ADLS client credential creation docs to match the current Azure portal.

*and add a datestamp of when these docs were current*"
ABFS: Delegation SAS generator for integration with Ranger,13290965,Resolved,Minor,Fixed,11/Mar/20 02:38,12/May/20 18:42,3.2.1,HADOOP-16730 added support for Shared Access Signatures (SAS).  Azure Data Lake Storage Gen2 supports a new SAS type known as User Delegation SAS.  This Jira tracks an update to the ABFS driver that will include a Delegation SAS generator and tests to validate that this SAS type is working correctly with the driver.
ITestDynamoDBMetadataStore.testTableVersioning failure -DDB deleteItem consistency?,13279372,Resolved,Minor,Won't Fix,14/Jan/20 14:37,24/Mar/23 14:47,3.3.0,"transient failure of ITestDynamoDBMetadataStore.testTableVersioning; the deleted table version record was still there.

Looks like the delete is EC; finally I hit a consistency event

Proposed: the test iterates a bit awaiting the exception"
NPE in s3a byte buffer block upload,13291056,Resolved,Minor,Cannot Reproduce,11/Mar/20 11:20,04/Oct/22 16:44,3.3.0,NPE in s3a upload when fs.s3a.fast.upload.buffer = bytebuffer
Use JUnit TemporaryFolder Rule in TestFileUtils,13280165,Resolved,Minor,Fixed,17/Jan/20 15:32,25/Jan/20 15:13,,
"For AzureNativeFS, when BlockCompaction is enabled, FileSystem.create(path).close() would throw exception.",13286285,Open,Minor,,19/Feb/20 17:32,,2.9.2,"For AzureNativeFS, when BlockCompaction is enabled, FileSystem.create(path).close() would throw blob not existed exception.

Block Compaction Setting: fs.azure.block.blob.with.compaction.dir
Exception is thrown from close(), this would happen when no write happened. When actually write any content in the file, same context close() won't trigger the exception. 

When BlockCompaction is not enabled, this issue won't happen. 

Call Stack:

org.apache.hadoop.fs.azure.AzureException: Source blob _$azuretmpfolder$/956457df-4a3e-4285-bc68-29f68b9b36c4test1911.log does not exist.
org.apache.hadoop.fs.azure.AzureException: Source blob _$azuretmpfolder$/956457df-4a3e-4285-bc68-29f68b9b36c4test1911.log does not exist. 
at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2648) 
at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2608) 
at org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream.restoreKey(NativeAzureFileSystem.java:1199) 
at org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream.close(NativeAzureFileSystem.java:1068) 
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) 
at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)"
AWS AssumedRoleCredentialProvider needs ExternalId add,13279630,Resolved,Minor,Invalid,15/Jan/20 16:56,15/Jan/20 18:58,3.2.1,"AWS has added a security feature to the assume-role function in the form of the ""ExternalId"" key in the AWS Java SDK {{STSAssumeRoleSessionCredentialsProvider.Builder}} class.  To support this security feature, the hadoop aws {{AssumedRoleCredentialProvider}} needs a patch to include this value from the configuration as well as an added Constant to the {{org.apache.hadoop.fs.s3a.Constants}} file."
ITestS3GuardOutOfBandOperations.testListingDelete failing on versioned bucket,13293882,Resolved,Minor,Fixed,25/Mar/20 17:15,14/Apr/20 09:58,3.3.0,"Failure in versioned bucket on  testListingDelete[auth=true](org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations) ; looks like an error in one of the asserts where something is failing in the open().get() not the read, because we don't pass in a status to the rawFS.

"
The description of hadoop.http.authentication.signature.secret.file contains outdated information,13283373,Resolved,Minor,Fixed,05/Feb/20 04:51,25/Feb/20 02:12,,"{quote}The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.
{quote}
JT/TT should be changed to RM/NM"
Backport HADOOP-10848. Cleanup calling of sun.security.krb5.Config to branch-3.2,13293526,Resolved,Minor,Fixed,24/Mar/20 12:22,24/Mar/20 23:25,,"Backport HADOOP-10848 to lower branches like branch-3.2 so applications using older hadoop jars (e.g. Ozone) can get rid of the annoying message
{code}
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
{code}
when running with JDK11+"
mkdir on s3a should not be sensitive to trailing '/',13291590,Resolved,Minor,Duplicate,13/Mar/20 15:57,30/Nov/20 15:01,,"I would have expected to create the directory for both calls:

{code}
[hive@hiveserver2-0 lib]$ hdfs dfs -mkdir s3a://qe-s3-bucket-mst-xfpn-dwx-external/custom-jars2/
/usr/bin/hdfs: line 4: /usr/lib/bigtop-utils/bigtop-detect-javahome: No such file or directory
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
mkdir: get on s3a://qe-s3-bucket-mst-xfpn-dwx-external/custom-jars2/: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: One or more parameter values were invalid: An AttributeValue may not contain an empty string (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ValidationException; Request ID: 1AE0KA2Q5ADI47R75N8BDJE973VV4KQNSO5AEMVJF66Q9ASUAAJG): One or more parameter values were invalid: An AttributeValue may not contain an empty string (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ValidationException; Request ID: 1AE0KA2Q5ADI47R75N8BDJE973VV4KQNSO5AEMVJF66Q9ASUAAJG)
[hive@hiveserver2-0 lib]$ hdfs dfs -mkdir s3a://qe-s3-bucket-mst-xfpn-dwx-external/custom-jars2
/usr/bin/hdfs: line 4: /usr/lib/bigtop-utils/bigtop-detect-javahome: No such file or directory
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[hive@hiveserver2-0 lib]$ 
{code}"
distcp copy calls getFileStatus() needlessly and can fail against S3,13293116,Resolved,Minor,Fixed,21/Mar/20 18:14,09/Apr/20 17:30,3.0.0,"Distcp to AWS s3 was working fine on CDH 5.16 with distcp 2.6.0. but after upgrade to CDH 6.3 which comes with distcp-3.0 JAR which is through error as below.

The same error with repeats on Hadoop-distcp-3.2.1.jar as well. Tried with -direct option in 3.2.1, still same error.

 

Error: java.io.FileNotFoundException: No such file or directory: s3a://XXXXXXXXXXXXX/part-00012-baa6a706-3816-4dfa-ba07-0fb56fd38178-c000.snappy.parquet
 at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2255)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088)
 at org.apache.hadoop.tools.util.DistCpUtils.preserve(DistCpUtils.java:203)
 at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:220)
 at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:48)
 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
ABFS:  Combine append+flush calls for blockblob & appendblob,13280662,Resolved,Minor,Won't Fix,21/Jan/20 06:46,24/Jun/20 09:57,3.3.0,Combine append+flush calls for blockblob & appendblob
ABFS: Change in User-Agent header,13291252,Resolved,Minor,Fixed,12/Mar/20 08:25,22/Jun/20 04:05,3.4.0,"* Add more inforrmation to the User-Agent header like cluster name, cluster type, java vendor etc.
* Add APN/1.0 in the begining"
catch and downgrade all exceptions trying to load openssl native libs through wildfly,13282609,Resolved,Minor,Fixed,31/Jan/20 13:20,28/May/20 19:56,3.2.1,"I am getting reports of problems loading the wildfly/openSSL binding even on a build which should have HADOOP-16405 on. I'm not sure that really is the case, but given that we have had problems here which surface aa NPEs, and we may end up having other linkage problems in future -I think we should catch all exceptions raised trying to instantiate the wildfly SSL bindings and fall back to the JVM. 

I do want to log@warn here, because it is a sign of a problem. A full stack can come in at debug"
ABFS: Make list page size configurable,13291050,Resolved,Minor,Fixed,11/Mar/20 10:45,24/Apr/20 17:51,3.2.1,Make list page size configurable
"Backport HADOOP-16890- ""ABFS: Change in expiry calculation for MSI token provider"" & HADOOP-16825 ""ITestAzureBlobFileSystemCheckAccess failing"" to branch-2",13293385,Resolved,Minor,Fixed,23/Mar/20 20:57,08/Apr/20 04:40,3.3.0,"Backport ""ABFS: Change in expiry calculation for MSI token provider"" to branch-2"
ITestS3GuardOutOfBandOperations failing on versioned S3 buckets,13284634,Resolved,Minor,Fixed,11/Feb/20 17:56,24/Feb/20 18:49,3.3.0,"org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations.testListingDelete[auth=true]

failing because the deleted file can still be read when the s3guard entry has the versionId.

Proposed: if the FS is versioned and the file status has versionID then we switch to tests which assert the file is readable, rather than tests which assert it isn't there
"
Distcp print wrong log info when use -log,13293768,Patch Available,Minor,,25/Mar/20 09:30,,3.1.1,"when run distcp with -log /logpath -v, distcp will print copy status and file info to /logpath, but print log with wrong file zise. The logs print as follows:

FILE_COPIED: source=hdfs://ns1/test/stax2-api-3.1.4.jar, size=161867 --> target=hdfs://ns1/tmp/target/stax2-api-3.1.4.jar, size=0

As I analysis ,the root cause is as follows:

targrtFileStatus got before copying. So targrtFileStatus is null. Here should get targrtFileStatus again after file copying.

!image-2020-03-25-17-28-33-394.png!"
ABFS: Change in expiry calculation for MSI token provider,13288124,Resolved,Minor,Fixed,27/Feb/20 18:43,11/Mar/20 20:39,3.3.0,Set token expiry time as the value of expires_on field from the MSI response in case it is present
Add dropped connections metric for Server,13280481,Patch Available,Minor,,20/Jan/20 05:10,,3.3.0,With this metric we can see that the number of handled rpcs which weren't sent to clients.
fs.azure.user.agent.prefix adds customisation as suffix.,13289466,Open,Minor,,04/Mar/20 08:45,,3.2.0,"Setting fs.azure.user.agent.prefix adds customization as suffix

eg: when I set `fs.azure.user.agent.prefix` to my email the user agent value is 
Azure Blob FS/1.0 (DBFS) (JavaJRE 1.8.0_232; Linux 4.15.0-1050-azure; SunJSSE-1.8) arunravimv@gmail.com
 I believe this is inconsistent with the similar setting in AWS S3 fs.s3a.user.agent.prefix where the customization is added as a prefix. I believe that it would be great if we make this consistent with configuration name.

 

The proposed change is to fix the AbfsClient.initializeUserAgent.

 

We use a user agent information available in cloud service logs for extended tracking. 

 

 "
Update HdfsDesign.md to reduce ambiguity,13289040,Resolved,Minor,Fixed,02/Mar/20 20:34,04/Mar/20 02:13,,"A proposed update to [https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/HdfsDesign.md|http://example.com/] in the section ""Replica Placement: The First Baby Steps"" 4th paragraph, 2nd last line.

The sentence is leading to ambiguity of reader.

Considering the statement segmented in 3 parts by the commas:
 # the first part talks about ""one thirds of replicas"";
 # the second part talks about ""two thirds of replicas""
 # the third part talking about ""the other third"" is leading to ambiguity when one thirds and two thirds have already accounted for the whole.

Proposed solution:

Getting rid of the third part or rephrasing entire sentence to capture the overall essence of the sentence.

In other words, replacing 

_One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks._

with

-_One third of replicas are on one node, two thirds of replicas are on one rack._-
{quote}{{Two replicas are on different nodes of one rack and the remaining replica is on a node of one of the other racks.}}
{quote}
In addition to this, found 2 more sentences in same paragraph that will be corrected.

1.
{quote}However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three.
{quote}
will replace: does reduce -> does not reduce

2. 
{quote}With this policy, the replicas of a file do not evenly distribute across the racks.
{quote}
will replace: replicas of a file -> replicas of a block 

Please suggest if any additional meaning is getting lost with this replacement."
Sort fields in ReflectionUtils.java,13288733,Resolved,Minor,Fixed,02/Mar/20 10:09,02/Mar/20 17:54,3.3.0,"The tests in _org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl#testInitFirstVerifyCallBacks_ and _org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately_ can fail.

java.lang.AssertionError:

Element 0 for metrics expected:<MetricCounterLong{info=MetricsInfoImpl

{name=C1, description=C1 desc}

, value=1}>

but was:<MetricGaugeLong{info=MetricsInfoImpl

{name=G1, description=G1 desc}

, value=2}>

at org.junit.Assert.fail(Assert.java:88)

at org.junit.Assert.failNotEquals(Assert.java:834)

at org.junit.Assert.assertEquals(Assert.java:118)

at org.apache.hadoop.test.MoreAsserts.assertEquals(MoreAsserts.java:60)

at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.checkMetricsRecords(TestMetricsSystemImpl.java:439)

at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testInitFirstVerifyCallBacks(TestMetricsSystemImpl.java:178)

 

The root cause of this failure can be analyzed in the following stack trace:

_java.lang.Class.*getDeclaredFields*(Class.java:1916)_
 _org.apache.hadoop.util.ReflectionUtils.getDeclaredFieldsIncludingInherited(ReflectionUtils.java:353)_
 _org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.<init>(MetricsSourceBuilder.java:68)_
 _org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder(MetricsAnnotations.java:43)_
 _org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:223)_
 _org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testInitFirstVerifyCallBacks(TestMetricsSystemImpl.java:156)_

The specification about getDeclaredFields() says that ""the elements in the returned array are not sorted and are not in any particular order"". The documentation is here for your reference: [https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html#getDeclaredFields--]
 And the behaviour might be different for different JVM versions or vendors

 

The fix is to sort the fields returned by getDeclaredFields() so that the non-deterministic behaviour can be eliminated completely. In this way, the test becomes more stable and it will not suffer from the failure above any more.

 

 

 

 "
cmake is missing in the CentOS 8 section of BUILDING.txt,13284727,Resolved,Minor,Fixed,12/Feb/20 05:46,12/Feb/20 12:18,,"The following command does not install cmake by default:
{noformat}
$ sudo dnf group install 'Development Tools'{noformat}

cmake is an optional package and {{--with-optional}} should be specified."
start-build-env.sh behaves incorrectly when username is numeric only,13284250,Resolved,Minor,Fixed,10/Feb/20 10:42,12/Feb/20 05:13,,"When username is numaric only, the build environment does not run correctly.
Here is my case.

{noformat}
~/hadoop$ ./start-build-env.sh
...
Successfully tagged hadoop-build-1649860140:latest

 _   _           _                    ______
| | | |         | |                   |  _  \
| |_| | __ _  __| | ___   ___  _ __   | | | |_____   __
|  _  |/ _` |/ _` |/ _ \ / _ \| '_ \  | | | / _ \ \ / /
| | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\ V /
\_| |_/\__,_|\__,_|\___/ \___/| .__/  |___/ \___| \_(_)
                              | |
                              |_|

This is the standard Hadoop Developer build environment.
This has all the right tools installed required to build
Hadoop from source.

I have no name!@fceab279f8d1:~/hadoop$ whoami
whoami: cannot find name for user ID 1112533
I have no name!@fceab279f8d1:~/hadoop$ sudo ls
sudo: unknown uid 1112533: who are you?
{noformat}

I changed {{USER_NAME}} to {{USER_ID}} in the script. Then it worked correctly."
Test TestGroupsCaching fail if HashSet iterates in a different order,13283760,Resolved,Minor,Fixed,06/Feb/20 18:46,11/Feb/20 11:26,3.2.1,"The test `testNegativeGroupCaching` can fail if the iteration order of HashSet changes. In detail, the method `assertEquals` (line 331) compares `groups.getGroups(user)` with an ArrayList `myGroups`. The method `getGroups` converts `allGroups` (a HashSet) to a list and it calls iterator in HashSet. However, the iteration is non-deterministic.

This PR proposes to modify HashSet to LinkedHashSet for a deterministic order."
Replace com.sun.istack.Nullable with javax.annotation.Nullable in DNS.java,13282522,Resolved,Minor,Fixed,31/Jan/20 03:12,07/Feb/20 15:22,,com.sun.istack.Nullable is used in only DNS.java and javax.annotation.Nullable is widely used.
S3Guard listFiles will not query S3 if all listings are authoritative,13279427,Resolved,Minor,Fixed,14/Jan/20 19:28,30/Jan/20 10:20,3.3.0,"S3Guard does not respect authoritative metadatastore when listFiles is used with recursive=true. It queries S3 even when given directory tree is 1-level with no nested directories and the parent directory listing is authoritative. S3Guard should check the listings in given directory tree for authoritativeness and not query S3 when all listings in the tree are marked as authoritative in metadata table (given metadatastore is configured to be authoritative.

Below is the description of how the current code works:

S3AFileSystem#listFiles with recursive option, queries S3 even when directory listing is authoritative. FileStatusListingIterator is created with given entries from metadata store [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java#L126] . However, FileStatusListingIterator has an ObjectListingIterator that prefetches from s3 regardless of authoritative listing. We observed this behavior when using DynamDBMetadataStore.

I suppressed the unnecessary S3 calls by providing a dumb listing iterator to listFiles call in the provided patch. Obviously this is not a solution. Just demonstrating the source of the problem."
 Remove WARN log when ipc connection interrupted in Client#handleSaslConnectionFailure(),13278055,Resolved,Minor,Fixed,08/Jan/20 02:15,21/Jan/20 10:00,,"log info:
{code:java}
// Some comments here
2020-01-07,15:01:17,816 WARN org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.75.13.227:50415 remote=mb2-hadoop-prc-ct06.awsind/10.75.15.99:11230]. 60000 millis timeout left
{code}

With RequestHedgingProxyProvider, one rpc call will send multiple requests to all namenodes. After one request return successfully,  all other requests will be interrupted. It's not a big problem and should not print a warning log.
{code:java}
private synchronized void handleSaslConnectionFailure(
....
LOG.warn(""Exception encountered while connecting to ""
        + ""the server : "" + ex);
}


{code}
 "
Use forkCount and reuseForks parameters instead of forkMode in the config of maven surefire plugin,13279802,Resolved,Minor,Fixed,16/Jan/20 11:10,21/Jan/20 09:09,,"forkMode parameter is now deprecated.
https://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html
 
{noformat}
$ find . -name ""pom.xml"" | xargs grep ""forkMode""
./hadoop-tools/hadoop-distcp/pom.xml:          <forkMode>always</forkMode>
./hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml:              <forkMode>once</forkMode>
{noformat}"
Remove Superfluous Dependency on Zookeeper Recipes ,13280642,Resolved,Minor,Not A Problem,21/Jan/20 02:27,21/Jan/20 02:34,,ZooKeeper Recipes library is a dependency of hadoop-common even though it's not being used.  I noticed it when an upstream library was shading hadooo-common and also suspiciously shading ZK recipes library.
Add Write Convenience Methods,13277440,Resolved,Minor,Fixed,04/Jan/20 00:48,16/Jan/20 11:28,,Was working on something in HBase.  I just wanted to write a simple string to a HDFS file and I was surprised how tricky it was.  It inspired me to add some write convenience methods to HDFS common.
"In TestZKFailoverController, restore changes from HADOOP-11149 that were dropped by HDFS-6440",13277431,Resolved,Minor,Fixed,03/Jan/20 21:31,04/Jan/20 00:09,2.10.0,"In our automated tests, we are seeing intermittent failures in TestZKFailoverController.  I have been unable to reproduce the failures locally, but in examining the code, I found a difference that may explain the failures.

In trunk, HDFS-6440 ( Support more than 2 NameNodes. Contributed by Jesse Yates.) was checked in before HADOOP-11149. TestZKFailoverController times out), which changed the test added in HDFS-6440.

In branch-2, the order was reversed, and the test that was added in HDFS-6440 does not retain the fixes from HADOOP-11149.

Note that there was also a change from HDFS-10985. (o.a.h.ha.TestZKFailoverController should not use fixed time sleep before assertions.) that was missed in the HDFS-6440 backport.

My proposal is to restore the changes from HADOOP-11149.  I made this change internally and it seems to have fixed the intermittent failures."
log accepted/rejected fs.s3a.authoritative.path paths @ debug,13294420,Open,Trivial,,27/Mar/20 10:18,,3.3.0,"HADOOP-16939 added filtering of auth paths from other buckets. Typos and misconfigs may stop paths being auth when people expect them

Proposed: we log @ debug which paths were accepted, which werent"
Typo in distcp counters,13289852,Open,Trivial,,05/Mar/20 15:52,,,"The logging of distcp job counters includes a type (""btyes"" instead of ""bytes""):
{noformat}
        DistCp Counters
                Bandwidth in Btyes=1077528522
{noformat}
"
Reduce the execution time of the TestRolloverSignerSecretProvider,13293093,Patch Available,Trivial,,21/Mar/20 11:49,,,"In my Laptop it takes 52Second, I found out that the last Thread.sleep is redundant and could be removed. Also, we can reduce its execution time by decreasing ""rolloverFrequency"" without any problem"
Fix Hadoops AWS Integration GitHub links,13278804,Resolved,Trivial,Won't Fix,10/Jan/20 22:11,17/Jan/20 03:39,,"The links in the ""see also"" section here point to broken .html links instead of .md links:

[https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md]"
[ABFS]: NPE in AbfsManagedApacheHttpConnection.toString() when not connected,13591677,Resolved,Blocker,Fixed,10/Sep/24 16:52,16/Sep/24 20:34,3.4.1,"if {{AbfsManagedApacheHttpConnection.toString()}} is invoked and httpClientConnection is null, you get a stack trace."
[ABFS] Restore ETAGS_AVAILABLE to abfs path capabilities,13592935,Resolved,Critical,Fixed,23/Sep/24 10:52,23/Sep/24 18:24,3.4.1,"HADOOP-19131 accidentally deleted  {{CommonPathCapabilities.ETAGS_AVAILABLE}} from the patch capabilities of abfs. 

restore"
LZO files cannot be decompressed,13589545,Open,Critical,,21/Aug/24 06:12,,3.4.0,"The following command fails with the below exception:

hadoop fs -text [file:///home/hadoop/part-ak.lzo]
{code:java}
2024-08-21 05:05:07,418 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
2024-08-21 05:05:08,706 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 049362b7cf53ff5f739d6b1532457f2c6cd495e8]
2024-08-21 05:07:01,542 INFO compress.CodecPool: Got brand-new decompressor [.lzo]
2024-08-21 05:07:14,558 WARN lzo.LzopInputStream: Incorrect LZO file format: file did not end with four trailing zeroes.
java.io.IOException: Corrupted uncompressed block
    at com.hadoop.compression.lzo.LzopInputStream.verifyChecksums(LzopInputStream.java:219)
    at com.hadoop.compression.lzo.LzopInputStream.close(LzopInputStream.java:342)
    at org.apache.hadoop.fs.shell.Display$Cat.printToStdout(Display.java:102)
    at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:95)
    at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:383)
    at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:346)
    at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:319)
    at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:301)
    at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:285)
    at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:121)
    at org.apache.hadoop.fs.shell.Command.run(Command.java:192)
    at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
    at org.apache.hadoop.fs.FsShell.main(FsShell.java:390)
Exception in thread ""main"" java.lang.InternalError: lzo1x_decompress_safe returned: -5
    at com.hadoop.compression.lzo.LzoDecompressor.decompressBytesDirect(Native Method)
    at com.hadoop.compression.lzo.LzoDecompressor.decompress(LzoDecompressor.java:315)
    at com.hadoop.compression.lzo.LzopDecompressor.decompress(LzopDecompressor.java:122)
    at com.hadoop.compression.lzo.LzopInputStream.decompress(LzopInputStream.java:252)
    at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:110)
    at java.base/java.io.InputStream.read(InputStream.java:218)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:95)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:132)
    at org.apache.hadoop.fs.shell.Display$Cat.printToStdout(Display.java:100)
    at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:95)
    at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:383)
    at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:346)
    at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:319)
    at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:301)
    at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:285)
    at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:121)
    at org.apache.hadoop.fs.shell.Command.run(Command.java:192)
    at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
    at org.apache.hadoop.fs.FsShell.main(FsShell.java:390) {code}
From my analysis, i was pinpoint to the [change|https://github.com/apache/hadoop/pull/5912/files#diff-268b9968a4db21ac6eeb7bcaef10e4db744d00ba53989fc7251bb3e8d9eac7dfR904] which changed _io.compression.codec.lzo.buffersize_ from 64KB to 256KB.
Earlier, the default value was being picked from [here|https://github.com/twitter/hadoop-lzo/blob/master/src/main/java/com/hadoop/compression/lzo/LzoCodec.java#L51].

Let me know if my analysis looks good. What should be the proper approach to fixing it?

 "
S3ABlockOutputStream no longer sends progress events in close(),13587613,Resolved,Critical,Fixed,01/Aug/24 17:55,02/Aug/24 16:17,3.4.0,"We don't get progress events passed through from S3ABlockOutputStream to any Progress instance passed in which doesn't implement ProgressListener

This is due to a signature mismatch between the changed ProgressableListener interface and the {{S3ABlockOutputStream.ProgressListener}} impl.

* critical because distcp jobs will timeout on large uploads without this
* trivial to fix; does need a test"
S3A: Support S3 Conditional Writes,13589641,Open,Major,,21/Aug/24 18:07,,,"S3 Conditional Write (Put-if-absent) capability is now generally available - [https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/]

 

S3A should allow passing in this put-if-absent header to prevent over writing of files. 

There is a feature branch for this: HADOOP-19256-s3-conditional-writes

+ support etags to allow an overwrite to be restricted to overwriting a specific version. This can be done through a createFile option.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-writes.html

Fun fact; third party stores will not reject overwrites if they don't recognise the headers, so there's no way to be sure they are supported without testing.

we need a flag to enable/disable conditional writes which can be exposed in a hasPathCapability()"
upgrade to protobuf-java 3.25.5 due to CVE-2024-7254,13593240,Resolved,Major,Fixed,25/Sep/24 16:49,16/Dec/24 15:02,,"https://github.com/advisories/GHSA-735f-pc8j-v9w8

Presumably protobuf encoded messages in Hadoop come from trusted sources but it is still useful to upgrade the jar."
Upgrade Mockito version to 4.11.0,13587461,Resolved,Major,Fixed,31/Jul/24 15:50,05/Nov/24 17:36,3.4.1,"While Compiling test classes with JDK17, faced error related to Mockito:
*Mockito cannot mock this class.*
So to make it compatible with jdk17 we have to upgrade the version of mockito-core as well as mockito-inline."
upgrade to jackson 2.14.3,13590692,Open,Major,,01/Sep/24 21:20,,,"see HADOOP-19230 and HADOOP-18332

Jackson 2.18 is causing some trouble but Jackson 2.14 has some useful changes. I can follow up later and try to use a newer version of Jackson but this is a start."
Use stable sort in commandQueue,13591442,Open,Major,,09/Sep/24 06:16,,3.4.0,"h2. Purpose
 - To remove possibility of wrong-ordered log simulation

h2. Why this happens?
 - private DelayQueue<AuditReplayCommand> commandQueue is actually PriorityQueue that use unstable sort.
 - commandQueue can have order that is not same to original audit log order.
 - In real production, there is the commands that occur same time and should be fixed order.
{code:bash}
# getfileinfo before open
2024-07-01 19:27:12,886 INFO FSNamesystem.audit: allowed=true   ugi=xx-xx (auth:TOKEN) via hive/hadoop.example.com@EXAMPLE.PROD (auth:TOKEN)   ip=/10.xx.xxx.xxx       cmd=getfileinfo src=/user/hive/warehouse/a.db/b/date_id=2024-06-16/part-xxxx.gz.parquet     dst=null        perm=null       proto=rpc
2024-07-01 19:27:12,886 INFO FSNamesystem.audit: allowed=true   ugi=xx-xx (auth:TOKEN) via hive/hadoop.example.com@EXAMPLE.PROD (auth:TOKEN)   ip=/10.xx.xxx.xxx       cmd=open        src=/user/hive/warehouse/a.db/b/date_id=2024-06-16/part-xxxx.gz.parquet     dst=null        perm=null       proto=rpc

# create before setPermission
# this examples have not exactly same time, but could be same when rate factor is high enough
2024-07-01 17:25:30,867 INFO FSNamesystem.audit: allowed=true   ugi=yy-yy@EXAMPLE.PROD (auth:KERBEROS)    ip=/10.xxx.xx.xxx       cmd=create      src=/user/yy-yy/.staging/job_1716867484406_290658/job.xml dst=null        perm=yy-yy:zzz:rw-rw-r--  proto=rpc
2024-07-01 17:25:30,871 INFO FSNamesystem.audit: allowed=true   ugi=yy-yy@EXAMPLE.PROD (auth:KERBEROS)    ip=/10.xxx.xx.xxx       cmd=setPermission       src=/user/yy-yy/.staging/job_1716867484406_290658/job.xml dst=null        perm=yy-yy:zzz:rw-r--r--  proto=rpc
{code}

h2. How much improve test accuracy when use stable sort?
 - Using stable sort, wrong ordered simulation could not occur.
 -- I fixed code to use line number of audit log in sorting criteria.
 -- Because it is not simple to change DelayQueue data structure to use stable sort
 - Multi threading or client-ip-based-partitioning could be occur in real production and affect log order, but not critical.
 -- Client-ip-based-partitioning is even similar to real production choas log order
 - This is the graph that
 -- use real production hdfs audit log
 -- compare stable sort and unstable sort with different rate(1~4)
 -- use 5 minutes simulation(in rate 1) ip-based-partitioned-audit-log
 -- shows total valid command, total read latency, total write latency
!image-2024-09-19-16-40-34-947.png|width=615,height=740!
 - Conclusion
 -- Stable sort ensure almost similar valid command number.
 -- Unstable sort sometimes extremely high latency because of wrong ordered log simulation."
S3A: remove option to delete directory markers,13592379,Resolved,Major,Fixed,17/Sep/24 17:25,08/Jan/25 17:49,3.4.1,"S3A no longer supports the ability to disable deletion of parent directory
markers on file or directory creation.

The option ""fs.s3a.directory.marker.retention"" is no longer supported.

This is incompatible with all hadoop versions before 3.2.2 and with hadoop 3.3.0
    when applications using the s3a connector attempt to write to the same repository.

Fix: upgrade to a recent version of the Hadoop libraries when working with S3 storage, for which there are many reasons


h3. cherrypicking:
* include HADOOP-19464
"
Getting NullPointerException when the unauthorised user tries to perform the key operation,13588288,Resolved,Major,Fixed,08/Aug/24 13:42,20/Aug/24 13:06,,"While validating the tomcat 9.x in apache Ranger when user doesn't have appropriate permission in Ranger policies we faced the NPE for key operation using hadoop cmd.

*Problem :*

_Functionally -_ We are facing the NPE while performing key operations from hadoop cmd with the user not having permission in policy on cluster with tomcat v9.x. However with curl to Ranger KSM Server is working as expected.

_Technically -_ Getting response message as null on client side in hadoop-common at [KMSClientProvider.java|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java#L565]

*E.G.*

_with Ranger KMS tomcat v9.x_
{code:java}
 hadoop key list
The list subcommand displays the keynames contained within
a particular provider as configured in core-site.xml or
specified with the -provider argument. -metadata displays
the metadata. If -strict is supplied, fail immediately if
the provider requires a password and none is given.
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.KeyShell.prettifyException(KeyShell.java:541)
	at org.apache.hadoop.crypto.key.KeyShell.printException(KeyShell.java:536)
	at org.apache.hadoop.tools.CommandShell.run(CommandShell.java:79)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)
	at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:553) {code}
_on_ _Ranger KMS_ _tomcat v8.5.x_
{code:java}
hadoop key list
The list subcommand displays the keynames contained within
a particular provider as configured in core-site.xml or
specified with the -provider argument. -metadata displays
the metadata. If -strict is supplied, fail immediately ifthe provider requires a password and none is given.
Executing command failed with the following exception: AuthorizationException: User:xyzuser not allowed to do 'GET_KEYS'{code}
*Debug logs on Ranger KMS Server side*

1.) Added logs in [KMSExceptionsProvider.java|https://github.com/apache/ranger/blob/master/kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSExceptionsProvider.java] in method _createResponse()_ and _toResponse()_ where we are generating response to send it to client i.e. _hadoop-common_
Logs are exactly same on both the tomcat scenario. Refer below the added logs, detailed logs will be available in ranger kms log file on cluster. 
{code:java}
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ==== Entered into toResponse =========
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ==== exception =========org.apache.hadoop.security.authorize.AuthorizationException: User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ==== exception.getClass() =========class org.apache.hadoop.security.authorize.AuthorizationException
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ==== AuthorizationException =========
2024-07-25 11:35:51,452 WARN  org.apache.hadoop.crypto.key.kms.server.KMS: [https-jsse-nio-9494-exec-2]: User systest@ROOT.COMOPS.SITE (auth:KERBEROS) request GET https://ccycloud-1.ss-tomcat-test1.root.comops.site:9494/kms/v1/keys/names caused exception.
org.apache.hadoop.security.authorize.AuthorizationException: User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ===== Entered into createResponse ======
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ==== status ======= Forbidden
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ======= ex ======= org.apache.hadoop.security.authorize.AuthorizationException: User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ======= ex.getStackTrace() ======= [Ljava.lang.StackTraceElement;@3e75ae9d
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ======= ex.getMessage() ======= User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,452 INFO  org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider: [https-jsse-nio-9494-exec-2]: ======= ex.toString() ======= org.apache.hadoop.security.authorize.AuthorizationException: User:systest not allowed to do 'GET_KEYS'  {code}
2.) Also added logs in [KMSExceptionsProvider.java|https://github.com/apache/ranger/blob/master/kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSExceptionsProvider.java]

Adding logs in code base
{code:java}
public void setStatus(int sc, String sm) {
        LOG.info(""========= setStatus with message============ "");
      statusCode = sc;
      msg = sm;
        LOG.info(""========= sc ============ "" +sc);
        LOG.info(""========= msg ============ "" +msg);

            if(sc == 403) {
                LOG.info(""===== its 403 ===="");
                super.setStatus(sc, sm);
            } else{
                super.setStatus(sc, sm);
            }
    } {code}
LOGS:
{code:java}
2024-07-25 11:35:51,460 INFO  org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter: [https-jsse-nio-9494-exec-2]: ========= setStatus with message============
2024-07-25 11:35:51,460 INFO  org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter: [https-jsse-nio-9494-exec-2]: ========= sc ============ 403
2024-07-25 11:35:51,460 INFO  org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter: [https-jsse-nio-9494-exec-2]: ========= msg ============ Forbidden
2024-07-25 11:35:51,460 INFO  org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter: [https-jsse-nio-9494-exec-2]: ===== its 403 ==== {code}
This explains that the KMS server is sending the code and message appropriately.

*Debug logs on Hadoop Common Client side*
1.) Added logs in [HttpExceptionUtils.java|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HttpExceptionUtils.java] to make sure whether appropriate response is received.

Logs will be available in ranger kms log file.
{code:java}
2024-07-25 11:35:51,453 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ====== Entered into createJerseyExceptionResponse ====
2024-07-25 11:35:51,453 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ========== ex ========   org.apache.hadoop.security.authorize.AuthorizationException: User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,454 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ========== ex.getMessage ========   User:systest not allowed to do 'GET_KEYS'
2024-07-25 11:35:51,454 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ========== status ========   Forbidden
2024-07-25 11:35:51,454 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ========== status.getStatusCode ========   403
2024-07-25 11:35:51,454 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]: ========== status.getReasonPhrase ========   Forbidden
2024-07-25 11:35:51,454 INFO  org.apache.hadoop.util.HttpExceptionUtils: [https-jsse-nio-9494-exec-2]:  =======  response  ======== com.sun.jersey.core.spi.factory.ResponseImpl@5bd8a59b  {code}
2.) Added logs exactly before NPE occurs  i.e. [KMSClientProvider.java|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java#L564]
Adding logs in code base


LOG.info("" =========== conn ======== "" + conn);
Map<String, List<String>> map = conn.getHeaderFields();
LOG.info(""======= map ======== "" + map);for (Map.Entry<String, List<String>> entry : map.entrySet()) {
  LOG.info(""=============== "" + ""Key : "" + entry.getKey() +          "" ,Value : "" + entry.getValue());
}
LOG.info("" =========== conn.getResponseMessage ======== "" + conn.getResponseMessage());
LOG.info("" =========== conn.getResponseCode ======== "" + conn.getResponseCode());if ((conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN
    && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) ||
        conn.getResponseMessage().contains(INVALID_SIGNATURE)))
    || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) { 

LOGS: This logs gets printed on terminal where we execute hadoop cmd .

_with Ranger KMS tomcat v9.x_


{code:java}
hadoop key list
24/07/25 11:38:15 INFO kms.KMSClientProvider: ======== Entered into call ========
24/07/25 11:38:15 INFO kms.KMSClientProvider:  =========== conn ======== sun.net.www.protocol.https.DelegateHttpsURLConnection:https://ccycloud-1.ss-tomcat-test1.root.comops.site:9494/kms/v1/keys/names
24/07/25 11:38:15 INFO kms.KMSClientProvider: ======= map ======== {Keep-Alive=[timeout=60], null=[HTTP/1.1 403], Strict-Transport-Security=[max-age=31536000; includeSubDomains; preload], Server=[Apache Ranger], Connection=[keep-alive], Content-Length=[220], Date=[Thu, 25 Jul 2024 11:38:15 GMT], Content-Type=[application/json]}
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Keep-Alive ,Value : [timeout=60]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : null ,Value : [HTTP/1.1 403]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Strict-Transport-Security ,Value : [max-age=31536000; includeSubDomains; preload]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Server ,Value : [Apache Ranger]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Connection ,Value : [keep-alive]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Content-Length ,Value : [220]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Date ,Value : [Thu, 25 Jul 2024 11:38:15 GMT]
24/07/25 11:38:15 INFO kms.KMSClientProvider: =============== Key : Content-Type ,Value : [application/json]
24/07/25 11:38:15 INFO kms.KMSClientProvider:  =========== conn.getResponseMessage ======== null
24/07/25 11:38:15 INFO kms.KMSClientProvider:  =========== conn.getResponseCode ======== 403
list [-provider <provider>] [-strict] [-metadata] [-help]:


The list subcommand displays the keynames contained within
a particular provider as configured in core-site.xml or
specified with the -provider argument. -metadata displays
the metadata. If -strict is supplied, fail immediately if
the provider requires a password and none is given.
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.KeyShell.prettifyException(KeyShell.java:541)
	at org.apache.hadoop.crypto.key.KeyShell.printException(KeyShell.java:536)
	at org.apache.hadoop.tools.CommandShell.run(CommandShell.java:79)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)
	at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:553) {code}

_with Ranger KMS tomcat v8.5.x_
hadoop key list
24/07/25 11:02:25 INFO kms.KMSClientProvider: ======== Entered into call ========
24/07/25 11:02:25 INFO kms.KMSClientProvider:  =========== conn ======== sun.net.www.protocol.https.DelegateHttpsURLConnection:https://ccycloud-1.ds-tomcat-test1.root.comops.site:9494/kms/v1/keys/names24/07/25 11:02:25 INFO kms.KMSClientProvider: ======= map ======== {Keep-Alive=[timeout=60], null=[HTTP/1.1 403 Forbidden], Strict-Transport-Security=[max-age=31536000; includeSubDomains; preload], Server=[Apache Ranger], Connection=[keep-alive], Content-Length=[220], Date=[Thu, 25 Jul 2024 11:02:25 GMT], Content-Type=[application/json]}
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Keep-Alive ,Value : [timeout=60]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : null ,Value : [HTTP/1.1 403 Forbidden]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Strict-Transport-Security ,Value : [max-age=31536000; includeSubDomains; preload]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Server ,Value : [Apache Ranger]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Connection ,Value : [keep-alive]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Content-Length ,Value : [220]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Date ,Value : [Thu, 25 Jul 2024 11:02:25 GMT]
24/07/25 11:02:25 INFO kms.KMSClientProvider: =============== Key : Content-Type ,Value : [application/json]
24/07/25 11:02:25 INFO kms.KMSClientProvider:  =========== conn.getResponseMessage ======== Forbidden
24/07/25 11:02:25 INFO kms.KMSClientProvider:  =========== conn.getResponseCode ======== 403
Cannot list keys for KeyProvider: org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider@209da20d
list [-provider <provider>] [-strict] [-metadata] [-help]:


The list subcommand displays the keynames contained within
a particular provider as configured in core-site.xml or
specified with the -provider argument. -metadata displays
the metadata. If -strict is supplied, fail immediately ifthe provider requires a password and none is given.
Executing command failed with the following exception: AuthorizationException: User:xyzuser not allowed to do 'GET_KEYS' 

Please notice 
_with tomcat v9.x : *Key : null ,Value : [HTTP/1.1 403]*_ 
_with. tomcat v8.5.x : *Key : null ,Value : [HTTP/1.1 403 Forbidden]*_

**

Message ""Forbidden"" is not present with tomcat v9.x.

It seems that tomcat v9.x is not setting the message and hadoop-common is trying to get where we are facing  NPE.



Also checked for _*org.apache.coyote.USE_CUSTOM_STATUS_MSG_IN_HEADER*_ but its not available in tomcat 9.x
Ref:
Tomcat Doc for 8.5.x [https://tomcat.apache.org/tomcat-8.5-doc/api/org/apache/coyote/Constants.html#USE_CUSTOM_STATUS_MSG_IN_HEADER]

Tomcat Doc for 9.x [https://tomcat.apache.org/tomcat-9.0-doc/api/org/apache/coyote/Constants.html]

Thanks





 "
ABFS: [FnsOverBlob] Implementing Rename and Delete APIs over Blob Endpoint,13586660,Resolved,Major,Fixed,23/Jul/24 11:36,17/Feb/25 17:19,3.4.0,"Currently, we only support rename and delete operations on the DFS endpoint. The reason for supporting rename and delete operations on the Blob endpoint is that the Blob endpoint does not account for hierarchy. We need to ensure that the HDFS contracts are maintained when performing rename and delete operations. Renaming or deleting a directory over the Blob endpoint requires the client to handle the orchestration and rename or delete all the blobs within the specified directory.
 
The task outlines the considerations for implementing rename and delete operations for the FNS-blob endpoint to ensure compatibility with HDFS contracts.
 * {*}Blob Endpoint Usage{*}: The task addresses the need for abstraction in the code to maintain HDFS contracts while performing rename and delete operations on the blob endpoint, which does not support hierarchy.
 * {*}Rename Operations{*}: The {{AzureBlobFileSystem#rename()}} method will use a {{RenameHandler}} instance to handle rename operations, with separate handlers for the DFS and blob endpoints. This method includes prechecks, destination adjustments, and orchestration of directory renaming for blobs.
 * {*}Atomic Rename{*}: Atomic renaming is essential for blob endpoints, as it requires orchestration to copy or delete each blob within the directory. A configuration will allow developers to specify directories for atomic renaming, with a JSON file to track the status of renames.
 * {*}Delete Operations{*}: Delete operations are simpler than renames, requiring fewer HDFS contract checks. For blob endpoints, the client must handle orchestration, including managing orphaned directories created by Az-copy.
 * {*}Orchestration for Rename/Delete{*}: Orchestration for rename and delete operations over blob endpoints involves listing blobs and performing actions on each blob. The process must be optimized to handle large numbers of blobs efficiently.
 * {*}Need for Optimization{*}: Optimization is crucial because the {{ListBlob}} API can return a maximum of 5000 blobs at once, necessitating multiple calls for large directories. The task proposes a producer-consumer model to handle blobs in parallel, thereby reducing processing time and memory usage.
 * {*}Producer-Consumer Design{*}: The proposed design includes a producer to list blobs, a queue to store the blobs, and a consumer to process them in parallel. This approach aims to improve efficiency and mitigate memory issues.

More details will follow

Perquisites for this Patch:
1. HADOOP-19187 ABFS: [FnsOverBlob]Making AbfsClient Abstract for supporting both DFS and Blob Endpoint - ASF JIRA (apache.org)

2. HADOOP-19226 ABFS: [FnsOverBlob]Implementing Azure Rest APIs on Blob Endpoint for AbfsBlobClient - ASF JIRA (apache.org)

3. HADOOP-19207 ABFS: [FnsOverBlob]Response Handling of Blob Endpoint APIs and Metadata APIs - ASF JIRA (apache.org)"
ABFS: Initialize ABFS client timer only when metric collection is enabled,13592666,Resolved,Major,Fixed,20/Sep/24 05:22,30/Sep/24 15:59,3.4.0,"In the current flow, we are initializing the timer of the {{abfs-timer-client}} outside the metric collection enable check. As a result, for each file system, when the {{AbfsClient}} object is initialized, it spawns a thread to evaluate the time of the ABFS client. Since we are purging/closing the timer inside the metric collection check, these threads are not being closed, causing them to persist in a long-lived state. To fix this, we are moving the timer initialization inside the condition"
ABFS: [FnsOverBlob] Implementing Ingress Support with various Fallback Handling,13586659,Resolved,Major,Fixed,23/Jul/24 11:33,17/Feb/25 17:18,3.4.0,"Scope of this task is to refactor the AbfsOutputStream class to handle the ingress for DFS and Blob endpoint effectively.

More details will be added soon.

Perquisites for this Patch:
1. [HADOOP-19187] ABFS: [FnsOverBlob]Making AbfsClient Abstract for supporting both DFS and Blob Endpoint - ASF JIRA (apache.org)

2. [HADOOP-19226] ABFS: [FnsOverBlob]Implementing Azure Rest APIs on Blob Endpoint for AbfsBlobClient - ASF JIRA (apache.org)

3. [HADOOP-19207] ABFS: [FnsOverBlob]Response Handling of Blob Endpoint APIs and Metadata APIs - ASF JIRA (apache.org)"
AliyunOSS: Add a feature to disable redirection for the OSS connector.,13587426,Open,Major,,31/Jul/24 11:14,,3.1.0,"For security reasons, some users of the OSS connector wish to disable the connector's HTTP redirection functionality. The OSS Java SDK have the capability to turn off HTTP redirection, but the configuration is not exposed in the {{core-site.xml}} file. This change primarily involves adding a flag to disable HTTP redirection in the {{core-site.xml}} file"
ABFS: [FnsOverBlob] Adding Integration Tests for Special Scenarios in Blob Endpoint,13586663,Open,Major,,23/Jul/24 12:03,,3.4.0,"FNS accounts does not understand directories and to create that abstraction client has to handle the cases where hdfs operations include interactions with directory paths. This needs some additional testing to be done for each HDFS operation where path can exists as directory.

More details to follow

Perquisites: 
 # HADOOP-19187 ABFS: [FnsOverBlob]Making AbfsClient Abstract for supporting both DFS and Blob Endpoint
 # HADOOP-19207 ABFS: [FnsOverBlob]Response Handling of Blob Endpoint APIs and Metadata APIs[|https://issues.apache.org/jira/secure/DeleteLink.jspa?id=13579416&destId=13583033&linkType=12310460&atl_token=A5KQ-2QAV-T4JA-FDED_17fc7154167b7d6d6490aa6508db554fd6d7af24_lin]
 # HADOOP-19226 ABFS: [FnsOverBlob]Implementing Azure Rest APIs on Blob Endpoint for AbfsBlobClient
 # HADOOP-19232 ABFS: [FnsOverBlob] Implementing Ingress Support with various Fallback Handling
 # HADOOP-19233 ABFS: [FnsOverBlob] Implementing Rename and Delete APIs over Blob Endpoint
 "
Vector IO on cloud storage: what is a good minimum seek size?,13586172,Resolved,Major,Fixed,17/Jul/24 19:18,15/Jan/25 12:18,3.4.1,"vector iO has a max size to coalesce ranges, but it also needs a maximum gap between ranges to justify the merge. Right now we could have a read where two vectors of size 8 bytes can be merged with a 1 MB gap between them -and that's wasteful. 

We could also consider an ""efficiency"" metric which looks at the ratio of bytes-read to bytes-discarded. Not sure what we'd do with it, but we could track it as an IOStat

h2. Current values

The thresholds at which adjacent vector IO read ranges are coalesced into a
single range has been increased, as has the limit at which point they are 
considered large enough that parallel reads are faster.

* The min/max for local filesystems and any other FS without custom support are 
now 16K and 1M
* s3a and abfs use 128K as the minimum size, 2M for max.


"
Upgrade to 9.4.57.v20241219 due to CVE-2024-8184,13585575,In Progress,Major,,12/Jul/24 08:46,,,"Upgrade to jetty 9.4.56 due to

[CVE-2024-22201|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-22201]

[CVE-2023-44487|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-44487]

[CVE-2024-8184|https://nvd.nist.gov/vuln/detail/CVE-2024-8184] : [https://github.com/advisories/GHSA-g8m5-722r-8whq]"
[JDK17] Upgade wildfly-openssl:1.1.3.Final to 2.1.4.Final+,13590909,Resolved,Major,Fixed,03/Sep/24 19:19,14/Sep/24 05:29,3.5.0,Apparentlly wildfly has to be updated to  2.1.4.Final to work on java17+; without that s3 and azure need to be configured to not use openssl to connect to stores
Integration of Volcano Engine TOS in Hadoop.,13586864,Open,Major,,25/Jul/24 02:49,,3.4.0,"Volcano Engine is a fast growing cloud vendor launched by ByteDance, and TOS is the object storage service of Volcano Engine. A common way is to store data into TOS and run Hadoop/Spark/Flink applications to access TOS. But there is no original support for TOS in hadoop, thus it is not easy for users to build their Big Data System based on TOS.
 
This work aims to integrate TOS with Hadoop to help users run their applications on TOS. Users only need to do some simple configuration, then their applications can read/write TOS without any code change. This work is similar to AWS S3, AzureBlob, AliyunOSS, Tencnet COS and HuaweiCloud Object Storage in Hadoop.

 

 Please see the attached document ""Integration of Volcano Engine TOS in Hadoop"" for more details."
[JDK17] Add a JDK17 profile,13593827,Reopened,Major,,30/Sep/24 23:18,,,
Implement bulk delete command as hadoop fs command operation ,13589506,Open,Major,,20/Aug/24 20:20,,3.4.1,"
{code}
hadoop fs -bulkdelete <base-url> <file> 

{code}

Key uses
* QE: Testing from python and other scripting languages
* cluster maintenance: actual bulk deletion operations from the store

one thought there: we MUST qualify paths with / elements: if a passed in path ends in /, it means ""delete a marker"", not ""delete a dir""'. and if it doesn't have one then it's an object.. This makes it possible to be used to delete surplus markers or where there is a file above another file...cloudstore listobjects finds this"
 Update AWS Java SDK to 2.27.14,13590618,Open,Major,,30/Aug/24 22:38,,,"Upgrade SDK to add IfNoneMatch support for upcoming PutIfNotExist functionality

 "
ABFS: [FnsOverBlob] Implementing Azure Rest APIs on Blob Endpoint for AbfsBlobClient,13585785,Resolved,Major,Fixed,15/Jul/24 04:37,26/Nov/24 14:37,3.4.0,"This is second task in series of tasks for implementing Blob Endpoint support for FNS accounts.

This patch will have changes to implement all the APIs over Blob Endpoint as a part of implementing AbfsBlobClient."
NoSuchMethodError in aws sdk third party logger in hadoop aws 3.4,13587268,Open,Major,,30/Jul/24 04:18,,3.4.0,"{code:java}
""localizedMessage"": ""java.lang.NoSuchMethodError: 'software.amazon.awssdk.thirdparty.org.slf4j.Logger software.amazon.awssdk.utils.Logger.logger()'"",
""message"": ""java.lang.NoSuchMethodError: 'software.amazon.awssdk.thirdparty.org.slf4j.Logger software.amazon.awssdk.utils.Logger.logger()'"",
""name"": ""com.google.common.util.concurrent.ExecutionError"",
""cause"": {
  ""commonElementCount"": 1,
  ""localizedMessage"": ""'software.amazon.awssdk.thirdparty.org.slf4j.Logger software.amazon.awssdk.utils.Logger.logger()'"",
  ""message"": ""'software.amazon.awssdk.thirdparty.org.slf4j.Logger software.amazon.awssdk.utils.Logger.logger()'"",
  ""name"": ""java.lang.NoSuchMethodError"",
  ""extendedStackTrace"": [
    {
      ""class"": ""software.amazon.awssdk.transfer.s3.internal.GenericS3TransferManager"",
      ""method"": ""close"",
      ""file"": ""GenericS3TransferManager.java"",
      ""line"": 393,
      ""exact"": false,
      ""location"": ""bundle-2.23.19.jar"",
      ""version"": ""?""
    },
    {
      ""class"": ""org.apache.hadoop.fs.s3a.S3AUtils"",
      ""method"": ""closeAutocloseables"",
      ""file"": ""S3AUtils.java"",
      ""line"": 1553,
      ""exact"": false,
      ""location"": ""hadoop-aws-3.4.0.jar"",
      ""version"": ""?""
    },
    {
      ""class"": ""org.apache.hadoop.fs.s3a.S3AFileSystem"",
      ""method"": ""stopAllServices"",
      ""file"": ""S3AFileSystem.java"",
      ""line"": 4358,
      ""exact"": false,
      ""location"": ""hadoop-aws-3.4.0.jar"",
      ""version"": ""?""
    },
    {
      ""class"": ""org.apache.hadoop.fs.s3a.S3AFileSystem"",
      ""method"": ""initialize"",
      ""file"": ""S3AFileSystem.java"",
      ""line"": 758,
      ""exact"": false,
      ""location"": ""hadoop-aws-3.4.0.jar"",
      ""version"": ""?""
    },
    {
      ""class"": ""org.apache.hadoop.fs.FileSystem"",
      ""method"": ""createFileSystem"",
      ""file"": ""FileSystem.java"",
      ""line"": 3601,
      ""exact"": false,
      ""location"": ""hadoop-common-3.4.0.jar"",
      ""version"": ""?""
    },
    {
      ""class"": ""org.apache.hadoop.fs.FileSystem"",
      ""method"": ""get"",
      ""file"": ""FileSystem.java"",
      ""line"": 552,
      ""exact"": false,
      ""location"": ""hadoop-common-3.4.0.jar"",
      ""version"": ""?""
    }, {code}
 

This appears to be related to how shading works in the aws bundle sdk.

Hadoop Version : hadoop-aws-3.4.0

AWS-SDK-Bundle - 2.23.19

 "
Support S3A cross region access when S3 region/endpoint is set,13593036,Resolved,Major,Fixed,24/Sep/24 04:43,04/Oct/24 13:59,3.4.0,"Currently when S3 region nor endpoint is set, the default region is set to us-east-2 with cross region access enabled. But when region or endpoint is set, cross region access is not enabled.

A new option enables cross region access as a separate config and enable/disables it
irrespective of region/endpoint is set.
   s3a.cross.region.access.enabled

default: enables cross region access as a separate config and enable/disables it
irrespective of region/endpoint is set.

"
S3A: fs.s3a.connection.request.timeout too low for large uploads over slow links,13593771,Reopened,Major,,30/Sep/24 18:36,,3.4.0,"The value of {{fs.s3a.connection.request.timeout}} (default = 60s} is too low for large uploads over slow connections.

I suspect something changed between the v1 and v2 SDK versions so that put was exempt from the normal timeouts, It is not and now surfaces in failures to upload 1+ GB files over slower network connections. Smailer (for example 128 MB) files work.

The parallel queuing of writes in the S3ABlockOutputStream is helping create this problem as it queues multiple blocks at the same time, so per-block bandwidth becomes available/blocks ; four blocks cuts the capacity down by a quarter.

The fix is straightforward: use a much bigger timeout. I'm going to propose 15 minutes. We need to strike a balance between upload time allocation and other requests timing out.

I do worry about other consequences; we've found that timeout exception happy to hide the underlying causes of retry failures -so in fact this may be better for all but a server hanging after the HTTP request is initiated.

too bad we can't alter the timeout for different requests"
"ABFS: Allow ""fs.azure.account.hns.enabled"" to be set as Account Specific Config",13592889,Resolved,Major,Fixed,23/Sep/24 04:50,27/Sep/24 14:48,3.4.0,"There are a few reported requirements where users working with multiple file systems need to specify this config either only for some accounts or set it differently for different account.
ABFS driver today does not allow this to be set as account specific config.

This Jira is to allow that as a new support."
Fix KMSTokenRenewer#handleKind dependency on BouncyCastleProvider class,13593056,Open,Major,,24/Sep/24 08:26,,," 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/bouncycastle/jce/provider/BouncyCastleProvider
    at org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer.handleKind(KMSClientProvider.java:180)
    at org.apache.hadoop.security.token.Token.getRenewer(Token.java:467)
    at org.apache.hadoop.security.token.Token.renew(Token.java:500)
    at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider.$anonfun$getTokenRenewalInterval$3(HadoopFSDelegationTokenProvider.scala:147)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
    at scala.util.Try$.apply(Try.scala:217)
    at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider.$anonfun$getTokenRenewalInterval$2(HadoopFSDelegationTokenProvider.scala:146) {code}"
[JDK17] Upgrade maven.plugin-tools.version to 3.10.2,13593826,Resolved,Major,Fixed,30/Sep/24 23:14,08/Nov/24 00:56,3.4.0,
hadoop-aws exclude aws-java-sdk-bundle dependencies. Use aws-java-sdk-s3 instead,13592816,Open,Major,,21/Sep/24 15:57,,,"hadoop-aws directly depends on aws-java-sdk-bundle, and the size of aws-java-sdk-bundle is too large. "
`CombinedFileRange.merge` should not convert disjoint ranges into overlapped ones,13593615,Resolved,Major,Fixed,29/Sep/24 05:01,10/Oct/24 08:52,3.3.9,"Currently, Hadoop has a bug to convert disjoint ranges into overlapped ones and eventually fails by itself.
{code:java}
+  public void testMergeSortedRanges() {
+    List<FileRange> input = asList(
+        createFileRange(13816220, 24, null),
+        createFileRange(13816244, 7423960, null)
+    );
+    assertIsNotOrderedDisjoint(input, 100, 800);
+    final List<CombinedFileRange> outputList = mergeSortedRanges(
+        sortRangeList(input), 100, 1001, 2500);
+
+    assertRangeListSize(outputList, 1);
+    assertFileRange(outputList.get(0), 13816200, 7424100);
+  }
{code}

 !Screenshot 2024-09-28 at 22.02.01.png! "
Upgrade Guice from 4.0 to 5.1.0 to support Java 17,13584362,Resolved,Major,Fixed,01/Jul/24 12:00,06/Jul/24 07:44,,"Guice supports Java 17 since 5.1.0, see [google/guice#1536|https://github.com/google/guice/issues/1536]"
Resolve Certificate error in Hadoop-auth tests.,13584717,Resolved,Major,Fixed,04/Jul/24 10:36,03/Oct/24 07:18,3.4.2,"While compiling Hadoop-Trunk with JDK17, faced following errors in TestMultiSchemeAuthenticationHandler and 
TestLdapAuthenticationHandler classes.


{code:java}
[INFO] Running org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.256 s <<< FAILURE! - in org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler
[ERROR] org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler  Time elapsed: 1.255 s  <<< ERROR!
java.lang.IllegalAccessError: class org.apache.directory.server.core.security.CertificateUtil (in unnamed module @0x32e614e9) cannot access class sun.security.x509.X500Name (in module java.base) because module java.base does not export sun.security.x509 to unnamed module @0x32e614e9
        at org.apache.directory.server.core.security.CertificateUtil.createTempKeyStore(CertificateUtil.java:334)
        at org.apache.directory.server.factory.ServerAnnotationProcessor.instantiateLdapServer(ServerAnnotationProcessor.java:158)
        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:318)
        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:351) {code}"
[JDK17] Upgrade maven-war-plugin to 3.4.0,13593825,Resolved,Major,Fixed,30/Sep/24 23:12,03/Oct/24 14:32,3.4.0,"During the process of compiling JDK17, we needed a higher version, which has already been successfully applied in our internal builds."
"S3A: Unable to recover from failure of multipart block upload attempt ""Status Code: 400; Error Code: RequestTimeout""",13585039,Resolved,Major,Fixed,08/Jul/24 15:57,16/Sep/24 11:17,3.4.0,"If a multipart PUT request fails for some reason (e.g. networrk error) then all subsequent retry attempts fail with a 400 Response and ErrorCode RequestTimeout .

{code}
Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID:; S3 Extended Request ID:
{code}

The list of supporessed exceptions contains the root cause (the initial failure was a 500); all retries failed to upload properly from the source input stream {{RequestBody.fromInputStream(fileStream, size)}}.

Hypothesis: the mark/reset stuff doesn't work for input streams. On the v1 sdk we would build a multipart block upload request passing in (file, offset, length), the way we are now doing this doesn't recover.

probably fixable by providing our own {{ContentStreamProvider}} implementations for
# file + offset + length
# bytebuffer
# byte array

The sdk does have explicit support for the memory ones, but they copy the data blocks first. we don't want that as it would double the memory requirements of active blocks."
Avoid Subject.getSubject method on newer JVMs,13593745,Resolved,Major,Duplicate,30/Sep/24 14:45,02/Oct/24 14:51,,"In Java 23, Subject.getSubject requires setting the system property java.security.manager to allow, else it will throw an exception. More detail is available in the release notes: https://jdk.java.net/23/release-notes

This is in support of the eventual removal of the security manager, at which point, Subject.getSubject will be removed."
BlockDecompressorStream#rawReadInt wastes about 1% of overall CPU cycles creating new EOFException,13593728,Patch Available,Major,,30/Sep/24 11:56,,3.3.6,"On our HBase clusters, while looking at CPU profiles, I noticed that about 1% of overall CPU cycles are spent under BlockDecompressorStream#rawReadInt just throwing EOFException. This could be easily avoided."
Operating on / in ChecksumFileSystem throws NPE,13593479,Resolved,Major,Fixed,27/Sep/24 08:25,28/Sep/24 14:06,,"Operating on / on ChecksumFileSystem throws NPE

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:151)
	at org.apache.hadoop.fs.Path.<init>(Path.java:130)
	at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:121)
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:774)
	at org.apache.hadoop.fs.ChecksumFileSystem.setReplication(ChecksumFileSystem.java:884)
{noformat}

Internally I observed it for SetPermission but on my Mac LocalFs doesn't let me setPermission on ""/"", so I reproduced it via SetReplication which goes through the same code path"
hadoop-client-runtime exclude dnsjava InetAddressResolverProvider,13593101,Resolved,Major,Fixed,24/Sep/24 14:32,01/Oct/24 13:51,3.4.1,"[https://github.com/dnsjava/dnsjava/issues/338]

 
{code:java}
Exception in thread ""main"" java.util.ServiceConfigurationError: java.net.spi.InetAddressResolverProvider: Provider org.apache.hadoop.shaded.org.xbill.DNS.spi.DnsjavaInetAddressResolverProvider not found
    at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
    at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1219)
    at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228)
    at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
    at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
    at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
    at java.base/java.util.ServiceLoader.findFirst(ServiceLoader.java:1812)
    at java.base/java.net.InetAddress.loadResolver(InetAddress.java:508)
    at java.base/java.net.InetAddress.resolver(InetAddress.java:488)
    at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1826)
    at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:1139)
    at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1818)
    at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1931)
    at org.apache.logging.log4j.core.util.NetUtils.getLocalHostname(NetUtils.java:56)
    at org.apache.logging.log4j.core.LoggerContext.lambda$setConfiguration$0(LoggerContext.java:625) {code}"
NPE on maven enforcer with -Pnative on arm mac,13593757,Resolved,Major,Fixed,30/Sep/24 16:28,01/Oct/24 13:38,3.4.1,"If you try to build on an arm mac with -Pnative you get an npe in enforcer.

This is independent of whether or not maven can actually compare the native code.

Upgrading maven to 3.5.0 works."
Support force close a DomainSocket for server service,13590838,Resolved,Major,Fixed,03/Sep/24 08:58,30/Sep/24 17:07,,"Currently the DomainSocket#close will check the reference count to be 0 before it goes on to close the socket. In server service case, server calls DomainSocket#listen will add 1 to the reference count. When trying to close the server socket which is blocked by accept, the close call will doing endless count > 0 check, which prevent the server socket to be closed. "
MetricsSystemImpl should not print INFO message in CLI,13592381,Resolved,Major,Fixed,17/Sep/24 18:11,27/Sep/24 13:42,3.4.0,"Below is an example:
{code}
# hadoop fs  -Dfs.s3a.bucket.probe=0 -Dfs.s3a.change.detection.version.required=false -Dfs.s3a.change.detection.mode=none -Dfs.s3a.endpoint=http://some.site:9878 -Dfs.s3a.access.keysome=systest -Dfs.s3a.secret.key=8...1 -Dfs.s3a.endpoint=http://some.site:9878  -Dfs.s3a.path.style.access=true -Dfs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem   -ls  -R s3a://bucket1/
24/09/17 10:47:48 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
24/09/17 10:47:48 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
24/09/17 10:47:48 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started
24/09/17 10:47:48 WARN impl.ConfigurationHelper: Option fs.s3a.connection.establish.timeout is too low (5,000 ms). Setting to 15,000 ms instead
24/09/17 10:47:50 WARN s3.S3TransferManager: The provided S3AsyncClient is an instance of MultipartS3AsyncClient, and thus multipart download feature is not enabled. To benefit from all features, consider using S3AsyncClient.crtBuilder().build() instead.
drwxrwxrwx   - root root          0 2024-09-17 10:47 s3a://bucket1/dir1
24/09/17 10:47:53 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...
24/09/17 10:47:53 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.
24/09/17 10:47:53 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete. 
{code}
"
upgrade dnsjava to 3.6.1 due to CVEs,13586909,Resolved,Major,Fixed,25/Jul/24 11:48,03/Aug/24 06:13,,"See https://github.com/apache/hadoop/pull/6955 - but this is missing the necessary change to LICENSE-binary (which already has an out of date version for dnsjava).

* CVE-2024-25638 https://github.com/advisories/GHSA-cfxw-4h78-h7fw
* https://github.com/advisories/GHSA-mmwx-rj87-vfgr
* https://github.com/advisories/GHSA-crjg-w57m-rqqf

"
S3A: AWS SDK 2.25.53 warnings logged about transfer manager not using CRT client,13592093,Resolved,Major,Fixed,13/Sep/24 15:44,19/Sep/24 13:56,3.4.0,"
When an S3 transfer manager is created for renaming/download a new message is logged telling off the caller for not using the CRT client.

{code}
5645:2024-09-13 16:29:17,375 [setup] WARN  s3.S3TransferManager (LoggerAdapter.java:warn(225)) - The provided S3AsyncClient is an instance of MultipartS3AsyncClient, and thus multipart download feature is not enabled. To benefit from all features, consider using S3AsyncClient.crtBuilder().build() instead.
{code}

This is a change in the SDK to tell us developers off -yet it is visible to end users who don't benefit from it and for which it only creates confusion.

It appears to have been downgraded to debug in the AWS trunk code in PR ""S3 Async Client - Multipart download (#5164) -but:

* it is too late to upgrade and qualify a new version for 3.4.1; downgrading is all we can do
* there is no guarantee this log message or similar will reoccur.

Plan
1. Revert from 3.4.1
2. lift code from cloudstore library which uses reflection to access and manipulate log4j logs where present
3. downgrade all transfer manager log levels to NONE. 
4. File an AWS report about how this is an incompatible regression, identify how their process can evolve, particularly in the area of code guidelines about safe logging use.

I also intend to tighten up our review process to support more rigorous detection of new .warn() messages in the AWS SDK. I'm going to propose that as well as requiring review of our test/CLI output, we require ripgrep scans of .warn(/.error( in SDK source, audit of any new changes. by saving the output of the previous iteration, it'll be straightforward to identify new changes -but not changes in codepaths which change their frequency of appearance.

I think we should revisit whether or not to move off the xfer manager in the past. We've discussed it in the past, and avoided it just due to maintenance costs. However, it is pushing maintenance costs anyway.

meanwhile: no new AWS SDK updates until we are confident we have our processes under control.



"
Release Hadoop Third-Party 1.3.0,13589111,Resolved,Major,Fixed,16/Aug/24 15:02,05/Sep/24 20:04,thirdparty-1.3.0,Create a release of thirdparty jar with the protobuf version compatible with all java8 builds.
dtutil cancel command fails with NullPointerException,13592165,In Progress,Major,,14/Sep/24 15:01,,,"The hadoop dtutil cancel command gives NullPointerException.
{code}
java.lang.NullPointerException
	at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:422)
	at org.apache.hadoop.security.User.<init>(User.java:48)
	at org.apache.hadoop.security.User.<init>(User.java:43)
	at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1418)
	at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1402)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)
	at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81)
	at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91)
	at java.base/java.lang.String.valueOf(String.java:2951)
	at java.base/java.lang.StringBuilder.append(StringBuilder.java:168)
	at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.stringifyToken(DelegationTokenIdentifier.java:123)
	at org.apache.hadoop.hdfs.DFSClient$Renewer.cancel(DFSClient.java:804)
	at org.apache.hadoop.security.token.Token.cancel(Token.java:527)
	at org.apache.hadoop.security.token.DtFileOperations.removeTokenFromFile(DtFileOperations.java:283)
	at org.apache.hadoop.security.token.DtUtilShell$Remove.execute(DtUtilShell.java:320)
	at org.apache.hadoop.tools.CommandShell.run(CommandShell.java:72)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)
	at org.apache.hadoop.security.token.DtUtilShell.main(DtUtilShell.java:361)
{code}

This is because it is trying to create a User and in User.java we are calling [this|https://github.com/apache/hadoop/blob/ea6e0f7cd58d0129897dfc7870aee188be80a904/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/User.java#L48] line.

But due to this direct call it doesn't initialize the auth_to_local rules and causes NPE in [here|https://github.com/apache/hadoop/blob/ea6e0f7cd58d0129897dfc7870aee188be80a904/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java#L422] in KerberosName.java"
Fix test TestServiceInterruptHandling.testRegisterAndRaise,13588400,Resolved,Major,Fixed,09/Aug/24 11:04,30/Aug/24 11:06,,"If test on some slow server, testRegisterAndRaise may fail, error stack are

{code}

[ERROR] testRegisterAndRaise(org.apache.hadoop.service.launcher.TestServiceInterruptHandling) Time elapsed: 0.513 s <<< FAILURE! java.lang.AssertionError: interrupt data at org.junit.Assert.fail(Assert.java:89) at org.junit.Assert.assertTrue(Assert.java:42) at org.junit.Assert.assertNotNull(Assert.java:713) at org.apache.hadoop.service.launcher.TestServiceInterruptHandling.testRegisterAndRaise(TestServiceInterruptHandling.java:48) at
{code}


h3. backporting note:

there are two commits for this issue, please cherrypick both when backporting"
S3A: Commiter ITests failing unless job.id set,13592099,Open,Major,,13/Sep/24 16:37,,3.4.1,"the job.id maven setup is broken on parallel runs: unless explicitly set on the command line tests which try to parse its traling bits as a number will fail.

{code}
[ERROR] testBinding[File committer in task-fs=[]-task=[file]-[class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]](org.apache.hadoop.fs.s3a.commit.ITestS3ACommitterFactory)  Time elapsed: 0.181 s  <<< ERROR!
java.lang.Exception: Failed to parse b-00
{code}

Emergency workaround: set it
{code}
-Djob.id=0001
{code}
"
Add Options.Rename.THROW_NON_ATOMIC,13588855,Open,Major,,14/Aug/24 14:16,,3.3.6,"I propose we add an option `Options.Rename.THROW_NON_ATOMIC` to change `rename()` behavior to throw when the underlying filesystem's rename operation is not atomic.

This would be useful for callers that expect to perform an atomic op but want to fail if when an atomic rename fails.

 

At first this might seem something that can be done by querying capabilities of the filesystem but that would only work on real filesystems. A motivating example would be a virtual filesystem for which paths can resolve to any concrete filesystem (s3, etc). If `rename()` is called with two virtual paths that resolve to different filesystems (s3 and gcs for example) then obviously the operation can't be atomic since bytes must be copied from one fs to another.

 

What do you think [~steve_l] ?"
Upgrade esdk to the latest version 3.24.3,13585424,Open,Major,,11/Jul/24 08:08,,3.4.1,"The current version relies on okhttp 3.x and would like to upgrade to the latest version, which relies on okhttp 4.12"
Upgrade maven-shade-plugin to 3.6.0,13591200,Resolved,Major,Fixed,05/Sep/24 16:35,05/Sep/24 19:30,,"I found an issue when testing with Jackson 2.18.0-rc1.
Hadoop bundles the Jackson and other 3rd party classes into Hadoop fat jars using maven-shade-plugin. 
Jackson is a Multi-Release jar. https://openjdk.org/jeps/238
The most recent jackson-core jars have classes that support META-INF/versions/21 classes. The existing maven-shade-plugin version in Hadoop build fails with ASM issues with Java 21 classes. maven-shade-plugin handles them fine."
Improve Logging for Empty or Missing Socket Factory Configuration,13591133,Open,Major,,05/Sep/24 08:48,,3.3.6,"In the current implementation of the {{NetUtils.getDefaultSocketFactory}} method, when the configuration parameter {{hadoop.rpc.socket.factory.class.default}} is not set or is empty, the method silently defaults to using the standard {{{}SocketFactory{}}}. However, this lack of logging can make it difficult to diagnose configuration issues. We propose enhancing the code with logging to warn users when this situation occurs.
 
*Expected Behavior:* 
When the {{hadoop.rpc.socket.factory.class.default}} configuration parameter is missing or empty, a warning log should be generated, informing the user that the system is defaulting to the standard {{{}SocketFactory{}}}.
 
*How-to-Fix:*
We propose to expose this relationship by logging a warning when the {{hadoop.rpc.socket.factory.class.default}} parameter is missing or empty."
Logging Enhancement for Shutdown Timeout Configuration in ShutdownHookManager,13591131,Open,Major,,05/Sep/24 08:46,,3.3.6,"The issue was encountered in scenarios where Hadoop services are expected to shut down gracefully within a specified time frame. Misconfigured shutdown timeouts can lead to abrupt terminations or extended shutdown periods. The original code does not provide sufficient visibility into misconfigurations related to the shutdown timeout duration, which can result in unexpected behavior during service shutdowns.
 
*Expected Behavior:* 
The system should log a warning message when the configured shutdown timeout is less than the minimum allowed value, and it should reset the duration to this minimum value. Additionally, the user should be informed to set the {{hadoop.service.shutdown.timeout}} configuration parameter to a valid value.
 
*How-to-Fix:* 
Enhance the {{getShutdownTimeout}} method by adding logging statements that warn when the shutdown timeout is below the allowed minimum, and reset it to the minimum value. This logging will help users identify and correct misconfigurations."
Improve Logging for User Account Update Time Configuration in ShellBasedIdMapping Constructor,13591130,Open,Major,,05/Sep/24 08:45,,3.3.6,"The original implementation of the {{ShellBasedIdMapping}} constructor lacks detailed logging about the configuration of the user account update time ({{{}usergroupid.update.millis{}}}). When this time is set to a value less than 1 minute, the system automatically adjusts it to the minimum of 1 minute without providing clear feedback to the user. This can lead to confusion and difficulty in troubleshooting if the user is unaware that their configuration was overridden. By adding detailed logs, we can ensure that users are informed about the exact configuration being applied, making it easier to diagnose potential issues related to configuration.
 
*Expected Behavior:* 
When the {{ShellBasedIdMapping}} constructor is invoked, the system should log the user-configured update time for user accounts. If the update time is below the minimum threshold (1 minute), the system should log a warning indicating that the minimum value of 1 minute will be used instead.
 
*How-to-Fix:*
We propose to expose such a relationship by adding detailed logging statements to inform the user about the configuration status."
Enhance Logging for Localized Trash Configuration in moveToAppropriateTrash Method,13591129,Open,Major,,05/Sep/24 08:43,,3.3.6,"The {{moveToAppropriateTrash}} method in {{Trash.java}} handles file deletion by moving files to the appropriate trash directory based on the configuration. However, the current implementation lacks logging to indicate whether the localized trash feature is enabled or not. This enhancement proposal introduces logging to provide better visibility into the decision-making process when moving files to the trash, particularly in environments where the ViewFileSystem is used.
 
*Expected Behavior:* 
When {{CONFIG_VIEWFS_TRASH_FORCE_INSIDE_MOUNT_POINT}} is enabled, the system should log that the localized trash feature is enabled and that the {{Trash}} is being initialized with a {{ViewFileSystem}} object. Conversely, if the feature is disabled, the system should log that it is using a fully resolved {{FileSystem}} object.
 
*How-to-Fix:*
This logging enhancement adds informative messages to indicate whether the localized trash feature is enabled or not, thus providing more transparency during the file deletion process. The test cases verify the functionality under different configurations."
 Logging Enhancements for setConf Method in WhitelistBasedResolver,13591128,Open,Major,,05/Sep/24 08:42,,3.3.6,"The original implementation does not provide any logging information about the status of the variable whitelist configuration. As a result, it is difficult to determine whether the variable whitelist is enabled or disabled, which file is being used, and the expiry time set for the whitelist. This lack of visibility can lead to confusion during troubleshooting and configuration, as administrators may not be aware if the variable whitelist is correctly configured or if it is using the intended file and settings.
 
*Expected Behavior:* 
The system should log detailed information about whether the variable whitelist is enabled, the file being used, and the expiry time. This will assist in debugging and ensure that the configuration is applied as expected.
 
*How-to-Fix:*
We propose exposing the configuration behavior through logging. This includes indicating whether the variable whitelist is enabled, specifying the file in use, and logging the expiry time to provide greater visibility into the configuration process."
Enhance Logging for Timeout Configuration in Client.getTimeout,13591127,Open,Major,,05/Sep/24 08:40,,3.3.6,"The original implementation of the {{Client.getTimeout}} method does not provide detailed logging when it falls back to default behaviors based on the configuration. Specifically, when the {{ipc.client.rpc.timeout}} parameter is set to an invalid value (less than or equal to zero), the method either uses the ping interval as the timeout or returns a default value of {{-1}} depending on whether {{ipc.client.ping}} is enabled. This lack of logging can make it difficult for users to understand why certain timeout values are being used, leading to potential confusion and difficulty in troubleshooting. By adding logging statements, the system can inform users about the decisions being made based on their configurations.
 
*Expected Behavior:* 
When the {{Client.getTimeout}} method is invoked, the system should log detailed information about the timeout configuration, including whether the {{ipc.client.ping}} is enabled or disabled, and how this affects the timeout value being used.
 
*How-to-Fix:*
We propose to enhance the method by adding logging statements that inform the user about the configuration decisions being made. This will improve transparency and make the system easier to debug."
"removal of gcm TLS cyphers blocking abfs access ""No negotiable cipher suite""",13590727,Open,Major,,02/Sep/24 12:24,,3.4.0,"we've seen instances of client-abfs TLS negotiation failing ""No negotiable cipher suite"". this can be fixed by switching to using ""Default_JSSE_with_GCM"" as the SSL options.

However, DelegatingSSLSocketFactory ""Default"" attempts OpenSSL, falling back to 
{code}
Default indicates Ordered, preferred OpenSSL, if failed to load then fall
 back to Default_JSSE
{code}

And "" Default_JSSE is not truly the the default JSSE implementation because
the GCM cipher is disabled when running on Java ""

What does that mean? it means that if you use the ""Default"" TLS option of ""try openssl and fall back to java"" doesn't ever turn on gcm encryption.

Proposed:
* ""Default"" falls back to GCM
* add an option {{Default_JSSE_No_GCM}}

Once we move off java8 turning off GCM is no longer needed for performance, hopefully (benchmarks would be good here)"
upgrade to jackson 2.14.3,13586403,Open,Major,,19/Jul/24 21:37,,,"Follow up to HADOOP-18332

I have what I believe fixes the Jackson JAX-RS incompatibility.

https://github.com/pjfanning/jsr311-compat/

The reason that I want to start by just going to Jackson 2.14 is that Jackson has new StreamReadConstraints in Jackson 2.15 to protect against malicious JSON inputs. The constraints are generous but can cause issues with very large or deeply nested inputs.

Jackson has had a lot of security hardening fixes recently and it seems problematic to be stuck on an unsupported version of Jackson (2.12)."
add JacksonUtil to centralise some code,13586452,Open,Major,,21/Jul/24 14:54,,3.4.0,"To future proof Hadoop against Jackson changes, it makes sense to not just create ObjectMappers and JsonFactories in many different places in the Hadoop code.

One of the main drivers of this is https://www.javadoc.io/doc/com.fasterxml.jackson.core/jackson-core/latest/com/fasterxml/jackson/core/StreamReadConstraints.html 

Jackson 3 (not yet scheduled for release) has some fairly big API and behaviour changes too."
Protobuf code generate and replace should happen together,13587719,Resolved,Major,Fixed,02/Aug/24 13:20,30/Aug/24 10:30,,
Enhance FileSystem.Cache to honor security token and expiration,13587110,Open,Major,,28/Jul/24 04:02,,3.3.4,"We have an online service which uses Hadoop FileSystem to load files from a remote cloud storage.

The current cache in FileSystem is a [HashMap|https://github.com/apache/hadoop/blob/4525c7e35ea22d7a6350b8af10eb8d2ff68376e7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3635C1-L3635C62], and its key honors scheme, authority (like [user@host:port|https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax]), ugi and a unique long for its [hash code|https://github.com/apache/hadoop/blob/4525c7e35ea22d7a6350b8af10eb8d2ff68376e7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3891C1-L3894C8]. And among those 4 fields, only ""scheme"" and ""authority"" could be controlled externally.

That results in a wrong case like: A FileSystem entry in the cache was created with schemeA + authorityA, and with read + write access, and an expiration. Later, an API to get FileSystem comes still using schemeA + authorityA, but with less access (maybe read only), or it already expires, that FileSystem entry in the cache is hit by mistake, while no new FilleSystem is created. It does not lead to a security issue, but subsequent calls (maybe to read the file) will be rejected with 403 by the remote storage.

Our proposal is like
 * Short term
 ** Add a new field in FileSystem.Cache.Key to affect hashCode() and equals(). This field could be specified when contructing a Key.
 ** Add a simple expiration mechanism in FileSystem.Cache
 *** Each cache entry is created with a expiration
 *** When getting a FileSystem, if the cache entry is hit but already expires, close it and remove it from the cache. And return a new created FileSystem.
 * Long term
 ** Replace the internal HashMap by a more modern and full functional cache framework, like [https://github.com/ben-manes/caffeine]

 

 

 

 "
Google GCS changes fail due to VectorIO changes,13589129,Resolved,Major,Fixed,16/Aug/24 19:05,19/Aug/24 18:59,3.4.1,"the changes of HADOOP-19098 broken google gcs


{code}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project gcs-connector: Compilation failure
[ERROR] /Users/mthakur/Sandbox/open_source/hadoop-connectors/gcs/src/main/java/com/google/cloud/hadoop/fs/gcs/VectoredIOImpl.java:[317,60] incompatible types: java.util.List<capture#1 of ? extends org.apache.hadoop.fs.FileRange> cannot be converted to org.apache.hadoop.fs.FileRange[]
[ERROR]

{code}

failing line is

{code}
FileRange[] sortedRanges = VectoredReadUtils.sortRanges(input);

{code}


need to restore the original  {{sortRanges}}, renaming the changed signature one first. plus a test, obviously"
Enhance FileSystem.Cache to honor a user defined field,13587111,Open,Major,,28/Jul/24 05:22,,3.3.4,"Add a new field in FileSystem.Cache.Key to affect hashCode() and equals(). This field could be specified when constructing a Key
 * For new users, they are able to control the hash code generation with respect to this field (could be a security string/token/object...), which may contain access (read/write/list...) and/or expiration time ...
 * For old users, this field defaults to null and is not honored in hashCode() and equals()"
Update the yasm rpm download address,13587655,Resolved,Major,Fixed,02/Aug/24 04:48,05/Aug/24 01:59,,"Some ci run failed, for example:

[https://ci-hadoop.apache.org/blue/rest/organizations/jenkins/pipelines/hadoop-multibranch/branches/PR-6972/runs/1/nodes/56/steps/61/log/?start=0]

I found the yasm rpm url `[https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/y/yasm-1.2.0-4.el7.x86_64.rpm]` is outdated

 "
Pullout arch-agnostic maven javadoc plugin configurations in hadoop-common,13587546,Resolved,Major,Fixed,01/Aug/24 08:04,05/Aug/24 07:31,3.4.1,
IPC client uses CompletableFuture to support asynchronous operations.,13586692,Resolved,Major,Fixed,23/Jul/24 15:27,24/Jul/24 17:10,,"h3. Description

In the implementation of asynchronous Ipc.client, the main methods used include HADOOP-13226, HDFS-10224, etc.

However, the existing implementation does not support `CompletableFuture`; instead, it relies on setting up callbacks, which can lead to the ""callback hell"" problem. Using `CompletableFuture` can better organize asynchronous callbacks. Therefore, on the basis of the existing implementation, by using `CompletableFuture`, once the `client.call` is completed, the asynchronous thread handles the response of this call without blocking the main thread.

 

*Test*

new UT  TestAsyncIPC#testAsyncCallWithCompletableFuture()"
Authentification failed in Azure Kubernetes with HTTP1.1 and Chunked transfer encoding,13587672,Open,Major,,02/Aug/24 07:15,,3.3.4," 

The problem is related to Azure authentication on Kubernetes.

When I run my Spark program, I have this error when I try to authenticate the pod :

 
{code:java}
java.lang.NullPointerException
    at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.consumeInputStream(AzureADAuthenticator.java:340)
    at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:270)
    at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:211)
    at org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenFromMsi(AzureADAuthenticator.java:137)
    at org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider.refreshToken(MsiTokenProvider.java:45)
    at org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)
    at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:554)
    at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:151)
    at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:125)
    at org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:181)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:569)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:536)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:359) {code}
 

 

My configuration is a spark-driver deployed on Azure kubernetes with managed identity.

I used [this method|https://medium.com/datamindedbe/running-spark-3-on-aks-with-azure-ad-integration-c1fc0032c550] with aad-pod-identity.

 

There are two different scenarios we can observe when trying to authenticate on Kubernetes to Azure Instance Metadata Service :
 * The returned token is short and its size is less than 2048 chars. The Token have all headers and explicitly the ""Content-Length"" header

!TokenOK.png!
 * The returned token is long and its size is more than 2048 chars. The Token have [the HTTP1.1 capacity with transfer encoding property|https://en.wikipedia.org/wiki/Chunked_transfer_encoding] in Response and don't have the ""Content-length"" header due to Chunked transfer encoding mechanism.

!TokenKO.png!

 

NB : I run a curl command in pod to generate these sceenshots according to the [Azure Documentation|https://learn.microsoft.com/en-us/azure/virtual-machines/instance-metadata-service?tabs=linux]

In a GitHub repository I found my ""AzureADAuthenticator.java"" and this piece of code :

!CodeResponse.png!

The ""Content-length"" property is mandatory when the returned HTTP code is 200 and it's not compatible with the HTTP1.1 Chunked transfer encoding fonctionality.

Is it possible to update this authentification to support this mechanism implemented by Microsoft on kubernetes (and may be in virtual machine)."
Fix create-release script for arm64 based MacOS,13586983,Resolved,Major,Fixed,25/Jul/24 22:09,01/Aug/24 19:46,3.4.0,
Avoid DNS lookup while creating IPC Connection object,13584550,Resolved,Major,Fixed,02/Jul/24 20:18,16/Jul/24 13:10,,"Been running HADOOP-18628 in production for quite sometime, everything works fine as long as DNS servers in HA are available. Upgrading single NS server at a time is also a common case, not problematic. Every DNS lookup takes 1ms in general.

However, recently we encountered a case where 2 out of 4 NS servers went down (temporarily but it's a rare case). With small duration DNS cache and 2s of NS fallback timeout configured in resolv.conf, now any client performing DNS lookup can encounter 4s+ delay. This caused namenode outage as listener thread is single threaded and it was not able to keep up with large num of unique clients (in direct proportion with num of DNS resolutions every few seconds) initiating connection on listener port.

While having 2 out of 4 DNS servers offline is rare case and NS fallback settings could also be improved, it is important to note that we don't need to perform DNS resolution for every new connection if the intention is to improve the insights into VersionMistmatch errors thrown by the server.

The proposal is the delay the DNS resolution until the server throws the error for incompatible header or version mismatch. This would also help with ~1ms extra time spent even for healthy DNS lookup."
ipc.Server accelerate token negotiation only for the default mechanism,13585909,Resolved,Major,Fixed,16/Jul/24 06:20,20/Jul/24 07:19,,"{code}
//Server.java
      // accelerate token negotiation by sending initial challenge
      // in the negotiation response
      if (enabledAuthMethods.contains(AuthMethod.TOKEN)) {
        ...
      }
{code}
In Server.Connection.buildSaslNegotiateResponse() above, it accelerates token negotiation by sending initial challenge in the negotiation response.  However, it is a non-standard SASL
negotiation.  We should do it only for the default SASL mechanism."
ShellCommandFencer#setConfAsEnvVars should also replace '-' with '_'.,13585282,Resolved,Major,Fixed,10/Jul/24 08:43,20/Jul/24 08:14,,"When setting configuration into environment variables, '-' should also be replaced with '_'."
Continue running the pipeline even if stages fail,13585309,Open,Major,,10/Jul/24 11:29,,,
Switch yum repo baseurl due to CentOS 7 sunset,13585165,Resolved,Major,Fixed,09/Jul/24 12:06,11/Jul/24 22:18,3.4.0,"Similar to HADOOP-18151 (which handled sunset for CentOS 8), CentOS 7 reached EOL on July 1, 2024"
S3A : S3AInputStream positioned readFully Expectation,13584922,Resolved,Major,Works for Me,07/Jul/24 10:45,08/Jul/24 18:22,,"So basically i was testing to write some unit test - for S3AInputStream readFully Method 

package org.apache.hadoop.fs.s3a;

import java.io.EOFException;
import java.io.FilterInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.SocketException;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;

import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.s3a.audit.impl.NoopSpan;
import org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets;
import org.apache.hadoop.util.BlockingThreadPoolExecutorService;
import org.apache.hadoop.util.functional.CallableRaisingIOE;
import org.assertj.core.api.Assertions;
import org.junit.Before;
import org.junit.Test;
import software.amazon.awssdk.awscore.exception.AwsErrorDetails;
import software.amazon.awssdk.awscore.exception.AwsServiceException;
import software.amazon.awssdk.core.ResponseInputStream;
import software.amazon.awssdk.http.AbortableInputStream;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;
import software.amazon.awssdk.services.s3.model.GetObjectResponse;

import static java.lang.Math.min;
import static java.nio.charset.StandardCharsets.UTF_8;
import static org.apache.hadoop.fs.s3a.Constants.ASYNC_DRAIN_THRESHOLD;
import static org.apache.hadoop.fs.s3a.Constants.AWS_REGION;
import static org.apache.hadoop.fs.s3a.Constants.FS_S3A;
import static org.apache.hadoop.fs.s3a.Constants.MULTIPART_MIN_SIZE;
import static org.apache.hadoop.fs.s3a.Constants.S3_CLIENT_FACTORY_IMPL;
import static org.apache.hadoop.util.functional.FutureIO.eval;
import static org.assertj.core.api.Assertions.assertThat;
import static org.assertj.core.api.AssertionsForClassTypes.assertThatExceptionOfType;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.verify;

public class TestReadFullyAndPositionalRead {

    private S3AFileSystem fs;
    private S3AInputStream input;
    private S3Client s3;
    private static final String EMPTY = """";
    private static final String INPUT = ""test_content"";

    @Before
    public void setUp() throws IOException {
        Configuration conf = createConfiguration();
        fs = new S3AFileSystem();
        URI uri = URI.create(FS_S3A + ""://"" + MockS3AFileSystem.BUCKET);
        // Unset S3CSE property from config to avoid pathIOE.
        conf.unset(Constants.S3_ENCRYPTION_ALGORITHM);
        fs.initialize(uri, conf);
        s3 = fs.getS3AInternals().getAmazonS3Client(""mocking"");
    }

    public Configuration createConfiguration() {
        Configuration conf = new Configuration();
        conf.setClass(S3_CLIENT_FACTORY_IMPL, MockS3ClientFactory.class, S3ClientFactory.class);
        // use minimum multipart size for faster triggering
        conf.setLong(Constants.MULTIPART_SIZE, MULTIPART_MIN_SIZE);
        conf.setInt(Constants.S3A_BUCKET_PROBE, 1);
        // this is so stream draining is always blocking, allowing assertions to be safely made without worrying about any race conditions
        conf.setInt(ASYNC_DRAIN_THRESHOLD, Integer.MAX_VALUE);
        // set the region to avoid the getBucketLocation on FS init.
        conf.set(AWS_REGION, ""eu-west-1"");
        return conf;
    }

    @Test
    public void testReadFullyFromBeginning() throws IOException {
        input = getMockedS3AInputStream(INPUT);
        byte[] byteArray = new byte[INPUT.length()];
        input.readFully(0, byteArray, 0, byteArray.length);
        assertThat(new String(byteArray, UTF_8)).isEqualTo(INPUT);
    }

    @Test
    public void testReadFullyWithOffsetAndLength() throws IOException {
        input = getMockedS3AInputStream(INPUT);
        byte[] byteArray = new byte[4];
        input.readFully(5, byteArray, 0, 4);
        assertThat(new String(byteArray, UTF_8)).isEqualTo(""cont"");
    }

    @Test
    public void testReadFullyWithOffsetBeyondStream() throws IOException {
        input = getMockedS3AInputStream(INPUT);
        byte[] byteArray = new byte[10];
        assertThatExceptionOfType(EOFException.class)
                .isThrownBy(() -> input.readFully(20, byteArray, 0, 10));
    }

    private S3AInputStream getMockedS3AInputStream(String input) {
        Path path = new Path(""test-path"");
        String eTag = ""test-etag"";
        String versionId = ""test-version-id"";
        String owner = ""test-owner"";
        S3AFileStatus s3AFileStatus = new S3AFileStatus(input.length(), 0, path, input.length(), owner, eTag, versionId);
        S3ObjectAttributes s3ObjectAttributes = new S3ObjectAttributes(
                fs.getBucket(), path, fs.pathToKey(path), fs.getS3EncryptionAlgorithm(), new EncryptionSecrets().getEncryptionKey(), eTag, versionId, input.length());
        S3AReadOpContext s3AReadOpContext = fs.createReadContext(s3AFileStatus, NoopSpan.INSTANCE);
        return new S3AInputStream(s3AReadOpContext, s3ObjectAttributes, getMockedInputStreamCallback(input), s3AReadOpContext.getS3AStatisticsContext().newInputStreamStatistics(), BlockingThreadPoolExecutorService.newInstance(2, 40, 60, TimeUnit.SECONDS, ""s3a-bounded""));
    }

    private S3AInputStream.InputStreamCallbacks getMockedInputStreamCallback(String input) {
        GetObjectResponse objectResponse = GetObjectResponse.builder().eTag(""test-etag"").build();
        ResponseInputStream<GetObjectResponse>[] responseInputStreams = new ResponseInputStream[] {
                getMockedInputStream(objectResponse, true, input),
                getMockedInputStream(objectResponse, true, input),
                getMockedInputStream(objectResponse, false, input)
        };
        return new S3AInputStream.InputStreamCallbacks() {
            private Integer mockedS3ObjectIndex = 0;
            @Override
            public ResponseInputStream<GetObjectResponse> getObject(GetObjectRequest request) {
                mockedS3ObjectIndex++;
                if (mockedS3ObjectIndex == 3) {
                    throw AwsServiceException.builder()
                            .message(""Failed to get S3Object"")
                            .awsErrorDetails(AwsErrorDetails.builder().errorCode(""test-code"").build())
                            .build();
                }
                return responseInputStreams[min(mockedS3ObjectIndex, responseInputStreams.length) - 1];
            }

            @Override
            public GetObjectRequest.Builder newGetRequestBuilder(String key) {
                return GetObjectRequest.builder().bucket(fs.getBucket()).key(key);
            }

            @Override
            public <T> CompletableFuture<T> submit(final CallableRaisingIOE<T> task) {
                return eval(task);
            }

            @Override
            public void close() {
            }
        };
    }

    private ResponseInputStream<GetObjectResponse> getMockedInputStream(
            GetObjectResponse response, boolean success, String input) {
        FilterInputStream stream = new FilterInputStream(AbortableInputStream.create(
                IOUtils.toInputStream(input, StandardCharsets.UTF_8), () -> {
                })) {
            @Override
            public void close() throws IOException {
                super.close();
                if (!success) {
                    throw new SocketException(""Socket closed"");
                }
            }
        };
        return new ResponseInputStream<>(response, stream);
    }

}

Now this -

[ERROR]   TestReadFullyAndPositionalRead.testPositionalReadWithOffsetAndLength:136 expected:<""[con]t""> but was:<""[tes]t"">

is the failure its not adhering to the position parameter and reading the inital bytes only

What is the expectation of the readFully Function in S3AInputStream?

"
Introduce getTrashPolicy to FileSystem API,13584461,Open,Major,,02/Jul/24 10:32,,,"Hadoop FileSystem supports multiple FileSystem implementations awareness (e.g. client is aware of both hdfs:// and ofs:// protocols).

However, it seems that currently Hadoop TrashPolicy remains the same regardless of the URI scheme. The TrashPolicy is governed by ""fs.trash.classname"" configuration and stays the same regardless of the FileSystem implementation. For example, HDFS defaults to TrashPolicyDefault and Ozone defaults to TrashPolicyOzone, but only one will be picked since the the configuration will be overwritten by the other.

Therefore, I propose to tie the TrashPolicy implementation to each FileSystem implementation by introducing a new FileSystem#getTrashPolicy interface. TrashPolicy#getInstance can call FileSystem#getTrashPolicy to get the appropriate TrashPolicy."
ABFS: Disabling Apache Http Client as Default Http Client for ABFS Driver,13592510,Resolved,Minor,Fixed,18/Sep/24 17:46,24/Sep/24 13:04,3.4.0,"As part of work done under HADOOP-19120 [ABFS]: ApacheHttpClient adaptation as network library - ASF JIRA
Apache Http Client was introduced as an alternative Network Library that can be used with ABFS Driver. Earlier JDK Http Client was the only supported network library.

Apache Http Client was found to be more helpful in terms of controls and knobs it provides to better manage the Network aspects of driver. Hence it was made the default Network Client to be used with ABFS Driver.

Recently while running scale workloads, we observed a regression where some unexpected wait time was observed while establishing connections. A possible fix has been identified and we are working on getting it fixed.
One scenario identified during internal tests revealed that, with the current Apache client implementation, a connection cannot become stale. As a result, it is safe to remove the check for stale connections when closing them. This change will optimize the connection handling process by eliminating unnecessary delays and reducing the risk of potential failures caused by redundant checks.
There was also a possible NPE scenario which was identified on the new network client code recently.

Until we are done with the code fixes and have revalidated the whole Apache client flow, we would like to make JDK Client as default client again. The new support for Apache Http Client will still be there, but it will be disabled behind a config."
Move all DistCp execution logic to execute(),13590813,Resolved,Minor,Fixed,03/Sep/24 07:05,23/Sep/24 07:44,,Many code flows create a DistCp instance and call the public method execute() to get the Job object for better control over the distcp job but some logics are only called by the run() method. Should move these lines over to execute().
S3A: new test failures on branch-3.4.1,13592102,Open,Minor,,13/Sep/24 16:58,,3.4.1,"test failures on a -Dscale run; s3 usw-1, caller has IAM role.
{code}
[INFO] Running org.apache.hadoop.fs.s3a.ITestS3AClosedFS
[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.497 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3AClosedFS
[ERROR] testClosedInstrumentation(org.apache.hadoop.fs.s3a.ITestS3AClosedFS)  Time elapsed: 0.143 s  <<< FAILURE!
org.junit.ComparisonFailure: [S3AInstrumentation.hasMetricSystem()] expected:<[fals]e> but was:<[tru]e>
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at org.apache.hadoop.fs.s3a.ITestS3AClosedFS.testClosedInstrumentation(ITestS3AClosedFS.java:111)
{code}
"
S3A: ITestAssumeRole.testAssumeRoleBadInnerAuth failure,13590489,Resolved,Minor,Fixed,29/Aug/24 19:13,03/Sep/24 20:36,3.4.0,"Not sure when this changed, but I've only just noticed today while setting up a new test config.

The test {{testAssumeRoleBadInnerAuth}} is failing because the error string coming back from STS is slightly different.

{code}
[ERROR] testAssumeRoleBadInnerAuth(org.apache.hadoop.fs.s3a.auth.ITestAssumeRole)  Time elapsed: 4.182 s  <<< FAILURE!
java.lang.AssertionError: 
 Expected to find 'not a valid key=value pair (missing equal-sign) in Authorization header' but got unexpected exception: org.apache.hadoop.fs.s3a.AWSBadRequestException: Instantiate org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on /: software.amazon.awssdk.services.sts.model.StsException: Invalid key=value pair (missing equal-sign) 
{code}

Rather than change the string to look for, lets just remove the string so it it less brittle in future
"
Fix unit tests testSlowConnection and testBadSetup failed in TestRPC,13584318,Resolved,Minor,Fixed,01/Jul/24 05:50,05/Jul/24 06:42,3.4.0,"Fix unit tests testSlowConnection and testBadSetup failed in TestRPC.

We should use ProtobufRpcEngine2 ProtocolEngine."
Create hadoop-runner based on JDK 17,13592205,Resolved,Trivial,Done,16/Sep/24 06:04,16/Sep/24 18:56,,"Publish a separate hadoop-runner image with JDK 17, to allow testing JDK 17 runtime compatibility.  (Keep existing JDK 8/11 images for compatibility.)"
Files and directories mixed up in TreeScanResults#dump,13592246,Resolved,Trivial,Fixed,16/Sep/24 13:48,17/Sep/24 10:26,,"{code}
AssertionError: listStatus(): directory count in 6 directories and 4 files
Files:
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0000
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0000
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0001
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0001
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0002
  ""s3a://obs-bucket/test/testComplexDirActions/dir-0-0002
Directories:
  ""s3a://obs-bucket/test/testComplexDirActions/file--0-0000.txt
  ""s3a://obs-bucket/test/testComplexDirActions/file--0-0001.txt
  ""s3a://obs-bucket/test/testComplexDirActions/file--0-0002.txt
  ""s3a://obs-bucket/test/testComplexDirActions/file--0-0003.txt expected:<3> but was:<6>
{code}

""Files"" and ""Directories"" are mixed up in the output."
Update Netty to 4.1.48.Final,13298138,Resolved,Blocker,Fixed,13/Apr/20 21:45,14/Apr/20 22:29,3.3.0,We are currently on Netty 4.1.45.Final. We should update to the latest 4.1.48.Final
Fix the CosCrendentials Provider in core-site.xml for unit tests.,13300176,In Progress,Blocker,,22/Apr/20 06:32,,,Fix the CosCredentials Provider classpath in core-site.xml for unit tests.
Upgrade jackson-databind to 2.9.10.4,13301052,Resolved,Blocker,Fixed,26/Apr/20 07:23,27/Apr/20 20:37,3.1.4,"trunk (3.3.0) now has HADOOP-16905. But branch 3.2/3.1/.. still uses jackson-databind 2.9.

I'm opening this jira since I'm unsure whether we are backporting HADOOP-16905 to lower version branches due to compatibility concern or whatever.

GH PR: https://github.com/apache/hadoop/pull/1981

CC [~weichiu]"
ZStandardCodec compression mail fail(generic error) when encounter specific file,13314197,Resolved,Blocker,Duplicate,30/Jun/20 03:41,04/Oct/21 02:45,2.6.5,"*Problem:* 

In our production environment,  we put file in hdfs with zstd compressor, recently, we find that a specific file may leads to zstandard compressor failures. 

And we can reproduce the issue with specific file(attached file: badcase.data)

!image-2020-06-30-11-51-18-026.png|width=1031,height=230!

 

*Analysis*: 

ZStandarCompressor use buffersize( From zstd recommended compress out buffer size)  for both inBufferSize and outBufferSize 

!image-2020-06-30-11-35-46-859.png|width=1027,height=387!

but zstd indeed provides two separately recommending inputBufferSize and outputBufferSize  

!image-2020-06-30-11-39-17-861.png!

 

*Workaround*

One workaround,  using recommended in/out buffer size provided by zstd lib can avoid the problem, but we don't know why. 

zstd recommended input buffer size:  1301072 (128 * 1024)

zstd recommended ouput buffer size: 131591 

!image-2020-06-30-11-42-44-585.png|width=1023,height=196!

 

 

 

 

 

 

 "
Ignore AuthenticationFilterInitializer for KMSWebServer,13297880,Resolved,Blocker,Fixed,12/Apr/20 14:25,17/Apr/20 21:42,3.3.0,KMS does not work if hadoop.http.filter.initializers is set to AuthenticationFilterInitializer since KMS uses its own authentication filter. This is problematic when KMS is on the same node with other Hadoop services and shares core-site.xml with them. The filter initializers configuration should be tweaked as done for httpfs in HDFS-14845.
HADOOP-16582 changed mkdirs() behavior,13297177,Open,Critical,,08/Apr/20 18:21,,2.10.0,"HADOOP-16582 changed behavior of {{mkdirs()}}
Some Hive tests depend on the old behavior and they fail miserably.

{quote}
earlier:

all plain mkdirs(somePath) were fast-tracked to FileSystem.mkdirs which have rerouted them to mkdirs(somePath, somePerm) method with some defaults (which were static)
an implementation of FileSystem have only needed implement ""mkdirs(somePath, somePerm)"" - because the other was not neccessarily called if it was always in a FilterFileSystem or something like that
now:

especially FilterFileSystem forwards the call of mkdirs(p) to the actual fs implementation...which may skip overriden mkdirs(somPath,somePerm) methods
...and could cause issues for existing FileSystem implementations
{quote}

File this jira to address this problem.

[~kgyrtkirk] [~stevel@apache.org] [~kihwal]"
start-build-env.sh fails in branch-3.1,13313831,Resolved,Critical,Won't Fix,28/Jun/20 08:17,10/Jun/21 08:03,,"./start-build-env.sh fails to install ember-cli
{noformat}
npm ERR! Linux 5.4.0-37-generic
npm ERR! argv ""/usr/bin/nodejs"" ""/usr/bin/npm"" ""install"" ""-g"" ""ember-cli""
npm ERR! node v4.2.6
npm ERR! npm  v3.5.2
npm ERR! code EMISSINGARG

npm ERR! typeerror Error: Missing required argument #1
npm ERR! typeerror     at andLogAndFinish (/usr/share/npm/lib/fetch-package-metadata.js:31:3)
npm ERR! typeerror     at fetchPackageMetadata (/usr/share/npm/lib/fetch-package-metadata.js:51:22)
npm ERR! typeerror     at resolveWithNewModule (/usr/share/npm/lib/install/deps.js:456:12)
npm ERR! typeerror     at /usr/share/npm/lib/install/deps.js:457:7
npm ERR! typeerror     at /usr/share/npm/node_modules/iferr/index.js:13:50
npm ERR! typeerror     at /usr/share/npm/lib/fetch-package-metadata.js:37:12
npm ERR! typeerror     at addRequestedAndFinish (/usr/share/npm/lib/fetch-package-metadata.js:82:5)
npm ERR! typeerror     at returnAndAddMetadata (/usr/share/npm/lib/fetch-package-metadata.js:117:7)
npm ERR! typeerror     at pickVersionFromRegistryDocument (/usr/share/npm/lib/fetch-package-metadata.js:134:20)
npm ERR! typeerror     at /usr/share/npm/node_modules/iferr/index.js:13:50
npm ERR! typeerror This is an error with npm itself. Please report this error at:
npm ERR! typeerror     <http://github.com/npm/npm/issues>

npm ERR! Please include the following file with any support request:
npm ERR!     /root/npm-debug.log
{noformat}"
WASB: Update azure-storage-java SDK,13313252,Resolved,Critical,Fixed,24/Jun/20 13:14,25/Jun/20 05:51,2.7.0,"WASB depends on the Azure Storage Java SDK.  There is a concurrency bug in the Azure Storage Java SDK that can cause the results of a list blobs operation to appear empty.  This causes the Filesystem listStatus and similar APIs to return empty results.  This has been seen in Spark work loads when jobs use more than one executor core. 

See [https://github.com/Azure/azure-storage-java/pull/546] for details on the bug in the Azure Storage SDK.

This issue can cause data loss."
NPE when hadoop.security.authorization is enabled but the input PolicyProvider for ZKFCRpcServer is NULL,13296421,Resolved,Critical,Fixed,06/Apr/20 06:52,13/Apr/20 20:48,3.2.1,"During initialization, ZKFCRpcServer refreshes the service authorization ACL for the service handled by this server if config hadoop.security.authorization is enabled, by calling refreshServiceAcl with the input PolicyProvider and Configuration.
{code:java}
ZKFCRpcServer(Configuration conf,
 InetSocketAddress bindAddr,
 ZKFailoverController zkfc,
 PolicyProvider policy) throws IOException {
 this.server = ...
 
 // set service-level authorization security policy
 if (conf.getBoolean(
 CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {
 server.refreshServiceAcl(conf, policy);
 }
}{code}

refreshServiceAcl calls ServiceAuthorizationManager#refreshWithLoadedConfiguration which directly gets services from the provider with provider.getServices(). When the provider is NULL, the code throws NPE without an informative message. In addition, the default value of config `hadoop.security.authorization.policyprovider` (which controls PolicyProvider here) is NULL and the only usage of ZKFCRpcServer initializer provides only an abstract method getPolicyProvider which does not enforce that PolicyProvider should not be NULL.

The suggestion here is to either add a guard check or exception handling with an informative logging message on ZKFCRpcServer to handle input PolicyProvider being NULL.

 

I am very happy to provide a patch for it if the issue is confirmed :)"
Remove unused joda-time,13302962,Resolved,Major,Fixed,05/May/20 20:21,03/Nov/20 00:08,3.4.0,Joda-time is defined in the hadoop-project/pom.xml but it's not used anywhere. It should be easy to remove it without problems.
Update Mockserver,13298615,Resolved,Major,Fixed,15/Apr/20 16:44,13/Oct/20 01:14,3.2.2,We are on Mockserver 3.9.2 which is more than 5 years old. Time to update.
Remove residual code of Ozone,13308091,Resolved,Major,Fixed,28/May/20 16:47,29/May/20 07:52,3.3.1,
Support disabling buffered reads in ABFS positional reads,13304437,Resolved,Major,Fixed,12/May/20 19:00,16/Feb/21 17:00,3.3.1,"Right now it will do a seek to the position , read and then seek back to the old position.  (As per the impl in the super class)
In HBase kind of workloads we rely mostly on short preads. (like 64 KB size by default).  So would be ideal to support a pure pos read API which will not even keep the data in a buffer but will only read the required data as what is asked for by the caller. (Not reading ahead more data as per the read size config)

Allow an optional boolean config to be specified while opening file for read using which buffered pread can be disabled. 
FutureDataInputStreamBuilder openFile(Path path)"
Replace Guava Supplier with Java8+ Supplier in Hadoop,13314186,Resolved,Major,Fixed,30/Jun/20 02:00,22/Jul/20 13:53,3.2.2,"Replacing Usage of {{guava.Supplier<>}} are in Unit tests {{GenereicTestUtils.waitFor()}} in Hadoop project.
 * To make things more convenient for reviewers, I decided:
 ** Not to replace Object instantiation by lambda expressions because this will increase the patch size significantly and require code adjustments to pass the checkstyle scripts.
 ** Not to refactor the imports because this will make reading the patch more difficult.
 * Merge should be done to the following branches: trunk, branch-3.3, branch-3.2, branch-3.1

The task is straightforward because {{java.util.Supplier}} has the same API as {{guava.Supplier<>}} and the vast majority of usage comes from Test-units.
 Therefore, we need only to do the following a ""one-line"" change in all 147 files.
{code:bash}
 
-import com.google.common.base.Supplier;
+import java.util.function.Supplier;
{code}
The code change needs to be applied to the following list of files:
{code:java}
 
Targets 
    Occurrences of 'com.google.common.base.Supplier' in project with mask '*.java' 
Found Occurrences (146 usages found) 
    org.apache.hadoop.conf (1 usage found) 
        TestReconfiguration.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.crypto.key.kms.server (1 usage found) 
        TestKMS.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.fs (2 usages found) 
        FCStatisticsBaseTest.java (1 usage found) 
            40 import com.google.common.base.Supplier; 
        TestEnhancedByteBufferAccess.java (1 usage found) 
            75 import com.google.common.base.Supplier; 
    org.apache.hadoop.fs.viewfs (1 usage found) 
        TestViewFileSystemWithTruncate.java (1 usage found) 
            23 import com.google.common.base.Supplier; 
    org.apache.hadoop.ha (1 usage found) 
        TestZKFailoverController.java (1 usage found) 
            25 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs (20 usages found) 
        DFSTestUtil.java (1 usage found) 
            79 import com.google.common.base.Supplier; 
        MiniDFSCluster.java (1 usage found) 
            78 import com.google.common.base.Supplier; 
        TestBalancerBandwidth.java (1 usage found) 
            29 import com.google.common.base.Supplier; 
        TestClientProtocolForPipelineRecovery.java (1 usage found) 
            30 import com.google.common.base.Supplier; 
        TestDatanodeRegistration.java (1 usage found) 
            44 import com.google.common.base.Supplier; 
        TestDataTransferKeepalive.java (1 usage found) 
            47 import com.google.common.base.Supplier; 
        TestDeadNodeDetection.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestDecommission.java (1 usage found) 
            41 import com.google.common.base.Supplier; 
        TestDFSShell.java (1 usage found) 
            37 import com.google.common.base.Supplier; 
        TestEncryptedTransfer.java (1 usage found) 
            35 import com.google.common.base.Supplier; 
        TestEncryptionZonesWithKMS.java (1 usage found) 
            22 import com.google.common.base.Supplier; 
        TestFileCorruption.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestLeaseRecovery2.java (1 usage found) 
            32 import com.google.common.base.Supplier; 
        TestLeaseRecoveryStriped.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestMaintenanceState.java (1 usage found) 
            63 import com.google.common.base.Supplier; 
        TestPread.java (1 usage found) 
            61 import com.google.common.base.Supplier; 
        TestQuota.java (1 usage found) 
            39 import com.google.common.base.Supplier; 
        TestReplaceDatanodeOnFailure.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestReplication.java (1 usage found) 
            27 import com.google.common.base.Supplier; 
        TestSafeMode.java (1 usage found) 
            62 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.client.impl (2 usages found) 
        TestBlockReaderLocalMetrics.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestLeaseRenewer.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.qjournal (1 usage found) 
        MiniJournalCluster.java (1 usage found) 
            31 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.qjournal.client (1 usage found) 
        TestIPCLoggerChannel.java (1 usage found) 
            43 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.qjournal.server (1 usage found) 
        TestJournalNodeSync.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.blockmanagement (7 usages found) 
        TestBlockManagerSafeMode.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestBlockReportRateLimiting.java (1 usage found) 
            25 import com.google.common.base.Supplier; 
        TestNameNodePrunesMissingStorages.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestPendingInvalidateBlock.java (1 usage found) 
            43 import com.google.common.base.Supplier; 
        TestPendingReconstruction.java (1 usage found) 
            34 import com.google.common.base.Supplier; 
        TestRBWBlockInvalidation.java (1 usage found) 
            49 import com.google.common.base.Supplier; 
        TestSlowDiskTracker.java (1 usage found) 
            48 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.datanode (13 usages found) 
        DataNodeTestUtils.java (1 usage found) 
            40 import com.google.common.base.Supplier; 
        TestBlockRecovery.java (1 usage found) 
            120 import com.google.common.base.Supplier; 
        TestBlockScanner.java (1 usage found) 
            43 import com.google.common.base.Supplier; 
        TestBPOfferService.java (1 usage found) 
            92 import com.google.common.base.Supplier; 
        TestCorruptMetadataFile.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestDataNodeLifeline.java (1 usage found) 
            74 import com.google.common.base.Supplier; 
        TestDataNodeMetrics.java (1 usage found) 
            37 import com.google.common.base.Supplier; 
        TestDataNodeMetricsLogger.java (1 usage found) 
            57 import com.google.common.base.Supplier; 
        TestDataNodeMultipleRegistrations.java (1 usage found) 
            33 import com.google.common.base.Supplier; 
        TestDataNodeMXBean.java (1 usage found) 
            31 import com.google.common.base.Supplier; 
        TestDatanodeProtocolRetryPolicy.java (1 usage found) 
            32 import com.google.common.base.Supplier; 
        TestDataNodeVolumeFailure.java (1 usage found) 
            94 import com.google.common.base.Supplier; 
        TestDiskError.java (1 usage found) 
            31 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.datanode.checker (1 usage found) 
        TestThrottledAsyncChecker.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl (7 usages found) 
        TestCacheByPmemMappableBlockLoader.java (1 usage found) 
            65 import com.google.common.base.Supplier; 
        TestFsDatasetCache.java (1 usage found) 
            94 import com.google.common.base.Supplier; 
        TestFsDatasetImpl.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestFsVolumeList.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestLazyPersistLockedMemory.java (1 usage found) 
            22 import com.google.common.base.Supplier; 
        TestPmemCacheRecovery.java (1 usage found) 
            65 import com.google.common.base.Supplier; 
        TestSpaceReservation.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.datanode.metrics (1 usage found) 
        TestDataNodeOutlierDetectionViaMetrics.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.datanode.web.webhdfs (1 usage found) 
        TestDataNodeUGIProvider.java (1 usage found) 
            54 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.diskbalancer (2 usages found) 
        TestDiskBalancer.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestDiskBalancerWithMockMover.java (1 usage found) 
            23 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.federation (1 usage found) 
        FederationTestUtils.java (1 usage found) 
            95 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.federation.router (4 usages found) 
        TestRouterAdminCLI.java (1 usage found) 
            67 import com.google.common.base.Supplier; 
        TestRouterQuota.java (1 usage found) 
            77 import com.google.common.base.Supplier; 
        TestRouterRpc.java (1 usage found) 
            131 import com.google.common.base.Supplier; 
        TestRouterRPCClientRetries.java (1 usage found) 
            57 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.mover (1 usage found) 
        TestMover.java (1 usage found) 
            98 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.namenode (17 usages found) 
        TestAddStripedBlockInFBR.java (1 usage found) 
            43 import com.google.common.base.Supplier; 
        TestBackupNode.java (1 usage found) 
            59 import com.google.common.base.Supplier; 
        TestCacheDirectives.java (1 usage found) 
            99 import com.google.common.base.Supplier; 
        TestCheckpoint.java (1 usage found) 
            98 import com.google.common.base.Supplier; 
        TestDeadDatanode.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestEditLogAutoroll.java (1 usage found) 
            49 import com.google.common.base.Supplier; 
        TestEditLogRace.java (1 usage found) 
            41 import com.google.common.base.Supplier; 
        TestFsck.java (1 usage found) 
            62 import com.google.common.base.Supplier; 
        TestFSNamesystemLock.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestMetaSave.java (1 usage found) 
            33 import com.google.common.base.Supplier; 
        TestNameNodeMetadataConsistency.java (1 usage found) 
            33 import com.google.common.base.Supplier; 
        TestNameNodeMetricsLogger.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestNameNodeMXBean.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestNameNodeStatusMXBean.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestPersistentStoragePolicySatisfier.java (1 usage found) 
            40 import com.google.common.base.Supplier; 
        TestReencryption.java (1 usage found) 
            34 import com.google.common.base.Supplier; 
        TestUpgradeDomainBlockPlacementPolicy.java (1 usage found) 
            52 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.namenode.ha (11 usages found) 
        HATestUtil.java (1 usage found) 
            64 import com.google.common.base.Supplier; 
        TestBootstrapStandby.java (1 usage found) 
            30 import com.google.common.base.Supplier; 
        TestDNFencing.java (1 usage found) 
            32 import com.google.common.base.Supplier; 
        TestDNFencingWithReplication.java (1 usage found) 
            39 import com.google.common.base.Supplier; 
        TestEditLogTailer.java (1 usage found) 
            65 import com.google.common.base.Supplier; 
        TestHASafeMode.java (1 usage found) 
            75 import com.google.common.base.Supplier; 
        TestPendingCorruptDnMessages.java (1 usage found) 
            44 import com.google.common.base.Supplier; 
        TestPipelinesFailover.java (1 usage found) 
            65 import com.google.common.base.Supplier; 
        TestStandbyCheckpoints.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        TestStandbyInProgressTail.java (1 usage found) 
            52 import com.google.common.base.Supplier; 
        TestStandbyIsHot.java (1 usage found) 
            46 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.namenode.snapshot (1 usage found) 
        TestRandomOpsWithSnapshots.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.namenode.sps (1 usage found) 
        TestStoragePolicySatisfierWithStripedFile.java (1 usage found) 
            55 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.server.sps (1 usage found) 
        TestExternalStoragePolicySatisfier.java (1 usage found) 
            101 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.shortcircuit (1 usage found) 
        TestShortCircuitCache.java (1 usage found) 
            93 import com.google.common.base.Supplier; 
    org.apache.hadoop.hdfs.tools (2 usages found) 
        TestDFSAdmin.java (1 usage found) 
            28 import com.google.common.base.Supplier; 
        TestDFSZKFailoverController.java (1 usage found) 
            58 import com.google.common.base.Supplier; 
    org.apache.hadoop.http (1 usage found) 
        TestSSLHttpServerConfigs.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.ipc (1 usage found) 
        TestIPC.java (1 usage found) 
            101 import com.google.common.base.Supplier; 
    org.apache.hadoop.mapred (2 usages found) 
        TestTaskAttemptListenerImpl.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
        UtilsForTests.java (1 usage found) 
            64 import com.google.common.base.Supplier; 
    org.apache.hadoop.mapreduce.v2.app (4 usages found) 
        TestFetchFailure.java (1 usage found) 
            29 import com.google.common.base.Supplier; 
        TestMRApp.java (1 usage found) 
            31 import com.google.common.base.Supplier; 
        TestRecovery.java (1 usage found) 
            31 import com.google.common.base.Supplier; 
        TestTaskHeartbeatHandler.java (1 usage found) 
            28 import com.google.common.base.Supplier; 
    org.apache.hadoop.mapreduce.v2.app.rm (1 usage found) 
        TestRMContainerAllocator.java (1 usage found) 
            156 import com.google.common.base.Supplier; 
    org.apache.hadoop.mapreduce.v2.hs (1 usage found) 
        TestJHSDelegationTokenSecretManager.java (1 usage found) 
            30 import com.google.common.base.Supplier; 
    org.apache.hadoop.metrics2.impl (1 usage found) 
        TestMetricsSystemImpl.java (1 usage found) 
            42 import com.google.common.base.Supplier; 
    org.apache.hadoop.metrics2.lib (1 usage found) 
        TestMutableRollingAverages.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.security (1 usage found) 
        TestGroupsCaching.java (1 usage found) 
            37 import com.google.common.base.Supplier; 
    org.apache.hadoop.security.ssl (1 usage found) 
        TestReloadingX509TrustManager.java (1 usage found) 
            24 import com.google.common.base.Supplier; 
    org.apache.hadoop.security.token.delegation (1 usage found) 
        TestZKDelegationTokenSecretManager.java (1 usage found) 
            27 import com.google.common.base.Supplier; 
    org.apache.hadoop.test (2 usages found) 
        GenericTestUtils.java (1 usage found) 
            64 import com.google.common.base.Supplier; 
        TestGenericTestUtils.java (1 usage found) 
            26 import com.google.common.base.Supplier; 
    org.apache.hadoop.tracing (1 usage found) 
        SetSpanReceiver.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.util (1 usage found) 
        TestShell.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.applications.distributedshell (1 usage found) 
        TestDistributedShell.java (1 usage found) 
            43 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.client (1 usage found) 
        TestRMFailover.java (1 usage found) 
            64 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.client.api.impl (1 usage found) 
        TestYarnClientWithReservation.java (1 usage found) 
            20 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.containermanager (1 usage found) 
        TestContainerManager.java (1 usage found) 
            51 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher (1 usage found) 
        TestContainerLaunch.java (1 usage found) 
            57 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer (1 usage found) 
        TestContainerLocalizer.java (1 usage found) 
            97 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation (1 usage found) 
        TestLogAggregationService.java (1 usage found) 
            150 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor (1 usage found) 
        TestContainersMonitor.java (1 usage found) 
            40 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker (1 usage found) 
        TestNMLogAggregationStatusTracker.java (1 usage found) 
            24 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.resourcemanager (6 usages found) 
        TestApplicationMasterLauncher.java (1 usage found) 
            95 import com.google.common.base.Supplier; 
        TestLeaderElectorService.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestRM.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestRMHA.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
        TestRMRestart.java (1 usage found) 
            137 import com.google.common.base.Supplier; 
        TestWorkPreservingRMRestart.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.resourcemanager.recovery (1 usage found) 
        TestZKRMStateStore.java (1 usage found) 
            75 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity (1 usage found) 
        TestCapacityScheduler.java (1 usage found) 
            192 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair (1 usage found)
        TestContinuousScheduling.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.resourcemanager.security (2 usages found) 
        TestDelegationTokenRenewer.java (1 usage found) 
            117 import com.google.common.base.Supplier; 
        TestRMDelegationTokens.java (1 usage found) 
            29 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.router.webapp (1 usage found) 
        TestRouterWebServicesREST.java (1 usage found) 
            135 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.server.webproxy.amfilter (1 usage found) 
        TestAmFilter.java (1 usage found) 
            53 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.service (1 usage found) 
        MockServiceAM.java (1 usage found) 
            21 import com.google.common.base.Supplier; 
    org.apache.hadoop.yarn.sls.nodemanager (1 usage found) 
        TestNMSimulator.java (1 usage found) 
            20 import com.google.common.base.Supplier; 

{code}"
Replace Guava Function with Java8+ Function,13314204,Resolved,Major,Fixed,30/Jun/20 04:43,15/Jul/20 16:06,3.2.2,"{code:java}
Targets
    Occurrences of 'com.google.common.base.Function'
Found Occurrences  (7 usages found)
    hadoop-hdfs-project/hadoop-hdfs/dev-support/jdiff  (1 usage found)
        Apache_Hadoop_HDFS_2.6.0.xml  (1 usage found)
            13603 <field name=""GET_START_TXID"" type=""com.google.common.base.Function""
    org.apache.hadoop.hdfs.server.blockmanagement  (1 usage found)
        HostSet.java  (1 usage found)
            20 import com.google.common.base.Function;
    org.apache.hadoop.hdfs.server.datanode.checker  (1 usage found)
        AbstractFuture.java  (1 usage found)
            58 * (ListenableFuture, com.google.common.base.Function) Futures.transform}
    org.apache.hadoop.hdfs.server.namenode.ha  (1 usage found)
        HATestUtil.java  (1 usage found)
            40 import com.google.common.base.Function;
    org.apache.hadoop.hdfs.server.protocol  (1 usage found)
        RemoteEditLog.java  (1 usage found)
            20 import com.google.common.base.Function;
    org.apache.hadoop.mapreduce.lib.input  (1 usage found)
        TestFileInputFormat.java  (1 usage found)
            58 import com.google.common.base.Function;
    org.apache.hadoop.yarn.api.protocolrecords.impl.pb  (1 usage found)
        GetApplicationsRequestPBImpl.java  (1 usage found)
            38 import com.google.common.base.Function;

{code}
"
Tolerate leading and trailing spaces in fs.defaultFS,13300666,Resolved,Major,Fixed,23/Apr/20 21:56,30/Apr/20 21:27,3.4.0,"*Problem:*

Currently, `getDefaultUri` is using `conf.get` to get the value of `fs.defaultFS`, which means that the trailing whitespace after a valid URI won’t be removed and could stop namenode and datanode from starting up.

 

*How to reproduce (Hadoop-2.8.5):*

Set the configuration
{code:java}
<property>
     <name>fs.defaultFS</name>
     <value>hdfs://localhost:9000 </value>
</property>{code}
In core-site.xml (there is a whitespace after 9000) and start HDFS.

Namenode and datanode won’t start and the log message is:
{code:java}
2020-04-23 11:09:48,198 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.lang.IllegalArgumentException: Illegal character in authority at index 7: hdfs://localhost:9000 
    at java.net.URI.create(URI.java:852)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.setClientNamenodeAddress(NameNode.java:440)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:897)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:885)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1626)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1694)
Caused by: java.net.URISyntaxException: Illegal character in authority at index 7: hdfs://localhost:9000 
    at java.net.URI$Parser.fail(URI.java:2848)
    at java.net.URI$Parser.parseAuthority(URI.java:3186)
    at java.net.URI$Parser.parseHierarchical(URI.java:3097)
    at java.net.URI$Parser.parse(URI.java:3053)
    at java.net.URI.<init>(URI.java:588)
    at java.net.URI.create(URI.java:850)
    ... 5 more
{code}
 

*Solution:*

Use `getTrimmed` instead of `get` for `fs.defaultFS`:
{code:java}
public static URI getDefaultUri(Configuration conf) {
  URI uri =
    URI.create(fixName(conf.getTrimmed(FS_DEFAULT_NAME_KEY, DEFAULT_FS)));
  if (uri.getScheme() == null) {
    throw new IllegalArgumentException(""No scheme in default FS: "" + uri);
  }
  return uri;
}
{code}
I have submitted a patch for trunk about this."
NodeBase.normalize doesn't removing all trailing slashes.,13296341,Resolved,Major,Fixed,05/Apr/20 08:52,30/Apr/20 14:29,3.4.0,"As per javadoc 
/** Normalize a path by stripping off any trailing {@link #PATH_SEPARATOR}

But it removes only one."
Optimize UGI#getGroups by adding UGI#getGroupsSet,13312649,Resolved,Major,Fixed,21/Jun/20 02:28,09/Jul/20 18:35,3.4.0,"UGI#getGroups has been optimized with HADOOP-13442 by avoiding the List->Set->List conversion. However the returned list is not optimized to contains lookup, especially the user's group membership list is huge (thousands+) . This ticket is opened to add a UGI#getGroupsSet and use Set#contains() instead of List#contains() to speed up large group look up while minimize List->Set conversions in Groups#getGroups() call. "
Update commons-codec from 1.11 to 1.14,13303453,Resolved,Major,Fixed,07/May/20 16:56,11/May/20 15:41,3.4.0,We are on commons-codec 1.11 which is slightly outdated. The latest is 1.14. We should update it if it's not too much of a hassle.
Update Dockerfile_aarch64 to use Bionic,13313138,Resolved,Major,Fixed,24/Jun/20 01:18,01/Jul/20 09:58,3.3.1,"Dockerfile for x86 have been updated to apply Ubuntu Bionic, JDK11 and other changes, we should make Dockerfile for aarch64 following these changes, keep same behavior."
Increase precommit job timeout from 5 hours to 20 hours,13313300,Resolved,Major,Fixed,24/Jun/20 17:12,01/Jul/20 07:55,3.3.1,"Now we frequently increase the timeout for testing and undo the change before committing.

* https://github.com/apache/hadoop/pull/2026
* https://github.com/apache/hadoop/pull/2051
* https://github.com/apache/hadoop/pull/2012
* https://github.com/apache/hadoop/pull/2098
* and more...

I'd like to increase the timeout by default to reduce the work."
[Test] Use Yetus 0.12.0 in precommit job,13299391,Resolved,Major,Done,18/Apr/20 18:28,19/Apr/20 07:57,,
Support downstreams' existing Hadoop-rpc implementations using non-shaded protobuf classes.,13305596,Resolved,Major,Fixed,18/May/20 06:22,12/Jun/20 18:04,3.3.0,"After upgrade/shade of protobuf to 3.7 version, existing Hadoop-Rpc client-server implementations using ProtobufRpcEngine will not work.

So, this Jira proposes to keep existing ProtobuRpcEngine as-is (without shading and with protobuf-2.5.0 implementation) to support downstream implementations.

Use new ProtobufRpcEngine2 to use shaded protobuf classes within Hadoop and later projects who wish to upgrade their protobufs to 3.x."
Umbrella Jira for improving the Hadoop-cos support in Hadoop,13295848,Patch Available,Major,,02/Apr/20 17:43,,,"This Umbrella Jira focus on fixing some known bugs and adding some important features.

 

bugfix:
 # resolve the dependency conflict;
 # fix the upload buffer returning failed when some exceptions occur;
 # fix the issue that the single file upload can not be retried;
 # fix the bug of checking if a file exists through listing the file frequently.

features:
 # support SessionCredentialsProvider and InstanceCredentialsProvider, which allows users to specify the credentials in URI or get it from the CVM (Tencent Cloud Virtual Machine) bound to the CAM role that can access the COS bucket;
 # support the server encryption  based on SSE-COS and SSE-C;
 # support the HTTP proxy settings;
 # support the storage class settings;
 # support the CRC64 checksum."
Optimizing the upload buffer in Hadoop-cos,13297757,Patch Available,Major,,11/Apr/20 09:34,,,"This task focus on fixing the bug of returning an upload buffer failed when some exceptions occur.

 

What's more, the optimizing upload buffer management would be provided.

 "
Supporting the new credentials provider in Hadoop-cos,13297759,In Progress,Major,,11/Apr/20 09:42,,,"This task aims to support three credentials provider in Hadoop-cos:
 * SessionCredentialsProvider
 * InstanceCredentialsProvider"
S3A delegation token binding to support secondary binding list,13312278,Resolved,Major,Won't Fix,18/Jun/20 17:00,04/Oct/22 16:43,3.3.0,"(followon from HADOOP-17050)

Add the ability of an S3A FS instance to support multiple instances of delegation token bindings.

The property ""fs.s3a.delegation.token.secondary.bindings"" will list the classnames of all secondary bindings.

for each one, an instance shall be created with the canonical service name being: fs URI + [ tokenKind ]. This is to ensure that the URIs are unique for each FS instance -but also that a single fs instance can have multiple tokens in the credential list.

the instance is just a AbstractDelegationTokenBinding provider of an AWS credential provider chain, with the normal lifecycle and operations to bind to a DT, issue tokens, etc

* the final list of AWS Credential providers will be built by appending those provided by each binding in turn.

Token binding at launch

If the primary token binding binds to a delegation token, then the whole binding is changed such that all secondary tokens MUST also bind. That is: it will be an error if one cannot be found. This is  possibly overstrict-but it avoids situations where an incomplete set of tokens are retrieved and This does not surface until later.

Only the encryption secrets in the primary DT will be used for FS encryption settings.

Testing: yes.

Probably also by adding a test-only DT provider which doesn't actually issue any real credentials and so which can be deployed in both ITests and staging tests where we can verify that the chained instantiation works.

Compatibility: the goal is to be backwards compatible with any already released token provider plugin."
ABFS: Make PUT and POST operations idempotent,13301153,Resolved,Major,Fixed,27/Apr/20 03:54,24/Jun/20 16:16,3.2.1,"Initially changes were made as part of this PR to handle idempotency including rename operation with the understanding that last modified time gets updated. But that assumption was wrong and the rename idempotency handling has since evolved.


For a job clean up, if the Manifest Committer in below Jira is used, then rename idempotency works using the previously fetched etag :

[HADOOP-18163] hadoop-azure support for the Manifest Committer of MAPREDUCE-7341 - ASF JIRA (apache.org)

 

A part of the commit tracked under current Jira to handle DELETE idempotency is still relevant.

A means to handle idempotency between driver and backend inherently is being worked upon.

-- Older notes


Currently when a PUT or POST operation timeouts and the server has already successfully executed the operation, there is no check in driver to see if the operation did succeed or not and just retries the same operation again. This can cause driver to through invalid user errors.

 

Sample scenario:
 # Rename request times out. Though server has successfully executed the operation.
 # Driver retries rename and get source not found error.

In the scenario, driver needs to check if rename is being retried and success if source if not found, but destination is present.

 "
Handle release package related issues,13298258,Resolved,Major,Fixed,14/Apr/20 12:29,15/Apr/20 17:44,,"Same issue as mentioned in HADOOP-16919 is present in hadoop distribution generation as well.

Handle following comments from [~elek] in 1.0.0-RC0 voting mail thread here[[https://lists.apache.org/thread.html/r1f2e8325ecef239f0d713c683a16336e2a22431a9f6bfbde3c763816%40%3Ccommon-dev.hadoop.apache.org%3E]]
{quote}3. Yetus seems to be included in the source package. I am not sure if
 it's intentional but I would remove the patchprocess directory from the
 tar file.

7. Minor nit: I would suggest to use only the filename in the sha512
 files (instead of having the /build/source/target prefix). It would help
 to use `sha512 -c` command to validate the checksum.
{quote}
 "
Introduce StreamContext for Abfs Input and Output streams.,13297273,Resolved,Major,Fixed,09/Apr/20 06:14,14/May/20 17:18,,"The number of configuration keeps growing in AbfsOutputStream and AbfsInputStream as we keep on adding new features. It is time to refactor the configurations in a separate class like StreamContext and pass them around. 

This is will improve the readability of code and reduce cherry-pick-backport pain. "
Fix ZStandardCompressor input buffer offset,13313687,Resolved,Major,Fixed,26/Jun/20 20:22,10/Nov/20 19:44,3.2.1,"A bug in index handling causes ZStandardCompressor.c to pass a malformed ZSTD_inBuffer to libzstd. libzstd then returns an ""Error (generic)"" that gets thrown. The crux of the issue is two variables, uncompressedDirectBufLen and uncompressedDirectBufOff. The hadoop code counts uncompressedDirectBufOff from the start of uncompressedDirectBuf, then uncompressedDirectBufLen is counted from uncompressedDirectBufOff. However, libzstd considers pos and size to both be counted from the start of the buffer. As a result, this line https://github.com/apache/hadoop/blob/rel/release-3.2.1/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.c#L228 causes a malformed buffer to be passed to libzstd, where pos>size. Here's a longer description of the bug in case this abstract explanation is unclear:

----

Suppose we initialize uncompressedDirectBuf (via setInputFromSavedData) with five bytes of input. This results in uncompressedDirectBufOff=0 and uncompressedDirectBufLen=5 (https://github.com/apache/hadoop/blob/rel/release-3.2.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java#L140-L146).

Then we call compress(), which initializes a ZSTD_inBuffer (https://github.com/apache/hadoop/blob/rel/release-3.2.1/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.c#L195-L196). The definition of those libzstd structs is here https://github.com/facebook/zstd/blob/v1.3.1/lib/zstd.h#L251-L261 - note that we set size=uncompressedDirectBufLen and pos=uncompressedDirectBufOff. The ZSTD_inBuffer gets passed to libzstd, compression happens, etc. When libzstd returns from the compression function, it updates the ZSTD_inBuffer struct to indicate how many bytes were consumed (https://github.com/facebook/zstd/blob/v1.3.1/lib/compress/zstd_compress.c#L3919-L3920). Note that pos is advanced, but size is unchanged.

Now, libzstd does not guarantee that the entire input will be compressed in a single call of the compression function. (Some of the compression libraries used by hadoop, such as snappy, _do_ provide this guarantee, but libzstd is not one of them.) So the hadoop native code updates uncompressedDirectBufOff and uncompressedDirectBufLen using the updated ZSTD_inBuffer: https://github.com/apache/hadoop/blob/rel/release-3.2.1/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.c#L227-L228

Now, returning to our example, we started with 5 bytes of uncompressed input. Suppose libzstd compressed 4 of those bytes, leaving one unread. This would result in a ZSTD_inBuffer struct with size=5 (unchanged) and pos=4 (four bytes were consumed). The hadoop native code would then set uncompressedDirectBufOff=4, but it would also set uncompressedDirectBufLen=1 (five minus four equals one).

Since some of the input was not consumed, we will eventually call compress() again. Then we instantiate another ZSTD_inBuffer struct, this time with size=1 and pos=4. This is a bug - libzstd expects size and pos to both be counted from the start of the buffer, therefore pos>size is unsound. So it returns an error https://github.com/facebook/zstd/blob/v1.3.1/lib/compress/zstd_compress.c#L3932 which gets escalated as a java.lang.InternalError.

I will be submitting a pull request on github with a fix for this bug. The key is that the hadoop code should handle offsets the same way libzstd does, ie uncompressedDirectBufLen should be counted from the start of uncompressedDirectBuf, not from uncompressedDirectBufOff."
Reduce Guava dependency in Hadoop source code,13314092,In Progress,Major,,29/Jun/20 15:59,,,"Relying on Guava implementation in Hadoop has been painful due to compatibility and vulnerability issues.
 Guava updates tend to break/deprecate APIs. This made It hard to maintain backward compatibility within hadoop versions and clients/downstreams.

With 3.x uses java8+, the java 8 features should preferred to Guava, reducing the footprint, and giving stability to source code.

This jira should serve as an umbrella toward an incremental effort to reduce the usage of Guava in the source code and to create subtasks to replace Guava classes with Java features.

Furthermore, it will be good to add a rule in the pre-commit build to warn against introducing a new Guava usage in certain modules.

Any one willing to take part in this code refactoring has to:
 # Focus on one module at a time in order to reduce the conflicts and the size of the patch. This will significantly help the reviewers.
 # Run all the unit tests related to the module being affected by the change. It is critical to verify that any change will not break the unit tests, or cause a stable test case to become flaky.
 # Merge should be done to the following branches:  trunk, branch-3.3, branch-3.2, branch-3.1

 

A list of sub tasks replacing Guava APIs with java8 features:
{code:java}
com.google.common.io.BaseEncoding#base64()	java.util.Base64
com.google.common.io.BaseEncoding#base64Url()	java.util.Base64
com.google.common.base.Joiner.on()	                        java.lang.String#join() or 
                                                                                         java.util.stream.Collectors#joining()
com.google.common.base.Optional#of()	                java.util.Optional#of()
com.google.common.base.Optional#absent()	        java.util.Optional#empty()
com.google.common.base.Optional#fromNullable()	java.util.Optional#ofNullable()
com.google.common.base.Optional	                        java.util.Optional
com.google.common.base.Predicate	                        java.util.function.Predicate
com.google.common.base.Function	                        java.util.function.Function
com.google.common.base.Supplier	                        java.util.function.Supplier
{code}
 

I also vote for the replacement of {{Precondition}} with either a wrapper, or Apache commons lang.

I believe you guys have dealt with Guava compatibilities in the past and probably have better insights. Any thoughts? [~weichiu], [~gabor.bota], [~stevel@apache.org], [~ayushtkn], [~busbey], [~jeagles], [~kihwal]

 "
Add checkstyle rule to prevent further usage of Guava classes,13314320,Resolved,Major,Done,30/Jun/20 15:08,17/Oct/21 14:38,,"We should have precommit rules to prevent further usage of Guava classes that are available in Java8+


A list replacing Guava APIs with java8 features:
{code:java}
com.google.common.io.BaseEncoding#base64()	java.util.Base64
com.google.common.io.BaseEncoding#base64Url()	java.util.Base64
com.google.common.base.Joiner.on()	                        java.lang.String#join() or 
                                                                                         java.util.stream.Collectors#joining()
com.google.common.base.Optional#of()	                java.util.Optional#of()
com.google.common.base.Optional#absent()	        java.util.Optional#empty()
com.google.common.base.Optional#fromNullable()	java.util.Optional#ofNullable()
com.google.common.base.Optional	                        java.util.Optional
com.google.common.base.Predicate	                        java.util.function.Predicate
com.google.common.base.Function	                        java.util.function.Function
com.google.common.base.Supplier	                        java.util.function.Supplier
{code}
"
ViewFS should initialize target filesystems lazily,13302792,Resolved,Major,Fixed,05/May/20 06:09,21/Jul/21 01:30,3.2.1,"Currently viewFS initialize all configured target filesystems when viewfs#init itself.

Some target file system initialization involve creating heavy objects and proxy connections. Ex: DistributedFileSystem#initialize will create DFSClient object which will create proxy connections to NN etc.
For example: if ViewFS configured with 10 target fs with hdfs uri and 2 targets with s3a.

If one of the client only work with s3a target, But ViewFS will initialize all targets irrespective of what clients interested to work with. That means, here client will create 10 DFS initializations and 2 s3a initializations. Its unnecessary to have DFS initialization here. So, it will be a good idea to initialize the target fs only when first time usage call come to particular target fs scheme. "
Enable setting clientId and tenantId for ADL MsiTokenProvider,13296833,Resolved,Major,Won't Fix,07/Apr/20 13:50,10/Jun/21 17:56,,"AdlFileSystem is [currently using|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java#L318] a [deprecated constructor for MsiTokenProvider|https://github.com/Azure/azure-data-lake-store-java/blob/2.3.6/src/main/java/com/microsoft/azure/datalake/store/oauth2/MsiTokenProvider.java#L42], only passing a port setting which is not used anymore.

The purpose of this improvement is to update AdlFileSystem to pass optional clientId and tenantId to MsiTokenProvider. This can ultimately enable MSI authentication on VMs with attached multiple MSIs."
Remove source code from branch-2,13298511,Resolved,Major,Done,15/Apr/20 09:03,10/Jun/21 08:06,,"Now, branch-2 is dead and unused. I think we can delete the entire source code from branch-2 to avoid committing or cherry-picking to the unused branch.

Chen Liang asked ASF INFRA for help but it didn't help for us: INFRA-19581"
hadoop-shaded-protobuf_3_7 depends on the wrong version.,13312621,Resolved,Major,Duplicate,20/Jun/20 16:00,10/Jun/21 08:05,3.0.1,"When using maven to compile hadoop source code, the following exception message appears:

[*INFO*] *------------------------------------------------------------------------*

[*INFO*] *BUILD FAILURE*

[*INFO*] *------------------------------------------------------------------------*

[*INFO*] Total time:  29.546 s

[*INFO*] Finished at: 2020-06-20T23:57:59+08:00

[*INFO*] *------------------------------------------------------------------------*

[*ERROR*] Failed to execute goal on project hadoop-common: *Could not resolve dependencies for project org.apache.hadoop:hadoop-common:jar:3.3.0-SNAPSHOT: Could not find artifact org.apache.hadoop.thirdparty:hadoop-shaded-protobuf_3_7:jar:1.0.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots)* -> *[Help 1]*

[*ERROR*] 

[*ERROR*] To see the full stack trace of the errors, re-run Maven with the *-e* switch.

[*ERROR*] Re-run Maven using the *-X* switch to enable full debug logging.

[*ERROR*] 

[*ERROR*] For more information about the errors and possible solutions, please read the following articles:

[*ERROR*] *[Help 1]* http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException

[*ERROR*] 

[*ERROR*] After correcting the problems, you can resume the build with the command

[*ERROR*]   *mvn <args> -rf :hadoop-common*"
Hadoop 3.1.2 download link is broken,13298927,Resolved,Major,Fixed,17/Apr/20 00:21,18/Apr/20 04:15,,"Remove broken Hadoop 3.1.2 download links from the website.
https://hadoop.apache.org/releases.html"
Fix shelldocs path in Jenkinsfile,13309110,Resolved,Major,Fixed,03/Jun/20 06:31,03/Jun/20 21:11,,"Shelldocs check is not enabled in the precommit jobs.
|{color:#0000FF}0{color}|{color:#0000FF}shelldocs{color}|{color:#0000FF}0m 1s{color}|{color:#0000FF}Shelldocs was not available.{color}|

Console log https://builds.apache.org/job/hadoop-multibranch/job/PR-2045/1/console
{noformat}
WARNING: shellcheck needs UTF-8 locale support. Forcing C.UTF-8.
executable '/testptch/hadoop/dev-support/bin/shelldocs' for 'shelldocs' does not exist.
{noformat}"
Handle an internal dir in viewfs having multiple children mount points pointing to different filesystems,13302973,Resolved,Major,Fixed,05/May/20 21:42,01/Jul/20 07:29,,"In case the viefs mount table is configured in a way where multiple child mount points are pointing to different file systems, the getContentSummary or getStatus don't return the expected result
{code:java}
mount link /a/b/ → hdfs://nn1/a/b
 mount link /a/d/ → file:///nn2/c/d{code}
b has two files and d has 1 file. So getContentSummary on / should return 3 files.

It also fails for the following scenario:
{code:java}
mount link  /internalDir -> /internalDir/linternalDir2
mount link  /internalDir -> /internalDir/linkToDir2 -> hdfs://nn1/dir2{code}
Exception:
{code:java}
java.io.IOException: Internal implementation error: expected file name to be /java.io.IOException: Internal implementation error: expected file name to be /
 at org.apache.hadoop.fs.viewfs.InternalDirOfViewFs.checkPathIsSlash(InternalDirOfViewFs.java:88) at org.apache.hadoop.fs.viewfs.InternalDirOfViewFs.getFileStatus(InternalDirOfViewFs.java:154) at org.apache.hadoop.fs.FileSystem.getContentSummary(FileSystem.java:1684) at org.apache.hadoop.fs.FileSystem.getContentSummary(FileSystem.java:1695) at org.apache.hadoop.fs.viewfs.ViewFileSystem.getContentSummary(ViewFileSystem.java:918) at org.apache.hadoop.fs.viewfs.ViewFileSystemBaseTest.testGetContentSummary(ViewFileSystemBaseTest.java:1106){code}"
shelldoc fails in hadoop-common,13308460,Resolved,Major,Fixed,30/May/20 07:50,04/Jun/20 07:44,,"{noformat}
[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---
> ERROR: yetus-dl: gpg unable to import
> /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/sourcedir/patchprocess/KEYS_YETUS
> [INFO]
> ------------------------------------------------------------------------
> [INFO] BUILD FAILURE
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Total time:  9.377 s
> [INFO] Finished at: 2020-05-28T17:37:41Z
> [INFO]
> ------------------------------------------------------------------------
> [ERROR] Failed to execute goal
> org.codehaus.mojo:exec-maven-plugin:1.3.1:exec (shelldocs) on project
> hadoop-common: Command execution failed. Process exited with an error: 1
> (Exit value: 1) -> [Help 1]
> [ERROR]
> [ERROR] To see the full stack trace of the errors, re-run Maven with the
> -e switch.
> [ERROR] Re-run Maven using the -X switch to enable full debug logging.
> [ERROR]
> [ERROR] For more information about the errors and possible solutions,
> please read the following articles:
> [ERROR] [Help 1]
> http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{noformat}
* https://builds.apache.org/job/PreCommit-HADOOP-Build/16957/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt
* https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/155/artifact/out/patch-mvnsite-root.txt
* https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/157/artifact/out/patch-mvnsite-root.txt"
ViewFS does not return correct user/group and ACL,13302959,Resolved,Major,Fixed,05/May/20 20:10,20/Jun/20 07:46,,"When doing ls on a mount point parent, the returned user/group ACL is incorrect. It always showing the user and group being current user, with some arbitrary ACL. Which could misleading any application depending on this API.

cc [~cliang] [~virajith] "
listStatus and getFileStatus behave inconsistent in the case of ViewFs implementation for isDirectory,13306502,Resolved,Major,Fixed,21/May/20 13:38,10/Jun/20 22:01,3.0.0,"listStatus implementation in ViewFs and getFileStatus does not return consistent values for an element on isDirectory value. listStatus returns isDirectory of all softlinks as false and getFileStatus returns isDirectory as true.
{code:java}
[hdfs@c3121-node2 ~]$ /usr/jdk64/jdk1.8.0_112/bin/java -cp `hadoop classpath`:./hdfs-append-1.0-SNAPSHOT.jar LauncherGetFileStatus ""/""
FileStatus of viewfs://c3121/testme21may isDirectory:false
FileStatus of viewfs://c3121/tmp isDirectory:false
FileStatus of viewfs://c3121/foo isDirectory:false
FileStatus of viewfs://c3121/tmp21may isDirectory:false
FileStatus of viewfs://c3121/testme isDirectory:false
FileStatus of viewfs://c3121/testme2 isDirectory:false <--- returns false
FileStatus of / isDirectory:true
[hdfs@c3121-node2 ~]$ /usr/jdk64/jdk1.8.0_112/bin/java -cp `hadoop classpath`:./hdfs-append-1.0-SNAPSHOT.jar LauncherGetFileStatus /testme2
FileStatus of viewfs://c3121/testme2/dist-copynativelibs.sh isDirectory:false
FileStatus of viewfs://c3121/testme2/newfolder isDirectory:true
FileStatus of /testme2 isDirectory:true <--- returns true
[hdfs@c3121-node2 ~]$ {code}"
"Revert ""HADOOP-8143. Change distcp to have -pb on by default""",13305201,Resolved,Major,Fixed,15/May/20 13:05,15/May/20 13:10,3.0.3,"revert the HADOOP-8143. ""distcp -pb as default"" feature as it was

* breaking s3a uploads
* breaking incremental uploads to any object store"
"ListStatus on ViewFS root (ls ""/"") should list the linkFallBack root (configured target root).",13302246,Resolved,Major,Fixed,01/May/20 01:52,19/May/20 05:32,3.2.2,"As part of the design doc HDFS-15289, [~sanjay.radia] and me discussed the following scenarios when fallback enabled.

*Behavior when fallback enabled:*

       Assume FS trees and mount mappings like below:

           mount link /a/b/c/d  → hdfs://nn1/a/b

           mount link /a/p/q/r  → hdfs://nn2/a/b    

           fallback → hdfs://nn3/  $  /a/c
                                                 /x/z
 # Open(/x/y) then it goes to nn3 (fallback)      - WORKS
 # Create(/x/foo) then foo is created in nn3 in dir /x   - WORKS
 # ls /  should list   /a  /x .Today this does not work and IT IS A BUG!!! Because it conflicts with the open(/x/y)
 # Create /y  : fails  - also fails when not using  fallback  - WORKS
 # Create /a/z : fails - also fails when not using  fallback - WORKS
 # ls /a should list /b /p  as expected and will not show fallback in nn3 - WORKS

 

This Jira will fix issue of #3. So, when fallback enabled it should show merged ls view with mount links + fallback root. ( this will only be at root level)"
client fails forever when namenode ipaddr changed,13309671,Resolved,Major,Fixed,05/Jun/20 09:17,23/Jun/20 09:04,,"For machine replacement, I replace my standby namenode with a new ipaddr and keep the same hostname. Also update the client's hosts to make it resolve correctly

When I try to run failover to transite the new namenode(let's say nn2), the client will fail to read or write forever until it's restarted.

That make yarn nodemanager in sick state. Even the new tasks will encounter this exception  too. Until all nodemanager restart.

 
{code:java}
20/06/02 15:12:25 WARN ipc.Client: Address change detected. Old: nn2-192-168-1-100/192.168.1.100:9000 New: nn2-192-168-1-100/192.168.1.200:9000
20/06/02 15:12:25 DEBUG ipc.Client: closing ipc connection to nn2-192-168-1-100/192.168.1.200:9000: Connection refused
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:608)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1517)
        at org.apache.hadoop.ipc.Client.call(Client.java:1440)
        at org.apache.hadoop.ipc.Client.call(Client.java:1401)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:193)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
{code}
 

We can see the client has {{Address change detected}}, but it still fails. I find out that's because when method {{updateAddress()}} return true,  the {{handleConnectionFailure()}} thow an exception that break the next retry with the right ipaddr.

Client.java: setupConnection()
{code:java}
        } catch (ConnectTimeoutException toe) {
          /* Check for an address change and update the local reference.
           * Reset the failure counter if the address was changed
           */
          if (updateAddress()) {
            timeoutFailures = ioFailures = 0;
          }
          handleConnectionTimeout(timeoutFailures++,
              maxRetriesOnSocketTimeouts, toe);
        } catch (IOException ie) {
          if (updateAddress()) {
            timeoutFailures = ioFailures = 0;
          }
// because the namenode ip changed in updateAddress(), the old namenode ipaddress cannot be accessed now
// handleConnectionFailure will thow an exception, the next retry never have a chance to use the right server updated in updateAddress()
          handleConnectionFailure(ioFailures++, ie);
        }
{code}
 "
ArrayIndexOfboundsException in ViewFileSystem#listStatus,13309000,Resolved,Major,Fixed,02/Jun/20 17:10,10/Jun/20 16:08,,"In Viewfilesystem#listStatus , we get groupnames of ugi , If groupnames doesn't exists  it will throw AIOBE
{code:java}
        else {
          result[i++] = new FileStatus(0, true, 0, 0,
            creationTime, creationTime, PERMISSION_555,
            ugi.getShortUserName(), ugi.getGroupNames()[0],
            new Path(inode.fullPath).makeQualified(
                myUri, null));
        } {code}
 "
Release Hadoop 3.3.0,13300344,Resolved,Major,Done,22/Apr/20 19:27,02/Jun/21 06:51,,"Thanks [~brahma] for making the effort to release Hadoop 3.3.0. Raise this jira to track the progress, and use this jira to link other Apache projects' Hadoop 3.3.0 support progress."
Adding Network Counters in ABFS,13309459,Resolved,Major,Fixed,04/Jun/20 13:51,27/Aug/20 14:16,3.3.0,"Network Counters to be added in ABFS:
|CONNECTIONS_MADE|Number of times connection was made with Azure Data Lake|
|SEND_REQUESTS|Number of send requests|
|GET_RESPONSE|Number of response gotten|
|BYTES_SEND|Number of bytes send|
|BYTES_RECEIVED|Number of bytes received|
|READ_THROTTLE|Number of times throttled while read operation|
|WRITE_THROTTLE|Number of times throttled while write operation|

propose:
 * Adding these counters as part of AbfsStatistic already made in HADOOP-17016.
 * Increment of counters across Abfs Network services."
S3A staging committer committing duplicate files,13309685,Resolved,Major,Duplicate,05/Jun/20 09:54,09/Nov/20 19:29,3.1.3,SPARK-31911 reporting concurrent jobs double writing files.
Update JaegerTracing,13298613,Open,Major,,15/Apr/20 16:35,,thirdparty-1.0.0,We currently use JaegerTracing 0.34.0. The latest is 1.2.0. We are several versions behind and should update. Note this update requires the latest version fo OpenTracing and has several breaking changes.
ABFS: Long waits and unintended retries when multiple threads try to fetch token using ClientCreds,13313370,Resolved,Major,Fixed,25/Jun/20 03:49,23/Jul/20 09:10,3.3.0,"Issue reported by DB:

we recently experienced some problems with ABFS driver that highlighted a possible issue with long hangs following synchronized retries when using the _ClientCredsTokenProvider_ and calling _AbfsClient.getAccessToken_. We have seen [https://github.com/apache/hadoop/pull/1923|https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fapache%2Fhadoop%2Fpull%2F1923&data=02%7c01%7csnvijaya%40microsoft.com%7c7362c5ba4af24a553c4308d807ec459d%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c637268058650442694&sdata=FePBBkEqj5kI2Ty4kNr3a2oJgB8Kvy3NvyRK8NoxyH4%3D&reserved=0], but it does not directly apply since we are not using a custom token provider, but instead _ClientCredsTokenProvider_ that ultimately relies on _AzureADAuthenticator_. 

 

The problem was that the critical section of getAccessToken, combined with a possibly redundant retry policy, made jobs hanging for a very long time, since only one thread at a time could make progress, and this progress amounted to basically retrying on a failing connection for 30-60 minutes.

 "
Support for Appendblob in abfs driver,13308928,Resolved,Major,Fixed,02/Jun/20 10:17,07/Jul/20 16:50,3.3.0,add changes to support appendblob in the hadoop-azure abfs driver.
update-method-param for ProtocolSignature,13298463,Open,Major,,15/Apr/20 05:16,,3.2.1,update-method-param for ProtocolSignature
ITestS3AConfiguration proxy tests fail when bucket probes == 0,13299127,Resolved,Major,Fixed,17/Apr/20 13:08,05/Jan/21 14:49,3.3.0,"
when bucket probes are disabled, proxy config tests in ITestS3AConfiguration fail because the probes aren't being attempted in initialize()

{code}
  <property>
    <name>fs.s3a.bucket.probe</name>
    <value>0</value>
  </property>   
{code}

Cause: HADOOP-16711
Fix: call unsetBaseAndBucketOverrides for bucket probe in test conf, then set the probe value to 2 just to be resilient to future default changes.
"
Optimise s3a Listing to be fully asynchronous.,13311956,Resolved,Major,Fixed,17/Jun/20 12:31,25/Aug/20 10:32,3.3.0,"Listing api returns a Remote iterator. Currently it is blocking in the constructor Listing class. Also it is done in batches, so if a batch is exhausted , the next batch will be fetched from s3 

thus causing small pauses on the client side every time a batch is fetched. We can add the following optimisations to fix these:
 * Make the constructor list call to s3 asynchronous.
 * Fetch the next batch asynchronously while the current batch is getting processed."
Tune listStatus() api of s3a.,13302085,Resolved,Major,Fixed,30/Apr/20 12:50,21/Sep/20 16:31,3.2.1,"Similar optimisation which was done for listLocatedStatus() HADOOP-16465  can be done for listStatus() api as well. 

This is going to reduce the number of remote calls in case of directory listing.

 

CC [~stevel@apache.org] [~shwethags]"
Tune S3A listFiles() api.,13302084,Resolved,Major,Fixed,30/Apr/20 12:49,15/Jul/20 10:14,3.2.1,"Similar optimisation which was done for listLocatedStatus() https://issues.apache.org/jira/browse/HADOOP-16465  can done for listFiles() and listStatus() api as well. 

This is going to reduce the number of remote calls in case of directory listing.

 

CC [~stevel@apache.org] [~shwethags]"
[JDK11] Fix Javadoc errors,13312586,Resolved,Major,Fixed,20/Jun/20 06:17,03/Aug/20 01:52,,"{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  17.982 s
[INFO] Finished at: 2020-06-20T01:56:28Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc (default-cli) on project hadoop-hdfs: An error has occurred in Javadoc report generation: 
[ERROR] Exit code: 1 - javadoc: warning - You have specified the HTML version as HTML 4.01 by using the -html4 option.
[ERROR] The default is currently HTML5 and the support for HTML 4.01 will be removed
[ERROR] in a future release. To suppress this warning, please ensure that any HTML constructs
[ERROR] in your comments are valid in HTML5, and remove the -html4 option.
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:25197: error: cannot find symbol
[ERROR]       com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                          ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:25319: error: cannot find symbol
[ERROR]         com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                            ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:26068: error: cannot find symbol
[ERROR]         com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                            ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:26073: error: package com.google.protobuf.GeneratedMessageV3 does not exist
[ERROR]       private PersistToken(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
{noformat}
"
ABFS: Upgrade Store REST API Version to 2019-12-12,13297287,Resolved,Major,Fixed,09/Apr/20 07:28,18/Aug/20 03:37,3.3.0,"Store REST API version on the backend clusters has been upgraded to 2019-12-12. This Jira will align the Driver requests to reflect this latest API version.

 "
findbugs warnings building branch-2.10,13313495,Resolved,Major,Fixed,25/Jun/20 19:16,31/Aug/20 04:54,2.10.0,"precommit build for branch-2.10 generates finbugs warnings in several components.
 This is an umbrella to analyze those warnings and fix/ignore them as necessary.

 
|{color:#FF0000}-1{color}|{color:#FF0000}findbugs{color}|{color:#FF0000}2m 1s{color}|{color:#FF0000}hadoop-common-project/hadoop-common in branch-2.10 has 14 extant findbugs warnings.{color}|
|{color:#FF0000}-1{color}|{color:#FF0000}findbugs{color}|{color:#FF0000}2m 54s{color}|{color:#FF0000}hadoop-hdfs-project/hadoop-hdfs in branch-2.10 has 10 extant findbugs warnings.{color}|
|{color:#FF0000}-1{color}|{color:#FF0000}findbugs{color}|{color:#FF0000}2m 12s{color}|{color:#FF0000}hadoop-hdfs-project/hadoop-hdfs-client in branch-2.10 has 1 extant findbugs warnings.{color}|
|{color:#FF0000}-1{color}|{color:#FF0000}findbugs{color}|{color:#FF0000}1m 35s{color}|{color:#FF0000}hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core in branch-2.10 has 3 extant findbugs warnings.{color}|
|{color:#FF0000}-1{color}|{color:#FF0000}findbugs{color}|{color:#FF0000}1m 50s{color}|{color:#FF0000}hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common in branch-2.10 has 1 extant findbugs warnings.{color}|"
ABFS: Adding metrics to AbfsInputStream (AbfsInputStreamStatistics),13297109,Resolved,Major,Fixed,08/Apr/20 15:27,03/Jul/20 10:47,3.3.0,"Adding metrics to AbfsInputStream (AbfsInputStreamStatistics) can improve the testing and diagnostics of the connector.
Also adding some logging.
"
ABFS: FS initialize fails for incompatible account-agnostic Token Provider setting ,13307480,Resolved,Major,Fixed,26/May/20 16:00,28/May/20 03:58,3.2.1,"When AuthType and Auth token provider configs are set for both generic and account specific config, as below:

// account agnostic

fs.azure.account.auth.type=CUSTOM

fs.azure.account.oauth.provider.type=ClassExtendingCustomTokenProviderAdapter

// account specific

fs.azure.account.auth.type.account_name=OAuth

fs.azure.account.oauth.provider.type.account_name=ClassExtendingAccessTokenProvider

 For account_name, OAuth with provider as ClassExtendingAccessTokenProvider is expected to be in effect.

When the token provider class is being read from the config, account agnostic config setting is read first in the assumption that it can serve as default if account-specific config setting is absent. But this logic leads to failure when AuthType set for account specific and otherwise are different as the Interface implementing the token provider is different for various Auth Types. This leads to a Runtime exception when trying to create the oAuth access token provider.

This Jira is to track the fix for it."
ABFS: Fix idempotency test failures when SharedKey is set as AuthType,13307481,Resolved,Major,Fixed,26/May/20 16:11,24/Jun/20 12:53,3.2.1,"Idempotency related tests added as part of 

https://issues.apache.org/jira/browse/HADOOP-17015

create a test AbfsClient instance. This mock instance wrongly accepts valid sharedKey and oauth token provider instance. This leads to test failures with exceptions:
[ERROR] testRenameRetryFailureAsHTTP404(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRename)  Time elapsed: 9.133 s  <<< ERROR!
 Invalid auth type: SharedKey is being used, expecting OAuth
         at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getTokenProvider(AbfsConfiguration.java:643)
This Jira is to fix these tests."
Update guava to 27.0-jre in hadoop branch-2.10,13313082,Resolved,Major,Won't Do,23/Jun/20 16:14,22/Jul/20 16:37,2.10.0,"com.google.guava:guava should be upgraded to 27.0-jre due to new CVE's found [CVE-2018-10237|https://nvd.nist.gov/vuln/detail/CVE-2018-10237].

 

The upgrade should not affect the version of java used. branch-2.10 still sticks to JDK7"
Replace Guava Supplier with Java8+ Supplier in hdfs,13314334,Resolved,Major,Implemented,30/Jun/20 15:49,17/Jul/20 16:25,,"Replacing Usage of Guava supplier are in Unit tests {{GenereicTestUtils.waitFor()}} in hadoop-hdfs-project subdirectory.
{code:java}
Targets
    Occurrences of 'com.google.common.base.Supplier' in directory hadoop-hdfs-project with mask '*.java'
Found Occurrences  (99 usages found)
    org.apache.hadoop.fs  (1 usage found)
        TestEnhancedByteBufferAccess.java  (1 usage found)
            75 import com.google.common.base.Supplier;
    org.apache.hadoop.fs.viewfs  (1 usage found)
        TestViewFileSystemWithTruncate.java  (1 usage found)
            23 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs  (20 usages found)
        DFSTestUtil.java  (1 usage found)
            79 import com.google.common.base.Supplier;
        MiniDFSCluster.java  (1 usage found)
            78 import com.google.common.base.Supplier;
        TestBalancerBandwidth.java  (1 usage found)
            29 import com.google.common.base.Supplier;
        TestClientProtocolForPipelineRecovery.java  (1 usage found)
            30 import com.google.common.base.Supplier;
        TestDatanodeRegistration.java  (1 usage found)
            44 import com.google.common.base.Supplier;
        TestDataTransferKeepalive.java  (1 usage found)
            47 import com.google.common.base.Supplier;
        TestDeadNodeDetection.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestDecommission.java  (1 usage found)
            41 import com.google.common.base.Supplier;
        TestDFSShell.java  (1 usage found)
            37 import com.google.common.base.Supplier;
        TestEncryptedTransfer.java  (1 usage found)
            35 import com.google.common.base.Supplier;
        TestEncryptionZonesWithKMS.java  (1 usage found)
            22 import com.google.common.base.Supplier;
        TestFileCorruption.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestLeaseRecovery2.java  (1 usage found)
            32 import com.google.common.base.Supplier;
        TestLeaseRecoveryStriped.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestMaintenanceState.java  (1 usage found)
            63 import com.google.common.base.Supplier;
        TestPread.java  (1 usage found)
            61 import com.google.common.base.Supplier;
        TestQuota.java  (1 usage found)
            39 import com.google.common.base.Supplier;
        TestReplaceDatanodeOnFailure.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestReplication.java  (1 usage found)
            27 import com.google.common.base.Supplier;
        TestSafeMode.java  (1 usage found)
            62 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.client.impl  (2 usages found)
        TestBlockReaderLocalMetrics.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestLeaseRenewer.java  (1 usage found)
            20 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.qjournal  (1 usage found)
        MiniJournalCluster.java  (1 usage found)
            31 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.qjournal.client  (1 usage found)
        TestIPCLoggerChannel.java  (1 usage found)
            43 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.qjournal.server  (1 usage found)
        TestJournalNodeSync.java  (1 usage found)
            20 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.blockmanagement  (7 usages found)
        TestBlockManagerSafeMode.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestBlockReportRateLimiting.java  (1 usage found)
            25 import com.google.common.base.Supplier;
        TestNameNodePrunesMissingStorages.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestPendingInvalidateBlock.java  (1 usage found)
            43 import com.google.common.base.Supplier;
        TestPendingReconstruction.java  (1 usage found)
            34 import com.google.common.base.Supplier;
        TestRBWBlockInvalidation.java  (1 usage found)
            49 import com.google.common.base.Supplier;
        TestSlowDiskTracker.java  (1 usage found)
            48 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.datanode  (13 usages found)
        DataNodeTestUtils.java  (1 usage found)
            40 import com.google.common.base.Supplier;
        TestBlockRecovery.java  (1 usage found)
            120 import com.google.common.base.Supplier;
        TestBlockScanner.java  (1 usage found)
            43 import com.google.common.base.Supplier;
        TestBPOfferService.java  (1 usage found)
            92 import com.google.common.base.Supplier;
        TestCorruptMetadataFile.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestDataNodeLifeline.java  (1 usage found)
            74 import com.google.common.base.Supplier;
        TestDataNodeMetrics.java  (1 usage found)
            37 import com.google.common.base.Supplier;
        TestDataNodeMetricsLogger.java  (1 usage found)
            57 import com.google.common.base.Supplier;
        TestDataNodeMultipleRegistrations.java  (1 usage found)
            33 import com.google.common.base.Supplier;
        TestDataNodeMXBean.java  (1 usage found)
            31 import com.google.common.base.Supplier;
        TestDatanodeProtocolRetryPolicy.java  (1 usage found)
            32 import com.google.common.base.Supplier;
        TestDataNodeVolumeFailure.java  (1 usage found)
            94 import com.google.common.base.Supplier;
        TestDiskError.java  (1 usage found)
            31 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.datanode.checker  (1 usage found)
        TestThrottledAsyncChecker.java  (1 usage found)
            21 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (7 usages found)
        TestCacheByPmemMappableBlockLoader.java  (1 usage found)
            65 import com.google.common.base.Supplier;
        TestFsDatasetCache.java  (1 usage found)
            94 import com.google.common.base.Supplier;
        TestFsDatasetImpl.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestFsVolumeList.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestLazyPersistLockedMemory.java  (1 usage found)
            22 import com.google.common.base.Supplier;
        TestPmemCacheRecovery.java  (1 usage found)
            65 import com.google.common.base.Supplier;
        TestSpaceReservation.java  (1 usage found)
            21 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.datanode.metrics  (1 usage found)
        TestDataNodeOutlierDetectionViaMetrics.java  (1 usage found)
            21 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.datanode.web.webhdfs  (1 usage found)
        TestDataNodeUGIProvider.java  (1 usage found)
            54 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.diskbalancer  (2 usages found)
        TestDiskBalancer.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestDiskBalancerWithMockMover.java  (1 usage found)
            23 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.federation  (1 usage found)
        FederationTestUtils.java  (1 usage found)
            95 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.federation.router  (4 usages found)
        TestRouterAdminCLI.java  (1 usage found)
            67 import com.google.common.base.Supplier;
        TestRouterQuota.java  (1 usage found)
            77 import com.google.common.base.Supplier;
        TestRouterRpc.java  (1 usage found)
            131 import com.google.common.base.Supplier;
        TestRouterRPCClientRetries.java  (1 usage found)
            57 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.mover  (1 usage found)
        TestMover.java  (1 usage found)
            98 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.namenode  (17 usages found)
        TestAddStripedBlockInFBR.java  (1 usage found)
            43 import com.google.common.base.Supplier;
        TestBackupNode.java  (1 usage found)
            59 import com.google.common.base.Supplier;
        TestCacheDirectives.java  (1 usage found)
            99 import com.google.common.base.Supplier;
        TestCheckpoint.java  (1 usage found)
            98 import com.google.common.base.Supplier;
        TestDeadDatanode.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestEditLogAutoroll.java  (1 usage found)
            49 import com.google.common.base.Supplier;
        TestEditLogRace.java  (1 usage found)
            41 import com.google.common.base.Supplier;
        TestFsck.java  (1 usage found)
            62 import com.google.common.base.Supplier;
        TestFSNamesystemLock.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestMetaSave.java  (1 usage found)
            33 import com.google.common.base.Supplier;
        TestNameNodeMetadataConsistency.java  (1 usage found)
            33 import com.google.common.base.Supplier;
        TestNameNodeMetricsLogger.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestNameNodeMXBean.java  (1 usage found)
            21 import com.google.common.base.Supplier;
        TestNameNodeStatusMXBean.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestPersistentStoragePolicySatisfier.java  (1 usage found)
            40 import com.google.common.base.Supplier;
        TestReencryption.java  (1 usage found)
            34 import com.google.common.base.Supplier;
        TestUpgradeDomainBlockPlacementPolicy.java  (1 usage found)
            52 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.namenode.ha  (11 usages found)
        HATestUtil.java  (1 usage found)
            64 import com.google.common.base.Supplier;
        TestBootstrapStandby.java  (1 usage found)
            30 import com.google.common.base.Supplier;
        TestDNFencing.java  (1 usage found)
            32 import com.google.common.base.Supplier;
        TestDNFencingWithReplication.java  (1 usage found)
            39 import com.google.common.base.Supplier;
        TestEditLogTailer.java  (1 usage found)
            65 import com.google.common.base.Supplier;
        TestHASafeMode.java  (1 usage found)
            75 import com.google.common.base.Supplier;
        TestPendingCorruptDnMessages.java  (1 usage found)
            44 import com.google.common.base.Supplier;
        TestPipelinesFailover.java  (1 usage found)
            65 import com.google.common.base.Supplier;
        TestStandbyCheckpoints.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        TestStandbyInProgressTail.java  (1 usage found)
            52 import com.google.common.base.Supplier;
        TestStandbyIsHot.java  (1 usage found)
            46 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.namenode.snapshot  (1 usage found)
        TestRandomOpsWithSnapshots.java  (1 usage found)
            20 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.namenode.sps  (1 usage found)
        TestStoragePolicySatisfierWithStripedFile.java  (1 usage found)
            55 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.server.sps  (1 usage found)
        TestExternalStoragePolicySatisfier.java  (1 usage found)
            101 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.shortcircuit  (1 usage found)
        TestShortCircuitCache.java  (1 usage found)
            93 import com.google.common.base.Supplier;
    org.apache.hadoop.hdfs.tools  (2 usages found)
        TestDFSAdmin.java  (1 usage found)
            28 import com.google.common.base.Supplier;
        TestDFSZKFailoverController.java  (1 usage found)
            58 import com.google.common.base.Supplier;


{code}"
Replace Guava Supplier with Java8+ Supplier in MAPREDUCE,13314330,Resolved,Major,Implemented,30/Jun/20 15:39,17/Jul/20 16:24,,"Replacing Usage of Guava supplier are in Unit tests {{GenereicTestUtils.waitFor()}} in hadoop-mapreduce-project subdirectory.
{code:java}
Targets
hadoop-mapreduce-project with mask '*.java'
Found Occurrences  (8 usages found)
    org.apache.hadoop.mapred  (2 usages found)
        TestTaskAttemptListenerImpl.java  (1 usage found)
            20 import com.google.common.base.Supplier;
        UtilsForTests.java  (1 usage found)
            64 import com.google.common.base.Supplier;
    org.apache.hadoop.mapreduce.v2.app  (4 usages found)
        TestFetchFailure.java  (1 usage found)
            29 import com.google.common.base.Supplier;
        TestMRApp.java  (1 usage found)
            31 import com.google.common.base.Supplier;
        TestRecovery.java  (1 usage found)
            31 import com.google.common.base.Supplier;
        TestTaskHeartbeatHandler.java  (1 usage found)
            28 import com.google.common.base.Supplier;
    org.apache.hadoop.mapreduce.v2.app.rm  (1 usage found)
        TestRMContainerAllocator.java  (1 usage found)
            156 import com.google.common.base.Supplier;
    org.apache.hadoop.mapreduce.v2.hs  (1 usage found)
        TestJHSDelegationTokenSecretManager.java  (1 usage found)
            30 import com.google.common.base.Supplier;

{code}"
WASB : NativeAzureFsOutputStream#close() throwing IllegalArgumentException,13299344,Resolved,Major,Fixed,18/Apr/20 08:03,14/Jul/20 14:32,,"During HFile create, at the end when called close() on the OutputStream, there is some pending data to get flushed. When this flush happens, an Exception is thrown back from Storage. The Azure-storage SDK layer will throw back IOE. (Even if it is a StorageException thrown from the Storage, the SDK converts it to IOE.) But at HBase, we end up getting IllegalArgumentException which causes the RS to get aborted. If we get back IOE, the flush will get retried instead of aborting RS.
The reason is this
NativeAzureFsOutputStream uses Azure-storage SDK's BlobOutputStreamInternal. But the BlobOutputStreamInternal is wrapped within a SyncableDataOutputStream which is a FilterOutputStream. During the close op, NativeAzureFsOutputStream calls close on SyncableDataOutputStream and it uses below method from FilterOutputStream
{code}
public void close() throws IOException {
  try (OutputStream ostream = out) {
              flush();
  }
}
{code}
Here the flush call caused an IOE to be thrown to here. The finally will issue close call on ostream (Which is an instance of BlobOutputStreamInternal)
When BlobOutputStreamInternal#close() is been called, if there was any exception already occured on that Stream, it will throw back the same Exception
{code}
public synchronized void close() throws IOException {
  try {
              // if the user has already closed the stream, this will throw a STREAM_CLOSED exception
              // if an exception was thrown by any thread in the threadExecutor, realize it now
              this.checkStreamState();
              ...
}
private void checkStreamState() throws IOException {
  if (this.lastError != null) {
              throw this.lastError;
  }
}
{code}
So here both try and finally block getting Exceptions and Java uses Throwable#addSuppressed() 
Within this method if both Exceptions are same objects, it throws back IllegalArgumentException
{code}
public final synchronized void addSuppressed(Throwable exception) {
              if (exception == this)
                             throw new IllegalArgumentException(SELF_SUPPRESSION_MESSAGE, exception);
              ....
}
{code}
"
No Log compression and retention in Hadoop,13299746,Patch Available,Major,,20/Apr/20 16:15,,,"Hadoop logging lacks several important features. Logs generated end up eating disk space
We need an implementation that satisfies the following three features:  1) time-based rolling, 2) retention and 3) compression.

For example KMS logs have no retention or compression. 
{code:bash}
-rw-r--r-- 1 hkms users 704M Mar 20 23:59 kms.log.2020-03-20
-rw-r--r-- 1 hkms users 731M Mar 21 23:59 kms.log.2020-03-21
-rw-r--r-- 1 hkms users 750M Mar 22 23:59 kms.log.2020-03-22
-rw-r--r-- 1 hkms users 757M Mar 23 23:59 kms.log.2020-03-23
-rw-r--r-- 1 hkms users 805M Mar 24 23:59 kms.log.2020-03-24
-rw-r--r-- 1 hkms users 858M Mar 25 23:59 kms.log.2020-03-25
-rw-r--r-- 1 hkms users 875M Mar 26 23:59 kms.log.2020-03-26
-rw-r--r-- 1 hkms users 754M Mar 27 23:59 kms.log.2020-03-27
{code}
"
ABFS: GetAccessToken unrecoverable failures are being retried,13313371,Resolved,Major,Duplicate,25/Jun/20 03:56,13/Jul/20 04:30,,"When there is an invalid config set, call to fetch token fails with exception:

throw new UnexpectedResponseException(httpResponseCode,
 requestId,
 operation
 + "" Unexpected response.""
 + "" Check configuration, URLs and proxy settings.""
 + "" proxies="" + proxies,
 authEndpoint,
 responseContentType,
 responseBody);
 }

Issue here is that UnexpectedResponseException is not recognized as irrecoverable state and ends up being retried. This needs to be fixed."
javadoc failing in the yetus report with the latest trunk,13313164,Resolved,Major,Duplicate,24/Jun/20 04:02,13/Jul/20 06:00,,"javadoc is failing in the latest yetus report on  trunk. below is a report from an empty PR where it is failing.

 

💔 *-1 overall*
||Vote||Subsystem||Runtime||Comment||
|+0 🆗|reexec|26m 14s|Docker mode activated.|
| | |_ Prechecks _| |
|+1 💚|dupname|0m 0s|No case conflicting files found.|
|+1 💚|[@author|https://github.com/author]|0m 0s|The patch does not contain any [@author|https://github.com/author] tags.|
|-1 ❌|test4tests|0m 0s|The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.|
| | |_ trunk Compile Tests _| |
|+1 💚|mvninstall|23m 14s|trunk passed|
|+1 💚|compile|0m 46s|trunk passed with JDK Ubuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04|
|+1 💚|compile|0m 30s|trunk passed with JDK Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|+1 💚|checkstyle|0m 24s|trunk passed|
|+1 💚|mvnsite|0m 35s|trunk passed|
|+1 💚|shadedclient|16m 53s|branch has no errors when building and testing our client artifacts.|
|-1 ❌|javadoc|0m 25s|hadoop-azure in trunk failed with JDK Ubuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04.|
|+1 💚|javadoc|0m 23s|trunk passed with JDK Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|+0 🆗|spotbugs|0m 53s|Used deprecated FindBugs config; considering switching to SpotBugs.|
|+1 💚|findbugs|0m 51s|trunk passed|
| | |_ Patch Compile Tests _| |
|+1 💚|mvninstall|0m 27s|the patch passed|
|+1 💚|compile|0m 27s|the patch passed with JDK Ubuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04|
|+1 💚|javac|0m 27s|the patch passed|
|+1 💚|compile|0m 22s|the patch passed with JDK Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|+1 💚|javac|0m 22s|the patch passed|
|+1 💚|checkstyle|0m 15s|the patch passed|
|+1 💚|mvnsite|0m 24s|the patch passed|
|+1 💚|whitespace|0m 0s|The patch has no whitespace issues.|
|+1 💚|shadedclient|15m 29s|patch has no errors when building and testing our client artifacts.|
|-1 ❌|javadoc|0m 22s|hadoop-azure in the patch failed with JDK Ubuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04.|
|+1 💚|javadoc|0m 20s|the patch passed with JDK Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|+1 💚|findbugs|0m 53s|the patch passed|
| | |_ Other Tests _| |
|+1 💚|unit|1m 19s|hadoop-azure in the patch passed.|
|+1 💚|asflicense|0m 28s|The patch does not generate ASF License warnings.|
| | |92m 45s| |

||Subsystem||Report/Notes||
|Docker|ClientAPI=1.40 ServerAPI=1.40 base: [https://builds.apache.org/job/hadoop-multibranch/job/PR-2091/1/artifact/out/Dockerfile]|
|GITHUB PR|[#2091|https://github.com/apache/hadoop/pull/2091]|
|Optional Tests|dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient findbugs checkstyle|
|uname|Linux ddd84b65f91e 4.15.0-101-generic [#102|https://github.com/apache/hadoop/pull/102]-Ubuntu SMP Mon May 11 10:07:26 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux|
|Build tool|maven|
|Personality|personality/hadoop.sh|
|git revision|trunk / [{{7c02d18}}|https://github.com/apache/hadoop/commit/7c02d1889bbeabc73c95a4c83f0cd204365ff410]|
|Default Java|Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|Multi-JDK versions|/usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_252-8u252-b09-1~18.04-b09|
|javadoc|[https://builds.apache.org/job/hadoop-multibranch/job/PR-2091/1/artifact/out/branch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04.txt]|
|javadoc|[https://builds.apache.org/job/hadoop-multibranch/job/PR-2091/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.7+10-post-Ubuntu-2ubuntu218.04.txt]|
|Test Results|[https://builds.apache.org/job/hadoop-multibranch/job/PR-2091/1/testReport/]|
|Max. process+thread count|308 (vs. ulimit of 5500)|
|modules|C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure|
|Console output|[https://builds.apache.org/job/hadoop-multibranch/job/PR-2091/1/console]|
|versions|git=2.17.1 maven=3.6.0 findbugs=3.1.0-RC1|
|Powered by|Apache Yetus 0.12.0 [https://yetus.apache.org|https://yetus.apache.org/]|

This message was automatically generated."
ABFS: Reuse DSAS fetched in ABFS Input and Output stream,13299376,Resolved,Major,Duplicate,18/Apr/20 15:31,13/Jul/20 04:36,3.2.1,"This Jira will track the update where ABFS input and output streams can re-use D-SAS token fetched. If the SAS is within 1 minute of expiry, ABFS will request a new SAS.  When the stream is closed the SAS will be released. "
Improvement to the AccessControlException thrown by Azure abfs driver,13311995,Resolved,Major,Not A Bug,17/Jun/20 16:10,13/Jul/20 04:00,,"Currently when an AccessControlException happens in Abfs driver call it prints the entire stack trace. To be consistent with the HDFS way of showing the Permission denied, could we modify this in ABFSClient?
 
e.g:
>$ hdfs dfs -ls /hbase/mobdir
ls: Permission denied: user=user1, access=READ_EXECUTE, inode=""/hbase/mobdir"":hbase:hbase:drwx------"
ABFS: Fix the parsing errors in ABFS Driver with creation Time (being returned in ListPath),13313171,Resolved,Major,Fixed,24/Jun/20 06:36,03/Jul/20 18:03,3.3.0,"I am seeing errors while running ABFS Driver against stg75 build in canary. This is related to parsing errors as we receive creationTIme in the ListPath API. Here are the errors:

RestVersion: 2020-02-10

 mvn -T 1C -Dparallel-tests=abfs -Dscale -DtestsThreadCount=8 clean verify -Dit.test=ITestAzureBlobFileSystemRenameUnicode

[ERROR] testRenameFileUsingUnicode[0](org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRenameUnicode)  Time elapsed: 852.083 s  <<< ERROR!

Status code: -1 error code: null error message: InvalidAbfsRestOperationExceptionorg.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field ""creationTime"" (Class org.apache.hadoop.

.azurebfs.contracts.services.ListResultEntrySchema), not marked as ignorable

 at [Source: [sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@49e30796|mailto:sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@49e30796];%20line:%201,%20column:%2048] (through reference chain: org.apache.hadoop.fs.azurebfs.contracts.services.ListResultSchema[""pat

""]->org.apache.hadoop.fs.azurebfs.contracts.services.ListResultEntrySchema[""creationTime""])

        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:273)

        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:188)

        at org.apache.hadoop.fs.azurebfs.services.AbfsClient.listPath(AbfsClient.java:237)

        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:773)

        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.listStatus(AzureBlobFileSystemStore.java:735)

        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.listStatus(AzureBlobFileSystem.java:373)

        at org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRenameUnicode.testRenameFileUsingUnicode(ITestAzureBlobFileSystemRenameUnicode.java:92)

        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.base/java.lang.reflect.Method.invoke(Method.java:566)

        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)

        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)

        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)

        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)

        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)

        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)

        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)

        at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field ""creationTime"" (Class org.apache.hadoop.fs.azurebfs.contracts.services.ListResultEntrySchema), not marked as i

orable

 at [Source: [sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@49e30796|mailto:sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@49e30796];%20line:%201,%20column:%2048] (through reference chain: org.apache.hadoop.fs.azurebfs.contracts.services.ListResultSchema[""pat

""]->org.apache.hadoop.fs.azurebfs.contracts.services.ListResultEntrySchema[""creationTime""])

        at org.codehaus.jackson.map.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:53)

        at org.codehaus.jackson.map.deser.StdDeserializationContext.unknownFieldException(StdDeserializationContext.java:267)

        at org.codehaus.jackson.map.deser.std.StdDeserializer.reportUnknownProperty(StdDeserializer.java:673)

        at org.codehaus.jackson.map.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:659)

        at org.codehaus.jackson.map.deser.BeanDeserializer.handleUnknownProperty(BeanDeserializer.java:1365)

        at org.codehaus.jackson.map.deser.BeanDeserializer._handleUnknown(BeanDeserializer.java:725)

        at org.codehaus.jackson.map.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:703)

        at org.codehaus.jackson.map.deser.BeanDeserializer.deserialize(BeanDeserializer.java:580)

        at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217)

        at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:194)

        at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:30)

        at org.codehaus.jackson.map.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:299)

        at org.codehaus.jackson.map.deser.SettableBeanProperty$FieldProperty.deserializeAndSet(SettableBeanProperty.java:579)

        at org.codehaus.jackson.map.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:697)

        at org.codehaus.jackson.map.deser.BeanDeserializer.deserialize(BeanDeserializer.java:580)

        at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732)

        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1909)

        at org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation.parseListFilesResponse(AbfsHttpOperation.java:501)

        at org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation.processResponse(AbfsHttpOperation.java:366)

        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:255)"
vulnerabilities reported in jackson and jackson-databind in branch-2.10,13313488,Resolved,Major,Fixed,25/Jun/20 17:24,30/Jun/20 03:21,2.10.0,"There are known vulnerabilities in the com.fasterxml.jackson.core:jackson-databind package [,2.9.10.5).

[List of vulnerabilities|https://snyk.io/vuln/maven:com.fasterxml.jackson.core%3Ajackson-databind].

Upgrading jackson and jackson-databind to 2.10 should get rid of those vulnerabilities."
Add EC flag to stat commands,13313228,Resolved,Major,Not A Problem,24/Jun/20 10:44,25/Jun/20 19:44,,"We currently do not have a brief way to judge an ec file.  {{hdfs fsck}}  can do but shows too much information. Neither {{du}} nor {{ls}} can accurately judge the ec file. 

So I added ec flag to stat cli.

old result: 
{code:java}
$ hadoop fs -stat ""%F"" /user/ec/ec.txt
regular file
$ hadoop fs -stat ""%F"" /user/rep/rep.txt 
regular file
{code}
new result:
{code:java}
$ hadoop fs -stat ""%F"" /user/ec/ec.txt 
erasure coding file
$ hadoop fs -stat ""%F"" /user/rep/rep.txt 
replica file
{code}
 "
Add getClusterRoot and getClusterRoots methods to FileSystem and ViewFilesystem,13311856,Open,Major,,17/Jun/20 02:26,,,"In a federated setting (HDFS federation, federation across multiple buckets on S3, multiple containers across Azure storage), certain system tools/pipelines require the ability to map paths to the clusters/accounts.

Consider the example of GDPR compliance/retention jobs that need to go over various datasets, ingested over a period of T days and remove/quarantine datasets that are not properly annotated/have reached their retention period. Such jobs can rely on renames to a global trash/quarantine directory to accomplish their task. However, in a federated setting, efficient, atomic renames (as those within a single HDFS cluster) are not supported across the different clusters/shards in federation. As a result, such jobs will need to leverage a trash/quarantine directory per cluster/shard. Further, they would need to map from a particular path to the cluster/shard that contains this path.

To address such cases, this JIRA proposes to get add two new methods to {{FileSystem}}: {{getClusterRoot}} and {{getClusterRoots()}}."
Increase number of IPC calls and lock creating contention for low latency queries,13305801,Patch Available,Major,,19/May/20 03:51,,,"After HADOOP-16126 and HADOOP-16127, we noticed lock issues even in local FS, caused by IPC

""Task-Executor-11"" #273 daemon prio=5 os_prio=0 tid=0x00007fe204664800 nid=0x2343d2 waiting on condition [0x00007fe1fcfda000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.ipc.Client.stop(Client.java:1329)
	at org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:113)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.close(ProtobufRpcEngine.java:302)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:677)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.close(ClientNamenodeProtocolTranslatorPB.java:304)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:672)
	at 
[org.apache.hadoop.io|http://org.apache.hadoop.io/]
.retry.DefaultFailoverProxyProvider.close(DefaultFailoverProxyProvider.java:57)
	at 
[org.apache.hadoop.io|http://org.apache.hadoop.io/]
.retry.RetryInvocationHandler$ProxyDescriptor.close(RetryInvocationHandler.java:234)
	at 
[org.apache.hadoop.io|http://org.apache.hadoop.io/]
.retry.RetryInvocationHandler.close(RetryInvocationHandler.java:444)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:677)
	at org.apache.hadoop.hdfs.DFSClient.closeConnectionToNamenode(DFSClient.java:592)
	at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:633)
	- locked <0x00007fed071063a0> (a org.apache.hadoop.hdfs.DFSClient)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1358)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3463)
	- locked <0x00007fe63d000000> (a org.apache.hadoop.fs.FileSystem$Cache)
	at org.apache.hadoop.fs.FileSystem.closeAllForUGI(FileSystem.java:576)
	at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:299)
	at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:93)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)""New I/O worker #44"" #80 prio=5 os_prio=0 tid=0x00007fede2a03000 nid=0x233f2b waiting for monitor entry [0x00007fe20cd3d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3345)
	- waiting to lock <0x00007fe63d000000> (a org.apache.hadoop.fs.FileSystem$Cache)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:435)
	at org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.<init>(TezSpillRecord.java:65)
	at org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.<init>(TezSpillRecord.java:58)
	at org.apache.hadoop.hive.llap.shufflehandler.IndexCache.readIndexFileToCache(IndexCache.java:121)
	at org.apache.hadoop.hive.llap.shufflehandler.IndexCache.getIndexInformation(IndexCache.java:70)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.getMapOutputInfo(ShuffleHandler.java:887)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.populateHeaders(ShuffleHandler.java:908)
	at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:805)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:142)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
 

Reverting the two jiras fixed the issue. "
"The default password(none) in JavaKeyStoreProvider, is no-longer useful. ",13310148,Resolved,Major,Invalid,08/Jun/20 14:24,09/Jun/20 12:33,3.2.1,"Since, the java keytool does not allow us to create a keystore with password length less than 6 characters(i.e. none), we should consider updating the password to a 6 char length (e.g. nopass).

{code}
$ keytool -genkeypair -storetype jceks -keyalg RSA -alias kms -keystore `pwd`/keystore4 -storepass none
keytool error: java.lang.Exception: Keystore password must be at least 6 characters
$ java -version
openjdk version ""1.8.0_252""
OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1ubuntu1-b09)
OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)
{code}
"
LocalFs rename is broken.,13310234,Resolved,Major,Duplicate,08/Jun/20 22:09,08/Jun/20 22:15,,"In LocalFs and any other FileContext based on ChecksumFs, the {{renameInternal(src, dest, overwrite)}} method is broken since it is not implemented. The method in FilterFs will be invoked, which is checksum unaware. This can result in file leaks.
"
Drop MRv1 binary compatibility in 4.0.0,13309365,Open,Major,,04/Jun/20 07:04,,,"A code comment suggests making the setJobConf method deprecated along with mapred package HADOOP-1230. HADOOP-1230 has been closed a long time ago, but the method is still not annotated as deprecated.
{code:java}
 /**
   * This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.
   */ {code}
Comment location: [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java#L88]

From the previous discussion, it seems that this method is still required if we ensure binary compatibility with MRv1. 
 https://issues.apache.org/jira/browse/HADOOP-17047?focusedCommentId=17111702&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17111702

Mingliang Liu suggested to Drop MRv1 binary compatibility in 4.0.0
 https://issues.apache.org/jira/browse/HADOOP-17047?focusedCommentId=17112442&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17112442"
AuthenticatedURL.Token does not store the token if there are redirects.,13309686,Open,Major,,05/Jun/20 09:55,,,"When a http call which requires SPNEGO auth is redirected to another server which also required SPNEGO auth. The token is overwritten in second call which invalidates the token in the first call. This causes a performance impact in all redirect cases and the authentication to fail in some cases.
{code:java}
    AuthenticatedURL.Token token = new AuthenticatedURL.Token();
    // Is this right, can you have spnego over http, which would mean we'll have sslFactory for all cases.
    AuthenticatedURL authenticatedURL = new AuthenticatedURL(null, sslFactory);
    conn = authenticatedURL.openConnection(url, token);
{code}
The above code results in 2 OPTION calls to the server, the first call gets the token for first server and stores it into cookieHandler in token and then then the client redirects to server2, which overwrites the token first call made. When getInputStream is called on the conn, the jdk does a SPNEGO auth again since the tokens sent to the servers are not valid anymore. This mean the KerberosAuthenticator authenticate method is skipped for the second call and works only with the jdk call. I believe the fix should be to make the cookie handler multi-domain aware and also add APIs to extract token for a given domain.

 

I've attached the logs where I saw this behavior. There is another issue which can be seen in the log, where the token type is sent as alt-kerberos by the server, which is ignore by the handler and make a call again.

 

Effectively, we are making 6 http calls, whereas with token reuse it should have been 2 calls, except for the first call which will be 4 calls."
Adding Common Counters in ABFS,13301182,Resolved,Major,Fixed,27/Apr/20 07:36,02/Jun/20 17:35,3.3.0,"Common Counters to be added to ABFS:
|OP_CREATE|
|OP_OPEN|
|OP_GET_FILE_STATUS|
|OP_APPEND|
|OP_CREATE_NON_RECURSIVE|
|OP_DELETE|
|OP_EXISTS|
|OP_GET_DELEGATION_TOKEN|
|OP_LIST_STATUS|
|OP_MKDIRS|
|OP_RENAME|

|DIRECTORIES_CREATED|
|DIRECTORIES_DELETED|
|FILES_CREATED|
|FILES_DELETED|
|ERROR_IGNORED|

 propose:
 * Have an enum class to define all the counters.
 * Have an Instrumentation class for making a MetricRegistry and adding all the counters.
 * Incrementing the counters in AzureBlobFileSystem.
 * Integration and Unit tests to validate the counters."
NetUtils.connect() throws unchecked exception (UnresolvedAddressException) causing clients to abort,13307345,Resolved,Major,Fixed,26/May/20 06:21,01/Jun/20 18:19,2.10.0,"Hadoop components are increasingly being deployed on VMs and containers. One aspect of this environment is that DNS is dynamic. Hostname records get modified (or deleted/recreated) as a container in Kubernetes (or even VM) is being created/recreated. In such dynamic environments, the initial DNS resolution request might return resolution failure briefly as DNS client doesn't always get the latest records. This has been observed in Kubernetes in particular. In such cases NetUtils.connect() appears to throw java.nio.channels.UnresolvedAddressException.  In much of Hadoop code (like DFSInputStream and DFSOutputStream), the code is designed to retry IOException. However, since UnresolvedAddressException is not child of IOException, no retry happens and the code aborts immediately. It is much better if NetUtils.connect() throws java.net.UnknownHostException as that is derived from IOException and the code will treat this as a retry-able error."
javax.activation-api and jakarta.activation-api define overlapping classes,13305811,Resolved,Major,Fixed,19/May/20 04:52,22/May/20 02:22,,"There are some warnings in hadoop-client-runtime module.
{noformat}
[WARNING] javax.activation-api-1.2.0.jar, jakarta.activation-api-1.2.1.jar define 31 overlapping classes: 
[WARNING]   - javax.activation.CommandInfo$Beans$1
[WARNING]   - javax.activation.ObjectDataContentHandler
[WARNING]   - javax.activation.DataContentHandlerFactory
[WARNING]   - javax.activation.DataContentHandler
[WARNING]   - javax.activation.CommandObject
[WARNING]   - javax.activation.SecuritySupport$2
[WARNING]   - javax.activation.FileTypeMap
[WARNING]   - javax.activation.CommandInfo
[WARNING]   - javax.activation.MailcapCommandMap
[WARNING]   - javax.activation.DataHandler$1
[WARNING]   - 21 more...
{noformat}"
UserGroupInformation#getCurrentUser is always unix user in linux env?,13304573,Open,Major,,13/May/20 08:48,,2.7.3,"After UserGroupInformation#loginUserFromKeytab(...) in my program, invoke UserGroupInformation#getCurrentuser() method and return a unix user in linux OS, but return a loginUser in my Mac OS. Why is this so?"
Change scope of InternalDirOfViewFs and InodeTree to make ViewFileSystem extendable outside common package,13304490,Open,Major,,13/May/20 00:56,,,"In a use case where ViewFileSystem to be extended outside of hadoop-common package, the scope of InternalDirOfViewFs and InodeTree needs to be public. 

 

cc [~virajith] [~shv]"
Fix failure of TestSnappyCompressorDecompressor on CentOS 8,13303572,Resolved,Major,Duplicate,08/May/20 07:54,11/May/20 07:55,,testSnappyCompressDecompress testSnappyCompressDecompressInMultiThreads reproducibly fails on CentOS 8. These tests has no issue on CentOS 7.
Add tests for reading fair call queue capacity weight configs,13302490,Resolved,Major,Fixed,03/May/20 06:31,08/May/20 00:06,,"This is to add more tests for changes introduced in https://issues.apache.org/jira/browse/HADOOP-17010

specifically adding tests for more comprehensive flow by testing CallQueueManager reading conf and constructs the right FairCallQueue"
Add queue capacity weights support in FairCallQueue,13300472,Resolved,Major,Fixed,23/Apr/20 07:40,28/Apr/20 23:15,,"Right now in FairCallQueue all subqueues share the same capacity by evenly distributing total capacity. This requested feature is to make subqueues able to have different queue capacity where more important queues can have more capacity, thus less queue overflow and client backoffs."
Declare ProtobufHelper a public API,13301696,Resolved,Major,Not A Problem,29/Apr/20 02:40,30/Apr/20 20:21,3.3.0,"HADOOP-16621 removed two public API methods:

1. o.a.h.security.token.Token(TokenProto tokenPB) --> replaced by o.a.h.security.ipc.ProtobufHelper.tokenFromProto()
2. o.a.h.security.token.Token.toTokenProto() --> replaced by o.a.h.security.ipc.ProtobufHelper.protoFromToken()

Protobuf is declared private. Should we make it public now?"
hadoop-cos fails to build,13300339,Resolved,Major,Fixed,22/Apr/20 18:59,26/Apr/20 07:28,3.3.0,"Found the following compilation error in a PR precommit. The failure doesn't seem related to the PR itself. Cant' reproduce locally though.

https://builds.apache.org/job/hadoop-multibranch/job/PR-1972/1/artifact/out/patch-compile-root.txt

{noformat}
[INFO] Apache Hadoop Tencent COS Support .................. FAILURE [  0.074 s]
[INFO] Apache Hadoop Cloud Storage ........................ SKIPPED
[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 17:31 min
[INFO] Finished at: 2020-04-22T07:37:51+00:00
[INFO] Final Memory: 192M/1714M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.0.2:copy-dependencies (package) on project hadoop-cos: Artifact has not been packaged yet. When used on reactor artifact, copy should be executed after packaging: see MDEP-187. -> [Help 1]
{noformat}"
Resolve hadoop-cos dependency conflict,13296545,Resolved,Major,Fixed,06/Apr/20 15:35,20/Apr/20 10:55,,"There are some dependency conflicts between the Hadoop-common and Hadoop-cos. For example, joda time lib, HTTP client lib and etc."
Remove Ratis dependencies from Hadoop branch-3.2,13300817,Open,Major,,24/Apr/20 14:18,,3.2.2,"These are not used in the core Hadoop codebase. Remove them to avoid confusion.

{code}
    <!-- Apache Ratis version -->
    <ratis.version>0.3.0-eca3531-SNAPSHOT</ratis.version>

      <dependency>
        <groupId>org.apache.ratis</groupId>
        <artifactId>ratis-proto-shaded</artifactId>
        <version>${ratis.version}</version>
      </dependency>
      <dependency>
        <artifactId>ratis-common</artifactId>
        <groupId>org.apache.ratis</groupId>
        <version>${ratis.version}</version>
      </dependency>
      <dependency>
        <artifactId>ratis-client</artifactId>
        <groupId>org.apache.ratis</groupId>
        <version>${ratis.version}</version>
      </dependency>
      <dependency>
        <artifactId>ratis-server</artifactId>
        <groupId>org.apache.ratis</groupId>
        <version>${ratis.version}</version>
      </dependency>
      <dependency>
        <artifactId>ratis-netty</artifactId>
        <groupId>org.apache.ratis</groupId>
        <version>${ratis.version}</version>
      </dependency>
      <dependency>
        <artifactId>ratis-grpc</artifactId>
        <groupId>org.apache.ratis</groupId>
        <version>${ratis.version}</version>
      </dependency>
{code}

These are removed from trunk already."
The suffix name of the unified compression class,13299449,Resolved,Major,Fixed,19/Apr/20 08:04,22/Apr/20 20:27,3.2.1,"The suffix name of the unified compression class,I think the suffix name in the compression class should be extracted into a constant class, which is helpful for developers to understand the structure of the compression class as a whole.
{quote}public static final String OPT_EXTENSION =
 ""io.compress.passthrough.extension"";

/**
 * This default extension is here so that if no extension has been defined,
 * some value is still returned: \{@value}..
 */
public static final String DEFAULT_EXTENSION = "".passthrough"";

private Configuration conf;

private String extension = DEFAULT_EXTENSION;

public PassthroughCodec() {
}
{quote}

The above code, the use of constants is a bit messy."
"in javaApi, UGI params should be overidden through FileSystem conf",13297923,Resolved,Major,Won't Fix,13/Apr/20 02:33,17/Apr/20 14:47,2.7.2,"org.apache.hadoop.security.UserGroupInformation#ensureInitialized,will always get the configure from the configuration files. Like below：
{code:java}
private static void ensureInitialized() {
  if (conf == null) {
    synchronized(UserGroupInformation.class) {
      if (conf == null) { // someone might have beat us
        initialize(new Configuration(), false);
      }
    }
  }
}{code}
So that, if FileSystem is created through FileSystem#get or FileSystem#newInstance with conf, the conf values different from the configuration files will not take effect in UserGroupInformation.  E.g:
{code:java}
Configuration conf = new Configuration();
conf.set(""k1"",""v1"");
conf.set(""k2"",""v2"");
FileSystem fs = FileSystem.get(uri, conf);{code}
""k1"" or ""k2"" will not work in UserGroupInformation."
s3a to not need wildfly on the classpath,13298360,Resolved,Major,Fixed,14/Apr/20 18:16,20/Apr/20 13:46,3.3.0,"see : https://github.com/apache/hadoop/pull/1948 and HADOOP-16855

* remove a hard dependency on wildfly.jar being on the classpath for S3; it's used if present, but handled if not
* even if openssl is requested
* and NPEs are caught and swallowed in case wildfly 1.0.4.Final ever gets on the classpath again"
Provide GroupIdentityProvider and MappedUserIdentityProvider for FairCallQueue namenode RPC Queue throttling for grouping user requests,13299269,Open,Major,,18/Apr/20 03:13,,3.0.3,"Currently, in a multi-tenant cluster FairCallQueue Identity is limited to UserIdentityProvider. Tenants tend to get passed burst RPC loads by using different service ID. This Jira requests that GroupIdentityProvider and MappedUserIdentityProvider be implemented to allow better tenant experience for clusters with multiple tenants."
Support the CRC64 checksum,13298099,Open,Major,,13/Apr/20 18:29,,,"Support the CRC64 checksum in Hadoop-cos, and implement the 'getFileChecksum' method."
Optimizing the retry mechanism when a server error occurs,13297758,In Progress,Major,,11/Apr/20 09:36,,,
Making `getBoolean` log warning message for unrecognized value,13297115,Resolved,Major,Fixed,08/Apr/20 16:02,09/Apr/20 18:04,,"*Problem:*

In `Configuration.java`, the `getBoolean` can accept any valueString and return the default value for any string except “true” or “false” (ignoring case):
{code:java}
if (StringUtils.equalsIgnoreCase(""true"", valueString))
  return true;
else if (StringUtils.equalsIgnoreCase(""false"", valueString))
  return false;
else return defaultValue;{code}
If the user misspells some boolean configuration value, for example, “true” to “ture”, then getBoolean will directly return the default value without logging any warning message. If the default value is “false”, then Hadoop is actually using a totally different value (“false”) compared to the user’s expectation (“true”) and the user even doesn’t know it.

This can lead to serious issues, especially regarding security features.

Other projects such as Alluxio are doing more rigorous and explicit check.
[https://github.com/xlab-uiuc/ctest-alluxio/blob/master/core/common/src/main/java/alluxio/conf/InstancedConfiguration.java#L366]
in which the getBoolean method will fail immediately if the value is invalid.

 

*Solution:*

We can log one warning message before getBoolean return the default value for unrecognized value:
{code:java}
if (StringUtils.equalsIgnoreCase(""true"", valueString))
  return true;
else if (StringUtils.equalsIgnoreCase(""false"", valueString))
  return false;
else {
  LOG.warn(""Invalid value for boolean: "" + valueString +
           "", choose default value: "" + defaultValue + "" for "" + name);
  return defaultValue;
}{code}
I attach a patch to log the warning message."
Tencent Cloud COS: Package it into hadoop-dist,13295995,Open,Major,,03/Apr/20 09:30,,3.3.0,"[~stevel@apache.org]  Hi, Steve, the Hadoop-COS has been submitted to the trunk branch last year. (Related jira: https://issues.apache.org/jira/browse/HADOOP-15616). 

I am the contributor and the maintainer of this patch.  Considering that you are the expert on the object storage support in Hadoop, I have some questions for you.
 # The Hadoop-cos patch is placed in the *hadoop-cloud-storage-project directory. How to package it into the hadoop-dist directory as the distribution package?*
 # How to manage the dependencies of the Hadoop-cos, such as cos-java-sdk? I use the maven to compile and package it, but only the hadoop-cos-3.4.0-SNAPSHOT.jar is generated. I compare the pom.xml between the hadoop-cos and hadoop-oss, and find the dependency scope is '*compile*' in their pom.xml. The hadoop-oss has oss-java-sdk.jar in dist directory, but the hadoop-cos does not. 

Can you guide me how to solve these two problems? 

 

Thanks for your time and attention. :)

 

 "
"S3A client retries on SSL Auth exceptions triggered by ""."" bucket names",13301212,Open,Minor,,27/Apr/20 10:00,,3.2.1,"If you have a ""."" in bucket names (it's allowed!) then virtual host HTTPS connections fail with a  java.net.ssl exception. Except we retry and the inner cause is wrapped by generic ""client exceptions""

I'm not going to try and be clever about fixing this, but we should
* make sure that the inner exception is raised up
* avoid retries
* document it in the troubleshooting page. 
* if there is a well known public ""."" bucket (cloudera has some:)) we can test

I get a vague suspicion the AWS SDK is retrying too. Not much we can do there."
S3A deleteObjects hanging/retrying forever,13309295,Open,Minor,,04/Jun/20 00:00,,3.2.1,"{code}
sun.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:523) com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:82) org.apache.hadoop.fs.s3a.S3ABlockOutputStream.putObject(S3ABlockOutputStream.java:446) org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:365) org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101) org.apache.parquet.hadoop.util.HadoopPositionOutputStream.close(HadoopPositionOutputStream.java:64) org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:685) org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:122) org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165) org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42) org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57) org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74) org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247) org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242) org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394) org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248) org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170) org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) org.apache.spark.scheduler.Task.run(Task.scala:123) org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) java.lang.Thread.run(Thread.java:748)
{code}

 
we are using spark 2.4.4 with hadoop 3.2.1 on kubernetes/spark-operator, sometimes we see this hang with the stacktrace above. it looks like the putObject never return, we have to kill the executor to make the job move forward. 
 "
Embrace Immutability of Java Collections,13300413,Resolved,Minor,Fixed,23/Apr/20 02:51,19/Jun/20 17:24,,
Replace Guava Predicate with Java8+ Predicate,13314175,Resolved,Minor,Fixed,29/Jun/20 23:34,15/Jul/20 17:21,3.2.2,"{{com.google.common.base.Predicate}} can be replaced with {{java.util.function.Predicate}}. 
The change involving 9 occurrences is straightforward:


{code:java}
Targets
    Occurrences of 'com.google.common.base.Predicate' in project with mask '*.java'
Found Occurrences  (9 usages found)
    org.apache.hadoop.hdfs.server.blockmanagement  (1 usage found)
        CombinedHostFileManager.java  (1 usage found)
            43 import com.google.common.base.Predicate;
    org.apache.hadoop.hdfs.server.namenode  (1 usage found)
        NameNodeResourceChecker.java  (1 usage found)
            38 import com.google.common.base.Predicate;
    org.apache.hadoop.hdfs.server.namenode.snapshot  (1 usage found)
        Snapshot.java  (1 usage found)
            41 import com.google.common.base.Predicate;
    org.apache.hadoop.metrics2.impl  (2 usages found)
        MetricsRecords.java  (1 usage found)
            21 import com.google.common.base.Predicate;
        TestMetricsSystemImpl.java  (1 usage found)
            41 import com.google.common.base.Predicate;
    org.apache.hadoop.yarn.logaggregation  (1 usage found)
        AggregatedLogFormat.java  (1 usage found)
            77 import com.google.common.base.Predicate;
    org.apache.hadoop.yarn.logaggregation.filecontroller  (1 usage found)
        LogAggregationFileController.java  (1 usage found)
            22 import com.google.common.base.Predicate;
    org.apache.hadoop.yarn.logaggregation.filecontroller.ifile  (1 usage found)
        LogAggregationIndexedFileController.java  (1 usage found)
            22 import com.google.common.base.Predicate;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation  (1 usage found)
        AppLogAggregatorImpl.java  (1 usage found)
            75 import com.google.common.base.Predicate;

{code}
"
Add .diff to gitignore,13295696,Resolved,Minor,Fixed,02/Apr/20 06:00,03/Apr/20 08:29,2.10.1,"Add .diff to gitignore.
Else on git add . 
it even stages the .diff files too."
Tidy Up Text and ByteWritables Classes,13295653,Resolved,Minor,Fixed,01/Apr/20 23:22,17/Apr/20 15:16,3.4.0,"# Remove superfluous code
 # Remove superfluous comments
 # Checkstyle fixes
 # Remove methods that simply call {{super}}.method()
 # Use Java 8 facilities to streamline code where applicable
 # Simplify and unify some of the constructs between the two classes

 

The one meaningful change is that I am suggesting that the expanding of the arrays be 1.5x instead of 2x per expansion.  I pulled this idea from open JDK."
S3AFS globStatus attempts to resolve symlinks,13314350,Resolved,Minor,Fixed,30/Jun/20 17:40,13/Jul/20 18:12,,"The S3AFileSystem implementation of the globStatus API has a setting configured to resolve symlinks. Under certain circumstances, this will cause additional file existence checks to be performed in order to determine if a FileStatus signifies a symlink. As symlinks are not supported in S3AFileSystem, these calls are unnecessary.

Code snapshot (permalink): [https://github.com/apache/hadoop/blob/2a67e2b1a0e3a5f91056f5b977ef9c4c07ba6718/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L4002]

Causes additional getFileStatus call here (permalink): [https://github.com/apache/hadoop/blob/1921e94292f0820985a0cfbf8922a2a1a67fe921/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java#L308]

Current code snippet:
{code:java}
/**
   * Override superclass so as to disable symlink resolution and so avoid
   * some calls to the FS which may have problems when the store is being
   * inconsistent.
   * {@inheritDoc}
   */
  @Override
  public FileStatus[] globStatus(
      final Path pathPattern,
      final PathFilter filter)
      throws IOException {
    entryPoint(INVOCATION_GLOB_STATUS);
    return Globber.createGlobber(this)
        .withPathPattern(pathPattern)
        .withPathFiltern(filter)
        .withResolveSymlinks(true)
        .build()
        .glob();
  }
{code}
 

The fix should be pretty simple, just flip ""withResolveSymlinks"" to false."
TestSequenceFile#testRecursiveSeqFileCreate fails in subsequent run ,13297702,Resolved,Minor,Fixed,10/Apr/20 22:01,12/Apr/20 06:13,3.2.1,"The test expects an IOException when creating a writer for file `target/test/data/recursiveCreateDir/file` with `createParent=false`. And it expects to create the writer successfully when `createParent=True`. `createParent` means `create parent directory if non-existent`.

The test will pass if it is run for the first time, but it will fail for the second run. This is because the test did not clean the parent directory created during the first run.

The parent directory `recursiveCreateDir` was created, but it was not deleted before the test finished. So, when the test was run again, it still treated the parent directory `recursiveCreateDir` as non-existent and expected an IOException from creating a writer with `createParent=false`. Then the test did not get the expected IOException because `recursiveCreateDir` has been created in the first test run.




{code:java}
@SuppressWarnings(""deprecation"")
 @Test
 public void testRecursiveSeqFileCreate() throws IOException {
 FileSystem fs = FileSystem.getLocal(conf);
 Path name = new Path(new Path(GenericTestUtils.getTempPath(
 ""recursiveCreateDir"")), ""file""); // FILE SUCCESSULLY CREATED HERE
 boolean createParent = false;
try {
 SequenceFile.createWriter(fs, conf, name, RandomDatum.class,
 RandomDatum.class, 512, (short) 1, 4096, createParent,
 CompressionType.NONE, null, new Metadata());
 fail(""Expected an IOException due to missing parent"");
 } catch (IOException ioe) {
 // Expected
 }
createParent = true;
 SequenceFile.createWriter(fs, conf, name, RandomDatum.class,
 RandomDatum.class, 512, (short) 1, 4096, createParent,
 CompressionType.NONE, null, new Metadata());
 // should succeed, fails if exception thrown
 }
{code}


Suggested patch:

 
{code:java}
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java
index 044824356ed..1aff2936264 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java
@@ -649,8 +649,9 @@ public void testCreateWriterOnExistingFile() throws IOException {
 @Test
 public void testRecursiveSeqFileCreate() throws IOException {
 FileSystem fs = FileSystem.getLocal(conf);
- Path name = new Path(new Path(GenericTestUtils.getTempPath(
- ""recursiveCreateDir"")), ""file"");
+ Path parentDir = new Path(GenericTestUtils.getTempPath(
+ ""recursiveCreateDir""));
+ Path name = new Path(parentDir, ""file"");
 boolean createParent = false;
 
 try {
@@ -667,6 +668,9 @@ public void testRecursiveSeqFileCreate() throws IOException {
 RandomDatum.class, 512, (short) 1, 4096, createParent,
 CompressionType.NONE, null, new Metadata());
 // should succeed, fails if exception thrown
+
+ fs.deleteOnExit(parentDir);
+ fs.close();
 }
 
 @Test{code}"
"Hadoop distcp throws ""ERROR: Tools helper ///usr/lib/hadoop/libexec/tools/hadoop-distcp.sh was not found""",13305002,Resolved,Minor,Fixed,14/May/20 17:41,18/May/20 06:38,3.1.3,"On Hadoop 3.x, we see following ""ERROR: Tools helper ///usr/lib/hadoop/libexec/tools/hadoop-distcp.sh was not found."" message on the first line of the command output when running Hadoop DistCp.
{code:java}
$ hadoop distcp /path/to/src /user/hadoop/
ERROR: Tools helper ///usr/lib/hadoop/libexec/tools/hadoop-distcp.sh was not found.
2020-05-14 17:11:53,173 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true
..
{code}
This message was added by HADOOP-12857 and it would be an expected behavior.
 DistCp calls 'hadoop_add_to_classpath_tools hadoop-distcp' when [it starts|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-distcp/src/main/shellprofile.d/hadoop-distcp.sh], and the error is returned because the hadoop-distcp.sh does not exist in the tools directory.

However, that error message confuses us. Since this is not an user end configuration issue, I would think it's better to change the log level to debug (hadoop_debug)."
TestFTPFileSystem failing as ftp server dir already exists,13304104,Resolved,Minor,Fixed,11/May/20 12:16,14/May/20 17:29,3.4.0,"TestFTPFileSystem failing as the test dir exists.

need to delete in setup/teardown of each test case"
ABFS: Avoid storage calls to check if the account is HNS enabled or not,13299735,Resolved,Minor,Fixed,20/Apr/20 15:34,24/Apr/20 04:46,3.4.0,"Each time an FS instance is created a Getacl call is made. If the call fails with 400 Bad request, the account is determined to be a non-HNS account. 

Recommendation is to create a config and be able to avoid store calls to determine account HNS status,

If config is available, use that to determine account HNS status. If config is not present in core-site, default behaviour will be calling getAcl. "
MetricsSystem doesn't start the sink adapters on restart,13312846,Resolved,Minor,Fixed,22/Jun/20 14:06,06/Jul/20 15:30,,"In HBase we use dynamic metrics and when a metric is removed, we have to refresh the JMX beans, since there is no API from Java to do it, a hack like stopping the metrics system and restarting it was used (Read the comment on the class [https://github.com/mmpataki/hbase/blob/master/hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java])

 

It calls the below APIs in the same order

     MetricsSystem.stop

     MetricsSystem.start

 

MetricsSystem.stop stops all the SinkAdapters, *but doesn't remove them from the sink list* (allSinks is the variable). When the metrics system is started again, *it is assumed that the SinkAdapters are restarted, but they are not* due to the check done in the beginning of the function register."
Add error Message when thread join throw exception,13297887,Open,Minor,,12/Apr/20 15:53,,3.2.1,Add error Message when thread join throw exception.
Fix typo in JMXJsonServlet.java,13297891,Open,Minor,,12/Apr/20 16:02,,3.2.1,Fix typo in JMXJsonServlet.java
Fix typo getLogSupressionMessage to getLogSuppressionMessage,13297976,Open,Minor,,13/Apr/20 08:13,,3.2.1,Fix typo getLogSupressionMessage to getLogSuppressionMessage
Add concat fs command,13302063,Resolved,Minor,Fixed,30/Apr/20 11:11,08/Oct/20 09:37,,We should add one concat fs command for ease of use. It concatenates existing source files into the target file using FileSystem.concat().
Failed to load XInclude files with relative path.,13313232,Resolved,Minor,Fixed,24/Jun/20 11:08,21/Sep/20 18:15,3.2.1,"When we create a configuration file, which load a external XML file with relative path, and try to load it via calling `Configuration.addResource` with `Path(URI)`, we got an error, which failed to load a external XML, after https://issues.apache.org/jira/browse/HADOOP-14216 is merged.
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: java.io.IOException: Fetch fail on include for 'mountTable.xml' with no fallback while loading 'file:/opt/hadoop/etc/hadoop/core-site.xml'
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3021)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2973)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2848)
	at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2896)
	at com.company.test.Main.main(Main.java:29)
Caused by: java.io.IOException: Fetch fail on include for 'mountTable.xml' with no fallback while loading 'file:/opt/hadoop/etc/hadoop/core-site.xml'
	at org.apache.hadoop.conf.Configuration$Parser.handleEndElement(Configuration.java:3271)
	at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3331)
	at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3114)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3007)
	... 4 more
{noformat}
The cause is that the URI is passed as string to java.io.File constructor and File does not support the file URI, so my suggestion is trying to convert from string to URI at first."
Add support to read history files from the done directory,13298167,Open,Minor,,14/Apr/20 02:01,,2.9.2,Currently history server implementation moves files from intermediate to done directory. We would like to add an additional server that can function as an alternate history server. To support it we would need to provide a functionality to read from done directory only. 
S3A to support additional token issuers,13306166,Resolved,Minor,Fixed,20/May/20 11:21,24/Jun/20 13:50,,"In {{org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding}} the {{createDelegationToken}} should return a list of tokens.
With this functionality, the {{AbstractDelegationTokenBinding}} can get two different tokens at the same time.
{{AbstractDelegationTokenBinding.TokenSecretManager}} should be extended to retrieve secrets and lookup delegation tokens (use the public API for secretmanager in hadoop)
"
Intermittent failing of ITestAbfsStreamStatistics in ABFS,13301526,Resolved,Minor,Fixed,28/Apr/20 11:07,02/Jun/20 04:48,3.3.0,"There are intermittent failures of a test inside ITestAbfsStreamStatistics in ABFS.
Did consecutive runs of the test and failure seemed random. Stack Trace in the comments.

Propose:
- Change the assertion of the test for it to be passed, Since the production code seems fine."
ABFS: Delegation SAS Generator Updates,13312075,Resolved,Minor,Fixed,17/Jun/20 22:55,18/Jun/20 02:14,3.2.1,"# The authentication version in the service has been updated from Dec19 to Feb20, so need to update the client.
 # Add support and test cases for getXattr and setXAttr.
 # Update DelegationSASGenerator and related to use Duration instead of int for time periods.
 # Cleanup DelegationSASGenerator switch/case statement that maps operations to permissions.
 # Cleanup SASGenerator classes to use String.equals instead of ==."
ABFS: Improve the ABFS driver documentation,13299784,Resolved,Minor,Fixed,20/Apr/20 19:08,19/Jun/20 19:59,3.4.0,"* Add the missing configuration/settings details
* Mention the default vales"
Improve RawFileSystem Performance,13301788,Resolved,Minor,Fixed,29/Apr/20 12:10,17/Jun/20 15:18,3.3.0,"Improving RawFileSystem performance.

Changes:
 * RawLocalFileSystem could localize the default block size to avoid sync bottleneck with a Configuration object.
[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java#L666]
 * Exists() override method for optimization."
Fix broken links in AWS documentation,13309050,Open,Minor,,02/Jun/20 21:20,,,"Broken links are found in the following page:

[hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md|https://github.com/apache/hadoop/compare/trunk...piotte13:trunk#diff-3043a79259e7448fccbf133c3612b700]"
Fix intermittent failure of ITestBlockingThreadPoolExecutorService,13304566,Resolved,Minor,Fixed,13/May/20 08:32,22/May/20 09:52,,ITestBlockingThreadPoolExecutorService intermittently fails due to load on test node.
add hadoop-azure ITest to fail if all abfs or wasb tests are skipped,13306533,Open,Minor,,21/May/20 16:25,,3.2.1,"Currently the hadoop-azure 'mvn verify' test run will complete if no credentials are provided -the wasb tests skipped on the assumption there is an abfs test setup; the abfs tests assume the run is for wasb

we need a test which fails when both credential sets are missing."
"`Configuration` javadoc describes supporting environment variables, but the feature is not available",13305282,Resolved,Minor,Fixed,15/May/20 19:51,18/May/20 02:14,2.10.0,"In Hadoop 2.10.0, the javadoc on the `Configuration` class describes the ability to read values from environment variables. However, this feature isn't implemented until HADOOP-9642, which shipped in 3.0.0."
Fix invalid metastore configuration in S3GuardTool tests,13302394,Resolved,Minor,Fixed,02/May/20 03:33,07/May/20 03:33,3.3.0,"The WARN message shown in S3GuardTool tests implies mismatch between property name and the value.
{noformat}
2020-05-02 11:57:44,266 [setup] WARN  conf.Configuration (Configuration.java:getBoolean(1694)) - Invalid value for boolean: org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore, choose default value: false for fs.s3a.metadatastore.authoritative
{noformat}"
HADOOP-16953. tune s3guard disabled warnings,13295746,Resolved,Minor,Fixed,02/Apr/20 09:38,05/May/20 18:20,3.3.0,"config option org.apache.hadoop.fs.s3a.s3guard.disabled.warn.level should be  fs.s3a.s3guard.disabled.warn.level

need to fix that and add the existing one as deprecated"
Add capability in hadoop-client to automatically login from a client/service keytab,13300127,Open,Minor,,22/Apr/20 01:16,,3.2.1,"With existing Hadoop client implementation, client applications for services that are using kerberized clusters, need to handle Keytab based login in their code, before doing HDFS or M/R API calls.

To avoid that, we are proposing adding Keytab based auto login to hadoop client library with configurable and default paths for Keytabs. 
This functionality helps new service owners as well as those transitioning from non-kerberized cluster to kerberized ones.

Auto login, should avoid extra login attempts in case a valid TGT is already available."
TestFileContextResolveAfs#testFileContextResolveAfs creates dangling link and fails for subsequent runs,13297815,Resolved,Minor,Fixed,11/Apr/20 22:22,19/Apr/20 20:37,3.2.1,"In the test testFileContextResolveAfs, the symlink TestFileContextResolveAfs2 (linked to TestFileContextResolveAfs1) cannot be deleted when the test finishes.

This is because TestFileContextResolveAfs1 was always deleted before TestFileContextResolveAfs2 when they were both passed into FileSystem#deleteOnExit. This caused TestFileContextResolveAfs2 to become a dangling link, which FileSystem in Hadoop currently cannot delete. (This is because Files#exists will return false for dangling links.)

As a result, the test `testFileContextResolveAfs` only passed for the first run. And for later runs of this test, it will fail by throwing the following exception: 
{code:java}
fs.FileUtil (FileUtil.java:symLink(821)) - Command 'ln -s mypath/TestFileContextResolveAfs1 mypath/TestFileContextResolveAfs2' failed 1 with: ln: mypath/TestFileContextResolveAfs2: File exists

java.io.IOException: Error 1 creating symlink file:mypath/TestFileContextResolveAfs2 to mypath/TestFileContextResolveAfs1
{code}"
Modify constant for AbstractFileSystem,13297259,Open,Minor,,09/Apr/20 04:17,,3.2.1,Modify constant for AbstractFileSystem
S3Guard auth mode should be set to false by default in integration tests.,13298013,Resolved,Minor,Fixed,13/Apr/20 11:53,16/Apr/20 11:02,3.2.0,"As per the doc [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md#testing-s3a-with-s3guard-enabled]

, it says if s3Guard profile is enabled all tests runs in ""non-authoritative"" mode. But as per the code [https://github.com/apache/hadoop/blob/8d49229c3764a205f21750225005a2c9a8124ab9/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java#L485]

the default value is set to true.  So either we fix the doc or code. "
Supporting the server encryption based on SSE-COS and SSE-C,13298098,Open,Minor,,13/Apr/20 18:26,,,Support the server encryption  based on SSE-COS and SSE-C in Hadoop-cos.
"Trivial typo(s) which are 'timout', 'interruped' in comment, LOG and documents",13303975,Resolved,Trivial,Fixed,11/May/20 00:18,12/May/20 15:56,3.4.0,There are typos 'Interruped' and 'timout'.
TODO comments exist in trunk while the related issues are already fixed.,13305616,Resolved,Trivial,Fixed,18/May/20 08:23,08/Jun/20 18:31,2.10.1,"In a research project, we analyzed the source code of Hadoop looking for comments with on-hold SATDs (self-admitted technical debt) that could be fixed already. An on-hold SATD is a TODO/FIXME comment blocked by an issue. If this blocking issue is already resolved, the related todo can be implemented (or sometimes it is already implemented, but the comment is left in the code causing confusions). As we found a few instances of these in Hadoop, we decided to collect them in a ticket, so they are documented and can be addressed sooner or later.

A list of code comments that mention already closed issues.
 * A code comment suggests making the setJobConf method deprecated along with a mapred package HADOOP-1230. HADOOP-1230 has been closed a long time ago, but the method is still not annotated as deprecated.
{code:java}
 /**
   * This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.
   */ {code}
Comment location: [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java#L88]

 * A comment mentions that the return type of the getDefaultFileSystem method should be changed to AFS when HADOOP-6223 is completed.
 Indeed, this change was done in the related commit of HADOOP-6223: ([https://github.com/apache/hadoop/commit/3f371a0a644181b204111ee4e12c995fc7b5e5f5#diff-cd86a2b9ce3efd2232c2ace0e9084508L395)]
 Thus, the comment could be removed.
{code:java}
@InterfaceStability.Unstable /* return type will change to AFS once
HADOOP-6223 is completed */
{code}
Comment location: [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java#L512]"
Fix typo in ReconfigurationServlet,13297885,Open,Trivial,,12/Apr/20 15:38,,3.2.1,Fix typo in ReconfigurationServlet
Fix code style in JMXJsonServlet,13297893,Open,Trivial,,12/Apr/20 16:19,,3.2.1,Fix code style in JMXJsonServlet
WordMedian example has a logical error,13304300,Open,Trivial,,12/May/20 07:40,,3.2.1,"This is the WordMedian example in `Hadoop-mapreduce-examples-3.2.1.jar`. The 'else if' statement is inaccessible because it is a subset of the 'if' statement.

!image-2020-05-12-15-35-51-583.png!"
Install Intel ISA-L library in Dockerfile,13324360,Resolved,Blocker,Fixed,25/Aug/20 01:22,22/Jan/21 01:31,3.3.1,"Currently, there is not isa-l library in the docker container, and jenkins skips the natvie tests, TestNativeRSRawCoder and TestNativeXORRawCoder."
update  org.apache.httpcomponents:httpclient to 4.5.13 and httpcore to 4.4.13,13324357,Resolved,Blocker,Fixed,25/Aug/20 01:12,13/Oct/20 08:59,3.3.0,"Update the dependencies
 * org.apache.httpcomponents:httpclient from 4.5.6 to 4.5.12
 * org.apache.httpcomponents:httpcore from 4.4.10 to 4.4.13"
whitespace not allowed in paths when saving files to s3a via committer,13314899,Resolved,Blocker,Fixed,03/Jul/20 14:08,25/Apr/21 18:00,3.2.0,"When saving results through spark dataframe on latest 3.0.1-snapshot compiled against hadoop-3.2 with the following specs
 --conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory 
 --conf spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter 
 --conf spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol 
 --conf spark.hadoop.fs.s3a.committer.name=partitioned 
 --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=replace 
 we are unable to save the file with whitespace character in the path. It works fine without.

I was looking into the recent commits with regards to qualifying the path, but couldn't find anything obvious. Is this a known bug?

When saving results through spark dataframe on latest 3.0.1-snapshot compiled against hadoop-3.2 with the following specs
--conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory  
--conf spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter 
--conf spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol 
--conf spark.hadoop.fs.s3a.committer.name=partitioned 
--conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=replace 
we are unable to save the file with whitespace character in the path. It works fine without.

I was looking into the recent commits with regards to qualifying the path, but couldn't find anything obvious. Is this a known bug?

!image-2020-07-03-16-08-52-340.png!"
HADOOP-17244. S3A directory delete tombstones dir markers prematurely.,13326101,Resolved,Blocker,Fixed,04/Sep/20 18:07,18/Nov/20 13:28,3.3.1,"Test failure: {{ITestS3AFileContextMainOperations#testRenameDirectoryAsNonExistentDirectory}}

This is repeatable on -Dauth runs (we haven't been running them, have we?)

Either its from the recent dir marker changes (initial hypothesis) or its been lurking a while and not been picked up.
"
Java 17 support,13320586,Open,Major,,03/Aug/20 04:38,,3.5.0,Umbrella JIRA to support Java 17 LTS.
ABFS: Test testAbfsStreamOps timing out ,13327350,Open,Major,,14/Sep/20 05:15,,3.3.0,"Test testAbfsStreamOps is timing out when log4j settings are at DEBUG/TRACE level for AbfsInputStream.

log4j.logger.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream=TRACE

 

org.junit.runners.model.TestTimedOutException: test timed out after 900000 millisecondsorg.junit.runners.model.TestTimedOutException: test timed out after 900000 milliseconds
 at java.lang.Throwable.getStackTraceElement(Native Method) at java.lang.Throwable.getOurStackTrace(Throwable.java:828) at java.lang.Throwable.getStackTrace(Throwable.java:817) at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.log4j.spi.LocationInfo.<init>(LocationInfo.java:139) at org.apache.log4j.spi.LoggingEvent.getLocationInformation(LoggingEvent.java:253) at org.apache.log4j.helpers.PatternParser$LocationPatternConverter.convert(PatternParser.java:500) at org.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65) at org.apache.log4j.PatternLayout.format(PatternLayout.java:506) at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310) at org.apache.log4j.WriterAppender.append(WriterAppender.java:162) at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251) at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66) at org.apache.log4j.Category.callAppenders(Category.java:206) at org.apache.log4j.Category.forcedLog(Category.java:391) at org.apache.log4j.Category.log(Category.java:856) at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:273) at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:150) at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:131) at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:104) at java.io.FilterInputStream.read(FilterInputStream.java:83) at org.apache.hadoop.fs.azurebfs.AbstractAbfsTestWithTimeout.validateContent(AbstractAbfsTestWithTimeout.java:117) at org.apache.hadoop.fs.azurebfs.ITestAbfsStreamStatistics.testAbfsStreamOps(ITestAbfsStreamStatistics.java:155) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)"
ITestAbfsInputStreamStatistics#testReadAheadCounters timing out always,13319673,Reopened,Major,,28/Jul/20 05:05,,3.3.0,The test ITestAbfsInputStreamStatistics#testReadAheadCounters timing out always is timing out always
Add .asf.yaml to allow github and jira integration,13324869,Resolved,Major,Fixed,27/Aug/20 15:32,28/Aug/20 12:20,3.4.0,"As of now the default for github is set only to worklog, To enable link and label, We need to add this."
add guava BaseEncoding to illegalClasses,13314495,Resolved,Major,Fixed,01/Jul/20 14:25,17/Feb/21 01:21,3.4.0,"One important thing to not here as pointed out by [~jeagles] in [his comment on the parent task|https://issues.apache.org/jira/browse/HADOOP-17098?focusedCommentId=17147935&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17147935]

{quote}One note to be careful about is that base64 translation is not a standard, so the two implementations could produce different results. This might matter in the case of serialization, persistence, or client server different versions.{quote}


*Base64Url:*

{code:java}
Targets
    Occurrences of 'base64Url' in project with mask '*.java'
Found Occurrences  (6 usages found)
    org.apache.hadoop.mapreduce  (3 usages found)
        CryptoUtils.java  (3 usages found)
            wrapIfNecessary(Configuration, FSDataOutputStream, boolean)  (1 usage found)
                138 + Base64.encodeBase64URLSafeString(iv) + ""]"");
            wrapIfNecessary(Configuration, InputStream, long)  (1 usage found)
                183 + Base64.encodeBase64URLSafeString(iv) + ""]"");
            wrapIfNecessary(Configuration, FSDataInputStream)  (1 usage found)
                218 + Base64.encodeBase64URLSafeString(iv) + ""]"");
    org.apache.hadoop.util  (2 usages found)
        KMSUtil.java  (2 usages found)
            toJSON(KeyVersion)  (1 usage found)
                104 Base64.encodeBase64URLSafeString(
            toJSON(EncryptedKeyVersion)  (1 usage found)
                117 .encodeBase64URLSafeString(encryptedKeyVersion.getEncryptedKeyIv()));
    org.apache.hadoop.yarn.server.resourcemanager.webapp  (1 usage found)
        TestRMWebServicesAppsModification.java  (1 usage found)
            testAppSubmit(String, String)  (1 usage found)
                837 .put(""test"", Base64.encodeBase64URLSafeString(""value12"".getBytes(""UTF8"")));

{code}

*Base64:*

{code:java}
Targets
    Occurrences of 'base64;' in project with mask '*.java'
Found Occurrences  (51 usages found)
    org.apache.hadoop.crypto.key.kms  (1 usage found)
        KMSClientProvider.java  (1 usage found)
            20 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.crypto.key.kms.server  (1 usage found)
        KMS.java  (1 usage found)
            22 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.fs  (2 usages found)
        XAttrCodec.java  (2 usages found)
            23 import org.apache.commons.codec.binary.Base64;
            56 BASE64;
    org.apache.hadoop.fs.azure  (3 usages found)
        AzureBlobStorageTestAccount.java  (1 usage found)
            23 import com.microsoft.azure.storage.core.Base64;
        BlockBlobAppendStream.java  (1 usage found)
            50 import org.apache.commons.codec.binary.Base64;
        ITestBlobDataValidation.java  (1 usage found)
            50 import com.microsoft.azure.storage.core.Base64;
    org.apache.hadoop.fs.azurebfs  (2 usages found)
        AzureBlobFileSystemStore.java  (1 usage found)
            99 import org.apache.hadoop.fs.azurebfs.utils.Base64;
        TestAbfsConfigurationFieldsValidation.java  (1 usage found)
            34 import org.apache.hadoop.fs.azurebfs.utils.Base64;
    org.apache.hadoop.fs.azurebfs.diagnostics  (2 usages found)
        Base64StringConfigurationBasicValidator.java  (1 usage found)
            26 import org.apache.hadoop.fs.azurebfs.utils.Base64;
        TestConfigurationValidators.java  (1 usage found)
            25 import org.apache.hadoop.fs.azurebfs.utils.Base64;
    org.apache.hadoop.fs.azurebfs.extensions  (2 usages found)
        MockDelegationSASTokenProvider.java  (1 usage found)
            37 import org.apache.hadoop.fs.azurebfs.utils.Base64;
        MockSASTokenProvider.java  (1 usage found)
            27 import org.apache.hadoop.fs.azurebfs.utils.Base64;
    org.apache.hadoop.fs.azurebfs.services  (1 usage found)
        SharedKeyCredentials.java  (1 usage found)
            47 import org.apache.hadoop.fs.azurebfs.utils.Base64;
    org.apache.hadoop.fs.cosn  (1 usage found)
        CosNativeFileSystemStore.java  (1 usage found)
            61 import com.qcloud.cos.utils.Base64;
    org.apache.hadoop.fs.s3a  (1 usage found)
        EncryptionTestUtils.java  (1 usage found)
            26 import org.apache.commons.net.util.Base64;
    org.apache.hadoop.hdfs.protocol.datatransfer.sasl  (3 usages found)
        DataTransferSaslUtil.java  (1 usage found)
            39 import org.apache.commons.codec.binary.Base64;
        SaslDataTransferClient.java  (1 usage found)
            47 import org.apache.commons.codec.binary.Base64;
        SaslDataTransferServer.java  (1 usage found)
            44 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.hdfs.server.common.blockaliasmap.impl  (1 usage found)
        TextFileRegionAliasMap.java  (1 usage found)
            31 import java.util.Base64;
    org.apache.hadoop.hdfs.server.federation.store.driver.impl  (1 usage found)
        StateStoreSerializerPBImpl.java  (1 usage found)
            22 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.hdfs.server.federation.store.protocol.impl.pb  (1 usage found)
        FederationProtocolPBTranslator.java  (1 usage found)
            23 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.hdfs.server.namenode.web.resources  (1 usage found)
        NamenodeWebHdfsMethods.java  (1 usage found)
            31 import java.util.Base64;
    org.apache.hadoop.hdfs.web  (1 usage found)
        WebHdfsFileSystem.java  (1 usage found)
            43 import java.util.Base64;
    org.apache.hadoop.io  (1 usage found)
        DefaultStringifier.java  (1 usage found)
            26 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.io.compress  (1 usage found)
        TestCodec.java  (1 usage found)
            51 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.mapreduce  (1 usage found)
        CryptoUtils.java  (1 usage found)
            24 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.mapreduce.security  (1 usage found)
        SecureShuffleUtils.java  (1 usage found)
            29 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.registry.server.dns  (2 usages found)
        RegistryDNS.java  (1 usage found)
            22 import org.apache.commons.net.util.Base64;
        TestRegistryDNS.java  (1 usage found)
            19 import org.apache.commons.net.util.Base64;
    org.apache.hadoop.security  (1 usage found)
        SaslRpcServer.java  (1 usage found)
            42 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.security.authentication.client  (2 usages found)
        KerberosAuthenticator.java  (2 usages found)
            18 import org.apache.commons.codec.binary.Base64;
            152 private Base64 base64;
    org.apache.hadoop.security.authentication.server  (5 usages found)
        KerberosAuthenticationHandler.java  (1 usage found)
            19 import org.apache.commons.codec.binary.Base64;
        LdapAuthenticationHandler.java  (1 usage found)
            34 import org.apache.commons.codec.binary.Base64;
        TestKerberosAuthenticationHandler.java  (1 usage found)
            20 import org.apache.commons.codec.binary.Base64;
        TestLdapAuthenticationHandler.java  (1 usage found)
            24 import org.apache.commons.codec.binary.Base64;
        TestMultiSchemeAuthenticationHandler.java  (1 usage found)
            33 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.security.authentication.util  (2 usages found)
        Signer.java  (1 usage found)
            16 import org.apache.commons.codec.binary.Base64;
        TestKerberosUtil.java  (1 usage found)
            26 import java.util.Base64;
    org.apache.hadoop.security.token  (1 usage found)
        Token.java  (1 usage found)
            24 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.util  (1 usage found)
        KMSUtil.java  (1 usage found)
            20 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.applications.distributedshell  (2 usages found)
        ApplicationMaster.java  (1 usage found)
            44 import java.util.Base64;
        Client.java  (1 usage found)
            34 import java.util.Base64;
    org.apache.hadoop.yarn.client.util  (1 usage found)
        YarnClientUtils.java  (1 usage found)
            29 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher  (1 usage found)
        TestContainerLaunch.java  (1 usage found)
            59 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.server.resourcemanager.webapp  (2 usages found)
        RMWebAppUtil.java  (1 usage found)
            30 import org.apache.commons.codec.binary.Base64;
        TestRMWebServicesAppsModification.java  (1 usage found)
            52 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.service.client  (1 usage found)
        ApiServiceClient.java  (1 usage found)
            35 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.service.utils  (1 usage found)
        HttpUtil.java  (1 usage found)
            30 import org.apache.commons.codec.binary.Base64;
    org.apache.hadoop.yarn.util  (1 usage found)
        AuxiliaryServiceHelper.java  (1 usage found)
            24 import org.apache.commons.codec.binary.Base64;
{code}



"
Use shaded guava from thirdparty,13329636,Resolved,Major,Fixed,27/Sep/20 16:34,17/Oct/20 07:25,3.3.1,Use the shaded version of guava in hadoop-thirdparty
Replace Guava Optional with Java8+ Optional,13314499,Resolved,Major,Fixed,01/Jul/20 14:38,06/Jul/20 08:03,3.3.1,"{code:java}
Targets
    Occurrences of 'com.google.common.base.Optional' in project with mask '*.java'
Found Occurrences  (3 usages found)
    org.apache.hadoop.yarn.server.nodemanager  (2 usages found)
        DefaultContainerExecutor.java  (1 usage found)
            71 import com.google.common.base.Optional;
        LinuxContainerExecutor.java  (1 usage found)
            22 import com.google.common.base.Optional;
    org.apache.hadoop.yarn.server.resourcemanager.recovery  (1 usage found)
        TestZKRMStateStorePerf.java  (1 usage found)
            21 import com.google.common.base.Optional;

{code}
"
Bump up snakeyaml to 1.26 to mitigate CVE-2017-18640,13325256,Resolved,Major,Fixed,31/Aug/20 03:23,28/Oct/20 16:29,3.2.3,Bump up snakeyaml to 1.26 to mitigate CVE-2017-18640
Upgrade to jQuery 3.5.1 in hadoop-yarn-common,13329507,Resolved,Major,Fixed,26/Sep/20 01:08,26/Sep/20 18:20,3.3.1,"Found via HDFS-15598. Looks like HADOOP-17283 didn't update rat exclusion rule.

{noformat}
Lines that start with ????? in the ASF License  report indicate files that do not have an Apache license header:
 !????? /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2339/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/webapps/static/jquery/jquery-3.4.1.min.js
{noformat}"
Add RootedOzFS AbstractFileSystem to core-default.xml,13326102,Resolved,Major,Fixed,04/Sep/20 18:22,05/Sep/20 01:39,3.4.0,"When ""ofs"" is default, when running mapreduce job, YarnClient fails with below exception.
{code:java}
Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: fs.AbstractFileSystem.ofs.impl=null: No AbstractFileSystem configured for scheme: ofs
 at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:176)
 at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:265)
 at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:341)
 at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:338)
 at java.security.AccessController.doPrivileged(Native Method){code}

Observed that o3fs is also not defined, will use this jira to add those too."
Upgrade slf4j to 1.7.30 ( To Address: CVE-2018-8088),13324242,Resolved,Major,Fixed,24/Aug/20 10:51,24/Aug/20 13:37,3.3.1,"To address the following CVE, upgrade the slf4j to latest stable release 1.7.30.

[https://nvd.nist.gov/vuln/detail/CVE-2018-8088]
 

Note: We don't use EventData but should consider upgrading."
Jetty upgrade to 9.4.x causes MR app fail with IOException,13315688,Resolved,Major,Fixed,08/Jul/20 17:17,20/Jul/20 17:01,3.1.1,"I think we should catch IOException here instead of BindException in HttpServer2#bindForPortRange
{code:java}
 for(Integer port : portRanges) {
      if (port == startPort) {
        continue;
      }
      Thread.sleep(100);
      listener.setPort(port);
      try {
        bindListener(listener);
        return;
      } catch (BindException ex) {
        // Ignore exception. Move to next port.
        ioException = ex;
      }
    }
{code}

Stacktrace:

{code:java}
 HttpServer.start() threw a non Bind IOException | HttpServer2.java:1142
java.io.IOException: Failed to bind to xxxxx/xxx.xx.xx.xx:27101
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:346)
	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:307)
	at org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1190)
	at org.apache.hadoop.http.HttpServer2.bindForPortRange(HttpServer2.java:1258)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1282)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:451)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:440)
	at org.apache.hadoop.mapreduce.v2.app.client.MRClientService.serviceStart(MRClientService.java:148)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1378)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$7.run(MRAppMaster.java:1998)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1994)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1890)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:220)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:342)
	... 17 more
{code}
"
Update Hadoop's lz4 to v1.9.2,13318441,Resolved,Major,Fixed,21/Jul/20 09:04,18/Oct/20 13:09,3.4.0,Update hadoop's native lz4 to v1.9.2 
Extend CallerContext to make it include many items,13328440,Resolved,Major,Fixed,21/Sep/20 02:05,03/Oct/20 17:03,3.3.5,"Now context is string. We need to extend the CallerContext because context may contains many items.
Items include 
* router ip
* MR or CLI
* etc"
Service-user cost shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost.,13328789,Resolved,Major,Fixed,22/Sep/20 14:02,30/Sep/20 04:20,3.4.0,HADOOP-17165 has introduced a very useful feature: service-user. After this feature I think we shouldn't add the service-user's cost into totalDecayedCallCost and totalRawCallCost anymore. Because it may give all the identities the priority 0(Supposing we have a big service-user).
Support BCFKS keystores for Hadoop Credential Provider,13329281,Resolved,Major,Fixed,24/Sep/20 20:16,30/Sep/20 03:16,3.3.0,"Hadoop Credential Provider provides an extensible mechanism to manage sensitive tokens like passwords for the cluster. It currently only support JCEKS store type from JDK. 

This ticket is opened to add support BCFKS (Bouncy Castle FIPS) key store type for some higher security requirement use cases assuming OS/JDK has been updated with FIPS security provider for Java Security. "
Hadoop - Upgrade to JQuery 3.5.1,13328984,Resolved,Major,Fixed,23/Sep/20 12:12,24/Sep/20 12:19,3.3.1,jQuery version is being upgraded from jquery-3.4.1.min.js to jquery-3.5.1.min.js
Adding ReadAhead Counters in ABFS,13315372,Resolved,Major,Fixed,07/Jul/20 10:49,22/Jul/20 17:25,3.3.0,"Adding ReadAheads Counters in ABFS to track the behavior of the ReadAhead feature in ABFS. This would include 2 counters:
|READ_AHEAD_BYTES_READ|number of bytes read by readAhead|
|READ_AHEAD_REMOTE_BYTES_READ|number of bytes not used after readAhead was used|"
Implement service-user feature in DecayRPCScheduler,13320141,Resolved,Major,Fixed,30/Jul/20 08:11,09/Sep/20 16:57,3.4.0,"In our cluster, we want to use FairCallQueue to limit heavy users, but not want to restrict certain users who are submitting important requests. This jira proposes to implement the service-user feature that the user is always scheduled high-priority queue.
According to HADOOP-9640, the initial concept of FCQ has this feature, but not implemented finally."
Replace Guava Sets usage by Hadoop's own Sets in hadoop-common and hadoop-tools,13315425,Resolved,Major,Fixed,07/Jul/20 14:57,20/May/21 16:15,3.4.0,"Unjustified usage of Guava API to initialize a {{HashSet}}. This should be replaced by Java APIs.


{code:java}
Targets
    Occurrences of 'Sets.newHashSet' in project
Found Occurrences  (223 usages found)
    org.apache.hadoop.crypto.key  (2 usages found)
        TestValueQueue.java  (2 usages found)
            testWarmUp()  (2 usages found)
                106 Assert.assertEquals(Sets.newHashSet(""k1"", ""k2"", ""k3""),
                107 Sets.newHashSet(fillInfos[0].key,
    org.apache.hadoop.crypto.key.kms  (6 usages found)
        TestLoadBalancingKMSClientProvider.java  (6 usages found)
            testCreation()  (6 usages found)
                86 assertEquals(Sets.newHashSet(""http://host1:9600/kms/foo/v1/""),
                87 Sets.newHashSet(providers[0].getKMSUrl()));
                95 assertEquals(Sets.newHashSet(""http://host1:9600/kms/foo/v1/"",
                98 Sets.newHashSet(providers[0].getKMSUrl(),
                108 assertEquals(Sets.newHashSet(""http://host1:9600/kms/foo/v1/"",
                111 Sets.newHashSet(providers[0].getKMSUrl(),
    org.apache.hadoop.crypto.key.kms.server  (1 usage found)
        KMSAudit.java  (1 usage found)
            59 static final Set<KMS.KMSOp> AGGREGATE_OPS_WHITELIST = Sets.newHashSet(
    org.apache.hadoop.fs.s3a  (1 usage found)
        TestS3AAWSCredentialsProvider.java  (1 usage found)
            testFallbackToDefaults()  (1 usage found)
                183 Sets.newHashSet());
    org.apache.hadoop.fs.s3a.auth  (1 usage found)
        AssumedRoleCredentialProvider.java  (1 usage found)
            AssumedRoleCredentialProvider(URI, Configuration)  (1 usage found)
                113 Sets.newHashSet(this.getClass()));
    org.apache.hadoop.fs.s3a.commit.integration  (1 usage found)
        ITestS3ACommitterMRJob.java  (1 usage found)
            test_200_execute()  (1 usage found)
                232 Set<String> expectedKeys = Sets.newHashSet();
    org.apache.hadoop.fs.s3a.commit.staging  (5 usages found)
        TestStagingCommitter.java  (3 usages found)
            testSingleTaskMultiFileCommit()  (1 usage found)
                341 Set<String> keys = Sets.newHashSet();
            runTasks(JobContext, int, int)  (1 usage found)
                603 Set<String> uploads = Sets.newHashSet();
            commitTask(StagingCommitter, TaskAttemptContext, int)  (1 usage found)
                640 Set<String> files = Sets.newHashSet();
        TestStagingPartitionedTaskCommit.java  (2 usages found)
            verifyFilesCreated(PartitionedStagingCommitter)  (1 usage found)
                148 Set<String> files = Sets.newHashSet();
            buildExpectedList(StagingCommitter)  (1 usage found)
                188 Set<String> expected = Sets.newHashSet();
    org.apache.hadoop.hdfs  (5 usages found)
        DFSUtil.java  (2 usages found)
            getNNServiceRpcAddressesForCluster(Configuration)  (1 usage found)
                615 Set<String> availableNameServices = Sets.newHashSet(conf
            getNNLifelineRpcAddressesForCluster(Configuration)  (1 usage found)
                660 Set<String> availableNameServices = Sets.newHashSet(conf
        MiniDFSCluster.java  (1 usage found)
            597 private Set<FileSystem> fileSystems = Sets.newHashSet();
        TestDFSUtil.java  (2 usages found)
            testGetNNServiceRpcAddressesForNsIds()  (2 usages found)
                1046 assertEquals(Sets.newHashSet(""nn1""), internal);
                1049 assertEquals(Sets.newHashSet(""nn1"", ""nn2""), all);
    org.apache.hadoop.hdfs.net  (5 usages found)
        TestDFSNetworkTopology.java  (5 usages found)
            testChooseRandomWithStorageType()  (4 usages found)
                277 Sets.newHashSet(""host2"", ""host4"", ""host5"", ""host6"");
                278 Set<String> archiveUnderL1 = Sets.newHashSet(""host1"", ""host3"");
                279 Set<String> ramdiskUnderL1 = Sets.newHashSet(""host7"");
                280 Set<String> ssdUnderL1 = Sets.newHashSet(""host8"");
            testChooseRandomWithStorageTypeWithExcluded()  (1 usage found)
                363 Set<String> expectedSet = Sets.newHashSet(""host4"", ""host5"");
    org.apache.hadoop.hdfs.qjournal.server  (2 usages found)
        JournalNodeSyncer.java  (2 usages found)
            getOtherJournalNodeAddrs()  (1 usage found)
                276 HashSet<String> sharedEditsUri = Sets.newHashSet();
            getJournalAddrList(String)  (1 usage found)
                318 Sets.newHashSet(jn.getBoundIpcAddress()));
    org.apache.hadoop.hdfs.server.datanode  (5 usages found)
        BlockPoolManager.java  (1 usage found)
            doRefreshNamenodes(Map<String, Map<String, InetSocketAddress>>, Map<String, Map<String, InetSocketAddress>>)  (1 usage found)
                198 toRemove = Sets.newHashSet(Sets.difference(
        BPOfferService.java  (2 usages found)
            refreshNNList(String, List<String>, ArrayList<InetSocketAddress>, ArrayList<InetSocketAddress>)  (2 usages found)
                146 Set<InetSocketAddress> oldAddrs = Sets.newHashSet();
                150 Set<InetSocketAddress> newAddrs = Sets.newHashSet(addrs);
        TestRefreshNamenodes.java  (2 usages found)
            testRefreshNamenodes()  (2 usages found)
                74 Set<InetSocketAddress> nnAddrsFromCluster = Sets.newHashSet();
                80 Set<InetSocketAddress> nnAddrsFromDN = Sets.newHashSet();
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (1 usage found)
        FsDatasetImpl.java  (1 usage found)
            getInitialVolumeFailureInfos(Collection<StorageLocation>, DataStorage)  (1 usage found)
                430 Set<StorageLocation> failedLocationSet = Sets.newHashSetWithExpectedSize(
    org.apache.hadoop.hdfs.server.namenode  (11 usages found)
        FSImageTestUtil.java  (2 usages found)
            assertFileContentsSame(File...)  (1 usage found)
                418 if (Sets.newHashSet(md5s.values()).size() > 1) {
            assertFileContentsDifferent(int, File...)  (1 usage found)
                435 if (Sets.newHashSet(md5s.values()).size() != expectedUniqueHashes) {
        TestFsck.java  (6 usages found)
            testFsckMove()  (5 usages found)
                388 new CorruptedTestFile(fileNames[0], Sets.newHashSet(0),
                390 new CorruptedTestFile(fileNames[1], Sets.newHashSet(2, 3),
                392 new CorruptedTestFile(fileNames[2], Sets.newHashSet(4),
                394 new CorruptedTestFile(fileNames[3], Sets.newHashSet(0, 1, 2, 3),
                396 new CorruptedTestFile(fileNames[4], Sets.newHashSet(1, 2, 3, 4),
            testFsckMoveAfterCorruption()  (1 usage found)
                2217 Sets.newHashSet(0), dfsClient, numDatanodes, dfsBlockSize);
        TestNameNodeRecovery.java  (3 usages found)
            getValidTxIds()  (1 usage found)
                303 return Sets.newHashSet(0L);
            getValidTxIds()  (1 usage found)
                345 return Sets.newHashSet(0L);
            getValidTxIds()  (1 usage found)
                391 return Sets.newHashSet(1L , 2L, 3L, 5L, 6L, 7L, 8L, 9L, 10L);
    org.apache.hadoop.mapred  (1 usage found)
        TestMRTimelineEventHandling.java  (1 usage found)
            checkNewTimelineEvent(ApplicationId, ApplicationReport, String)  (1 usage found)
                302 Set<String> cfgsToCheck = Sets.newHashSet(""dummy_conf1"", ""dummy_conf2"",
    org.apache.hadoop.mapreduce.lib.input  (2 usages found)
        TestFileInputFormat.java  (2 usages found)
            verifyFileStatuses(List<Path>, List<FileStatus>, FileSystem)  (1 usage found)
                408 Set<Path> expectedPathSet = Sets.newHashSet(fqExpectedPaths);
            verifySplits(List<String>, List<InputSplit>)  (1 usage found)
                426 Set<String> expectedSet = Sets.newHashSet(expected);
    org.apache.hadoop.mapreduce.v2.app.webapp  (1 usage found)
        TestAMWebServices.java  (1 usage found)
            configureServlets()  (1 usage found)
                78 appContext.setBlacklistedNodes(Sets.newHashSet(""badnode1"", ""badnode2""));
    org.apache.hadoop.mapreduce.v2.hs.webapp  (4 usages found)
        TestHsWebServicesLogs.java  (4 usages found)
            testGetAggregatedLogsMetaForFinishedApp()  (1 usage found)
                302 Set<String> expectedIdStrings = Sets.newHashSet(
            testGetAggregatedLogsMetaForRunningApp()  (1 usage found)
                335 Set<String> expectedIdStrings = Sets.newHashSet(
            testGetAggregatedLogsMetaForFinishedAppAttempt()  (1 usage found)
                368 Set<String> expectedIdStrings = Sets.newHashSet(
            testGetAggregatedLogsMetaForRunningAppAttempt()  (1 usage found)
                401 Set<String> expectedIdStrings = Sets.newHashSet(
    org.apache.hadoop.metrics2.lib  (2 usages found)
        MutableRates.java  (1 usage found)
            47 private final Set<Class<?>> protocolCache = Sets.newHashSet();
        MutableRatesWithAggregation.java  (1 usage found)
            55 private final Set<Class<?>> protocolCache = Sets.newHashSet();
    org.apache.hadoop.tools  (2 usages found)
        CopyListing.java  (2 usages found)
            validateFinalListing(Path, DistCpContext)  (2 usages found)
                166 Set<URI> aclSupportCheckFsSet = Sets.newHashSet();
                167 Set<URI> xAttrSupportCheckFsSet = Sets.newHashSet();
    org.apache.hadoop.tools.dynamometer  (3 usages found)
        TestDynamometerInfra.java  (3 usages found)
            setupClass()  (3 usages found)
                272 Sets.newHashSet(NAMENODE_NODELABEL, DATANODE_NODELABEL));
                275 Sets.newHashSet(NAMENODE_NODELABEL));
                277 Sets.newHashSet(DATANODE_NODELABEL));
    org.apache.hadoop.yarn.api  (1 usage found)
        BasePBImplRecordsTest.java  (1 usage found)
            genTypeValue(Type)  (1 usage found)
                115 ret = Sets.newHashSet(genTypeValue(params[0]));
    org.apache.hadoop.yarn.api.protocolrecords.impl.pb  (1 usage found)
        GetNodesToLabelsResponsePBImpl.java  (1 usage found)
            initNodeToLabels()  (1 usage found)
                70 Sets.newHashSet(c.getNodeLabelsList()));
    org.apache.hadoop.yarn.api.resource  (3 usages found)
        TestPlacementConstraintParser.java  (3 usages found)
            testTargetExpressionParser()  (1 usage found)
                116 Set<TargetExpression> expectedTargetExpressions = Sets.newHashSet(
            testCardinalityConstraintParser()  (1 usage found)
                169 Set<TargetExpression> expectedTargetExpressions = Sets.newHashSet(
            testParseAllocationTagNameSpace()  (1 usage found)
                583 Set<TargetExpression> expectedTargetExpressions = Sets.newHashSet(
    org.apache.hadoop.yarn.client.cli  (10 usages found)
        TestYarnCLI.java  (10 usages found)
            testGetApplications()  (10 usages found)
                410 Sets.newHashSet(""tag1"", ""tag3""), false, Priority.UNDEFINED, """", """");
                421 null, Sets.newHashSet(""tag2"", ""tag3""), false, Priority.UNDEFINED,
                431 null, Sets.newHashSet(""tag1"", ""tag4""), false, Priority.UNDEFINED,
                441 ""NON-MAPREDUCE"", null, Sets.newHashSet(""tag1""), false,
                451 Sets.newHashSet(""tag2"", ""tag4""), false, Priority.UNDEFINED, """", """");
                756 Set<String> appTag1 = Sets.newHashSet(""tag1"");
                829 Set<String> appType9 = Sets.newHashSet(""YARN"");
                830 Set<String> appTag2 = Sets.newHashSet(""tag3"");
                858 Set<String> appType10 = Sets.newHashSet(""HIVE"");
                859 Set<String> appTag3 = Sets.newHashSet(""tag4"");
    org.apache.hadoop.yarn.logaggregation  (1 usage found)
        AggregatedLogFormat.java  (1 usage found)
            getFileCandidates(Set<File>, boolean)  (1 usage found)
                366 return Sets.newHashSet(mask);
    org.apache.hadoop.yarn.logaggregation.filecontroller  (1 usage found)
        LogAggregationFileController.java  (1 usage found)
            cleanOldLogs(Path, NodeId, UserGroupInformation)  (1 usage found)
                545 status = Sets.newHashSet(mask);
    org.apache.hadoop.yarn.logaggregation.filecontroller.ifile  (1 usage found)
        LogAggregationIndexedFileController.java  (1 usage found)
            parseCheckSumFiles(List<FileStatus>)  (1 usage found)
                718 status = Sets.newHashSet(mask);
    org.apache.hadoop.yarn.nodelabels  (7 usages found)
        NodeLabelTestBase.java  (1 usage found)
            toSet(E...)  (1 usage found)
                123 Set<E> set = Sets.newHashSet(elements);
        TestCommonNodeLabelsManager.java  (6 usages found)
            testAddRemovelabel()  (5 usages found)
                65 verifyNodeLabelAdded(Sets.newHashSet(""hello""), mgr.lastAddedlabels);
                69 verifyNodeLabelAdded(Sets.newHashSet(""hello1"", ""world1""), mgr.lastAddedlabels);
                72 Sets.newHashSet(""hello"", ""world"", ""hello1"", ""world1"")));
                103 assertCollectionEquals(Sets.newHashSet(""hello""), mgr.lastRemovedlabels);
                109 Assert.assertTrue(mgr.lastRemovedlabels.containsAll(Sets.newHashSet(
            testAddlabelWithCase()  (1 usage found)
                118 verifyNodeLabelAdded(Sets.newHashSet(""HeLlO""), mgr.lastAddedlabels);
    org.apache.hadoop.yarn.nodelabels.store.op  (1 usage found)
        RemoveClusterLabelOp.java  (1 usage found)
            write(OutputStream, CommonNodeLabelsManager)  (1 usage found)
                48 .newInstance(Sets.newHashSet(labels.iterator()))).getProto()
    org.apache.hadoop.yarn.server.api.protocolrecords  (1 usage found)
        TestProtocolRecords.java  (1 usage found)
            testNodeHeartBeatRequest()  (1 usage found)
                185 Sets.newHashSet(NodeAttribute.newInstance(""attributeA"",
    org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb  (1 usage found)
        ReplaceLabelsOnNodeRequestPBImpl.java  (1 usage found)
            initNodeToLabels()  (1 usage found)
                66 Sets.newHashSet(c.getNodeLabelsList()));
    org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer  (2 usages found)
        TestResourceLocalizationService.java  (2 usages found)
            doLocalization(ResourceLocalizationService, DrainDispatcher, DummyExecutor, DeletionService)  (1 usage found)
                1400 Sets.newHashSet(new Path(locPath1), new Path(locPath1 + ""_tmp""),
            doLocalizationAfterCleanup(ResourceLocalizationService, DrainDispatcher, DummyExecutor, DeletionService)  (1 usage found)
                1547 Sets.newHashSet(new Path(locPath1), new Path(locPath1 + ""_tmp""),
    org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation  (1 usage found)
        AppLogAggregatorImpl.java  (1 usage found)
            doContainerLogAggregation(LogAggregationFileController, boolean, boolean)  (1 usage found)
                674 this.uploadedFileMeta = Sets.newHashSet(mask);
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec  (1 usage found)
        TestNECVEPlugin.java  (1 usage found)
            testFindDevicesWithUdev()  (1 usage found)
                388 .thenReturn(Sets.newHashSet(testDevice));
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga  (1 usage found)
        FpgaDiscoverer.java  (1 usage found)
            discover()  (1 usage found)
                123 Set<String> minors = Sets.newHashSet(allowed.split("",""));
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu  (1 usage found)
        GpuDiscoverer.java  (1 usage found)
            lookupBinaryInDefaultDirsInternal()  (1 usage found)
                328 Set<String> triedBinaryPaths = Sets.newHashSet();
    org.apache.hadoop.yarn.server.resourcemanager  (5 usages found)
        TestAppManager.java  (1 usage found)
            testEscapeApplicationSummary()  (1 usage found)
                977 when(app.getApplicationTags()).thenReturn(Sets.newHashSet(""tag2"", ""tag1""));
        TestClientRMService.java  (3 usages found)
            testGetApplications()  (3 usages found)
                1442 tagSet = Sets.newHashSet(tags.get(0));
                1447 tagSet = Sets.newHashSet(tags.get(1));
                1452 tagSet = Sets.newHashSet(tags.get(2));
        TestRMRestart.java  (1 usage found)
            toSet(E...)  (1 usage found)
                2587 Set<E> set = Sets.newHashSet(elements);
    org.apache.hadoop.yarn.server.resourcemanager.nodelabels  (11 usages found)
        TestNodeAttributesManager.java  (11 usages found)
            testAddNodeAttributes()  (2 usages found)
                132 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[0]));
                138 .getClusterNodeAttributes(Sets.newHashSet(""non_exist_prefix""));
            testRemoveNodeAttributes()  (4 usages found)
                219 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[0]));
                222 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[1]));
                225 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[2]));
                263 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[0]));
            testReplaceNodeAttributes()  (5 usages found)
                299 Sets.newHashSet(NodeAttribute.PREFIX_DISTRIBUTED, PREFIXES[0]));
                313 Sets.newHashSet(NodeAttribute.PREFIX_DISTRIBUTED, PREFIXES[0]));
                340 Sets.newHashSet(NodeAttribute.PREFIX_DISTRIBUTED));
                357 .getClusterNodeAttributes(Sets.newHashSet(PREFIXES[1]));
                360 .getClusterNodeAttributes(Sets.newHashSet(
    org.apache.hadoop.yarn.server.resourcemanager.scheduler  (5 usages found)
        TestAbstractYarnScheduler.java  (3 usages found)
            testContainerReleaseWithAllocationTags()  (2 usages found)
                469 Sets.newHashSet(testTag1),
                477 Sets.newHashSet(testTag2),
            testNodeRemovedWithAllocationTags()  (1 usage found)
                578 Sets.newHashSet(testTag1),
        TestSchedulerUtils.java  (2 usages found)
            testValidateResourceRequestWithErrorLabelsPermission()  (1 usage found)
                246 Set<String> queueAccessibleNodeLabels = Sets.newHashSet();
            testNormalizeNodeLabelExpression()  (1 usage found)
                795 Set<String> queueAccessibleNodeLabels = Sets.newHashSet();
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity  (8 usages found)
        CapacitySchedulerTestBase.java  (1 usage found)
            toSet(E...)  (1 usage found)
                56 Set<E> set = Sets.newHashSet(elements);
        TestApplicationLimits.java  (1 usage found)
            toSet(String...)  (1 usage found)
                790 Set<String> set = Sets.newHashSet(elements);
        TestCapacityScheduler.java  (1 usage found)
            testParseQueueWithAbsoluteResource()  (1 usage found)
                1042 Sets.newHashSet(labelName));
        TestCapacitySchedulerMaxParallelApps.java  (1 usage found)
            executeCommonStepsAndChecks()  (1 usage found)
                200 Sets.newHashSet(
        TestCapacitySchedulerNodeLabelUpdate.java  (1 usage found)
            toSet(String...)  (1 usage found)
                144 Set<String> set = Sets.newHashSet(elements);
        TestNodeLabelContainerAllocation.java  (1 usage found)
            toSet(E...)  (1 usage found)
                157 Set<E> set = Sets.newHashSet(elements);
        TestUtils.java  (1 usage found)
            toSet(E...)  (1 usage found)
                262 Set<E> set = Sets.newHashSet(elements);
        TestWorkPreservingRMRestartForNodeLabel.java  (1 usage found)
            toSet(E...)  (1 usage found)
                77 Set<E> set = Sets.newHashSet(elements);
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint  (3 usages found)
        TestPlacementConstraintManagerService.java  (3 usages found)
            testGetRequestConstraint()  (3 usages found)
                209 Sets.newHashSet(""not_exist_tag""), null);
                228 Sets.newHashSet(sourceTag1), null);
                256 Sets.newHashSet(sourceTag1), c1);
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair  (2 usages found)
        TestFairScheduler.java  (1 usage found)
            testGetAppsInQueue()  (1 usage found)
                4455 Set<ApplicationAttemptId> appAttIds = Sets.newHashSet(apps.get(0), apps.get(1));
        TestQueueManager.java  (1 usage found)
            updateConfiguredLeafQueues(QueueManager, String...)  (1 usage found)
                185 .addAll(Sets.newHashSet(confLeafQueues));
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation  (1 usage found)
        AllocationFileParser.java  (1 usage found)
            81 Sets.newHashSet(QUEUE_MAX_RESOURCES_DEFAULT, USER_MAX_APPS_DEFAULT,
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter  (8 usages found)
        TestFSQueueConverter.java  (8 usages found)
            testConvertQueueHierarchy()  (1 usage found)
                168 Sets.newHashSet(""root"",
            testQueueMaxAMShare()  (1 usage found)
                192 Sets.newHashSet(""root.admins.bob"", ""root.admins.alice""));
            testQueueMaxParallelApps()  (1 usage found)
                208 Sets.newHashSet(""root.admins.alice""));
            testQueueMaxAllocations()  (1 usage found)
                232 Sets.newHashSet(""root.admins"", ""root.users.john""));
            testQueuePreemptionDisabled()  (1 usage found)
                251 Sets.newHashSet(""root.admins.alice"", ""root.users.joe""));
            testQueueAutoCreateChildQueue()  (2 usages found)
                319 Set<String> parentQueues = Sets.newHashSet(
                323 Set<String> leafQueues = Sets.newHashSet(
            64 Sets.newHashSet(""root"",
    org.apache.hadoop.yarn.server.resourcemanager.webapp  (25 usages found)
        ApplicationsRequestBuilder.java  (5 usages found)
            34 private Set<String> statesQuery = Sets.newHashSet();
            35 private Set<String> users = Sets.newHashSetWithExpectedSize(1);
            36 private Set<String> queues = Sets.newHashSetWithExpectedSize(1);
            45 private Set<String> appTypes = Sets.newHashSet();
            46 private Set<String> appTags = Sets.newHashSet();
        RMWebAppFilter.java  (1 usage found)
            71 private static final Set<String> NON_REDIRECTED_URIS = Sets.newHashSet(
        TestApplicationsRequestBuilder.java  (16 usages found)
            testRequestWithValidStateQuery()  (1 usage found)
                86 Sets.newHashSet(YarnApplicationState.NEW_SAVING.toString());
            testRequestWithEmptyStateQueries()  (1 usage found)
                96 .withStatesQuery(Sets.newHashSet()).build();
            testRequestWithInvalidStateQueries()  (1 usage found)
                106 .withStatesQuery(Sets.newHashSet(""a1"", ""a2"", """")).build();
            testRequestWithValidStateQueries()  (2 usages found)
                127 Sets.newHashSet(YarnApplicationState.NEW_SAVING.toString(),
                133 Sets.newHashSet(YarnApplicationState.NEW_SAVING.toString(),
            testRequestWithUserQuery()  (1 usage found)
                165 expectedRequest.setUsers(Sets.newHashSet(""user1""));
            testRequestWithQueueQueryExistingQueue()  (1 usage found)
                196 expectedRequest.setQueues(Sets.newHashSet(""queue1""));
            testRequestWithQueueQueryNotExistingQueue()  (1 usage found)
                212 expectedRequest.setQueues(Sets.newHashSet(""queue1""));
            testRequestWithEmptyApplicationTypesQuery()  (2 usages found)
                487 .withApplicationTypes(Sets.newHashSet()).build();
                490 expectedRequest.setApplicationTypes(Sets.newHashSet());
            testRequestWithValidApplicationTypesQuery()  (2 usages found)
                497 .withApplicationTypes(Sets.newHashSet(""type1"")).build();
                500 expectedRequest.setApplicationTypes(Sets.newHashSet(""type1""));
            testRequestWithEmptyApplicationTagsQuery()  (2 usages found)
                513 .withApplicationTags(Sets.newHashSet()).build();
                516 expectedRequest.setApplicationTags(Sets.newHashSet());
            testRequestWithValidApplicationTagsQuery()  (2 usages found)
                523 .withApplicationTags(Sets.newHashSet(""tag1"")).build();
                526 expectedRequest.setApplicationTags(Sets.newHashSet(""tag1""));
        TestRMWebServiceAppsNodelabel.java  (1 usage found)
            toSet(E...)  (1 usage found)
                242 Set<E> set = Sets.newHashSet(elements);
        TestRMWebServicesApps.java  (1 usage found)
            getApplicationIds(JSONArray)  (1 usage found)
                107 Set<String> ids = Sets.newHashSet();
        TestRMWebServicesForCSWithPartitions.java  (1 usage found)
            testPartitionInSchedulerActivities()  (1 usage found)
                273 .of(NodeId.newInstance(""127.0.0.1"", 0), Sets.newHashSet(LABEL_LX)));
    org.apache.hadoop.yarn.server.resourcemanager.webapp.fairscheduler  (4 usages found)
        FairSchedulerJsonVerifications.java  (2 usages found)
            FairSchedulerJsonVerifications(List<String>)  (1 usage found)
                49 this.customResourceTypes = Sets.newHashSet(customResourceTypes);
            42 Sets.newHashSet(""minResources"", ""amUsedResources"", ""amMaxResources"",
        FairSchedulerXmlVerifications.java  (2 usages found)
            FairSchedulerXmlVerifications(List<String>)  (1 usage found)
                53 this.customResourceTypes = Sets.newHashSet(customResourceTypes);
            46 private static final Set<String> RESOURCE_FIELDS = Sets.newHashSet(
    org.apache.hadoop.yarn.server.resourcemanager.webapp.helper  (1 usage found)
        ResourceRequestsXmlVerifications.java  (1 usage found)
            extractActualCustomResourceType(Element, List<String>)  (1 usage found)
                85 Sets.newHashSet(expectedResourceTypes));
    org.apache.hadoop.yarn.server.timeline  (3 usages found)
        EntityGroupPlugInForTest.java  (3 usages found)
            getTimelineEntityGroupId(String, NameValuePair, Collection<NameValuePair>)  (1 usage found)
                39 return Sets.newHashSet(getStandardTimelineGroupId(appId));
            getTimelineEntityGroupId(String, String)  (1 usage found)
                47 return Sets.newHashSet(getStandardTimelineGroupId(appId));
            getTimelineEntityGroupId(String, SortedSet<String>, Set<String>)  (1 usage found)
                54 return Sets.newHashSet();
    org.apache.hadoop.yarn.server.timelineservice.collector  (1 usage found)
        TestTimelineCollector.java  (1 usage found)
            testClearPreviousEntitiesOnAggregation()  (1 usage found)
                306 assertEquals(Sets.newHashSet(""type""), aggregationGroups.keySet());
    org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb  (1 usage found)
        CosmosDBDocumentStoreReader.java  (1 usage found)
            fetchEntityTypes(String, TimelineReaderContext)  (1 usage found)
                118 return Sets.newHashSet(client.queryDocuments(
    org.apache.hadoop.yarn.server.timelineservice.reader  (38 usages found)
        TestTimelineReaderWebServicesHBaseStorage.java  (16 usages found)
            loadData()  (16 usages found)
                266 Sets.newHashSet(""entity21"", ""entity22"", ""entity23"", ""entity24""));
                267 isRelatedTo1.put(""type4"", Sets.newHashSet(""entity41"", ""entity42""));
                268 isRelatedTo1.put(""type1"", Sets.newHashSet(""entity14"", ""entity15""));
                270 Sets.newHashSet(""entity31"", ""entity35"", ""entity32"", ""entity33""));
                274 Sets.newHashSet(""entity21"", ""entity22"", ""entity23"", ""entity24""));
                275 relatesTo1.put(""type4"", Sets.newHashSet(""entity41"", ""entity42""));
                276 relatesTo1.put(""type1"", Sets.newHashSet(""entity14"", ""entity15""));
                278 Sets.newHashSet(""entity31"", ""entity35"", ""entity32"", ""entity33""));
                327 Sets.newHashSet(""entity21"", ""entity22"", ""entity23"", ""entity24""));
                328 isRelatedTo2.put(""type5"", Sets.newHashSet(""entity51"", ""entity52""));
                329 isRelatedTo2.put(""type6"", Sets.newHashSet(""entity61"", ""entity66""));
                330 isRelatedTo2.put(""type3"", Sets.newHashSet(""entity31""));
                334 Sets.newHashSet(""entity21"", ""entity22"", ""entity23"", ""entity24""));
                335 relatesTo2.put(""type5"", Sets.newHashSet(""entity51"", ""entity52""));
                336 relatesTo2.put(""type6"", Sets.newHashSet(""entity61"", ""entity66""));
                337 relatesTo2.put(""type3"", Sets.newHashSet(""entity31""));
        TestTimelineReaderWebServicesUtils.java  (22 usages found)
            testRelationFiltersParsing()  (22 usages found)
                721 ""type1"", Sets.newHashSet((Object)""entity11"")),
                723 ""type2"", Sets.newHashSet((Object)""entity21"", ""entity22""))
                737 ""type1"", Sets.newHashSet((Object)""entity11"")),
                739 ""type2"", Sets.newHashSet((Object)""entity21"", ""entity22""))
                743 ""type3"", Sets.newHashSet(
                746 ""type1"", Sets.newHashSet((Object)""entity11"", ""entity12""))
                758 ""type1"", Sets.newHashSet((Object)""entity11"")),
                760 ""type2"", Sets.newHashSet((Object)""entity21"", ""entity22"")),
                762 ""type5"", Sets.newHashSet((Object)""entity51""))
                766 ""type3"", Sets.newHashSet(
                769 ""type1"", Sets.newHashSet((Object)""entity11"", ""entity12""))
                784 ""type1"", Sets.newHashSet((Object)""entity11"")),
                786 ""type2"", Sets.newHashSet(
                789 ""type5"", Sets.newHashSet((Object)""entity51""))
                793 ""type3"", Sets.newHashSet(
                796 ""type1"", Sets.newHashSet(
                803 ""type11"", Sets.newHashSet((Object)""entity111""))
                807 ""type4"", Sets.newHashSet((Object)""entity43"", ""entity44"",
                810 ""type7"", Sets.newHashSet((Object)""entity71""))
                817 ""type2"", Sets.newHashSet((Object)""entity2"")),
                819 ""type8"", Sets.newHashSet((Object)""entity88""))
                822 Sets.newHashSet((Object)""e"", ""e1""))
    org.apache.hadoop.yarn.service  (1 usage found)
        TestApiServer.java  (1 usage found)
            testUpgradeSingleInstance()  (1 usage found)
                533 mockServerClient.setExpectedInstances(Sets.newHashSet(
    org.apache.hadoop.yarn.service.utils  (2 usages found)
        ServiceApiUtil.java  (2 usages found)
            validateAndResolveCompsUpgrade(Service, Collection<String>)  (1 usage found)
                625 HashSet<String> requestedComps = Sets.newHashSet(compNames);
            validateAndResolveCompsStable(Service, Collection<String>)  (1 usage found)
                651 HashSet<String> requestedComps = Sets.newHashSet(compNames);
    org.apache.hadoop.yarn.service.webapp  (1 usage found)
        ApiServer.java  (1 usage found)
            updateComponent(HttpServletRequest, String, String, Component)  (1 usage found)
                374 Sets.newHashSet(componentName));
    org.apache.hadoop.yarn.webapp.hamlet  (2 usages found)
        HamletGen.java  (2 usages found)
            67 final Set<String> endTagOptional = Sets.newHashSet();
            68 final Set<String> inlineElements = Sets.newHashSet();
    org.apache.hadoop.yarn.webapp.hamlet2  (2 usages found)
        HamletGen.java  (2 usages found)
            65 final Set<String> endTagOptional = Sets.newHashSet();
            66 final Set<String> inlineElements = Sets.newHashSet();
    patches  (4 usages found)
        HADOOP-17101.001.patch  (2 usages found)
            226      Set<Path> expectedPathSet = Sets.newHashSet(fqExpectedPaths);
            244      Set<String> expectedSet = Sets.newHashSet(expected);
        HADOOP-17101.002.patch  (2 usages found)
            456      Set<Path> expectedPathSet = Sets.newHashSet(fqExpectedPaths);
            474      Set<String> expectedSet = Sets.newHashSet(expected);

{code}
"
Implement HttpServer2 metrics,13317101,Resolved,Major,Fixed,16/Jul/20 07:02,25/Mar/21 19:10,3.4.0,"I'd like to collect metrics (number of connections, average response time, etc...) from HttpFS and KMS but there are no metrics for HttpServer2."
Implement wrapper for guava newArrayList and newLinkedList,13318950,Resolved,Major,Fixed,23/Jul/20 13:29,03/Jun/21 10:22,3.3.5,"guava Lists class provide some wrappers to java ArrayList and LinkedList.

Replacing the method calls throughout the code can be invasive because guava offers some APIs that do not exist in java util. This Jira is the task of implementing those missing APIs in hadoop common in a step toward getting rid of guava.
 * create a wrapper class org.apache.hadoop.util.unguava.Lists 
 * implement the following interfaces in Lists:
 ** public static <E> ArrayList<E> newArrayList()
 ** public static <E> ArrayList<E> newArrayList(E... elements)
 ** public static <E> ArrayList<E> newArrayList(Iterable<? extends E> elements)
 ** public static <E> ArrayList<E> newArrayList(Iterator<? extends E> elements)
 ** public static <E> ArrayList<E> newArrayListWithCapacity(int initialArraySize)
 ** public static <E> LinkedList<E> newLinkedList()
 ** public static <E> LinkedList<E> newLinkedList(Iterable<? extends E> elements)
 ** public static <E> List<E> asList(@Nullable E first, E[] rest)"
[JDK11] Fix javadoc errors in hadoop-common module,13320576,Resolved,Major,Fixed,03/Aug/20 02:13,22/Oct/20 18:16,3.4.0,"Full error log: https://gist.github.com/aajisaka/5504dd485cc4bc98a6a245830d450919

{noformat}
[ERROR] /Users/aajisaka/git/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java:57: error: unexpected end tag: </p>
[ERROR]  * </p>
[ERROR]    ^
[ERROR] /Users/aajisaka/git/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java:33: error: self-closing element not allowed
[ERROR]  * <p/>This allows for configuration of how heavily each of the operations
[ERROR]    ^
...
{noformat}

How to reproduce the failure:
* Remove {{<javadoc.skip.jdk11>true</javadoc.skip.jdk11>}} from pom.xml
* Run {{mvn process-sources javadoc:javadoc-no-fork}}"
[JDK 11] Fix javadoc error  in Java API link detection,13320601,Resolved,Major,Fixed,03/Aug/20 07:16,04/Aug/20 19:09,3.4.0,"{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc-no-fork (default-cli) on project hadoop-hdfs-rbf: An error has occurred in Javadoc report generation: 
[ERROR] Exit code: 1 - javadoc: warning - You have specified the HTML version as HTML 4.01 by using the -html4 option.
[ERROR] The default is currently HTML5 and the support for HTML 4.01 will be removed
[ERROR] in a future release. To suppress this warning, please ensure that any HTML constructs
[ERROR] in your comments are valid in HTML5, and remove the -html4 option.
[ERROR] javadoc: error - The code being documented uses modules but the packages defined in https://docs.oracle.com/javase/8/docs/api/ are in the unnamed module.
{noformat}"
Dead links in breadcrumbs,13320825,Resolved,Major,Fixed,04/Aug/20 09:14,08/Aug/20 06:30,3.4.0,"In the breadcrumbs, most of the links are dead.
Can we remove the breadcrumbs?"
Add --mvn-custom-repos parameter to yetus calls,13320926,Resolved,Major,Fixed,04/Aug/20 18:43,05/Aug/20 01:57,3.3.1,"In my request PR [#2188|https://github.com/apache/hadoop/pull/2188], I see the QA build fails with unrelated errors.

One example:
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.1:install (default-install) on project hadoop-yarn-applications-mawo-core: Failed to install metadata org.apache.hadoop.applications.mawo:hadoop-yarn-applications-mawo-core:3.4.0-SNAPSHOT/maven-metadata.xml: Could not read metadata /home/jenkins/.m2/repository/org/apache/hadoop/applications/mawo/hadoop-yarn-applications-mawo-core/3.4.0-SNAPSHOT/maven-metadata-local.xml: input contained no data -> [Help 1]
{code}
Another example:
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.1:install (default-install) on project hadoop-project: Failed to install metadata org.apache.hadoop:hadoop-project:3.4.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hadoop/hadoop-project/3.4.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got n (position: END_TAG seen ...</metadata>\nn... @21:2) -> [Help 1]
{code}
As reported by HBASE-22474 and HBASE-22801, PreCommit validation from yetus uses a shared .m2 repository. By adding {{\-\-mvn-custom-repos-}} paramter, yetus will use a custom .m2 directory for executions for PR validations.

This is a change to mimic that for Hadoop project.

CC: [~aajisaka]"
Move personality file from Yetus to Hadoop repository ,13322598,Resolved,Major,Fixed,13/Aug/20 19:24,18/Aug/20 03:14,3.3.1,"Currently for CI build and testing we maintain personality scripts (i.e., [here|https://github.com/apache/yetus/blob/master/precommit/src/main/shell/personality/hadoop.sh]) in both Apache Yetus and Apache Hadoop. This poses problem when one needs to change both places, for example HADOOP-17125. 

This proposes to move the personality file into the Hadoop repo itself, so that we can manage them in a single place. The downside for this is we may need to duplicate the scripts in every branch. "
LoadBalanceKMSClientProvider#deleteKey should invalidateCache via all KMSClientProvider instances,13322829,Resolved,Major,Fixed,14/Aug/20 23:09,17/Sep/20 17:42,2.8.4,"Without invalidateCache, the deleted key may still exists in the servers' key cache (CachingKeyProvider in KMSWebApp.java)  where the delete key was not hit. Client may still be able to access encrypted files by specifying to connect to KMS instances with a cached version of the deleted key before the cache entry (10 min by default) expired. 

"
Fix build the hadoop-build Docker image failed,13326147,Resolved,Major,Fixed,05/Sep/20 10:23,16/Sep/20 07:48,3.3.1,"When I build the docker-build image under macOS, it failed caused by:
{code:java}
    ----------------------------------------
Command ""/usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-vKHcWu/isort/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-odL0bY-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-vKHcWu/isort/
You are using pip version 8.1.1, however version 20.2.2 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
The command '/bin/bash -o pipefail -c pip2 install     configparser==4.0.2     pylint==1.9.2' returned a non-zero code: 1
{code}"
Allow SSLFactory fallback to input config if ssl-*.xml fail to load from classpath,13327165,Resolved,Major,Fixed,11/Sep/20 22:46,21/Sep/20 19:42,2.8.5,"Some applications like Tez does not have ssl-client.xml and ssl-server.xml in classpath. Instead, it directly pass the parsed SSL configuration as the input configuration object. This ticket is opened to allow this case. TEZ-4096 attempts to solve this issue but but take a different approach which may not work in existing Hadoop clients that use SSLFactory from hadoop-common. "
Switch to Yetus main branch,13327549,Resolved,Major,Fixed,15/Sep/20 01:22,15/Sep/20 02:14,3.4.0,YETUS-986 switched the development branch from master to main.
Support new Instance by non default constructor by ReflectionUtils,13329542,Resolved,Major,Fixed,26/Sep/20 15:47,30/Sep/20 21:23,3.4.0,
Compile Hadoop on Windows natively,13321688,Open,Major,,09/Aug/20 14:55,,3.1.3,"Hadoop compiles flawlessly on Linux whereas, one has to hack around to compile it on Windows. The reason is obvious, there's a very large proportion of C/C++ codebase that's written in a platform specific manner that's amenable to Linux.

Thus, compiling Hadoop on Windows involves using the MinGW or the Cygwin toolchains. This third party dependency might be seen as a risk for those who want to deploy Hadoop on Windows nodes in production. The primary reason for this is that the MinGW/Cygwin runtimes on Windows don't produce core dumps. Whereas the Visual C++ runtime is able to do so, which greatly enables debugging.

Another reason to use the Visual C++ compiler for compiling Hadoop on Windows is that Visual C++ is native to Windows and one will have access to a large collection of tools that can be used for instrumentation, telemetry and fine-tuning the settings of Hadoop to run better on Windows.

Modern C++ has a plethora of APIs that one can use to write safe, cross-platform code, which can be leveraged to refactor the existing Linux specific code, thereby enabling compilation on Windows using the native Visual C++ compiler."
Support Non-Path Based FileSystem API in Regex Mount Points,13326170,Open,Major,,06/Sep/20 04:44,,,"Regex mount points create ChoRootedFileSystem while accessing. And we couldn't know the underlying filesystems ahead. This won't work with non-path based APIs such as  getAdditionalTokenIssuers. Instead of totaly unsuport it, we should support APIs to some extend. The could be done recording the FileSystem created and perform APIs for FileSystem instances created for ViewFileSystem."
pid file delete when service stop (secure datanode ) show cat no directory,13326859,Patch Available,Major,,10/Sep/20 12:10,,3.4.0,"when stop running secure datanode

show cat no directory .

 

when stop unrunning secure datanode

also show cat no pid directory

 

It's both unreasonable"
Backport HADOOP-13230 list/getFileStatus changes for preserved directory markers,13321860,Resolved,Major,Fixed,10/Aug/20 18:54,25/Aug/20 21:53,3.1.3,"Our supported Hadoop releases all need to handle s3a stores where the s3a client does not delete directory markers.

This is only the minimal set of list changes; no intent of changing marker retention policy. It should be transparent to all clients

* list/getFileStatus changes
* backwards compatible test of new algorithm
* new tests to verify marker handling
* marker rename bug  (HADOOP-17200) surfacing in testing

"
Update jackson-mapper-asl-1.9.13 to atlassian version to mitigate: CVE-2019-10172,13324369,Patch Available,Major,,25/Aug/20 03:03,,,"Currently jersey depends on the jackson, and upgradation of jersey from 1.X to 2.x looks complicated(see HADOOP-15984 and HADOOP-16485).

Update jackson-mapper-asl-1.9.13 to atlassian version to mitigate: CVE-2019-10172.

 "
Add unguava implementation for Joiner in hadoop.StringUtils,13314403,Patch Available,Major,,01/Jul/20 03:39,,,"In order to replace  {{com.google.common.base.Joiner}} with a non-guava implementation:
* Extend implementation of {{org.apache.hadoop.util.StringUtils.join()}} to support interfaces offered by guava and used throughout hadoop source code.
* {{org.apache.hadoop.util.StringUtils.join()}} should have the same behavior as {{guava.Joiner}}. For example, by default it throws NPE if any of the elements is null.
* Another Jira should to be created to do the actual call replacement in 67 source files.  
* Another Jira should be created to do the same for apache.common.stringUtils. We do not want to have different implementations of the same functionality, while each may have different behavior. Also, when we have the implementation of Join, in 1 single source code, we can use apache.commons.StringUtils internally without doing invasive code changes to the entire source code.
 
{code:java}
Targets
    Occurrences of 'com.google.common.base.Joiner' in project with mask '*.java'
Found Occurrences  (103 usages found)
    org.apache.hadoop.crypto.key.kms.server  (1 usage found)
        SimpleKMSAuditLogger.java  (1 usage found)
            26 import com.google.common.base.Joiner;
    org.apache.hadoop.fs  (1 usage found)
        TestPath.java  (1 usage found)
            37 import com.google.common.base.Joiner;
    org.apache.hadoop.fs.s3a  (1 usage found)
        StorageStatisticsTracker.java  (1 usage found)
            25 import com.google.common.base.Joiner;
    org.apache.hadoop.ha  (1 usage found)
        TestHAAdmin.java  (1 usage found)
            34 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs  (8 usages found)
        DFSClient.java  (1 usage found)
            196 import com.google.common.base.Joiner;
        DFSTestUtil.java  (1 usage found)
            76 import com.google.common.base.Joiner;
        DFSUtil.java  (1 usage found)
            108 import com.google.common.base.Joiner;
        DFSUtilClient.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        HAUtil.java  (1 usage found)
            59 import com.google.common.base.Joiner;
        MiniDFSCluster.java  (1 usage found)
            145 import com.google.common.base.Joiner;
        StripedFileTestUtil.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        TestDFSUpgrade.java  (1 usage found)
            53 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.protocol  (1 usage found)
        LayoutFlags.java  (1 usage found)
            26 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.protocolPB  (1 usage found)
        TestPBHelper.java  (1 usage found)
            118 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.qjournal  (1 usage found)
        MiniJournalCluster.java  (1 usage found)
            43 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.qjournal.client  (5 usages found)
        AsyncLoggerSet.java  (1 usage found)
            38 import com.google.common.base.Joiner;
        QuorumCall.java  (1 usage found)
            32 import com.google.common.base.Joiner;
        QuorumException.java  (1 usage found)
            25 import com.google.common.base.Joiner;
        QuorumJournalManager.java  (1 usage found)
            62 import com.google.common.base.Joiner;
        TestQuorumCall.java  (1 usage found)
            29 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.blockmanagement  (4 usages found)
        HostSet.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        TestBlockManager.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        TestBlockReportRateLimiting.java  (1 usage found)
            24 import com.google.common.base.Joiner;
        TestPendingDataNodeMessages.java  (1 usage found)
            41 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.common  (1 usage found)
        StorageInfo.java  (1 usage found)
            37 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.datanode  (7 usages found)
        BlockPoolManager.java  (1 usage found)
            32 import com.google.common.base.Joiner;
        BlockRecoveryWorker.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        BPServiceActor.java  (1 usage found)
            75 import com.google.common.base.Joiner;
        DataNode.java  (1 usage found)
            226 import com.google.common.base.Joiner;
        ShortCircuitRegistry.java  (1 usage found)
            49 import com.google.common.base.Joiner;
        TestDataNodeHotSwapVolumes.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        TestRefreshNamenodes.java  (1 usage found)
            35 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (1 usage found)
        FsVolumeImpl.java  (1 usage found)
            90 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.namenode  (13 usages found)
        FileJournalManager.java  (1 usage found)
            49 import com.google.common.base.Joiner;
        FSDirectory.java  (1 usage found)
            24 import com.google.common.base.Joiner;
        FSEditLogLoader.java  (1 usage found)
            120 import com.google.common.base.Joiner;
        FSEditLogOp.java  (1 usage found)
            141 import com.google.common.base.Joiner;
        FSImage.java  (1 usage found)
            78 import com.google.common.base.Joiner;
        FSImageTestUtil.java  (1 usage found)
            66 import com.google.common.base.Joiner;
        NameNode.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        TestAuditLogAtDebug.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        TestCheckpoint.java  (1 usage found)
            97 import com.google.common.base.Joiner;
        TestFileJournalManager.java  (1 usage found)
            52 import com.google.common.base.Joiner;
        TestNNStorageRetentionFunctional.java  (1 usage found)
            39 import com.google.common.base.Joiner;
        TestNNStorageRetentionManager.java  (1 usage found)
            53 import com.google.common.base.Joiner;
        TestProtectedDirectories.java  (1 usage found)
            21 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.namenode.ha  (9 usages found)
        BootstrapStandby.java  (1 usage found)
            73 import com.google.common.base.Joiner;
        HATestUtil.java  (1 usage found)
            41 import com.google.common.base.Joiner;
        TestDelegationTokensWithHA.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        TestDFSUpgradeWithHA.java  (1 usage found)
            56 import com.google.common.base.Joiner;
        TestEditLogsDuringFailover.java  (1 usage found)
            47 import com.google.common.base.Joiner;
        TestFailureOfSharedDir.java  (1 usage found)
            48 import com.google.common.base.Joiner;
        TestHAConfiguration.java  (1 usage found)
            34 import com.google.common.base.Joiner;
        TestObserverReadProxyProvider.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        TestStandbyInProgressTail.java  (1 usage found)
            51 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.server.protocol  (3 usages found)
        BlockECReconstructionCommand.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        BlockRecoveryCommand.java  (1 usage found)
            31 import com.google.common.base.Joiner;
        RemoteEditLogManifest.java  (1 usage found)
            23 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.tools  (5 usages found)
        CacheAdmin.java  (1 usage found)
            47 import com.google.common.base.Joiner;
        DFSAdmin.java  (1 usage found)
            41 import com.google.common.base.Joiner;
        TestDFSHAAdmin.java  (1 usage found)
            52 import com.google.common.base.Joiner;
        TestDFSHAAdminMiniCluster.java  (1 usage found)
            47 import com.google.common.base.Joiner;
        TestGetConf.java  (1 usage found)
            61 import com.google.common.base.Joiner;
    org.apache.hadoop.hdfs.util  (1 usage found)
        TestAtomicFileOutputStream.java  (1 usage found)
            41 import com.google.common.base.Joiner;
    org.apache.hadoop.io.compress  (1 usage found)
        CompressDecompressTester.java  (1 usage found)
            42 import com.google.common.base.Joiner;
    org.apache.hadoop.ipc  (3 usages found)
        ProxyCombiner.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        RefreshRegistry.java  (1 usage found)
            23 import com.google.common.base.Joiner;
        RPCCallBenchmark.java  (1 usage found)
            20 import com.google.common.base.Joiner;
    org.apache.hadoop.mapreduce.counters  (1 usage found)
        FileSystemCounterGroup.java  (1 usage found)
            30 import com.google.common.base.Joiner;
    org.apache.hadoop.mapreduce.jobhistory  (1 usage found)
        JobUnsuccessfulCompletionEvent.java  (1 usage found)
            32 import com.google.common.base.Joiner;
    org.apache.hadoop.mapreduce.v2.app.webapp  (1 usage found)
        AppController.java  (1 usage found)
            44 import com.google.common.base.Joiner;
    org.apache.hadoop.mapreduce.v2.util  (1 usage found)
        MRWebAppUtil.java  (1 usage found)
            20 import com.google.common.base.Joiner;
    org.apache.hadoop.metrics2.impl  (1 usage found)
        MetricsConfig.java  (1 usage found)
            32 import com.google.common.base.Joiner;
    org.apache.hadoop.metrics2.lib  (1 usage found)
        UniqueNames.java  (1 usage found)
            23 import com.google.common.base.Joiner;
    org.apache.hadoop.metrics2.util  (1 usage found)
        SampleQuantiles.java  (1 usage found)
            30 import com.google.common.base.Joiner;
    org.apache.hadoop.security  (1 usage found)
        ShellBasedUnixGroupsMapping.java  (1 usage found)
            27 import com.google.common.base.Joiner;
    org.apache.hadoop.test  (1 usage found)
        GenericTestUtils.java  (1 usage found)
            63 import com.google.common.base.Joiner;
    org.apache.hadoop.tools.dynamometer  (3 usages found)
        ApplicationMaster.java  (1 usage found)
            20 import com.google.common.base.Joiner;
        Client.java  (1 usage found)
            21 import com.google.common.base.Joiner;
        DynoInfraUtils.java  (1 usage found)
            20 import com.google.common.base.Joiner;
    org.apache.hadoop.util  (1 usage found)
        JvmPauseMonitor.java  (1 usage found)
            31 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.applications.distributedshell  (1 usage found)
        Client.java  (1 usage found)
            36 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.client  (1 usage found)
        ClientRMProxy.java  (1 usage found)
            44 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.client.api.impl  (2 usages found)
        AMRMClientImpl.java  (1 usage found)
            83 import com.google.common.base.Joiner;
        TimelineConnector.java  (1 usage found)
            56 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.client.cli  (1 usage found)
        TestNodeAttributesCLI.java  (1 usage found)
            64 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.nodemanager.amrmproxy  (1 usage found)
        DefaultRequestInterceptor.java  (1 usage found)
            25 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources  (1 usage found)
        CGroupsHandlerImpl.java  (1 usage found)
            24 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.nodemanager.health  (2 usages found)
        NodeHealthCheckerService.java  (1 usage found)
            22 import com.google.common.base.Joiner;
        TestNodeHealthCheckerService.java  (1 usage found)
            31 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.resourcemanager.recovery  (1 usage found)
        TestZKRMStateStore.java  (1 usage found)
            74 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf  (1 usage found)
        MutableCSConfigurationProvider.java  (1 usage found)
            22 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies  (1 usage found)
        TestDominantResourceFairnessPolicy.java  (1 usage found)
            32 import org.apache.curator.shaded.com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.resourcemanager.webapp  (1 usage found)
        TestRMWebServicesNodes.java  (1 usage found)
            100 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.resourcemanager.webapp.dao  (1 usage found)
        AppInfo.java  (1 usage found)
            54 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.server.webapp  (3 usages found)
        LogServlet.java  (1 usage found)
            22 import com.google.common.base.Joiner;
        LogWebService.java  (1 usage found)
            22 import com.google.common.base.Joiner;
        LogWebServiceUtils.java  (1 usage found)
            21 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.service.webapp  (1 usage found)
        ApiServer.java  (1 usage found)
            20 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.util  (1 usage found)
        StringHelper.java  (1 usage found)
            21 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.webapp.hamlet  (1 usage found)
        HamletImpl.java  (1 usage found)
            21 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.webapp.hamlet2  (1 usage found)
        HamletImpl.java  (1 usage found)
            21 import com.google.common.base.Joiner;
    org.apache.hadoop.yarn.webapp.view  (1 usage found)
        DefaultPage.java  (1 usage found)
            21 import com.google.common.base.Joiner;

{code}"
Using snappy-java in SnappyCodec,13316391,Resolved,Major,Fixed,13/Jul/20 06:57,07/Oct/20 12:48,3.3.0,"In Hadoop, we use native libs for snappy codec which has several disadvantages:
 * It requires native *libhadoop* and *libsnappy* to be installed in system *LD_LIBRARY_PATH*, and they have to be installed separately on each node of the clusters, container images, or local test environments which adds huge complexities from deployment point of view. In some environments, it requires compiling the natives from sources which is non-trivial. Also, this approach is platform dependent; the binary may not work in different platform, so it requires recompilation.
 * It requires extra configuration of *java.library.path* to load the natives, and it results higher application deployment and maintenance cost for users.

Projects such as *Spark* and *Parquet* use [snappy-java|[https://github.com/xerial/snappy-java]] which is JNI-based implementation. It contains native binaries for Linux, Mac, and IBM in jar file, and it can automatically load the native binaries into JVM from jar without any setup. If a native implementation can not be found for a platform, it can fallback to pure-java implementation of snappy based on [aircompressor|[https://github.com/airlift/aircompressor/tree/master/src/main/java/io/airlift/compress/snappy]]."
Intermittent OutOfMemory error while performing hdfs CopyFromLocal to abfs ,13321774,Resolved,Major,Fixed,10/Aug/20 11:17,22/Sep/21 10:20,3.3.0,"OutOfMemory error due to new ThreadPools being made each time AbfsOutputStream is created. Since threadPool aren't limited a lot of data is loaded in buffer and thus it causes OutOfMemory error.

Possible fixes:
- Limit the number of ThreadCounts while performing hdfs copyFromLocal (Using -t property).
- Reducing OUTPUT_BUFFER_SIZE significantly which would limit the amount of buffer to be loaded in threads.
- Don't create new ThreadPools each time AbfsOutputStream is created and limit the number of ThreadPools each AbfsOutputStream could create."
JavaKeyStoreProvider fails to create a new key if the keystore is HDFS,13326665,Reopened,Major,,09/Sep/20 12:00,,,"The caller of JavaKeyStoreProvider#renameOrFail assumes that it throws FileNotFoundException if the src does not exist. However, JavaKeyStoreProvider#renameOrFail calls the old rename API. In DistributedFileSystem, the old API returns false if the src does not exist.

That way JavaKeyStoreProvider fails to create a new key if the keystore is HDFS."
ABFS: Support for conditional overwrite,13323685,Resolved,Major,Fixed,20/Aug/20 06:49,19/Sep/20 01:38,3.3.0,"Filesystem Create APIs that do not accept an argument for overwrite flag end up defaulting it to true. 

We are observing that request count of creates with overwrite=true is more and primarily because of the default setting of the flag is true of the called Create API. When a create with overwrite ends up timing out, we have observed that it could lead to race conditions between the first create and retried one running almost parallel.

To avoid this scenario for create with overwrite=true request, ABFS driver will always attempt to create without overwrite. If the create fails due to fileAlreadyPresent, it will resend the request with overwrite=true. 

 

 

 "
Clear abfs readahead requests on stream close,13319501,Resolved,Major,Fixed,27/Jul/20 09:13,07/Sep/21 12:38,3.3.0,It would be good to close/clear pending read ahead requests on stream close().
S3A statistics to support IOStatistics,13328236,Resolved,Major,Fixed,18/Sep/20 12:30,31/Dec/20 21:56,3.3.0,"S3A to rework statistics with

* API + Implementation split of the interfaces used by subcomponents when reporting stats
* S3A Instrumentation to implement all the interfaces
* streams, etc to all implement IOStatisticsSources and serve to callers
* Add some tracking of durations of remote requests
* RemoteIterator iterators returned from list* calls return list stats for callers to collect; toString will print them too"
Implement Incremental globStatus() -> RemoteIterator class,13328242,Open,Major,,18/Sep/20 12:44,,3.3.0,"globStatus/globber is inefficient, not incremental and doesn't provide a way to query any IOStatistics.

Proposed: 
* new IncrementalGlobber class which takes an interface to the file source (+impls for FS and FC) which offer the incremental ops
* use incremental listStatusIncremental calls
* or, if no wildcards, full deep listFiles(recursive)
* collect IOStatistics from source ops, aggregate and serve

Potentially a lot of work...need to look carefully here"
FSDataInput/Output Streams to automatically collect IOStatistics,13328239,Resolved,Major,Won't Fix,18/Sep/20 12:38,12/Jul/22 18:22,3.3.1,"the fs input/output streams to automatically collect stream IO statistics even if the inner classes don't; count invocations and durations

An issue here becomes ""how would you aggregate with the inner stream, *efficiently*. Not sure there, except maybe

* have some interface to access an updatable IOStatisticsStore which, if the stream implements, says ""here is something you can update""
* wrapper class updates that with counts, durations
* we extend DynamicIOStatistics for each counter/gauge etc to have a factory which can add new entries on demand (new AtomicLong, etc)
* so wrapper classes just update their stats, which  updates existing stats or triggers off an on-demand creation of a new entry

Not so informative in terms of low level details (HTTP/IPC requests and latency, errors, bytes discarded) but would give callers benefits of using the API for HDFS, Ozone, GCS"
implement non-guava Precondition checkNotNull,13316556,Resolved,Major,Fixed,13/Jul/20 21:16,20/Sep/21 15:01,,"As part In order to replace Guava Preconditions, we need to implement our own versions of the API.
 This Jira is to create {{checkNotNull}} in a new package dubbed {{unguava}}.

 +The plan is as follows+
 * create a new {{package org.apache.hadoop.util.unguava;}}
 * {{create class Validate}}
 * implement  {{package org.apache.hadoop.util.unguava.Validate;}} with the following interface
 ** {{checkNotNull(final T obj)}}
 ** {{checkNotNull(final T reference, final Object errorMessage)}}
 ** {{checkNotNull(final T obj, final String message, final Object... values)}}
 ** {{checkNotNull(final T obj,final Supplier<String> msgSupplier)}}
 * {{guava.preconditions used String.lenientformat which suppressed exceptions caused by string formatting of the exception message . So, in order to avoid changing the behavior, the implementation catches Exceptions triggered by building the message (IllegalFormat, InsufficientArg, NullPointer..etc)}}
 * {{After merging the new class, we can replace guava.Preconditions.checkNotNull by {{unguava.Validate.checkNotNull}}}}
 * We need the change to go into trunk, 3.1, 3.2, and 3.3

 

Similar Jiras will be created to implement checkState, checkArgument, checkIndex"
Fix C/C++ standard warnings,13321841,Resolved,Major,Fixed,10/Aug/20 17:06,11/Aug/20 07:49,3.1.3,"The C/C++ language standard is not specified in a cross-compiler manner. Even though it's as straight forward as passing *-std* as compiler arguments, not all the values are supported by all the compilers. For example, compilation with the Visual C++ compiler on Windows with *-std=gnu99* flag causes the following warning -
{code:java}
cl : command line warning D9002: ignoring unknown option '-std=gnu99' [Z:\hadoop-hdfs-project\hadoop-hdfs-native-client\target\native\main\native\libhdfs-examples\hdfs_read.vcxproj] {code}
Thus, we need to use the appropriate flags provided by CMake to specify the C/C++ standards so that it is compiler friendly."
Support S3 Access Points,13321859,Resolved,Major,Fixed,10/Aug/20 18:46,04/Oct/21 19:56,3.3.0,"Improve VPC integration by supporting access points for buckets
https://docs.aws.amazon.com/AmazonS3/latest/dev/access-points.html

*important*: when backporting, always include as followup patches
* HADOOP-17951 
* HADOOP-18085
"
Skip Retry INFO logging on first failover from a proxy,13315510,Resolved,Major,Fixed,08/Jul/20 00:21,13/Jul/20 21:14,,"RetryInvocationHandler logs an INFO level message on every failover except the first. This used to be ideal before when there were only 2 proxies in the FailoverProxyProvider. But if there are more than 2 proxies (as is possible with 3 or more NNs in HA), then there could be more than one failover to find the currently active proxy.

To avoid creating noise in clients logs/ console, RetryInvocationHandler should skip logging once for each proxy."
ABFS: Add Identifiers to Client Request Header,13329687,Resolved,Major,Fixed,28/Sep/20 06:20,02/Jul/21 13:45,3.3.0,"Adding unique values to the client request header to assist in correlating requests

the value of ""fs.azure.client.correlationid"" will be included in Azure storage logs for requests "
update log4j-1.2.17 to atlassian version( To Address: CVE-2019-17571),13324279,Resolved,Major,Duplicate,24/Aug/20 13:14,28/Jan/22 08:59,,"Currentlly there are no active release under 1.X in log4j and log4j2 is incompatiable to upgrade (see HADOOP-16206 ) for more details.

But following CVE is reported on log4j 1.2.17..I think,we should consider to update to Atlassian([https://mvnrepository.com/artifact/log4j/log4j/1.2.17-atlassian-0.4]) or redhat versions

[https://nvd.nist.gov/vuln/detail/CVE-2019-17571]"
Spark job with s3acommitter stuck at the last stage,13321878,Open,Major,,10/Aug/20 21:12,,3.2.1,"usually our spark job took 1 hour or 2 to finish, occasionally it runs for more than 3 hour and then we know it's stuck and usually the executor has stack like this

{{
""Executor task launch worker for task 78620"" #265 daemon prio=5 os_prio=0 tid=0x00007f73e0005000 nid=0x12d waiting on condition [0x00007f74cb291000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:349)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1457)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1717)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2785)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:2751)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:238)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper$$Lambda$210/1059071691.execute(Unknown Source)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
	at org.apache.hadoop.fs.s3a.Invoker$$Lambda$23/586859139.execute(Unknown Source)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:226)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:271)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:660)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:521)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:385)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.parquet.hadoop.util.HadoopPositionOutputStream.close(HadoopPositionOutputStream.java:64)
	at org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:685)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:122)
	at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- <0x00000003a57332e0> (a java.util.concurrent.ThreadPoolExecutor$Worker)


}}
captured jstack on the stuck executors in case it's useful."
Fix failure of docker image creation due to pip2 install error,13315751,Resolved,Major,Fixed,08/Jul/20 22:59,09/Jul/20 03:47,3.1.4,"{noformat}
The command '/bin/sh -c pip2 install     configparser==4.0.2     pylint==1.9.2' returned a non-zero code: 1
{noformat}
"
Support LZO using aircompressor,13316387,Open,Major,,13/Jul/20 06:28,,3.3.0,"LZO codec was removed in HADOOP-4874 because the original LZO binding is GPL which is problematic. However, many legacy data is still compressed by LZO codec, and companies often use vendor's GPL LZO codec in the classpath which might cause GPL contamination.

Presro and ORC-77 use [aircompressor| [https://github.com/airlift/aircompressor]] (Apache V2 licensed) to compress and decompress LZO data. Hadoop can add back LZO support using aircompressor without GPL violation."
Replace Guava Preconditions to avoid Guava dependency,13314498,In Progress,Major,,01/Jul/20 14:37,,,"By far, one of the most painful replacement in hadoop. There are two options:
# Using Apache commons
# Using Java wrapper without dependency on third party.

{code:java}
Targets
    Occurrences of 'com.google.common.base.Preconditions' in project with mask '*.java'
Found Occurrences  (577 usages found)
    org.apache.hadoop.conf  (2 usages found)
        Configuration.java  (1 usage found)
            108 import com.google.common.base.Preconditions;
        ReconfigurableBase.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.crypto  (7 usages found)
        AesCtrCryptoCodec.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        CryptoInputStream.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        CryptoOutputStream.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        CryptoStreamUtils.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        JceAesCtrCryptoCodec.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        OpensslAesCtrCryptoCodec.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        OpensslCipher.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.crypto.key  (2 usages found)
        JavaKeyStoreProvider.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        KeyProviderCryptoExtension.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.crypto.key.kms  (3 usages found)
        KMSClientProvider.java  (1 usage found)
            83 import com.google.common.base.Preconditions;
        LoadBalancingKMSClientProvider.java  (1 usage found)
            54 import com.google.common.base.Preconditions;
        ValueQueue.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
    org.apache.hadoop.crypto.key.kms.server  (5 usages found)
        KeyAuthorizationKeyProvider.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        KMS.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        KMSAudit.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        KMSWebApp.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        MiniKMS.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
    org.apache.hadoop.crypto.random  (1 usage found)
        OpensslSecureRandom.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs  (19 usages found)
        ByteBufferUtil.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        ChecksumFileSystem.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        FileContext.java  (1 usage found)
            68 import com.google.common.base.Preconditions;
        FileEncryptionInfo.java  (2 usages found)
            27 import static com.google.common.base.Preconditions.checkArgument;
            28 import static com.google.common.base.Preconditions.checkNotNull;
        FileSystem.java  (2 usages found)
            86 import com.google.common.base.Preconditions;
            91 import static com.google.common.base.Preconditions.checkArgument;
        FileSystemStorageStatistics.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        FSDataOutputStreamBuilder.java  (1 usage found)
            31 import static com.google.common.base.Preconditions.checkNotNull;
        FSInputStream.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        FsUrlConnection.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        GlobalStorageStatistics.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        Globber.java  (1 usage found)
            35 import static com.google.common.base.Preconditions.checkNotNull;
        MultipartUploader.java  (1 usage found)
            31 import static com.google.common.base.Preconditions.checkArgument;
        PartialListing.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        TestEnhancedByteBufferAccess.java  (1 usage found)
            74 import com.google.common.base.Preconditions;
        TestLocalFileSystem.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        UnionStorageStatistics.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        XAttrCodec.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.adl  (1 usage found)
        AdlFileSystem.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.aliyun.oss  (1 usage found)
        AliyunOSSUtils.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azure  (4 usages found)
        BlockBlobAppendStream.java  (1 usage found)
            46 import com.google.common.base.Preconditions;
        ClientThrottlingAnalyzer.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        NativeAzureFileSystem.java  (1 usage found)
            84 import com.google.common.base.Preconditions;
        NativeAzureFileSystemHelper.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs  (3 usages found)
        AbfsConfiguration.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        AzureBlobFileSystem.java  (1 usage found)
            41 import com.google.common.base.Preconditions;
        AzureBlobFileSystemStore.java  (1 usage found)
            52 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs.extensions  (2 usages found)
        ClassicDelegationTokenManager.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        StubAbfsTokenIdentifier.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs.oauth2  (6 usages found)
        AzureADAuthenticator.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        ClientCredsTokenProvider.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        CustomTokenProviderAdapter.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        IdentityTransformer.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        RefreshTokenBasedTokenProvider.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        UserPasswordTokenProvider.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs.security  (1 usage found)
        AbfsDelegationTokenManager.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs.services  (3 usages found)
        AbfsClientThrottlingAnalyzer.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        AbfsInputStream.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        AbfsOutputStream.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.azurebfs.utils  (1 usage found)
        TextFileBasedIdentityHandler.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.contract.s3a  (1 usage found)
        ITestS3AContractSeek.java  (1 usage found)
            47 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.fs.cosn.auth  (1 usage found)
        COSCredentialsProviderList.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.ftp  (2 usages found)
        FTPFileSystem.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        TestFTPFileSystem.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.http.client  (1 usage found)
        HttpFSFileSystem.java  (1 usage found)
            74 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.impl  (5 usages found)
        AbstractFSBuilderImpl.java  (2 usages found)
            39 import static com.google.common.base.Preconditions.checkArgument;
            40 import static com.google.common.base.Preconditions.checkNotNull;
        FsLinkResolution.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        PathCapabilitiesSupport.java  (1 usage found)
            28 import static com.google.common.base.Preconditions.checkArgument;
        WrappedIOException.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.loadGenerator  (1 usage found)
        LoadGenerator.java  (1 usage found)
            50 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.permission  (1 usage found)
        AclStatus.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a  (22 usages found)
        AWSClientIOException.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        AWSCredentialProviderList.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        FailureInjectionPolicy.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        InconsistentAmazonS3Client.java  (1 usage found)
            56 import com.google.common.base.Preconditions;
        Invoker.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        Listing.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
        MockS3AFileSystem.java  (1 usage found)
            45 import static com.google.common.base.Preconditions.checkNotNull;
        S3ABlockOutputStream.java  (1 usage found)
            38 import com.google.common.base.Preconditions;
        S3ADataBlocks.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        S3AFileSystem.java  (1 usage found)
            83 import com.google.common.base.Preconditions;
        S3AInputStream.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        S3ALocatedFileStatus.java  (1 usage found)
            24 import static com.google.common.base.Preconditions.checkNotNull;
        S3AMultipartUploader.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        S3AOpContext.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        S3AReadOpContext.java  (2 usages found)
            28 import com.google.common.base.Preconditions;
            30 import static com.google.common.base.Preconditions.checkNotNull;
        S3ARetryPolicy.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        S3ATestUtils.java  (1 usage found)
            72 import static com.google.common.base.Preconditions.checkNotNull;
        S3AUtils.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        WriteOperationHelper.java  (3 usages found)
            44 import com.google.common.base.Preconditions;
            58 import static com.google.common.base.Preconditions.checkArgument;
            59 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.fs.s3a.auth  (3 usages found)
        MarshalledCredentialProvider.java  (1 usage found)
            31 import static com.google.common.base.Preconditions.checkNotNull;
        RoleModel.java  (1 usage found)
            38 import static com.google.common.base.Preconditions.checkState;
        STSClientFactory.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a.auth.delegation  (6 usages found)
        AbstractDTService.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        FullCredentialsTokenBinding.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        MiniKerberizedHadoopCluster.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        RoleTokenBinding.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        S3ADelegationTokens.java  (2 usages found)
            48 import static com.google.common.base.Preconditions.checkArgument;
            49 import static com.google.common.base.Preconditions.checkState;
    org.apache.hadoop.fs.s3a.commit  (4 usages found)
        AbstractS3ACommitter.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        AbstractYarnClusterITest.java  (1 usage found)
            42 import static com.google.common.base.Preconditions.checkNotNull;
        CommitOperations.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
        MagicCommitPaths.java  (1 usage found)
            28 import static com.google.common.base.Preconditions.checkArgument;
    org.apache.hadoop.fs.s3a.commit.files  (1 usage found)
        SinglePendingCommit.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a.commit.magic  (1 usage found)
        MagicCommitTracker.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a.commit.staging  (3 usages found)
        Paths.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        StagingCommitter.java  (2 usages found)
            29 import com.google.common.base.Preconditions;
            54 import static com.google.common.base.Preconditions.*;
    org.apache.hadoop.fs.s3a.impl  (6 usages found)
        AbstractStoreOperation.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.checkNotNull;
        ChangeTracker.java  (1 usage found)
            43 import static com.google.common.base.Preconditions.checkNotNull;
        DeleteOperation.java  (1 usage found)
            47 import static com.google.common.base.Preconditions.checkArgument;
        ExecutingStoreOperation.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        MultiObjectDeleteSupport.java  (1 usage found)
            43 import static com.google.common.base.Preconditions.checkNotNull;
        RenameOperation.java  (1 usage found)
            47 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.fs.s3a.s3guard  (24 usages found)
        AbstractS3GuardToolTestBase.java  (1 usage found)
            40 import com.google.common.base.Preconditions;
        DescendantsIterator.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        DirListingMetadata.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        DumpS3GuardDynamoTable.java  (1 usage found)
            62 import static com.google.common.base.Preconditions.checkNotNull;
        DynamoDBClientFactory.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        DynamoDBMetadataStore.java  (1 usage found)
            66 import com.google.common.base.Preconditions;
        DynamoDBMetadataStoreTableManager.java  (1 usage found)
            55 import com.google.common.base.Preconditions;
        ImportOperation.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        ITestDynamoDBMetadataStore.java  (1 usage found)
            79 import static com.google.common.base.Preconditions.checkNotNull;
        ITestDynamoDBMetadataStoreScale.java  (1 usage found)
            64 import static com.google.common.base.Preconditions.checkNotNull;
        ITestS3GuardDDBRootOperations.java  (1 usage found)
            40 import static com.google.common.base.Preconditions.checkNotNull;
        LocalMetadataStore.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        MetadataStoreListFilesIterator.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        MetadataStoreTestBase.java  (1 usage found)
            51 import static com.google.common.base.Preconditions.checkNotNull;
        PathMetadata.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        PathMetadataDynamoDBTranslation.java  (1 usage found)
            38 import com.google.common.base.Preconditions;
        ProgressiveRenameTracker.java  (1 usage found)
            32 import static com.google.common.base.Preconditions.checkArgument;
        PurgeS3GuardDynamoTable.java  (1 usage found)
            46 import static com.google.common.base.Preconditions.checkNotNull;
        RenameTracker.java  (1 usage found)
            36 import static com.google.common.base.Preconditions.checkNotNull;
        S3Guard.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        S3GuardFsck.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
        S3GuardTableAccess.java  (1 usage found)
            42 import static com.google.common.base.Preconditions.checkNotNull;
        S3GuardTool.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
        TestPathMetadataDynamoDBTranslation.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a.scale  (1 usage found)
        ILoadTestS3ABulkDeleteThrottling.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.s3a.select  (6 usages found)
        CsvFile.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        ITestS3SelectCLI.java  (1 usage found)
            45 import static com.google.common.base.Preconditions.checkNotNull;
        SelectBinding.java  (2 usages found)
            33 import com.google.common.base.Preconditions;
            47 import static com.google.common.base.Preconditions.checkNotNull;
        SelectInputStream.java  (2 usages found)
            31 import com.google.common.base.Preconditions;
            47 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.fs.s3native  (1 usage found)
        S3xLoginHelper.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.shell  (2 usages found)
        SnapshotCommands.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        XAttrCommands.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.fs.viewfs  (1 usage found)
        InodeTree.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.ha  (5 usages found)
        ActiveStandbyElector.java  (1 usage found)
            47 import com.google.common.base.Preconditions;
        FailoverController.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
        HealthMonitor.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        MiniZKFCCluster.java  (1 usage found)
            40 import com.google.common.base.Preconditions;
        ZKFailoverController.java  (1 usage found)
            54 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs  (22 usages found)
        DFSClient.java  (1 usage found)
            197 import com.google.common.base.Preconditions;
        DFSOutputStream.java  (1 usage found)
            76 import com.google.common.base.Preconditions;
        DFSStripedOutputStream.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DFSTestUtil.java  (1 usage found)
            77 import com.google.common.base.Preconditions;
        DFSUtil.java  (1 usage found)
            109 import com.google.common.base.Preconditions;
        DFSUtilClient.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DistributedFileSystem.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        ErasureCodeBenchmarkThroughput.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        HAUtil.java  (1 usage found)
            60 import com.google.common.base.Preconditions;
        MiniDFSCluster.java  (1 usage found)
            146 import com.google.common.base.Preconditions;
        MiniDFSNNTopology.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        NameNodeProxiesClient.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        PeerCache.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        PositionStripeReader.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        StatefulStripeReader.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        StripeReader.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        TestAppendSnapshotTruncate.java  (1 usage found)
            54 import com.google.common.base.Preconditions;
        TestDFSStripedOutputStreamWithFailureBase.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        TestLeaseRecoveryStriped.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        TestMiniDFSCluster.java  (1 usage found)
            45 import com.google.common.base.Preconditions;
        UpgradeUtilities.java  (1 usage found)
            54 import com.google.common.base.Preconditions;
        XAttrHelper.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.client  (2 usages found)
        HdfsDataInputStream.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
        HdfsDataOutputStream.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.client.impl  (3 usages found)
        BlockReaderFactory.java  (1 usage found)
            78 import com.google.common.base.Preconditions;
        BlockReaderLocal.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DfsClientConf.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.net  (2 usages found)
        DFSNetworkTopology.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DFSTopologyNodeImpl.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.nfs.nfs3  (5 usages found)
        DFSClientCache.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        OffsetRange.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        OpenFileCtx.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
        OpenFileCtxCache.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
        WriteCtx.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.protocol  (13 usages found)
        BlockListAsLongs.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
        CacheDirective.java  (2 usages found)
            20 import static com.google.common.base.Preconditions.checkNotNull;
            31 import com.google.common.base.Preconditions;
        CacheDirectiveInfo.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        CacheDirectiveIterator.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        ErasureCodingPolicy.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        ErasureCodingPolicyInfo.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        HdfsPartialListing.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        LayoutFlags.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        LocatedBlock.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        ReencryptionStatus.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        SnapshotDiffReportListing.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        ZoneReencryptionStatus.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.protocol.datatransfer  (2 usages found)
        PacketHeader.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        PacketReceiver.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.protocolPB  (2 usages found)
        DatanodeProtocolServerSideTranslatorPB.java  (1 usage found)
            64 import com.google.common.base.Preconditions;
        PBHelperClient.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.qjournal.client  (7 usages found)
        AsyncLoggerSet.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        IPCLoggerChannel.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
        QuorumCall.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        QuorumException.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        QuorumJournalManager.java  (1 usage found)
            63 import com.google.common.base.Preconditions;
        SegmentRecoveryComparator.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        TestQJMWithFaults.java  (1 usage found)
            64 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.qjournal.server  (2 usages found)
        Journal.java  (1 usage found)
            76 import com.google.common.base.Preconditions;
        JournalNode.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.security.token.block  (1 usage found)
        BlockTokenSecretManager.java  (1 usage found)
            48 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.security.token.delegation  (1 usage found)
        DelegationTokenSecretManager.java  (1 usage found)
            52 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.balancer  (4 usages found)
        Balancer.java  (2 usages found)
            20 import static com.google.common.base.Preconditions.checkArgument;
            72 import com.google.common.base.Preconditions;
        Dispatcher.java  (1 usage found)
            87 import com.google.common.base.Preconditions;
        NameNodeConnector.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.blockmanagement  (19 usages found)
        AvailableSpaceBlockPlacementPolicy.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        AvailableSpaceRackFaultTolerantBlockPlacementPolicy.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        BlockIdManager.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        BlockInfo.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        BlockInfoContiguous.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        BlockInfoStriped.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        BlockManager.java  (1 usage found)
            130 import com.google.common.base.Preconditions;
        BlockManagerSafeMode.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        BlockManagerTestUtil.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        BlockPlacementPolicy.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        BlockPlacementPolicyDefault.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        BlockReportLeaseManager.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        BlockStoragePolicySuite.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        BlockToMarkCorrupt.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        CacheReplicationMonitor.java  (1 usage found)
            58 import com.google.common.base.Preconditions;
        DatanodeAdminDefaultMonitor.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        DatanodeAdminManager.java  (1 usage found)
            20 import static com.google.common.base.Preconditions.checkArgument;
        DatanodeManager.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        HostSet.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.common  (3 usages found)
        HdfsServerConstants.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
        Storage.java  (1 usage found)
            57 import com.google.common.base.Preconditions;
        Util.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.datanode  (17 usages found)
        BlockChecksumHelper.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        BlockPoolManager.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        BlockPoolSliceStorage.java  (1 usage found)
            51 import com.google.common.base.Preconditions;
        BlockRecoveryWorker.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        BlockScanner.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
        BlockSender.java  (1 usage found)
            59 import com.google.common.base.Preconditions;
        BPOfferService.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DataNode.java  (1 usage found)
            227 import com.google.common.base.Preconditions;
        DataXceiver.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DataXceiverServer.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
        DiskBalancer.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        DiskBalancerWorkItem.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        DiskBalancerWorkStatus.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        FsDatasetTestUtils.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        InternalDataNodeTestUtils.java  (1 usage found)
            49 import com.google.common.base.Preconditions;
        ShortCircuitRegistry.java  (1 usage found)
            50 import com.google.common.base.Preconditions;
        VolumeScanner.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.datanode.checker  (4 usages found)
        AbstractFuture.java  (2 usages found)
            25 import com.google.common.base.Preconditions;
            26 import static com.google.common.base.Preconditions.checkNotNull;
        DatasetVolumeChecker.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        TimeoutFuture.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.datanode.erasurecode  (3 usages found)
        ErasureCodingWorker.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        StripedReader.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        StripedWriter.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (10 usages found)
        FsDatasetCache.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        FsDatasetImpl.java  (1 usage found)
            124 import com.google.common.base.Preconditions;
        FsDatasetImplTestUtils.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        FsDatasetUtil.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
        FsVolumeImpl.java  (1 usage found)
            91 import com.google.common.base.Preconditions;
        LazyPersistTestCase.java  (1 usage found)
            48 import com.google.common.base.Preconditions;
        MappableBlockLoader.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        NativePmemMappableBlockLoader.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RamDiskReplicaTracker.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        TestScrLazyPersistFiles.java  (1 usage found)
            19 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.datanode.web.webhdfs  (1 usage found)
        WebHdfsHandler.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.diskbalancer  (3 usages found)
        DiskBalancerTestUtil.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        TestDiskBalancer.java  (1 usage found)
            19 import com.google.common.base.Preconditions;
        TestDiskBalancerWithMockMover.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.diskbalancer.command  (7 usages found)
        CancelCommand.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        Command.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        ExecuteCommand.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        HelpCommand.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        PlanCommand.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        QueryCommand.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        ReportCommand.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.diskbalancer.connectors  (2 usages found)
        DBNameNodeConnector.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        JsonNodeConnector.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.diskbalancer.datamodel  (3 usages found)
        DiskBalancerCluster.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        DiskBalancerDataNode.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        DiskBalancerVolumeSet.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.diskbalancer.planner  (2 usages found)
        GreedyPlanner.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NodePlan.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.federation.resolver.order  (1 usage found)
        AvailableSpaceResolver.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.federation.router  (1 usage found)
        RouterAdminServer.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.mover  (1 usage found)
        TestStorageMover.java  (1 usage found)
            68 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode  (55 usages found)
        AclFeature.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        BackupImage.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        CachePool.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        ContentSummaryComputationContext.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        EditLogBackupInputStream.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        EditLogFileInputStream.java  (1 usage found)
            49 import com.google.common.base.Preconditions;
        EditsDoubleBuffer.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        EncryptionZoneManager.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        ErasureCodingPolicyManager.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        FileJournalManager.java  (1 usage found)
            50 import com.google.common.base.Preconditions;
        FSDirAclOp.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSDirAppendOp.java  (1 usage found)
            43 import com.google.common.base.Preconditions;
        FSDirConcatOp.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSDirectory.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        FSDirEncryptionZoneOp.java  (1 usage found)
            55 import com.google.common.base.Preconditions;
        FSDirErasureCodingOp.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSDirMkdirOp.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        FSDirRenameOp.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSDirStatAndListingOp.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        FSDirWriteFileOp.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSDirXAttrOp.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        FSEditLog.java  (1 usage found)
            113 import com.google.common.base.Preconditions;
        FSEditLogAsync.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
        FSEditLogLoader.java  (1 usage found)
            121 import com.google.common.base.Preconditions;
        FSEditLogOp.java  (1 usage found)
            142 import com.google.common.base.Preconditions;
        FSImage.java  (1 usage found)
            79 import com.google.common.base.Preconditions;
        FSImageFormat.java  (1 usage found)
            79 import com.google.common.base.Preconditions;
        FSImageFormatPBINode.java  (1 usage found)
            74 import com.google.common.base.Preconditions;
        FSImageSerialization.java  (1 usage found)
            56 import com.google.common.base.Preconditions;
        FSNamesystem.java  (1 usage found)
            342 import com.google.common.base.Preconditions;
        FSPermissionChecker.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        FSTreeTraverser.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
        ImageServlet.java  (1 usage found)
            67 import com.google.common.base.Preconditions;
        INode.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        INodeDirectory.java  (1 usage found)
            44 import com.google.common.base.Preconditions;
        INodeDirectoryAttributes.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        INodeFile.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
        INodeMap.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        INodeReference.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        INodesInPath.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        INodeWithAdditionalFields.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        JournalSet.java  (1 usage found)
            43 import com.google.common.base.Preconditions;
        LeaseManager.java  (1 usage found)
            56 import com.google.common.base.Preconditions;
        NameNode.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        NNStorage.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
        NNStorageRetentionManager.java  (1 usage found)
            40 import com.google.common.base.Preconditions;
        NNThroughputBenchmark.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
        NNUpgradeUtil.java  (1 usage found)
            37 import com.google.common.base.Preconditions;
        RedundantEditLogInputStream.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        ReencryptionHandler.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ReencryptionUpdater.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        SaveNamespaceContext.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        SecondaryNameNode.java  (1 usage found)
            81 import com.google.common.base.Preconditions;
        XAttrFormat.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        XAttrPermissionFilter.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode.ha  (4 usages found)
        BootstrapStandby.java  (1 usage found)
            74 import com.google.common.base.Preconditions;
        EditLogTailer.java  (1 usage found)
            62 import com.google.common.base.Preconditions;
        RemoteNameNodeInfo.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        StandbyCheckpointer.java  (1 usage found)
            52 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode.snapshot  (9 usages found)
        AbstractINodeDiff.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        DiffListBySkipList.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        DirectorySnapshottableFeature.java  (1 usage found)
            51 import com.google.common.base.Preconditions;
        DirectoryWithSnapshotFeature.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FSImageFormatPBSnapshot.java  (1 usage found)
            85 import com.google.common.base.Preconditions;
        SnapshotDiffInfo.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
        SnapshotDiffListingInfo.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        SnapshotFSImageFormat.java  (1 usage found)
            43 import com.google.common.base.Preconditions;
        SnapshotManager.java  (1 usage found)
            60 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode.sps  (1 usage found)
        StoragePolicySatisfier.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode.top  (2 usages found)
        TopAuditLogger.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        TopConf.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.namenode.top.window  (1 usage found)
        RollingWindowManager.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.server.protocol  (3 usages found)
        BlocksWithLocations.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NamespaceInfo.java  (1 usage found)
            34 import com.google.common.base.Preconditions;
        RemoteEditLogManifest.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.shortcircuit  (7 usages found)
        DfsClientShm.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        DfsClientShmManager.java  (1 usage found)
            47 import com.google.common.base.Preconditions;
        DomainSocketFactory.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
        ShortCircuitCache.java  (1 usage found)
            58 import com.google.common.base.Preconditions;
        ShortCircuitReplica.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
        ShortCircuitShm.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
        TestShortCircuitCache.java  (1 usage found)
            92 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.tools  (4 usages found)
        AdminHelper.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        DFSAdmin.java  (1 usage found)
            106 import com.google.common.base.Preconditions;
        DFSHAAdmin.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        NNHAServiceTarget.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.tools.offlineImageViewer  (5 usages found)
        FileDistributionCalculator.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
        FSImageLoader.java  (1 usage found)
            60 import com.google.common.base.Preconditions;
        OfflineImageReconstructor.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        PBImageCorruptionDetector.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        PBImageTextWriter.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.util  (6 usages found)
        ByteArrayManager.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        Diff.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        EnumCounters.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        EnumDoubles.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        StripedBlockUtil.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
        TestStripedBlockUtil.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.hdfs.web  (2 usages found)
        JsonUtilClient.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        WebHdfsFileSystem.java  (1 usage found)
            137 import com.google.common.base.Preconditions;
    org.apache.hadoop.http  (1 usage found)
        HttpServer2.java  (1 usage found)
            53 import com.google.common.base.Preconditions;
    org.apache.hadoop.io  (2 usages found)
        DataOutputBuffer.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        ReadaheadPool.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.io.erasurecode  (1 usage found)
        CodecUtil.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.io.erasurecode.rawcoder  (1 usage found)
        RawErasureCoderBenchmark.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.io.retry  (2 usages found)
        AsyncCallHandler.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        CallReturn.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.ipc  (4 usages found)
        Client.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        ClientId.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        DecayRpcScheduler.java  (1 usage found)
            40 import com.google.common.base.Preconditions;
        RetryCache.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
    org.apache.hadoop.mapred.nativetask.buffer  (1 usage found)
        ByteBufferDataWriter.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
    org.apache.hadoop.mapred.nativetask.testutil  (1 usage found)
        BytesFactory.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.mapreduce.counters  (2 usages found)
        FileSystemCounterGroup.java  (1 usage found)
            31 import static com.google.common.base.Preconditions.*;
        FrameworkCounterGroup.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.mapreduce.lib.output  (4 usages found)
        FileOutputCommitter.java  (1 usage found)
            40 import com.google.common.base.Preconditions;
        FileOutputFormat.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        NamedCommitterFactory.java  (1 usage found)
            25 import com.google.common.base.Preconditions;
        PathOutputCommitter.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
    org.apache.hadoop.mapreduce.v2.app.job.impl  (1 usage found)
        TaskAttemptImpl.java  (1 usage found)
            149 import com.google.common.base.Preconditions;
    org.apache.hadoop.mapreduce.v2.app.webapp  (1 usage found)
        AMWebServices.java  (1 usage found)
            81 import com.google.common.base.Preconditions;
    org.apache.hadoop.metrics2  (2 usages found)
        AbstractMetric.java  (1 usage found)
            27 import static com.google.common.base.Preconditions.checkNotNull;
        MetricsTag.java  (1 usage found)
            27 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.metrics2.impl  (5 usages found)
        MetricsRecordImpl.java  (1 usage found)
            23 import static com.google.common.base.Preconditions.*;
        MetricsSinkAdapter.java  (1 usage found)
            25 import static com.google.common.base.Preconditions.*;
        MetricsSourceAdapter.java  (2 usages found)
            32 import static com.google.common.base.Preconditions.*;
            34 import com.google.common.base.Preconditions;
        MetricsSystemImpl.java  (1 usage found)
            36 import static com.google.common.base.Preconditions.*;
    org.apache.hadoop.metrics2.lib  (7 usages found)
        MethodMetric.java  (1 usage found)
            23 import static com.google.common.base.Preconditions.*;
        MetricsInfoImpl.java  (1 usage found)
            26 import static com.google.common.base.Preconditions.checkNotNull;
        MetricsSourceBuilder.java  (1 usage found)
            25 import static com.google.common.base.Preconditions.*;
        MutableCounter.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.*;
        MutableGauge.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.*;
        MutableRates.java  (1 usage found)
            24 import static com.google.common.base.Preconditions.*;
        MutableRollingAverages.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
    org.apache.hadoop.metrics2.source  (1 usage found)
        JvmMetrics.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
    org.apache.hadoop.metrics2.util  (2 usages found)
        MBeans.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
        SampleQuantiles.java  (1 usage found)
            31 import com.google.common.base.Preconditions;
    org.apache.hadoop.net  (3 usages found)
        NetUtils.java  (1 usage found)
            59 import com.google.common.base.Preconditions;
        NetworkTopology.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        SocketInputWrapper.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
    org.apache.hadoop.net.unix  (1 usage found)
        DomainSocketWatcher.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
    org.apache.hadoop.nfs  (1 usage found)
        NfsExports.java  (1 usage found)
            35 import com.google.common.base.Preconditions;
    org.apache.hadoop.oncrpc  (2 usages found)
        RpcReply.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        XDR.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.oncrpc.security  (2 usages found)
        CredentialsNone.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        VerifierNone.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.cli  (1 usage found)
        RegistryCli.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.client.api  (2 usages found)
        DNSOperationsFactory.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RegistryOperationsFactory.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.client.binding  (3 usages found)
        RegistryPathUtils.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RegistryTypeUtils.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RegistryUtils.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.client.impl  (1 usage found)
        FSRegistryOperationsService.java  (1 usage found)
            51 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.client.impl.zk  (4 usages found)
        CuratorService.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        RegistryOperationsService.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RegistrySecurity.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ZKPathDumper.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.client.types  (2 usages found)
        Endpoint.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        ServiceRecord.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.server.dns  (1 usage found)
        RegistryDNSServer.java  (1 usage found)
            19 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.server.integration  (1 usage found)
        SelectByYarnPersistence.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.registry.server.services  (1 usage found)
        MicroZookeeperService.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.security.authentication.server  (3 usages found)
        AuthenticationHandlerUtil.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        LdapAuthenticationHandler.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
        MultiSchemeAuthenticationHandler.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.security.authorize  (1 usage found)
        ProxyUsers.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.security.token.delegation  (2 usages found)
        AbstractDelegationTokenSecretManager.java  (1 usage found)
            44 import com.google.common.base.Preconditions;
        ZKDelegationTokenSecretManager.java  (1 usage found)
            70 import com.google.common.base.Preconditions;
    org.apache.hadoop.security.token.delegation.web  (2 usages found)
        DelegationTokenAuthenticatedURL.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        MultiSchemeDelegationTokenAuthenticationHandler.java  (1 usage found)
            39 import com.google.common.base.Preconditions;
    org.apache.hadoop.service.launcher  (3 usages found)
        InterruptEscalator.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        IrqHandler.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        ServiceLauncher.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
    org.apache.hadoop.test  (2 usages found)
        LambdaTestUtils.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        MetricsAsserts.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.*;
    org.apache.hadoop.tools  (3 usages found)
        DistCp.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        DistCpOptions.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        OptionsParser.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
    org.apache.hadoop.tools.dynamometer  (2 usages found)
        AMOptions.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        Client.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.tools.mapred  (1 usage found)
        DeletedDirTracker.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.util  (13 usages found)
        ChunkedArrayList.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        CloseableReferenceCount.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        GcTimeMonitor.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        IdentityHashStore.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        IntrusiveCollection.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        JarFinder.java  (1 usage found)
            16 import com.google.common.base.Preconditions;
        JsonSerialization.java  (1 usage found)
            38 import com.google.common.base.Preconditions;
        JvmPauseMonitor.java  (1 usage found)
            32 import com.google.common.base.Preconditions;
        LightWeightCache.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        LimitInputStream.java  (2 usages found)
            22 import static com.google.common.base.Preconditions.checkArgument;
            23 import static com.google.common.base.Preconditions.checkNotNull;
        ServletUtil.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
        StringUtils.java  (1 usage found)
            45 import com.google.common.base.Preconditions;
    org.apache.hadoop.util.curator  (2 usages found)
        ChildReaper.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ZKCuratorManager.java  (1 usage found)
            42 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.api.protocolrecords.impl.pb  (8 usages found)
        GetPluginInfoRequestPBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        GetPluginInfoResponsePBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NodePublishVolumeRequestPBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NodePublishVolumeResponsePBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NodeUnpublishVolumeRequestPBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        NodeUnpublishVolumeResponsePBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        ValidateVolumeCapabilitiesRequestPBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        ValidateVolumeCapabilitiesResponsePBImpl.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.api.records.impl.pb  (6 usages found)
        ApplicationAttemptIdPBImpl.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        ApplicationIdPBImpl.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        ContainerIdPBImpl.java  (1 usage found)
            28 import com.google.common.base.Preconditions;
        NodeIdPBImpl.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
        ReservationIdPBImpl.java  (1 usage found)
            26 import com.google.common.base.Preconditions;
        ResourceOptionPBImpl.java  (1 usage found)
            27 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.client  (2 usages found)
        ClientRMProxy.java  (1 usage found)
            45 import com.google.common.base.Preconditions;
        ServerProxy.java  (1 usage found)
            44 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.client.api  (1 usage found)
        AMRMClient.java  (1 usage found)
            51 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.client.api.async  (1 usage found)
        AMRMClientAsync.java  (1 usage found)
            59 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.client.api.impl  (2 usages found)
        AMRMClientImpl.java  (1 usage found)
            84 import com.google.common.base.Preconditions;
        TimelineConnector.java  (1 usage found)
            57 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.client.cli  (2 usages found)
        NodeAttributesCLI.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        RMAdminCLI.java  (1 usage found)
            84 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.logaggregation.filecontroller  (1 usage found)
        LogAggregationFileControllerFactory.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server  (1 usage found)
        AMHeartbeatRequestHandler.java  (1 usage found)
            36 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.api  (1 usage found)
        ServerRMProxy.java  (1 usage found)
            29 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.applicationhistoryservice  (1 usage found)
        ApplicationHistoryClientService.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.federation.failover  (1 usage found)
        FederationRMFailoverProxyProvider.java  (1 usage found)
            46 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.federation.policies.amrmproxy  (1 usage found)
        LocalityMulticastAMRMProxyPolicy.java  (1 usage found)
            54 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.federation.store.records.impl.pb  (1 usage found)
        SubClusterInfoPBImpl.java  (1 usage found)
            30 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.amrmproxy  (3 usages found)
        AbstractRequestInterceptor.java  (1 usage found)
            24 import com.google.common.base.Preconditions;
        AMRMProxyService.java  (1 usage found)
            84 import com.google.common.base.Preconditions;
        FederationInterceptor.java  (1 usage found)
            99 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager  (1 usage found)
        AuxServices.java  (1 usage found)
            83 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.container  (1 usage found)
        SlidingWindowRetryPolicy.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher  (1 usage found)
        ContainerCleanup.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker  (1 usage found)
        DockerImagesCommand.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer  (1 usage found)
        ContainerLocalizer.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor  (1 usage found)
        ContainersMonitorImpl.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga  (1 usage found)
        FpgaDevice.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler  (2 usages found)
        ClusterNodeTracker.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        SchedulerApplicationAttempt.java  (1 usage found)
            94 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity  (1 usage found)
        CapacityScheduler.java  (1 usage found)
            166 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair  (1 usage found)
        FairScheduler.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.timeline  (2 usages found)
        LeveldbTimelineStore.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        RollingLevelDBTimelineStore.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.timelineservice.collector  (1 usage found)
        AppLevelTimelineCollector.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.timelineservice.storage.reader  (6 usages found)
        ApplicationEntityReader.java  (1 usage found)
            61 import com.google.common.base.Preconditions;
        EntityTypeReader.java  (1 usage found)
            20 import com.google.common.base.Preconditions;
        FlowActivityEntityReader.java  (1 usage found)
            49 import com.google.common.base.Preconditions;
        FlowRunEntityReader.java  (1 usage found)
            60 import com.google.common.base.Preconditions;
        GenericEntityReader.java  (1 usage found)
            66 import com.google.common.base.Preconditions;
        SubApplicationEntityReader.java  (1 usage found)
            59 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.server.uam  (1 usage found)
        UnmanagedApplicationManager.java  (1 usage found)
            72 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service  (2 usages found)
        ServiceContext.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ServiceManager.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.client  (1 usage found)
        ApiServiceClient.java  (1 usage found)
            33 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.component  (2 usages found)
        Component.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        ComponentEvent.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.component.instance  (1 usage found)
        ComponentInstanceEvent.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.containerlaunch  (4 usages found)
        AbstractLauncher.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        CommandLineBuilder.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ContainerLaunchService.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        JavaCommandLineBuilder.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.provider  (1 usage found)
        ProviderService.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.registry  (1 usage found)
        YarnRegistryViewForProviders.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.service.utils  (6 usages found)
        ClientRegistryBinder.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        ConfigHelper.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        CoreFileSystem.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
        PublishedConfigurationOutputter.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
        ServiceApiUtil.java  (1 usage found)
            23 import com.google.common.base.Preconditions;
        ServiceUtils.java  (1 usage found)
            21 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.util  (1 usage found)
        BoundedAppender.java  (1 usage found)
            22 import com.google.common.base.Preconditions;
    org.apache.hadoop.yarn.webapp  (5 usages found)
        Dispatcher.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.checkState;
        Router.java  (2 usages found)
            21 import static com.google.common.base.Preconditions.checkNotNull;
            22 import static com.google.common.base.Preconditions.checkState;
        WebApp.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.checkNotNull;
        WebApps.java  (1 usage found)
            21 import static com.google.common.base.Preconditions.checkNotNull;
    org.apache.hadoop.yarn.webapp.hamlet  (1 usage found)
        HamletImpl.java  (1 usage found)
            22 import static com.google.common.base.Preconditions.*;
    org.apache.hadoop.yarn.webapp.hamlet2  (1 usage found)
        HamletImpl.java  (1 usage found)
            22 import static com.google.common.base.Preconditions.*;

{code}
"
remove guava Preconditions from Hadoop-common-project modules,13316300,Resolved,Major,Fixed,11/Jul/20 18:32,14/Oct/21 10:04,,Replace guava Preconditions by internal implementations that rely on java8+ APIs in the hadoop
Upgrade hbase to 1.4.13 on branch-2.10,13326606,Resolved,Major,Fixed,09/Sep/20 06:51,10/Sep/20 23:55,,hbase.version must be updated to address CVE-2018-8025 on branch-2.10.
ABFS: Add debug log for rename failures,13319804,Resolved,Major,Fixed,28/Jul/20 17:08,05/Aug/20 17:12,3.3.0,The JIRA [HADOOP-16281|https://issues.apache.org/jira/browse/HADOOP-16281] has not yet been concluded. Untill then the logline could help debugging.
Using lz4-java in Lz4Codec,13330019,Resolved,Major,Fixed,29/Sep/20 18:54,18/Nov/20 20:09,3.3.0,"In Hadoop, we use native libs for lz4 codec which has several disadvantages:

It requires native libhadoop to be installed in system LD_LIBRARY_PATH, and they have to be installed separately on each node of the clusters, container images, or local test environments which adds huge complexities from deployment point of view. In some environments, it requires compiling the natives from sources which is non-trivial. Also, this approach is platform dependent; the binary may not work in different platform, so it requires recompilation.
It requires extra configuration of java.library.path to load the natives, and it results higher application deployment and maintenance cost for users.
Projects such as Spark use [lz4-java|https://github.com/lz4/lz4-java] which is JNI-based implementation. It contains native binaries in jar file, and it can automatically load the native binaries into JVM from jar without any setup. If a native implementation can not be found for a platform, it can fallback to pure-java implementation of lz4.
"
Remove non-inclusive terminology from Hadoop Common,13320250,Open,Major,,30/Jul/20 18:22,,,"http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/202007.mbox/%3CCAAaVJWVXhsv4tn1KOQkKYTaQ441Yb8y7s%2BR_GnESwduB1iFxOA%40mail.gmail.com%3E

This JIRA is to remove offensive and non-inclusive terminology from Hadoop. The simple ones are whitelist/blacklist and master/slave. However this JIRA can also serve as a place to fix other non-inclusive terminology (e.g., binary gendered
examples, ""Alice"" doing the wrong security thing systematically).

As [~curino] posted in his email, the IETF has created a draft for proposed alternatives
https://tools.ietf.org/id/draft-knodel-terminology-00.html#rfc.section.1.1.1"
Upgrade zookeeper to 3.4.14 on branch-2.10,13326596,Resolved,Major,Fixed,09/Sep/20 06:24,10/Sep/20 11:39,,"Since versions of zookeeper and curator have different history between branch-2.10 and trunk, I filed this to upgrade both zookeeper and curator on branch-2.10."
MR FileInput/Output formats to aggregate IOStatistics,13328240,Resolved,Major,Won't Fix,18/Sep/20 12:39,07/Jul/21 13:14,3.3.1,"The MR input formats are where IO takes place, so collect stats. Justifiable if Hive/Spark use these (I think they do). "
ABFS: Random read perf improvement,13326445,Resolved,Major,Fixed,08/Sep/20 11:06,05/Jul/21 13:47,3.3.0,"Random read if marginally read ahead was seen to improve perf for a TPCH query. 

 

Introducing fs.azure.readahead.range parameter which can be set by user.
Data will be populated in buffer for random reads as well which leads to lesser
remote calls.
This patch also changes the seek implementation to perform a lazy seek. Actual
seek is done when a read is initiated and data is not present in buffer else
date is returned from buffer thus reducing the number of remote calls."
Replace Guava initialization of Lists.newArrayList,13315424,Resolved,Major,Duplicate,07/Jul/20 14:53,18/Jun/21 17:13,,"There are unjustified use of Guava APIs to initialize LinkedLists and ArrayLists. This could be simply replaced by Java API.

By analyzing hadoop code, the best way to replace guava  is to do the following steps:
 * create a wrapper class org.apache.hadoop.util.unguava.Lists 
 * implement the following interfaces in Lists:
 ** public static <E> ArrayList<E> newArrayList()
 ** public static <E> ArrayList<E> newArrayList(E... elements)
 ** public static <E> ArrayList<E> newArrayList(Iterable<? extends E> elements)
 ** public static <E> ArrayList<E> newArrayList(Iterator<? extends E> elements)
 ** public static <E> ArrayList<E> newArrayListWithCapacity(int initialArraySize)
 ** public static <E> LinkedList<E> newLinkedList()
 ** public static <E> LinkedList<E> newLinkedList(Iterable<? extends E> elements)
 ** public static <E> List<E> asList(@Nullable E first, E[] rest)

 

After this class is created, we can simply replace the import statement in all the source code.

 
{code:java}
Targets
    Occurrences of 'com.google.common.collect.Lists;' in project with mask '*.java'
Found Occurrences  (246 usages found)
    org.apache.hadoop.conf  (1 usage found)
        TestReconfiguration.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.crypto  (1 usage found)
        CryptoCodec.java  (1 usage found)
            35 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.azurebfs  (3 usages found)
        ITestAbfsIdentityTransformer.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        ITestAzureBlobFilesystemAcl.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        ITestAzureBlobFileSystemCheckAccess.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.http.client  (2 usages found)
        BaseTestHttpFSWith.java  (1 usage found)
            77 import com.google.common.collect.Lists;
        HttpFSFileSystem.java  (1 usage found)
            75 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.permission  (2 usages found)
        AclStatus.java  (1 usage found)
            27 import com.google.common.collect.Lists;
        AclUtil.java  (1 usage found)
            26 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a  (3 usages found)
        ITestS3AFailureHandling.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        ITestS3GuardListConsistency.java  (1 usage found)
            34 import com.google.common.collect.Lists;
        S3AUtils.java  (1 usage found)
            57 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a.auth  (1 usage found)
        RolePolicies.java  (1 usage found)
            26 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a.commit  (2 usages found)
        ITestCommitOperations.java  (1 usage found)
            28 import com.google.common.collect.Lists;
        TestMagicCommitPaths.java  (1 usage found)
            25 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a.commit.staging  (3 usages found)
        StagingTestBase.java  (1 usage found)
            47 import com.google.common.collect.Lists;
        TestStagingPartitionedFileListing.java  (1 usage found)
            31 import com.google.common.collect.Lists;
        TestStagingPartitionedTaskCommit.java  (1 usage found)
            28 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a.impl  (2 usages found)
        RenameOperation.java  (1 usage found)
            30 import com.google.common.collect.Lists;
        TestPartialDeleteFailures.java  (1 usage found)
            37 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.s3a.s3guard  (3 usages found)
        DumpS3GuardDynamoTable.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        DynamoDBMetadataStore.java  (1 usage found)
            67 import com.google.common.collect.Lists;
        ITestDynamoDBMetadataStore.java  (1 usage found)
            49 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.shell  (1 usage found)
        AclCommands.java  (1 usage found)
            25 import com.google.common.collect.Lists;
    org.apache.hadoop.fs.viewfs  (2 usages found)
        TestViewFileSystemWithAcls.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        TestViewFsWithAcls.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.ha  (4 usages found)
        DummyHAService.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        NodeFencer.java  (1 usage found)
            31 import com.google.common.collect.Lists;
        TestNodeFencer.java  (1 usage found)
            32 import com.google.common.collect.Lists;
        TestShellCommandFencer.java  (1 usage found)
            26 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs  (23 usages found)
        AdminStatesBaseTest.java  (1 usage found)
            31 import com.google.common.collect.Lists;
        DFSClient.java  (1 usage found)
            198 import com.google.common.collect.Lists;
        DFSTestUtil.java  (1 usage found)
            80 import com.google.common.collect.Lists;
        DFSUtil.java  (1 usage found)
            110 import com.google.common.collect.Lists;
        DistributedFileSystem.java  (1 usage found)
            24 import com.google.common.collect.Lists;
        HAUtil.java  (1 usage found)
            61 import com.google.common.collect.Lists;
        MiniDFSCluster.java  (1 usage found)
            147 import com.google.common.collect.Lists;
        MiniDFSNNTopology.java  (1 usage found)
            26 import com.google.common.collect.Lists;
        TestBatchedListDirectories.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        TestBlockStoragePolicy.java  (1 usage found)
            27 import com.google.common.collect.Lists;
        TestDecommission.java  (1 usage found)
            42 import com.google.common.collect.Lists;
        TestDFSFinalize.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        TestDFSRollback.java  (1 usage found)
            44 import com.google.common.collect.Lists;
        TestDFSShell.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        TestEncryptionZones.java  (1 usage found)
            46 import com.google.common.collect.Lists;
        TestErasureCodingExerciseAPIs.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        TestExtendedAcls.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        TestMaintenanceState.java  (1 usage found)
            64 import com.google.common.collect.Lists;
        TestQuota.java  (1 usage found)
            68 import com.google.common.collect.Lists;
        TestSafeMode.java  (1 usage found)
            63 import com.google.common.collect.Lists;
        TestSafeModeWithStripedFile.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        TestTrashWithSecureEncryptionZones.java  (1 usage found)
            35 import com.google.common.collect.Lists;
        XAttrHelper.java  (1 usage found)
            30 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.protocol  (2 usages found)
        LocatedBlock.java  (1 usage found)
            32 import com.google.common.collect.Lists;
        ReencryptionStatus.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.protocol.datatransfer  (1 usage found)
        PipelineAck.java  (1 usage found)
            27 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.protocol.datatransfer.sasl  (2 usages found)
        SaslDataTransferClient.java  (1 usage found)
            68 import com.google.common.collect.Lists;
        SaslDataTransferServer.java  (1 usage found)
            63 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.protocolPB  (3 usages found)
        ClientNamenodeProtocolTranslatorPB.java  (1 usage found)
            27 import com.google.common.collect.Lists;
        PBHelperClient.java  (1 usage found)
            35 import com.google.common.collect.Lists;
        TestPBHelper.java  (1 usage found)
            120 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.qjournal  (2 usages found)
        MiniJournalCluster.java  (1 usage found)
            44 import com.google.common.collect.Lists;
        QJMTestUtil.java  (1 usage found)
            45 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.qjournal.client  (3 usages found)
        QuorumJournalManager.java  (1 usage found)
            64 import com.google.common.collect.Lists;
        TestQuorumJournalManager.java  (1 usage found)
            72 import com.google.common.collect.Lists;
        TestQuorumJournalManagerUnit.java  (1 usage found)
            61 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.qjournal.server  (3 usages found)
        JournalNode.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        JournalNodeSyncer.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        TestJournalNodeSync.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.security.token.delegation  (1 usage found)
        DelegationTokenSecretManager.java  (1 usage found)
            53 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.aliasmap  (1 usage found)
        InMemoryAliasMap.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.blockmanagement  (5 usages found)
        BlockStoragePolicySuite.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        PendingDataNodeMessages.java  (1 usage found)
            27 import com.google.common.collect.Lists;
        SlowDiskTracker.java  (1 usage found)
            28 import com.google.common.collect.Lists;
        TestBlockManager.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        TestRBWBlockInvalidation.java  (1 usage found)
            50 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.common  (1 usage found)
        Util.java  (1 usage found)
            40 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.common.blockaliasmap.impl  (1 usage found)
        TestInMemoryLevelDBAliasMapClient.java  (1 usage found)
            19 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.datanode  (9 usages found)
        BlockPoolManager.java  (1 usage found)
            34 import com.google.common.collect.Lists;
        BlockPoolSliceStorage.java  (1 usage found)
            52 import com.google.common.collect.Lists;
        BPOfferService.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        DataNode.java  (1 usage found)
            231 import com.google.common.collect.Lists;
        DataStorage.java  (1 usage found)
            67 import com.google.common.collect.Lists;
        TestBPOfferService.java  (1 usage found)
            93 import com.google.common.collect.Lists;
        TestDataNodeHotSwapVolumes.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        TestDataNodeMetrics.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        TestDatanodeRegister.java  (1 usage found)
            36 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (2 usages found)
        FsDatasetImpl.java  (1 usage found)
            125 import com.google.common.collect.Lists;
        TestFsDatasetImpl.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.datanode.web.webhdfs  (1 usage found)
        TestDataNodeUGIProvider.java  (1 usage found)
            55 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.diskbalancer.command  (3 usages found)
        Command.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        ReportCommand.java  (1 usage found)
            37 import com.google.common.collect.Lists;
        TestDiskBalancerCommand.java  (1 usage found)
            74 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.mover  (1 usage found)
        Mover.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode  (44 usages found)
        AclStorage.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        AclTransformation.java  (1 usage found)
            32 import com.google.common.collect.Lists;
        BackupImage.java  (1 usage found)
            34 import com.google.common.collect.Lists;
        CacheManager.java  (1 usage found)
            95 import com.google.common.collect.Lists;
        Checkpointer.java  (1 usage found)
            46 import com.google.common.collect.Lists;
        EncryptionZoneManager.java  (1 usage found)
            34 import com.google.common.collect.Lists;
        FileJournalManager.java  (1 usage found)
            51 import com.google.common.collect.Lists;
        FSAclBaseTest.java  (1 usage found)
            61 import com.google.common.collect.Lists;
        FSDirAttrOp.java  (1 usage found)
            41 import com.google.common.collect.Lists;
        FSDirEncryptionZoneOp.java  (1 usage found)
            56 import com.google.common.collect.Lists;
        FSDirErasureCodingOp.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        FSDirSatisfyStoragePolicyOp.java  (1 usage found)
            36 import com.google.common.collect.Lists;
        FSDirXAttrOp.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        FSEditLog.java  (1 usage found)
            114 import com.google.common.collect.Lists;
        FSEditLogOp.java  (1 usage found)
            144 import com.google.common.collect.Lists;
        FSImage.java  (1 usage found)
            80 import com.google.common.collect.Lists;
        FSImageFormatProtobuf.java  (1 usage found)
            79 import com.google.common.collect.Lists;
        FSImageTestUtil.java  (1 usage found)
            69 import com.google.common.collect.Lists;
        FSImageTransactionalStorageInspector.java  (1 usage found)
            39 import com.google.common.collect.Lists;
        FSNamesystem.java  (1 usage found)
            344 import com.google.common.collect.Lists;
        FSXAttrBaseTest.java  (1 usage found)
            62 import com.google.common.collect.Lists;
        InotifyFSEditLogOpTranslator.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        JournalSet.java  (1 usage found)
            45 import com.google.common.collect.Lists;
        LeaseManager.java  (1 usage found)
            40 import com.google.common.collect.Lists;
        NameNode.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        NameNodeRpcServer.java  (1 usage found)
            48 import com.google.common.collect.Lists;
        NNStorage.java  (1 usage found)
            62 import com.google.common.collect.Lists;
        NNStorageRetentionManager.java  (1 usage found)
            42 import com.google.common.collect.Lists;
        ReencryptionUpdater.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        SecondaryNameNode.java  (1 usage found)
            32 import com.google.common.collect.Lists;
        TestAclConfigFlag.java  (1 usage found)
            40 import com.google.common.collect.Lists;
        TestAclTransformation.java  (1 usage found)
            30 import com.google.common.collect.Lists;
        TestAuditLogger.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        TestBackupNode.java  (1 usage found)
            62 import com.google.common.collect.Lists;
        TestCheckpoint.java  (1 usage found)
            101 import com.google.common.collect.Lists;
        TestEditLog.java  (1 usage found)
            107 import com.google.common.collect.Lists;
        TestFSDirectory.java  (1 usage found)
            50 import com.google.common.collect.Lists;
        TestFSImage.java  (1 usage found)
            38 import com.google.common.collect.Lists;
        TestFSImageWithAcl.java  (1 usage found)
            42 import com.google.common.collect.Lists;
        TestINodeAttributeProvider.java  (1 usage found)
            48 import com.google.common.collect.Lists;
        TestLeaseManager.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        TestNNStorageRetentionManager.java  (1 usage found)
            54 import com.google.common.collect.Lists;
        TransferFsImage.java  (1 usage found)
            61 import com.google.common.collect.Lists;
        XAttrPermissionFilter.java  (1 usage found)
            28 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode.ha  (7 usages found)
        StandbyCheckpointer.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        TestDNFencing.java  (1 usage found)
            33 import com.google.common.collect.Lists;
        TestEditLogsDuringFailover.java  (1 usage found)
            48 import com.google.common.collect.Lists;
        TestHASafeMode.java  (1 usage found)
            76 import com.google.common.collect.Lists;
        TestRequestHedgingProxyProvider.java  (1 usage found)
            59 import com.google.common.collect.Lists;
        TestStandbyCheckpoints.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        TestStandbyInProgressTail.java  (1 usage found)
            53 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode.snapshot  (3 usages found)
        DirectorySnapshottableFeature.java  (1 usage found)
            52 import com.google.common.collect.Lists;
        TestAclWithSnapshot.java  (1 usage found)
            58 import com.google.common.collect.Lists;
        TestFileWithSnapshotFeature.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode.top.metrics  (1 usage found)
        TopMetrics.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode.top.window  (1 usage found)
        RollingWindowManager.java  (1 usage found)
            28 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.server.namenode.web.resources  (1 usage found)
        NamenodeWebHdfsMethods.java  (1 usage found)
            118 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.tools  (3 usages found)
        TestDFSAdmin.java  (1 usage found)
            29 import com.google.common.collect.Lists;
        TestViewFileSystemOverloadSchemeWithDFSAdmin.java  (1 usage found)
            50 import com.google.common.collect.Lists;
        TestViewFileSystemOverloadSchemeWithFSCommands.java  (1 usage found)
            49 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.tools.offlineImageViewer  (5 usages found)
        FSImageLoader.java  (1 usage found)
            61 import com.google.common.collect.Lists;
        PBImageTextWriter.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        PBImageXmlWriter.java  (1 usage found)
            72 import com.google.common.collect.Lists;
        TestOfflineImageViewer.java  (1 usage found)
            138 import com.google.common.collect.Lists;
        TestOfflineImageViewerForAcl.java  (1 usage found)
            74 import com.google.common.collect.Lists;
    org.apache.hadoop.hdfs.web  (5 usages found)
        JsonUtil.java  (1 usage found)
            43 import com.google.common.collect.Lists;
        JsonUtilClient.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        TestJsonUtil.java  (1 usage found)
            59 import com.google.common.collect.Lists;
        TestURLConnectionFactory.java  (1 usage found)
            36 import com.google.common.collect.Lists;
        WebHdfsFileSystem.java  (1 usage found)
            140 import com.google.common.collect.Lists;
    org.apache.hadoop.http  (1 usage found)
        HttpServer2.java  (1 usage found)
            55 import com.google.common.collect.Lists;
    org.apache.hadoop.lib.wsrs  (2 usages found)
        Parameters.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        ParametersProvider.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.mapred  (1 usage found)
        TestFileInputFormat.java  (1 usage found)
            46 import com.google.common.collect.Lists;
    org.apache.hadoop.mapred.nativetask.kvtest  (1 usage found)
        KVTest.java  (1 usage found)
            45 import com.google.common.collect.Lists;
    org.apache.hadoop.mapred.uploader  (1 usage found)
        TestFrameworkUploader.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.mapreduce.counters  (1 usage found)
        CounterGroupFactory.java  (1 usage found)
            24 import com.google.common.collect.Lists;
    org.apache.hadoop.mapreduce.lib.input  (2 usages found)
        FileInputFormat.java  (1 usage found)
            50 import com.google.common.collect.Lists;
        TestFileInputFormat.java  (1 usage found)
            57 import com.google.common.collect.Lists;
    org.apache.hadoop.mapreduce.util  (1 usage found)
        CountersStrings.java  (1 usage found)
            24 import com.google.common.collect.Lists;
    org.apache.hadoop.mapreduce.v2.app  (1 usage found)
        MockJobs.java  (1 usage found)
            73 import com.google.common.collect.Lists;
    org.apache.hadoop.maven.plugin.resourcegz  (1 usage found)
        ResourceGzMojo.java  (1 usage found)
            16 import com.google.common.collect.Lists;
    org.apache.hadoop.metrics2.impl  (5 usages found)
        MBeanInfoBuilder.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        MetricsCollectorImpl.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        MetricsRecordBuilderImpl.java  (1 usage found)
            24 import com.google.common.collect.Lists;
        MetricsSystemImpl.java  (1 usage found)
            33 import com.google.common.collect.Lists;
        TestKafkaMetrics.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.metrics2.util  (1 usage found)
        Servers.java  (1 usage found)
            25 import com.google.common.collect.Lists;
    org.apache.hadoop.net  (1 usage found)
        NetworkTopology.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.registry.client.impl.zk  (1 usage found)
        RegistrySecurity.java  (1 usage found)
            23 import com.google.common.collect.Lists;
    org.apache.hadoop.tools  (2 usages found)
        CopyListingFileStatus.java  (1 usage found)
            44 import com.google.common.collect.Lists;
        SimpleCopyListing.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.tools.dynamometer  (2 usages found)
        ApplicationMaster.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        Client.java  (1 usage found)
            24 import com.google.common.collect.Lists;
    org.apache.hadoop.tools.dynamometer.workloadgenerator  (1 usage found)
        CreateFileMapper.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.tools.dynamometer.workloadgenerator.audit  (1 usage found)
        AuditReplayMapper.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.tools.util  (1 usage found)
        TestDistCpUtils.java  (1 usage found)
            46 import com.google.common.collect.Lists;
    org.apache.hadoop.util  (5 usages found)
        ChunkedArrayList.java  (1 usage found)
            29 import com.google.common.collect.Lists;
        JvmPauseMonitor.java  (1 usage found)
            33 import com.google.common.collect.Lists;
        TestApplicationClassLoader.java  (1 usage found)
            45 import com.google.common.collect.Lists;
        TestDirectBufferPool.java  (1 usage found)
            29 import com.google.common.collect.Lists;
        ZKUtil.java  (1 usage found)
            32 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.api  (1 usage found)
        BasePBImplRecordsTest.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.client.api.impl  (1 usage found)
        NMClientImpl.java  (1 usage found)
            30 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.client.cli  (1 usage found)
        NodeAttributesCLI.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager  (1 usage found)
        TestContainerExecutor.java  (1 usage found)
            33 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager  (1 usage found)
        TestContainerManager.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher  (1 usage found)
        TestContainerLaunch.java  (1 usage found)
            58 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu  (1 usage found)
        TestGpuResourceAllocator.java  (1 usage found)
            61 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec  (2 usages found)
        TestNECVEPlugin.java  (1 usage found)
            49 import org.apache.curator.shaded.com.google.common.collect.Lists;
        TestVEDeviceDiscoverer.java  (1 usage found)
            50 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga  (1 usage found)
        TestIntelFpgaOpenclPlugin.java  (1 usage found)
            25 import org.apache.curator.shaded.com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu  (2 usages found)
        GpuDiscoverer.java  (1 usage found)
            25 import com.google.common.collect.Lists;
        TestGpuResourcePlugin.java  (1 usage found)
            24 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager  (2 usages found)
        MockNodes.java  (1 usage found)
            46 import com.google.common.collect.Lists;
        TestAppManager.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager  (2 usages found)
        MockAsm.java  (1 usage found)
            59 import com.google.common.collect.Lists;
        TestAMRestart.java  (1 usage found)
            29 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.recovery  (1 usage found)
        TestZKRMStateStore.java  (1 usage found)
            77 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.rmapp  (1 usage found)
        TestRMAppTransitions.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler  (2 usages found)
        SchedulerUtils.java  (1 usage found)
            27 import com.google.common.collect.Lists;
        TestAbstractYarnScheduler.java  (1 usage found)
            36 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities  (1 usage found)
        ActivitiesManager.java  (1 usage found)
            22 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor  (1 usage found)
        PlacementConstraintProcessor.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair  (5 usages found)
        FairScheduler.java  (1 usage found)
            23 import com.google.common.collect.Lists;
        FairSchedulerConfiguration.java  (1 usage found)
            28 import com.google.common.collect.Lists;
        FairSchedulerTestBase.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        FSSchedulerNode.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        TestFairScheduler.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocationfile  (3 usages found)
        AllocationFileQueue.java  (1 usage found)
            19 import com.google.common.collect.Lists;
        AllocationFileQueuePlacementPolicy.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        AllocationFileQueuePlacementRule.java  (1 usage found)
            20 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter  (2 usages found)
        TestFSConfigToCSConfigArgumentHandler.java  (1 usage found)
            48 import com.google.common.collect.Lists;
        TestQueuePlacementConverter.java  (1 usage found)
            49 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.webapp  (1 usage found)
        ActivitiesTestUtils.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.resourcemanager.webapp.helper  (2 usages found)
        ResourceRequestsJsonVerifications.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        ResourceRequestsXmlVerifications.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.server.webproxy  (1 usage found)
        TestProxyUriUtils.java  (1 usage found)
            32 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service  (3 usages found)
        MockServiceAM.java  (1 usage found)
            22 import com.google.common.collect.Lists;
        TestDefaultUpgradeComponentsFinder.java  (1 usage found)
            20 import com.google.common.collect.Lists;
        TestYarnNativeServices.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service.client  (2 usages found)
        TestApiServiceClient.java  (1 usage found)
            29 import com.google.common.collect.Lists;
        TestServiceClient.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service.component.instance  (1 usage found)
        TestComponentInstance.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service.provider  (1 usage found)
        TestAbstractProviderService.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service.utils  (1 usage found)
        TestFilterUtils.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.service.webapp  (1 usage found)
        ApiServer.java  (1 usage found)
            21 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.util.resource  (1 usage found)
        CustomResourceTypesConfigurationProvider.java  (1 usage found)
            19 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.webapp  (2 usages found)
        ResponseInfo.java  (1 usage found)
            21 import com.google.common.collect.Lists;
        WebApp.java  (1 usage found)
            38 import com.google.common.collect.Lists;
    org.apache.hadoop.yarn.webapp.view  (2 usages found)
        JQueryUI.java  (1 usage found)
            31 import com.google.common.collect.Lists;
        TwoColumnLayout.java  (1 usage found)
            28 import com.google.common.collect.Lists;
{code}
 

 "
Backport HADOOP-13230 listing changes for preserved directory markers to 3.1.x,13324561,Resolved,Major,Won't Fix,25/Aug/20 21:57,10/Jun/21 08:03,3.1.4,"Backport a small subset of HADOOP-17199 to branch-3.1

No path capabities, declarative test syntax etc

just

-getFileStatus/list
-markers changes to bucket-info
-startup info message if option is set
-relevant test changes"
Configuration.getValByRegex() shouldn't update the results while fetching.,13316920,Resolved,Major,Fixed,15/Jul/20 12:39,17/Jul/20 09:42,3.1.3,"We have seen this stacktrace while using ABFS file system. After analysing the stack trace we can see that getValByRegex() is reading the properties and substituting the value in the same call. This may cause the ConcurrentModificationException. 
{code:java}
Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1877) ... 18 more Caused by: java.util.ConcurrentModificationException at java.util.Hashtable$Enumerator.next(Hashtable.java:1387) at org.apache.hadoop.conf.Configuration.getValByRegex(Configuration.java:3855) at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.validateStorageAccountKeys(AbfsConfiguration.java:689) at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.<init>(AbfsConfiguration.java:237) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:154) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:113) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3396) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3456) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3424) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:518) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
 


{code}"
Erasure Coding: Native library memory leak,13322860,Resolved,Major,Fixed,15/Aug/20 10:23,24/Aug/20 12:21,3.1.3,"We use both {{apache-hadoop-3.1.3}} and {{CDH-6.1.1-1.cdh6.1.1.p0.875250}} HDFS in production, and both of them have the memory increasing over {{-Xmx}} value. 

!image-2020-08-15-18-26-44-744.png!

 

We use EC strategy to to save storage costs.

This's the jvm options:
{code:java}
-Dproc_datanode -Dhdfs.audit.logger=INFO,RFAAUDIT -Dsecurity.audit.logger=INFO,RFAS -Djava.net.preferIPv4Stack=true -Xms8589934592 -Xmx8589934592 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError ...{code}
The max jvm heapsize is 8GB, but we can see the datanode RSS memory is 48g. All the other datanodes in this hdfs cluster has the same issue.
{code:java}
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 
226044 hdfs 20 0 50.6g 48g 4780 S 90.5 77.0 14728:27 /usr/java/jdk1.8.0_162/bin/java -Dproc_datanode{code}
 

This too much memory used leads to my machine unresponsive(if enable swap), or oom-killer happens.

 "
Make UGI support forceful relogin from keytab ignoring the last login time,13319658,Resolved,Major,Fixed,28/Jul/20 01:45,27/Aug/20 22:22,2.10.0,"Currently we have a relogin() method in UGI which attempts to login if there is no login attempted in the last 10 minutes or configured amount of time

We should also have provision for doing a forceful relogin irrespective of the time window that the client can choose to use it if needed . Consider the below scenario:
 # SASL Server is reimaged and new keytabs are fetched with refreshing the password
 # SASL client connection to the server would fail when it tries with the cached service ticket
 # We should try to logout to clear the service tickets in cache and then try to login back in such scenarios. But since the current relogin() doesn't guarantee a login, it could cause an issue
 # A forceful relogin in this case would help after logout

 "
Please fix CVEs by removing reference to htrace-core4,13320261,Resolved,Major,Duplicate,30/Jul/20 18:54,06/Aug/20 10:31,3.3.0,"htrace-core4 is a retired project and even on the latest version they Shade Jackson databind version 2.4.0 which has the following CVEs:
|cve|severity|cvss|
|CVE-2017-15095|critical|9.8|
|CVE-2018-1000873|medium|6.5|
|CVE-2018-14718|critical|9.8|
|CVE-2018-5968|high|8.1|
|CVE-2018-7489|critical|9.8|
|CVE-2019-14540|critical|9.8|
|CVE-2019-14893|critical|9.8|
|CVE-2019-16335|critical|9.8|
|CVE-2019-16942|critical|9.8|
|CVE-2019-16943|critical|9.8|
|CVE-2019-17267|critical|9.8|
|CVE-2019-17531|critical|9.8|
|CVE-2019-20330|critical|9.8|
|CVE-2020-10672|high|8.8|
|CVE-2020-10673|high|8.8|
|CVE-2020-10968|high|8.8|
|CVE-2020-10969|high|8.8|
|CVE-2020-11111|high|8.8|
|CVE-2020-11112|high|8.8|
|CVE-2020-11113|high|8.8|
|CVE-2020-11619|critical|9.8|
|CVE-2020-11620|critical|9.8|
|CVE-2020-14060|high|8.1|
|CVE-2020-14061|high|8.1|
|CVE-2020-14062|high|8.1|
|CVE-2020-14195|high|8.1|
|CVE-2020-8840|critical|9.8|
|CVE-2020-9546|critical|9.8|
|CVE-2020-9547|critical|9.8|
|CVE-2020-9548|critical|9.8|

 

Our security team is trying to block us from using hadoop because of this"
hadoop-azure parallel tests not working on recent JDKs,13314473,Resolved,Major,Fixed,01/Jul/20 12:59,17/May/21 03:11,3.3.0,"recent JDKs are failing to run the wasb or abfs parallel test runs -unable to instantiate the javascript engine.

Maybe it's been cut from the JVM or the ant script task can't bind to it.

Fix is as HADOOP-14696 -use our own plugin to set up the test dirs"
Test timeout for ITestAbfsInputStreamStatistics#testReadAheadCounters,13319553,Resolved,Major,Fixed,27/Jul/20 13:42,08/Sep/20 09:12,3.3.0,"Intermittent test timeout for ITestAbfsInputStreamStatistics#testReadAheadCounters happening due to race conditions in readAhead threads.

Test error:


{code:java}
[ERROR] testReadAheadCounters(org.apache.hadoop.fs.azurebfs.ITestAbfsInputStreamStatistics)  Time elapsed: 30.723 s  <<< ERROR!org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds        at java.lang.Thread.sleep(Native Method)        at org.apache.hadoop.fs.azurebfs.ITestAbfsInputStreamStatistics.testReadAheadCounters(ITestAbfsInputStreamStatistics.java:346)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:498)        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)        at java.util.concurrent.FutureTask.run(FutureTask.java:266)        at java.lang.Thread.run(Thread.java:748) {code}
Possible Reasoning:

- ReadAhead queue doesn't get completed and hence the counter values are not satisfied in 30 seconds time for some systems.

- The condition that readAheadBytesRead and remoteBytesRead counter values need to be greater than or equal to 4KB and 32KB respectively doesn't occur in some machines due to the fact that sometimes instead of reading for readAhead Buffer, remote reads are performed due to Threads still being in the readAhead queue to fill that buffer. Thus resulting in either of the 2 counter values to be not satisfying the condition and getting in an infinite loop and hence timing out the test eventually.

Possible Fixes:

- Write better test(That would pass under all conditions).
- Maybe UT instead of IT?

Possible fix to better the test would be preferable and UT as the last resort."
Implement FileSystem.listStatusIterator() in S3AFileSystem,13329008,Resolved,Major,Fixed,23/Sep/20 14:20,07/Oct/20 13:02,3.3.0,"Currently S3AFileSystem only implements listStatus() api which returns an array. Once we implement the listStatusIterator(), clients can benefit from the async listing done recently 

https://issues.apache.org/jira/browse/HADOOP-17074  by performing some tasks on files while iterating them.

 

CC [~stevel]"
Memory leak MemoryStore streaming temp files,13329306,Open,Major,,24/Sep/20 23:57,,," I saw a ticket that was mentioning the similar issue and said it was solved in 3.3.0. But I could not find the extract ticket, seems to be this one - https://issues.apache.org/jira/browse/HADOOP-15658.

 

But I am still observing the large amount of temp parquet files accumulated in the MemoryStore below.

 

!image-2020-09-25-09-57-44-849.png!

!image-2020-09-25-09-57-59-774.png!"
ABFS: Fix For Idempotency code,13317073,Resolved,Major,Fixed,16/Jul/20 04:09,23/Jul/20 05:54,3.4.0,"Trigger to handle the idempotency code introduced in https://issues.apache.org/jira/browse/HADOOP-17015 is incomplete. 

This PR is to fix the issue."
 Create socket address leveraging URI cache,13324320,Resolved,Major,Fixed,24/Aug/20 17:27,30/Mar/21 11:00,,"Note：Not only the hdfs client can get the current benefit, all callers of NetUtils.createSocketAddr will get the benefit. Just use hdfs client as an example.

 

Hdfs client selects best DN for hdfs Block. method call stack:

DFSInputStream.chooseDataNode -> getBestNodeDNAddrPair -> NetUtils.createSocketAddr

NetUtils.createSocketAddr creates the corresponding InetSocketAddress based on the host and port. There are some heavier operations in the NetUtils.createSocketAddr method, for example: URI.create(target), so NetUtils.createSocketAddr takes more time to execute.

The following is my performance report. The report is based on HBase calling hdfs. HBase is a high-frequency access client for hdfs, because HBase read operations often access a small DataBlock (about 64k) instead of the entire HFile. In the case of high frequency access, the NetUtils.createSocketAddr method is time-consuming.
h3. Test Environment：

 
{code:java}
HBase version: 2.1.0
JVM: -Xmx2g -Xms2g 
hadoop hdfs version: 2.7.4
disk:SSD
OS:CentOS Linux release 7.4.1708 (Core)
JMH Benchmark: @Fork(value = 1) 
@Warmup(iterations = 300) 
@Measurement(iterations = 300)
{code}
h4. Before Optimization FlameGraph:

In the figure, we can see that DFSInputStream.getBestNodeDNAddrPair accounts for 4.86% of the entire CPU, and the creation of URIs accounts for a larger proportion.

!Before Optimization remark.png!
h3. Optimization ideas:

NetUtils.createSocketAddr creates InetSocketAddress based on host and port. Here we can add Cache to InetSocketAddress. The key of Cache is host and port, and the value is InetSocketAddress.
h4. After Optimization FlameGraph:

In the figure, we can see that DFSInputStream.getBestNodeDNAddrPair accounts for 0.54% of the entire CPU. Here, ConcurrentHashMap is used as the Cache, and the ConcurrentHashMap.get() method gets data from the Cache. The CPU usage of DFSInputStream.getBestNodeDNAddrPair has been optimized from 4.86% to 0.54%.

!After Optimization remark.png!
h3. Original FlameGraph link:

[Before Optimization|https://drive.google.com/file/d/133L5m75u2tu_KgKfGHZLEUzGR0XAfUl6/view?usp=sharing]

[After Optimization FlameGraph|https://drive.google.com/file/d/133L5m75u2tu_KgKfGHZLEUzGR0XAfUl6/view?usp=sharing]"
ABFS: Support for Client Correlation ID,13327907,Resolved,Major,Abandoned,16/Sep/20 18:36,16/Mar/21 05:08,3.3.0,"Introducing a client correlation ID that appears in the Azure Storage diagnostic logs. This will modify the client request header to include the client-provided identifier for correlating requests. A valid correlation ID comprises alphanumeric characters and/or hyphens, and can be up to 72 characters in length. Invalid or missing entries for this configuration are substituted with the empty string default."
[JDK 11] Upgrade SpotBugs to 4.1.3 to fix false-positive warnings,13328117,Resolved,Major,Duplicate,17/Sep/20 19:02,11/Mar/21 02:00,,"In Java 11, there are a lot of false-positive findbugs warnings in try-with-resources.
Ref: https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java11-linux-x86_64/1/artifact/out/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html

This issue has been fixed by https://github.com/spotbugs/spotbugs/pull/1248 but now there are no releases that include this fix."
S3A FileSystem does not correctly delete directories with fake entries,13323932,Resolved,Major,Fixed,21/Aug/20 10:13,04/Feb/21 13:49,3.2.0,"h3. Summary

We are facing an issue where the Hadoop S3A Filesystem gets confused by fake directory objects in S3. Specifically trying to recursively remove a whole Hadoop directory (i.e. all objects with the same S3 prefix) doesn't work as expected.
h2. Background

We are using Alluxio together with S3 as our deep store. For some infrastructure reasons we decided to directly write to S3 (bypassing Alluxio) with our Spark applications, and only use Alluxio for reading data.

When we directly write our results into S3 with Spark, everything is fine. But once Alluxio accesses S3, it will create these fake directory entries. When we now try to overwrite existing data in S3 with a Spark application, the result is incorrect, since Spark will only remove the fake directory entry, but not all other objects below that prefix in S3.

Of course it is questionable if Alluxio is doing the right thing, but on the other hand it seems that Hadoop also does not behave as we expected.
h2. Steps to Reproduce

The following steps only require AWS CLI and Hadoop CLI to reproduce the issue we are facing:
h3. Initial setup
{code:bash}
# First step: Create an new and empty bucket in S3
$ aws s3 mb s3://dimajix-tmp                                                                                                                                                            make_bucket: dimajix-tmp

$ aws s3 ls
2020-08-21 11:19:50 dimajix-tmp

# Upload some data
$ aws s3 cp some_file.txt s3://dimajix-tmp/tmp/
upload: ./some_file.txt to s3://dimajix-tmp/tmp/some_file.txt

$ aws s3 ls s3://dimajix-tmp/tmp/                                                                                                                                                       2020-08-21 11:23:35          0 some_file.txt

# Check that Hadoop can list the file
$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/
Found 1 items
drwxrwxrwx   - kaya kaya          0 2020-08-21 11:24 s3a://dimajix-tmp/tmp

$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/tmp/
-rw-rw-rw-   1 kaya kaya          0 2020-08-21 11:23 s3a://dimajix-tmp/tmp/some_file.txt


# Evil step: Create fake directory entry in S3
$ aws s3api put-object --bucket dimajix-tmp --key tmp/
{
    ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\""""
}

# Look into S3, ensure that fake directory entry was created
$ aws s3 ls s3://dimajix-tmp/tmp/
2020-08-21 11:25:40          0
2020-08-21 11:23:35          0 some_file.txt

# Look into S3 using Hadoop CLI, ensure that everything looks okay (which is the case)
$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/tmp/
Found 1 items
-rw-rw-rw-   1 kaya kaya          0 2020-08-21 11:23 s3a://dimajix-tmp/tmp/some_file.txt
{code}
h3. Reproduce questionable behaviour: Try to recursively delete directory
{code:bash}
# Bug: Now try to delete the directory with Hadoop CLI
$ /opt/hadoop/bin/hdfs dfs -rm s3a://dimajix-tmp/tmp/
rm: `s3a://dimajix-tmp/tmp': Is a directory

# Okay, that didn't work out, Hadoop interprets the prefix as a directory (which is fine). It also did not delete anything, as we can see in S3:
$ aws s3 ls s3://dimajix-tmp/tmp/
2020-08-21 11:25:40          0
2020-08-21 11:23:35          0 some_file.txt

# Now let's try a little bit harder by trying to recursively delete the directory
$ /opt/hadoop/bin/hdfs dfs -rm -r s3a://dimajix-tmp/tmp/
Deleted s3a://dimajix-tmp/tmp

# Everything looked fine so far. But let's inspect S3 directly. We'll find that only the prefix (fake directory entry) has been removed. The file in the directory is still there.
$ aws s3 ls s3://dimajix-tmp/tmp/
2020-08-21 11:23:35          0 some_file.txt

# We can also use Hadoop CLI to check that the directory containing the file is still present, although we wanted to delete it above.
$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/tmp/
Found 1 items
-rw-rw-rw-   1 kaya kaya          0 2020-08-21 11:23 s3a://dimajix-tmp/tmp/some_file.txt
{code}
h3. Remedy by performing second delete
{code:bash}
# Now let's perform the same action again to remove the directory and all its contents
$ /opt/hadoop/bin/hdfs dfs -rm -r s3a://dimajix-tmp/tmp/
Deleted s3a://dimajix-tmp/tmp

# Finally everything was cleaned up.
$ aws s3 ls s3://dimajix-tmp/tmp/

$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/tmp/
ls: `s3a://dimajix-tmp/tmp/': No such file or directory

$ /opt/hadoop/bin/hdfs dfs -ls s3a://dimajix-tmp/                                                                                                                                      
{code}
h2. Actual Behaviour vs Expected Behaviour

When trying to recursively remove a directory using Hadoop CLI, I expect that the S3 prefix and all objects under that prefix are removed from S3. But actually only the prefix (fake directory entry) itself is removed."
MagicS3GuardCommitter fails with `pendingset` already exists,13327018,Resolved,Major,Fixed,11/Sep/20 07:05,26/Jan/21 13:35,3.2.0,"In `trunk/branch-3.3/branch-3.2`, `MagicS3GuardCommitter.innerCommitTask` has `false` at `pendingSet.save`.
{code}
    try {
      pendingSet.save(getDestFS(), taskOutcomePath, false);
    } catch (IOException e) {
      LOG.warn(""Failed to save task commit data to {} "",
          taskOutcomePath, e);
      abortPendingUploads(context, pendingSet.getCommits(), true);
      throw e;
    }
{code}

And, it can cause a job failure like the following.
{code}
WARN TaskSetManager: Lost task 1562.1 in stage 1.0 (TID 1788, 100.92.11.63, executor 26): org.apache.spark.SparkException: Task failed while writing rows.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:123)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: s3a://xxx/__magic/app-attempt-0000/task_20200911063607_0001_m_001562.pendingset already exists
    at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:761)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
    at org.apache.hadoop.util.JsonSerialization.save(JsonSerialization.java:269)
    at org.apache.hadoop.fs.s3a.commit.files.PendingSet.save(PendingSet.java:170)
    at org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter.innerCommitTask(MagicS3GuardCommitter.java:220)
    at org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter.commitTask(MagicS3GuardCommitter.java:165)
    at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)
    at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)
    at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:244)
    at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
{code}

{code}
20/09/11 07:44:38 ERROR TaskSetManager: Task 957.1 in stage 1.0 (TID 1412) can not write to output file: org.apache.hadoop.fs.FileAlreadyExistsException: s3a://xxx/t/__magic/app-attempt-0000/task_20200911073922_0001_m_000957.pendingset already exists; not retrying
{code}

The above happens in EKS with S3 environment and the job failure happens when some executor containers are killed by K8s"
ABFS Streams to  support IOStatistics API,13328237,Resolved,Major,Fixed,18/Sep/20 12:31,12/Jan/21 15:49,3.3.1,ABFS input/output streams to support IOStatistics API
Fixing javadoc in ListingOperationCallbacks,13321043,Resolved,Major,Fixed,05/Aug/20 08:27,05/Aug/20 11:41,,"{{mvn javadoc:javadoc -pl hadoop-tools/hadoop-aws}} is failing:
{noformat}
[ERROR] /Users/aajisaka/git/hadoop/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ListingOperationCallbacks.java:80: error: unexpected text
[ERROR]    * {@link this.getMaxKeys()}.
[ERROR]      ^
{noformat}"
Refactor S3A Listing code for better isolation,13316956,Resolved,Major,Fixed,15/Jul/20 14:52,04/Aug/20 16:13,3.3.0,"Currently Listing has a S3AFileSystem instance to use its required methods thus giving listing access to all of S3AFileSystem methods. Ideal way would be to use a callback interface in Listing which can in turn call s3Afs methods thus restricting the listing to methods which are absolutely required.

 

CC [~stevel@apache.org]"
upgrade jetty to 9.4.21,13318923,Resolved,Major,Duplicate,23/Jul/20 12:16,10/Nov/20 07:50,3.3.0,"I have tried to configure and start Hadoop KMS service, it was failed to start the error log messages:
{noformat}
2020-07-23 10:57:31,872 INFO  Server - jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_252-8u252-b09-1~18.04-b09
2020-07-23 10:57:31,899 INFO  session - DefaultSessionIdManager workerName=node0
2020-07-23 10:57:31,899 INFO  session - No SessionScavenger set, using defaults
2020-07-23 10:57:31,901 INFO  session - node0 Scavenging every 660000ms
2020-07-23 10:57:31,912 INFO  ContextHandler - Started o.e.j.s.ServletContextHandler@5bf0d49{logs,/logs,file:///opt/hadoop-3.4.0-SNAPSHOT/logs/,AVAILABLE}
2020-07-23 10:57:31,913 INFO  ContextHandler - Started o.e.j.s.ServletContextHandler@7c7a06ec{static,/static,jar:file:/opt/hadoop-3.4.0-SNAPSHOT/share/hadoop/common/hadoop-kms-3.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2020-07-23 10:57:31,986 INFO  TypeUtil - JVM Runtime does not support Modules
2020-07-23 10:57:32,015 INFO  KMSWebApp - -------------------------------------------------------------
2020-07-23 10:57:32,015 INFO  KMSWebApp -   Java runtime version : 1.8.0_252-8u252-b09-1~18.04-b09
2020-07-23 10:57:32,015 INFO  KMSWebApp -   User: hadoop
2020-07-23 10:57:32,015 INFO  KMSWebApp -   KMS Hadoop Version: 3.4.0-SNAPSHOT
2020-07-23 10:57:32,015 INFO  KMSWebApp - -------------------------------------------------------------
2020-07-23 10:57:32,023 INFO  KMSACLs - 'CREATE' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'DELETE' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'ROLLOVER' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'GET' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'GET_KEYS' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'GET_METADATA' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'SET_KEY_MATERIAL' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'GENERATE_EEK' ACL '*'
2020-07-23 10:57:32,024 INFO  KMSACLs - 'DECRYPT_EEK' ACL '*'
2020-07-23 10:57:32,025 INFO  KMSACLs - default.key.acl. for KEY_OP 'READ' is set to '*'
2020-07-23 10:57:32,025 INFO  KMSACLs - default.key.acl. for KEY_OP 'MANAGEMENT' is set to '*'
2020-07-23 10:57:32,025 INFO  KMSACLs - default.key.acl. for KEY_OP 'GENERATE_EEK' is set to '*'
2020-07-23 10:57:32,025 INFO  KMSACLs - default.key.acl. for KEY_OP 'DECRYPT_EEK' is set to '*'
2020-07-23 10:57:32,080 INFO  KMSAudit - Initializing audit logger class org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger
2020-07-23 10:57:32,537 INFO  KMSWebServer - SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down KMSWebServer at hadoop-benchmark/172.17.0.2{noformat}
I have googled the error and found there is a simlar issue： [https://github.com/eclipse/jetty.project/issues/4064]

It looks like a bug of jetty and has  been fixed in jetty>=9.4.21, currently Hadoop use the jetty is version of 9.4.20, see hadoop-project/pom.xml."
improve s3guard markers command line tool,13324535,Resolved,Major,Fixed,25/Aug/20 19:18,04/Sep/20 14:01,3.4.0,"The s3guard markers audit -expected N command is meant to verify that the marker count is N, but

* it isn't verified if the #of markers is 0, so you can't use it to assert that markers have been created
* it doesn't work for tests where you expect a minimum number of markers. 

It's essentially setting a max #of markers

Proposed: explicit -min, -max args to declare a specific range of values"
ABFS: Test testNegativeScenariosForCreateOverwriteDisabled fails for non-HNS account,13328784,Resolved,Major,Fixed,22/Sep/20 13:43,23/Sep/20 16:03,3.3.0,"Test testNegativeScenariosForCreateOverwriteDisabled fails when run against a non-HNS account. The test creates a mock AbfsClient to mimic negative scenarios.

Mock is triggered for valid values that come in for permission and umask while creating a file. Permission and umask get defaulted to null values with driver when creating a file for a nonHNS account. The mock trigger was not enabled for these null parameters.

 "
s3a rename() now requires s3:deleteObjectVersion permission,13327512,Resolved,Major,Fixed,14/Sep/20 19:53,14/Oct/20 09:00,3.4.0,"With the directory marker change (HADOOP-13230) you need the s3:deleteObjectVersion permission in your role, else the operation will fail in the bulk delete, *if S3Guard is in use*

Root cause
-if fileStatus has a versionId, we pass that in to the delete KeyVersion pair
-an unguarded listing doesn't get that versionId, so this is not an issue
-but if files in a directory were previously created such that S3Guard has their versionId in its tables, that is used in the request
-which then fails if the caller doesn't have the permission

Although we say ""you need s3:delete*"", this is a regression as any IAM role without the permission will have rename fail during delete"
ABFS: Enable checkaccess API,13320910,Resolved,Major,Fixed,04/Aug/20 17:18,01/Oct/20 20:30,3.3.0,Enable check access on ABFS. Currently by default the same if disabled.
Shade guava 29.0-jre in hadoop thirdparty,13328624,Resolved,Major,Fixed,21/Sep/20 18:55,27/Sep/20 14:56,,Shade guava 27.0-jre in hadoop-thirdparty
delta.io spark task commit encountering S3 cached 404/FileNotFoundException,13323878,Resolved,Major,Duplicate,21/Aug/20 03:41,21/Sep/20 13:30,3.1.2,"Hi,

When using spark streaming with deltalake, I got the following exception occasionally, something like 1 out of 100. Thanks.
{code:java}
Caused by: java.io.FileNotFoundException: No such file or directory: s3a://[pathToFolder]/date=2020-07-29/part-00005-046af631-7198-422c-8cc8-8d3adfb4413e.c000.snappy.parquet
 at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2255)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol$$anonfun$8.apply(DelayedCommitProtocol.scala:141)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol$$anonfun$8.apply(DelayedCommitProtocol.scala:139)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
 at scala.collection.AbstractTraversable.map(Traversable.scala:104)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol.commitTask(DelayedCommitProtocol.scala:139)
 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242){code}
 

-----Environment----

hadoop = ""3.1.2""
 hadoop-aws = ""3.1.2""

spark = ""2.4.5""

spark-on-k8s-operator = ""v1beta2-1.1.2-2.4.5""

 

deployed into AWS EKS kubernates. Version information below:

Server Version: version.Info\{Major:""1"", Minor:""16+"", GitVersion:""v1.16.8-eks-e16311"", GitCommit:""e163110a04dcb2f39c3325af96d019b4925419eb"", GitTreeState:""clean"", BuildDate:""2020-03-27T22:37:12Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}"
Remove whitelist/blacklist terminology from Hadoop Common,13320251,Patch Available,Major,,30/Jul/20 18:23,,,
Add RPC Quota to NameNode.,13328063,Patch Available,Major,,17/Sep/20 13:59,,,"My users recently complained 'The NameNode is much slower than before' to me. The reason is the cluster and jobs are getting bigger and bigger. So is the pressure of NameNode. I explained the pressure was heavy so the rpc requests must wait, but they were not satisfied. Because they thought the original quality of the service should be guaranteed. They were never told the NameNode would be so slow and all their services were built based on the assumption that the NameNode would always respond as fast as before.
 From the user's standpoint they are right. So my question is how to give the user a guarantee about RPC requests. The natural idea is RPC Quota, just like name quota and space quota. The quota can help users to understand the rpc requests are also a limit resource. And when they apply quota to the administrator, the admin would have the chance to distribute the resource and make a plan for the cluster. e.g. We have 200 quota for addBlock and they are all allocated. Even the peak doesn't reach 200, I should reject other users from applying to reserve the resource. The new user should be mounted to other namespaces.
 It's still an initial idea now. I'll think again carefully and make a detailed proposal. All advice are welcome!"
Test failures in ITestAzureBlobFileSystemCheckAccess in ABFS,13321977,Resolved,Major,Fixed,11/Aug/20 10:14,19/Sep/20 01:06,3.3.0,"ITestAzureBlobFileSystemCheckAccess is giving test failures while running both in parallel as well as in stand-alone(in IDE).

Tested by:  mvn -T 1C -Dparallel-tests=abfs clean verify
 Region: East US"
Sudo in hadoop-functions.sh should preserve environment variables ,13327954,Patch Available,Major,,17/Sep/20 03:55,,3.3.0,"Steps to reproduce:
1. Set {{HDFS_NAMENODE_USER=hdfs}} in {{/etc/default/hadoop-hdfs-namenode}} to enable user check (and switch to {{hdfs}} to start/stop NameNode daemon)
2. Stop NameNode with: {{service hadoop-hdfs-namenode stop}}
3. Got an error and NameNode is not stopped
{noformat}
ERROR: Cannot execute /usr/lib/hadoop-hdfs/bin/../libexec/hdfs-config.sh.
Failed to stop Hadoop namenode. Return value: 1. [FAILED]
{noformat}

The root cause is that after sudo, {{HADOOP_HOME=/usr/lib/hadoop}} is not preserved, and {{/usr/lib/hadoop-hdfs/bin/hdfs}} locates libexec by the following logic:

{noformat}
# let's locate libexec...
if [[ -n ""${HADOOP_HOME}"" ]]; then
  HADOOP_DEFAULT_LIBEXEC_DIR=""${HADOOP_HOME}/libexec""
else
  bin=$(cd -P -- ""$(dirname -- ""${MYNAME}"")"" >/dev/null && pwd -P)
  HADOOP_DEFAULT_LIBEXEC_DIR=""${bin}/../libexec""
fi
{noformat}

I believe the key point here is that we should preserve environment variables when doing sudo.

Note that this bug is not introduced by HDFS-15353, before which {{su -l}} is used, which will also discard environment variables."
S3Guard: Include 500 DynamoDB system errors in exponential backoff retries,13320607,Open,Major,,03/Aug/20 07:41,,3.1.3,"We get fatal failures from S3guard (that in turn fail our spark jobs) because of the inernal DynamoDB system errors.

{color:#000000}com.amazonaws.services.dynamodbv2.model.InternalServerErrorException: Internal server error (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalServerError; Request ID: 00EBRE6J6V8UGD7040C9DUP2MNVV4KQNSO5AEMVJF66Q9ASUAAJG): Internal server error (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalServerError; Request ID: 00EBRE6J6V8UGD7040C9DUP2MNVV4KQNSO5AEMVJF66Q9ASUAAJG){color}

{color:#000000}The DynamoDB has separate statistic for system errors:{color}

{color:#000000}!image-2020-08-03-09-58-54-102.png!{color}

{color:#000000}I contacted the AWS Support and got an explanation that those 500 errors are returned to the client once DynamoDB gets overwhelmed with client requests.{color}

{color:#000000}So essentially the traffic should had been throttled but it didn't and got 500 system errors.{color}

{color:#000000}My point is that the client should handle those errors just like throttling exceptions - {color}

{color:#000000}with exponential backoff retries.{color}

 

{color:#000000}Here is more complete exception stack trace:{color}

 

*{color:#000000}org.apache.hadoop.fs.s3a.AWSServiceIOException: get on s3a://rem-spark/persisted_step_data/15/0afb1ccb73854f1fa55517a77ec7cc5e__b67e2221-f0e3-4c89-90ab-f49618ea4557__SDTopology/parquet.all_ranges/topo_id=321: com.amazonaws.services.dynamodbv2.model.InternalServerErrorException: Internal server error (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalServerError; Request ID: 00EBRE6J6V8UGD7040C9DUP2MNVV4KQNSO5AEMVJF66Q9ASUAAJG): Internal server error (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalServerError; Request ID: 00EBRE6J6V8UGD7040C9DUP2MNVV4KQNSO5AEMVJF66Q9ASUAAJG) at{color}*{color:#000000} org.apache.hadoop.fs.s3a.S3AUtils.translateDynamoDBException(S3AUtils.java:389) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:181) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111) at org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore.get(DynamoDBMetadataStore.java:438) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2110) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1889) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1868) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1868) at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(InMemoryFileIndex.scala:277) at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$3$$anonfun$apply$2.apply(InMemoryFileIndex.scala:207) at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$3$$anonfun$apply$2.apply(InMemoryFileIndex.scala:206) at scala.collection.immutable.Stream.map(Stream.scala:418) at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$3.apply(InMemoryFileIndex.scala:206) at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$3.apply(InMemoryFileIndex.scala:204) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) Caused by: com.amazonaws.services.dynamodbv2.model.InternalServerErrorException: Internal server error (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalServerError; Request ID: 00EBRE6J6V8UGD7040C9DUP2MNVV4KQNSO5AEMVJF66Q9ASUAAJG) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1639) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.doInvoke(AmazonDynamoDBClient.java:2925) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.invoke(AmazonDynamoDBClient.java:2901) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.executeGetItem(AmazonDynamoDBClient.java:1640) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.getItem(AmazonDynamoDBClient.java:1616) at com.amazonaws.services.dynamodbv2.document.internal.GetItemImpl.doLoadItem(GetItemImpl.java:77) at com.amazonaws.services.dynamodbv2.document.internal.GetItemImpl.getItem(GetItemImpl.java:66) at com.amazonaws.services.dynamodbv2.document.Table.getItem(Table.java:608) at org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore.getConsistentItem(DynamoDBMetadataStore.java:423) at org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore.innerGet(DynamoDBMetadataStore.java:459) at org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore.lambda$get$2(DynamoDBMetadataStore.java:439) at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) ... 29 more{color}"
DistCp -update option will be invalid when distcp files from hdfs to S3,13326822,Resolved,Major,Duplicate,10/Sep/20 09:20,10/Sep/20 11:11,,"We use distcp with -update option to copy a dir from hdfs to S3. When we run distcp job once more, it will overwrite S3 dir directly, rather than skip the same files.
  
 Test Case:
Run twice the following cmd,  the modify time of S3 files will be modified every time.
 hadoop distcp -update /test/ s3a://${s3_buckect}/test/

 

Check code in CopyMapper.java and S3AFileSystem.java 

(1) For the first time, distcp job will create files in S3, but blockSize is unused!

!image-2020-09-10-17-45-16-998.png|width=542,height=485!

 

(2) For the second time, the distcp job will compare fileSize and blockSize between hdfs file and S3 file

!image-2020-09-10-17-47-01-653.png|width=524,height=248!

 

(3) blockSize is unused, when get blockSize of S3 file, it return a default value.

In S3AFileSystem.java, we find that the default value of fs.s3a.block.size is 32 * 1024 * 1024.

!image-2020-09-10-17-33-50-505.png|width=451,height=762!

 

!image-2020-09-10-17-52-32-290.png|width=527,height=87!
  

The blockSize of HDFS seems invalid in Object Store, like S3. So I think there's no need to compare blockSize when distcp with -update option."
Website to link to latest Hadoop wiki,13326560,Open,Major,,08/Sep/20 21:53,,,"Currently the website links to the [old wiki|https://wiki.apache.org/hadoop]. Shall we update that to the latest one: https://cwiki.apache.org/confluence/display/HADOOP2/Home

Or am I confused which one is latest, https://wiki.apache.org/hadoop or https://cwiki.apache.org/confluence/display/HADOOP2?"
Upgrade jackson-databind to 2.9.10.6 on branch-2.10,13326216,Resolved,Major,Fixed,06/Sep/20 23:37,09/Sep/20 00:56,2.10.0,This is filed to test backporting HADOOP-16905 to branch-2.10.
Upgrade netty-all to 4.1.50.Final on branch-2.10,13326488,Resolved,Major,Fixed,08/Sep/20 13:58,08/Sep/20 22:23,,netty-all seems to be easily updated to fix HADOOP-16918.
Test failure as failed request body counted in byte received metric - ITestAbfsNetworkStatistics#testAbfsHttpResponseStatistics,13324565,Resolved,Major,Fixed,25/Aug/20 22:11,08/Sep/20 09:15,3.3.0,"Bytes received counter increments for every request response received. 

[https://github.com/apache/hadoop/blob/d23cc9d85d887f01d72180bdf1af87dfdee15c5a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java#L251]

This increments even for failed requests. 

Observed during testing done for HADOOP-17215. A request failed with 409 Conflict contains response body as below:

{""error"":\{""code"":""PathAlreadyExists"",""message"":""The specified path already exists.\nRequestId:c3b2c55c-b01f-0061-7b31-7b6ee3000000\nTime:2020-08-25T22:44:07.2356054Z""}}

The error body of 168 size is incremented in bytes_received counter. 

This also breaks the testcase testAbfsHttpResponseStatistics. 
{code:java}
[ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.746 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics[ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.746 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics[ERROR] testAbfsHttpResponseStatistics(org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics)  Time elapsed: 13.183 s  <<< FAILURE!java.lang.AssertionError: Mismatch in bytes_received expected:<143> but was:<311> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest.assertAbfsStatistics(AbstractAbfsIntegrationTest.java:445) at org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics.testAbfsHttpResponseStatistics(ITestAbfsNetworkStatistics.java:291)
{code}
[~mehakmeetSingh] - is the bytes_received counter increment for failed requests an expected behaviour ?"
Support ChRootedFileSystem level cache for Regex Mount points,13326171,Open,Major,,06/Sep/20 07:11,,,"Support ChRootedFileSystem level cache for Regex Mount points, so users don't need default change rename strategy settings to use rename API."
Add FileContext based Regex Mountpoint implementation.,13326016,Open,Major,,04/Sep/20 04:09,,,This Jira is created to track tasks for FileContext based Regex Mountpoint implementation.
Allow file system caching to be disabled for all file systems,13323425,Open,Major,,18/Aug/20 19:29,,3.3.0,"Right now, FileSystem.get(URI uri, Configuration conf) allows caching of file systems to be disabled per scheme.

We can introduce a new global conf to disable caching for all FileSystem, the default would be false (or do not disable cache gobally)."
ViewFs#getDelegationTokens should include fallback fs tokens as well.,13325659,Open,Major,,02/Sep/20 02:40,,," 

getDelegationTokens API is using getMountPoints API and return tokens by iterating over.

But getMountPoints will not include fallbackfs. So, it will miss the tokens from fallback fs.
{code:java}
@Override
public List<Token<?>> getDelegationTokens(String renewer) throws IOException {
 List<InodeTree.MountPoint<AbstractFileSystem>> mountPoints = 
 fsState.getMountPoints();
 int initialListSize = 0;
 for (InodeTree.MountPoint<AbstractFileSystem> im : mountPoints) {
 initialListSize += im.target.targetDirLinkList.length; 
 }
 List<Token<?>> result = new ArrayList<Token<?>>(initialListSize);
 for ( int i = 0; i < mountPoints.size(); ++i ) {
 List<Token<?>> tokens = 
 mountPoints.get(i).target.targetFileSystem.getDelegationTokens(renewer);
 if (tokens != null) {
 result.addAll(tokens);
 }
 }
 return result;
}
{code}
 "
Improve and revise the performance description in the Tencent COS website document,13323147,Patch Available,Major,,18/Aug/20 03:30,,3.3.0,Improve the description of the maximum single file size limit and revise the performance data in the other issue section due to the performance improvement of COS backend architecture.
Validating storage keys in ABFS correctly,13316903,Resolved,Major,Fixed,15/Jul/20 11:57,27/Aug/20 14:15,3.3.0,"Storage Keys in ABFS should be validated after the keys have been loaded.

work:
 - Remove the previous validation of storage keys.
 - Validate at the correct place."
Adding Context class for AbfsClient to pass AbfsConfigurations to limit number of parameters ,13321710,Resolved,Major,Fixed,09/Aug/20 23:57,27/Aug/20 10:29,3.3.0,"To limit the growing number of parameters in AbfsClient for AbfsConfigurations, introducing a context class to pass the AbfsConfigurations from.

This would help in passing the AbfsConfigurations to further classes like AbfsRestOperation and AbfsHttpOperation also."
Fix unit test of HDFS-13934,13324802,Resolved,Major,Duplicate,27/Aug/20 09:45,27/Aug/20 09:56,,"unit test failed of org.apache.hadoop.fs.contract.AbstractContractMultipartUploaderTest#testConcurrentUploads.

Exception:
{code:java}
java.lang.IllegalArgumentExceptionjava.lang.IllegalArgumentException at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127) at org.apache.hadoop.test.LambdaTestUtils$ProportionalRetryInterval.<init>(LambdaTestUtils.java:907) at org.apache.hadoop.fs.contract.AbstractContractMultipartUploaderTest.testConcurrentUploads(AbstractContractMultipartUploaderTest.java:815){code}
h3. Reason:
{code:java}
public ProportionalRetryInterval(int intervalMillis,
    int maxIntervalMillis) {
  Preconditions.checkArgument(intervalMillis > 0);
  Preconditions.checkArgument(maxIntervalMillis > 0);
  this.intervalMillis = intervalMillis;
  this.current = intervalMillis;
  this.maxIntervalMillis = maxIntervalMillis;
}{code}
The constructor of ProportionalRetryInterval requires maxIntervalMillis> 0. But TestHDFSContractMultipartUploader does not override the timeToBecomeConsistentMillis method, so maxIntervalMillis = 0"
empty getDefaultExtension() is ignored,13324751,Open,Major,,27/Aug/20 03:53,,3.1.3,"Use case - source files are gz-compressed but have no extensions.

Attempt to auto-decompress them through 
{code:java}
package com.my.codec.test

import org.apache.hadoop.io.compress.GzipCodec

class GZCodec extends GzipCodec {
  override def getDefaultExtension(): String = """"
 }
{code}
 (notice empty getDefaultExtension ) and then setting *io.compression.codecs* to com.my.codec.test.GZCodec makes no effect 

Similar tests with one-character encoding for last possible names makes it work. So only the empty-string getDefaultExtension case is broken. 

I guess the issue is somewhere in [https://github.com/c9n/hadoop/blob/master/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java#L109] 

but it's not obvious. 

Folks have built some workarounds using custom readers, for example, 
 # [https://daynebatten.com/2015/11/override-hadoop-compression-codec-file-extension/]
 # [https://stackoverflow.com/questions/52011697/how-to-read-a-compressed-gzip-file-without-extension-in-spark?rq=1] 

Hopefully it would be an easy fix to support empty getDefaultExtension? 

 "
Bug in preserving Directory Attributes in DistCp with Atomic Copy,13316268,Resolved,Major,Fixed,11/Jul/20 04:55,22/Aug/20 17:51,3.1.2,"Description:

In case of Atomic Copy the copied data is commited and post that the preserve directory attributes runs. Preserving directory attributes is done over work path and not final path. I have fixed the base directory to point towards final path."
backport HADOOP-15691 PathCapabilities API to branch-3.2,13323029,Resolved,Major,Fixed,17/Aug/20 10:38,19/Aug/20 16:15,3.2.1,"Backport the PathCapabilities API of HADOOP-15691 as a precursor to the HADOOP-13230 backport, so making it easier to probe FS abilities.

going back further than is may be hard, but we'll see"
ABFS : Add support for authentication mechanism at container level,13323042,Open,Major,,17/Aug/20 11:55,,3.3.0,"ABFS supports using various authentication mechanisms at storage level. 

As part of access level policies , application might have access to various containers belonging to same storage account using different authentication mechanisms. For one container application can have read, write etc , for other it may have only read permission. Application should be able to leverage authentication mechanisms for same storage account at container level.

One should be able to use SAS as authentication mechanism for one container & OAuth as authentication mechanism for another container. "
Renaming a file under a sibling empty directory doesn't delete dest dir's marker,13321861,Resolved,Major,Duplicate,10/Aug/20 19:03,15/Aug/20 19:26,3.1.3,"This has probably existed for a long time -the fact that nobody has noticed is probably just luck.

Probably came with the *fix* of HADOOP-15079.

Given 

* a base directory /base
* A file /base/file
* an empty directory /base/empty (with empty dir marker)
* if you rename base/file under base/empty (key: with the dir passed in, not the full filename) the file is moved, but the marker /base/empty/ is not

The issue is that the  delete/recreate markers compares src and dest parent for equivalence when skipping marker delete, but if the final path of the rename is *under* the path ""dst"" then the comparison is wrong -it must be on the parent of the final path, not the path handed to rename()

{code}
    if (!src.getParent().equals(dst.getParent())) {
      LOG.debug(""source & dest parents are different; fix up dir markers"");
      deleteUnnecessaryFakeDirectories(dst.getParent());
      maybeCreateFakeParentDirectory(src);
    }
{code}"
ITestS3AHugeFilesSSECDiskBlock failing because of bucket overrides ,13321477,Resolved,Major,Fixed,07/Aug/20 09:09,13/Aug/20 13:33,3.3.0,"If we set the conf ""fs.s3a.bucket.mthakur-data.server-side-encryption.key"" in our test config 

tests in ITestS3AHugeFilesSSECDiskBlock failing because of we are overriding the bucket configuration thus overwriting the value of the base config set here. [https://github.com/apache/hadoop/blob/81da221c757bef9ec35cd190f14b2f872324c661/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFilesSSECDiskBlocks.java#L51] 

 

Full stack stace:
{code:java}
java.lang.IllegalArgumentException: Invalid base 64 character: ':'java.lang.IllegalArgumentException: Invalid base 64 character: ':'
 at com.amazonaws.util.Base64Codec.pos(Base64Codec.java:242) at com.amazonaws.util.Base64Codec.decode4bytes(Base64Codec.java:151) at com.amazonaws.util.Base64Codec.decode(Base64Codec.java:230) at com.amazonaws.util.Base64.decode(Base64.java:112) at com.amazonaws.services.s3.AmazonS3Client.populateSSE_C(AmazonS3Client.java:4379) at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1318) at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:1920) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407) at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:370) at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1913) at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1889) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3027) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2958) at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2842) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerMkdirs(S3AFileSystem.java:2798) at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:2772) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2369) at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.mkdirs(AbstractFSContractTestBase.java:361) at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:203) at org.apache.hadoop.fs.s3a.AbstractS3ATestBase.setup(AbstractS3ATestBase.java:59) at org.apache.hadoop.fs.s3a.scale.S3AScaleTestBase.setup(S3AScaleTestBase.java:90) at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.setup(AbstractSTestS3AHugeFiles.java:78) at org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesSSECDiskBlocks.setup(ITestS3AHugeFilesSSECDiskBlocks.java:41) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748){code}
 

I am not sure if we need to worry too much about this. We can just fix the local test config. 

CC [~stevel@apache.org]"
Unauthenticated users are not authorized to access this page message is misleading in HttpServer2.java,13318445,Patch Available,Major,,21/Jul/20 09:07,,,"Recently one of the users were misled by the message ""Unauthenticated users are not authorized to access this page"" when the user was not an admin user.
At that point the user is authenticated but has no admin access, so it's actually not an authentication issue but an authorization issue.
Also, 401 as error code would be better.
Something like ""User is unauthorized to access the page"" would help to users to find out what is the problem during access an http endpoint."
Decrease size of s3a dependencies,13321847,Resolved,Major,Won't Fix,10/Aug/20 17:22,10/Aug/20 18:30,,"S3A currently has a dependency on the aws-java-sdk-bundle, which includes the SDKs for all AWS services. The jar file for the current version is about 120 MB, but continues to grow (the latest is about 170 MB). Organic growth is expected as more and more AWS services are created.

The aws-java-sdk-bundle jar file is shaded as well, so it includes all transitive dependencies.

It would be nice if S3A could depend on smaller jar files in order to decrease the size of jar files pulled in transitively by clients. Decreasing the size of dependencies is particularly important for Docker files, where image pull times can be affected by image size.

One solution here would be for S3A to publish its own shaded jar which includes the SDKs for all needed AWS Services (e.g. S3, DynamoDB, etc.) along with the transitive dependencies for the individual SDKs."
Remove master/slave terminology from Hadoop Common,13320252,Open,Major,,30/Jul/20 18:24,,,
Report non-inclusive language as part of code contribution pre-commit check,13321139,Open,Major,,05/Aug/20 18:16,,,
UGI loginUserFromKeytab doesn't set the last login time,13320138,Resolved,Major,Fixed,30/Jul/20 07:41,05/Aug/20 16:10,2.10.0,UGI initial login from keytab doesn't set the last login time as a result of which the relogin can happen even before the configured minimum seconds to wait before relogin
Set MAVEN_OPTS in Jenkinsfile,13320996,Resolved,Major,Fixed,05/Aug/20 02:33,05/Aug/20 06:23,,"MAVEN_OPTS is not honored from Dockerfile, so we need to set MAVEN_OPTS in Jenkinsfile explicitly.

Especially, {{-Dhttps.protocols=TLSv1.2}} is required when running with Java 7."
[JDK 13] Javadoc HTML5 support,13320587,Open,Major,,03/Aug/20 04:46,,,"javadoc -html4 option has been removed since JDK 13. We need to remove the option and fix the errors.
https://bugs.openjdk.java.net/browse/JDK-8215578
"
Make ipc.Client.stop() sleep configurable,13319676,Open,Major,,28/Jul/20 05:32,,,"After identifying HADOOP-16126 might cause issues in few workloads, ipc.Client.stop() sleep was identified to be configurable to better suit multiple workloads"
ABFS: Test failure: Disable ITestAzureBlobFileSystemDelegationSAS tests,13318862,Resolved,Major,Works for Me,23/Jul/20 07:31,28/Jul/20 16:28,,ITestAzureBlobFileSystemDelegationSAS has tests for the SAS feature in preview stage. The tests should not run until the API version reflects the one in preview as when run against production clusters they will fail.
Add boost installation steps to build instruction on CentOS 8,13319184,Resolved,Major,Fixed,24/Jul/20 14:30,25/Jul/20 21:39,,"After HDFS-15385, -Pnative build fails without boost 1.72 used in libhdfs++. It must be installed from source since boost 1.66 packaged by the CentOS distribution does not match."
Create Classes to wrap Guava code replacement,13314494,Open,Major,,01/Jul/20 14:19,,,"Usage of Guava APIs in hadoop may not have one line replacement in Java8+. We need to create some classes to wrap those common functionalities instead of reinventing the wheel everywhere.
For example, we should have new package {{package org.apache.hadoop.util.collections}}.
Then we create classes like {{MultiMap}} which may have the entire implementation from scratch or we can use Apache Commons Collections 4.4 API.
The Pros of using wrapper is to avoid adding more dependencies in POM if we vote to use a third party jar."
Replace Guava.Splitter with common.util.StringUtils,13318520,Open,Major,,21/Jul/20 16:41,,,"Hadoop source code uses {{com.google.common.base.Splitter}} . We need to analyze the performance overhead of splitter and consider different implementations such as apache-commons.

Hadoop has an implementation {{org.apache.hadoop.util.StringUtils.split()}}. Therefore, all split() calls have to use the wrapper in common package. This will make the utility calls less invasive and confusing as the behavior of apache-commons.stringUtils is not the same as guava.

Once we have the wrapper, and all calls are using that wrapper, we can decide to use apache-commons or do specific optimizations without changing the entire source code.

 
{code:bash}
Targets
    Occurrences of 'import com.google.common.base.Splitter;' in project with mask '*.java'
Found Occurrences  (18 usages found)
    org.apache.hadoop.crypto  (1 usage found)
        CryptoCodec.java  (1 usage found)
            34 import com.google.common.base.Splitter;
    org.apache.hadoop.mapred.nativetask.kvtest  (1 usage found)
        KVTest.java  (1 usage found)
            44 import com.google.common.base.Splitter;
    org.apache.hadoop.mapreduce.v2.util  (1 usage found)
        MRWebAppUtil.java  (1 usage found)
            20 import com.google.common.base.Splitter;
    org.apache.hadoop.metrics2.impl  (1 usage found)
        MetricsConfig.java  (1 usage found)
            32 import com.google.common.base.Splitter;
    org.apache.hadoop.registry.client.impl.zk  (1 usage found)
        RegistrySecurity.java  (1 usage found)
            22 import com.google.common.base.Splitter;
    org.apache.hadoop.security.authentication.server  (1 usage found)
        MultiSchemeAuthenticationHandler.java  (1 usage found)
            34 import com.google.common.base.Splitter;
    org.apache.hadoop.security.token.delegation.web  (1 usage found)
        MultiSchemeDelegationTokenAuthenticationHandler.java  (1 usage found)
            40 import com.google.common.base.Splitter;
    org.apache.hadoop.tools.dynamometer  (1 usage found)
        Client.java  (1 usage found)
            22 import com.google.common.base.Splitter;
    org.apache.hadoop.tools.dynamometer.workloadgenerator.audit  (2 usages found)
        AuditLogDirectParser.java  (1 usage found)
            20 import com.google.common.base.Splitter;
        AuditReplayThread.java  (1 usage found)
            20 import com.google.common.base.Splitter;
    org.apache.hadoop.util  (2 usages found)
        TestApplicationClassLoader.java  (1 usage found)
            44 import com.google.common.base.Splitter;
        ZKUtil.java  (1 usage found)
            31 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.api.records.timeline  (1 usage found)
        TimelineEntityGroupId.java  (1 usage found)
            27 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.server.resourcemanager.scheduler  (1 usage found)
        QueueMetrics.java  (1 usage found)
            55 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.util  (1 usage found)
        StringHelper.java  (1 usage found)
            21 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.webapp  (1 usage found)
        WebApp.java  (1 usage found)
            37 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.webapp.hamlet  (1 usage found)
        HamletImpl.java  (1 usage found)
            22 import com.google.common.base.Splitter;
    org.apache.hadoop.yarn.webapp.hamlet2  (1 usage found)
        HamletImpl.java  (1 usage found)
            22 import com.google.common.base.Splitter;

{code}"
"KMSClientProvider Sends HTTP GET with null ""Content-Type"" Header",13318282,Open,Major,,20/Jul/20 15:12,,2.7.3,"Hive Server uses 'org.apache.hadoop.crypto.key.kms.KMSClientProvider' when interacting with HDFS TDE zones. This triggers a call to the KMS server. If the request method is a GET, the HTTP Header Content-Type is sent with a null value.

When using Ranger KMS, the embedded Tomcat server returns a HTTP 400 error with the following error message:
{quote}HTTP Status 400 - Bad Content-Type header value: ''
 The request sent by the client was syntactically incorrect.
{quote}
This only occurs with HTTP GET method calls. 

This is a captured HTTP request:

 
{code:java}
GET /kms/v1/key/xxx/_metadata?doAs=yyy&doAs=yyy HTTP/1.1
Cookie: hadoop.auth=""u=hive&p=hive/domain.com@DOMAIN.COM&t=kerberos-dt&e=123789456&s=xxx=""
Content-Type:
Cache-Control: no-cache
Pragma: no-cache
User-Agent: Java/1.8.0_241
Host: kms.domain.com:9292
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive{code}
 

Note the empty 'Content-Type' header.

And the corresponding response:

 
{code:java}
HTTP/1.1 400 Bad Request
Server: Apache-Coyote/1.1
Content-Type: text/html;charset=utf-8
Content-Language: en
Content-Length: 1034
Date: Thu, 16 Jul 2020 04:23:18 GMT
Connection: close{code}
 

This is the stack trace from the Hive server:

 
{code:java}
Caused by: java.io.IOException: HTTP status [400], message [Bad Request]
at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)
at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:608)
at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:597)
at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:566)
at org.apache.hadoop.crypto.key.kms.KMSClientProvider.getMetadata(KMSClientProvider.java:861)
at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.compareKeyStrength(Hadoop23Shims.java:1506)
at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.comparePathKeyStrength(Hadoop23Shims.java:1442)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.comparePathKeyStrength(SemanticAnalyzer.java:1990)
... 38 more{code}
 

This looks to occur in [https://github.com/hortonworks/hadoop-release/blob/HDP-2.6.5.165-3-tag/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java#L591-L599]
{code:java}
      if (authRetryCount > 0) {
        String contentType = conn.getRequestProperty(CONTENT_TYPE);
        String requestMethod = conn.getRequestMethod();
        URL url = conn.getURL();
        conn = createConnection(url, requestMethod);
        conn.setRequestProperty(CONTENT_TYPE, contentType);
        return call(conn, jsonOutput, expectedResponse, klass,
            authRetryCount - 1);
      }{code}
 I think when a GET method is received, the Content-Type header is not defined, then in line 592:
{code:java}
 String contentType = conn.getRequestProperty(CONTENT_TYPE);
{code}
The code attempts to retrieve the CONTENT_TYPE Request Property, which returns null.

Then in line 596:
{code:java}
conn.setRequestProperty(CONTENT_TYPE, contentType);
{code}
The null content type is used to construct the HTTP call to the KMS server.

A null Content-Type header is not allowed/considered malformed by the receiving KMS server.

I propose this code be updated to inspect the value returned by conn.getRequestProperty(CONTENT_TYPE), and not use a null value to construct the new KMS connection.

Proposed pseudo-patch:
{code:java}
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
@@ -593,7 +593,9 @@ public HttpURLConnection run() throws Exception {
         String requestMethod = conn.getRequestMethod();
         URL url = conn.getURL();
         conn = createConnection(url, requestMethod);
-        conn.setRequestProperty(CONTENT_TYPE, contentType);
+        if (conn.getRequestProperty(CONTENT_TYPE) != null) {
+          conn.setRequestProperty(CONTENT_TYPE, contentType);
+        }
         return call(conn, jsonOutput, expectedResponse, klass,
             authRetryCount - 1);
       }{code}
This should not impact any other use of this class and should only address cases where a null is returned for Content-Type."
UGI Credentials#addToken silently overrides the token with same service name,13315884,Open,Major,,09/Jul/20 12:44,,,"UGI Credentials#addToken silently overrides the token with same service name.

{code:java}
public void addToken(Text alias, Token<? extends TokenIdentifier> t) {
  if (t == null) {
    LOG.warn(""Null token ignored for "" + alias);
  } else if (tokenMap.put(alias, t) != null) {
    // Update private tokens
    Map<Text, Token<? extends TokenIdentifier>> tokensToAdd =
        new HashMap<>();
    for (Map.Entry<Text, Token<? extends TokenIdentifier>> e :
        tokenMap.entrySet()) {
      Token<? extends TokenIdentifier> token = e.getValue();
      if (token.isPrivateCloneOf(alias)) {
        tokensToAdd.put(e.getKey(), t.privateClone(token.getService()));
      }
    }
    tokenMap.putAll(tokensToAdd);
  }
} 
{code}
 
There are tokens which does not have service name like YARN_AM_RM_TOKEN, Localizer and these tokens gets overridden and causes access issues later which are tough to debug.

1. Need to check if they can be added with some random unique name.
2. Or at least a error message should be logged.

 "
double buffer memory size hard code,13316667,Open,Major,,14/Jul/20 10:16,,2.7.7,private int outputBufferCapacity = 512 * 1024;
 ITestS3AConfiguration.testProxyConnection failing when s3a bucket probe disabled,13322740,Open,Minor,,14/Aug/20 11:16,,3.4.0,"Failure of {{testProxyConnection(org.apache.hadoop.fs.s3a.ITestS3AConfiguration)}} when bucket probes set to

{code}
  <property>
    <name>fs.s3a.bucket.probe</name>
    <value>0</value>
  </property>   
{code}

and s3guard disabled
"
S3A (async) ObjectListingIterator to block in hasNext() for results,13325914,Open,Minor,,03/Sep/20 12:29,,3.4.0,"HADOOP-17074 made listing async in S3A, but the iterator's hasNext Call doesn't wait for the result. If invoked on an empty path it *may* return when it should be failing.

Note: surfaced in code review, not seen in the wild and all our tests were happy"
ABFS: Testcase failure ITestAbfsNetworkStatistics#testAbfsHttpResponseStatistics,13329652,Patch Available,Minor,,27/Sep/20 19:54,,3.4.0,The test case is failing when the fs.azure.test.appendblob.enabled=true.
Fix outdated properties of journal node when perform rollback,13318405,Resolved,Minor,Fixed,21/Jul/20 06:38,17/May/21 02:56,,"When rollback HDFS cluster, properties in JNStorage won't be refreshed after the storage dir changed. It leads to exceptions when starting namenode.

The exception like:
{code:java}
2020-07-09 19:04:12,810 FATAL [IPC Server handler 105 on 8022] org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: recoverUnfinalizedSegments failed for required journal (JournalAndStream(mgr=QJM to [10.0.118.217:8485, 10.0.117.208:8485, 10.0.118.179:8485], stream=null))
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:
10.0.118.217:8485: Incompatible namespaceID for journal Storage Directory /mnt/vdc-11176G-0/dfs/jn/nameservicetest1: NameNode has nsId 647617129 but storage has nsId 0
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.checkConsistentNamespace(JNStorage.java:236)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.newEpoch(Journal.java:300)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.newEpoch(JournalNodeRpcServer.java:136)
	at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.newEpoch(QJournalProtocolServerSideTranslatorPB.java:133)
	at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25417)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2274)
{code}"
Fix spotbugs warnings surfaced after upgrade to 4.0.6,13317389,Resolved,Minor,Fixed,17/Jul/20 11:04,22/Jul/20 04:41,3.2.3,"Spotbugs 4.0.6 generated additional warnings.
{noformat}
$ find . -name findbugsXml.xml | xargs -n 1 /opt/spotbugs-4.0.6/bin/convertXmlToText -longBugCodes
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to $L5 in org.apache.hadoop.ipc.Server$ConnectionManager.decrUserConnections(String)  At Server.java:[line 3729]
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to $L5 in org.apache.hadoop.ipc.Server$ConnectionManager.incrUserConnections(String)  At Server.java:[line 3717]
H D NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION NP: Method org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(Object) overrides the nullness annotation of parameter $L1 in an incompatible way  At DatasetVolumeChecker.java:[line 322]
H D NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION NP: Method org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler.onSuccess(VolumeCheckResult) overrides the nullness annotation of parameter result in an incompatible way  At DatasetVolumeChecker.java:[lines 358-376]
M D NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION NP: Method org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$2.onSuccess(Object) overrides the nullness annotation of parameter result in an incompatible way  At ThrottledAsyncChecker.java:[lines 170-175]
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to $L8 in org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount(FSEditLogOpCodes, EnumMap, Step, StartupProgress$Counter)  At FSEditLogLoader.java:[line 1241]
M D NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE NP: result must be non-null but is marked as nullable  At LocatedFileStatusFetcher.java:[lines 380-397]
M D NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE NP: result must be non-null but is marked as nullable  At LocatedFileStatusFetcher.java:[lines 291-309]
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to $L6 in org.apache.hadoop.yarn.sls.SLSRunner.increaseQueueAppNum(String)  At SLSRunner.java:[line 816]
H C UMAC_UNCALLABLE_METHOD_OF_ANONYMOUS_CLASS UMAC: Uncallable method org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage$1.getInstance() defined in anonymous class  At TestTimelineReaderWebServicesHBaseStorage.java:[line 87]
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to entities in org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown.checkQuery(HBaseTimelineReaderImpl)  At TestTimelineReaderHBaseDown.java:[line 190]
M V EI_EXPOSE_REP EI: org.apache.hadoop.fs.cosn.CosNInputStream$ReadBuffer.getBuffer() may expose internal representation by returning CosNInputStream$ReadBuffer.buffer  At CosNInputStream.java:[line 87]
{noformat}"
Erasure Coding: Remove dead code from common side,13325252,Resolved,Minor,Fixed,31/Aug/20 02:38,01/Sep/20 03:48,3.3.0,"These codes are unused, so remove them."
Add Capability To Get Text Length,13318328,Resolved,Minor,Fixed,20/Jul/20 19:59,24/Jul/20 09:41,3.4.0,"The Hadoop {{Text}} class contains an array of byte which contain a UTF-8 encoded string.  However, there is no way to quickly get the length of that string.  One can get the number of bytes in the byte array, but to figure out the length of the String, it needs to be decoded first.  In this simple example, sorting the {{Text}} objects by String length, the String needs to be decoded from the byte array repeatedly.  This was brought to my attention based on [HIVE-23870].

{code:java}
  public static void main(String[] args) {
    List<Text> list = Arrays.asList(new Text(""1""), new Text(""22""), new Text(""333""));
    list.sort((Text t1, Text t2) -> t1.toString().length() - t2.toString().length());
  }
{code}

Also helpful if I want to check the last letter in the {{Text}} object repeatedly:

{code:java}
    Text t = new Text(""4444"");
    System.out.println(t.charAt(t.toString().length() - 1));
{code}"
Dead link in hadoop-kms/index.md.vm,13318655,Resolved,Minor,Fixed,22/Jul/20 09:43,22/Jul/20 15:42,3.3.1,"There is a dead link (https://hadoop.apache.org/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html) in https://hadoop.apache.org/docs/r3.3.0/hadoop-kms/index.html#KMS_over_HTTPS_.28SSL.29

The link should be https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html"
libzstd-dev should be used instead of libzstd1-dev on Ubuntu 18.04 or higher,13329132,Resolved,Minor,Fixed,24/Sep/20 05:36,25/Sep/20 05:18,3.4.0,"libzstd1-dev is a transitional package on Ubuntu 18.04.
It is better to use libzstd-dev instead of libzstd1-dev in the Dockerfile (dev-support/docker/Dockerfile)."
Re-enable optimized copyFromLocal implementation in S3AFileSystem,13317485,Resolved,Minor,Fixed,17/Jul/20 19:29,02/Aug/21 15:12,3.2.1,"It looks like HADOOP-15932 disabled the optimized copyFromLocal implementation in S3A for correctness reasons.  innerCopyFromLocalFile should be fixed and re-enabled. The current implementation uses FileSystem.copyFromLocal which will open an input stream from the local fs and an output stream to the destination fs, and then call IOUtils.copyBytes. With default configs, this will cause S3A to read the file into memory, write it back to a file on the local fs, and then when the file is closed, upload it to S3.

The optimized version of copyFromLocal in innerCopyFromLocalFile, directly creates a PutObjectRequest request with the local file as the input."
JAVA_HOME with spaces not supported,13324675,Open,Minor,,26/Aug/20 16:26,,3.3.0,"When running on Windows, if JAVA_HOME contains a space (which is frequently since the default Java install path is ""C:\Program Files\Java"", running Hadoop fails to run. 

!image-2020-08-26-12-24-04-118.png!"
Failure of ITestAssumeRole.testRestrictedCommitActions ,13324524,Resolved,Minor,Fixed,25/Aug/20 18:03,05/Jan/22 16:06,3.3.1,"Counter of progress callbacks of parallel commits was 1, not two. As the callback wasn't using an atomic long, likely to be a (rare) race condition.

see in HADOOP-16830 test run; looks unrelated but I'll add the fix there anyway so I can see if it goes away.

"
transient ITestS3AFileContextStatistics failure -read buffer not filled,13327865,Resolved,Minor,Won't Fix,16/Sep/20 13:25,05/Jan/22 16:05,3.4.0,"Transient failure of test suite ITestS3AFileContextStatistics; read() didn't get the full buffer filled up. 

Note: Something is clearly odd on my dev setup, there's clearly smaller packets coming in. read() is free to return as much bytes as it has, but I've never seen this under-reporting until recently.
"
ABFS: configure output stream thread pool,13320184,Resolved,Minor,Fixed,30/Jul/20 12:46,14/Oct/20 22:56,3.3.0,"{code}
fs.azure.write.max.concurrent.requests -number of concurrent requests a single stream may POST at
fs.azure.write.max.requests.to.queue -number of blocks which will be queued before the client blocks.
{code}

the amount of memory consumed is fs.azure.write.max.concurrent.requests * fs.azure.write.max.requests.to.queue (default= 8MB)"
Handle transient stream read failures in FileSystem contract tests,13320730,Resolved,Minor,Fixed,03/Aug/20 18:45,09/Sep/20 11:02,3.3.0,"Seen 2x recently, failure in ITestS3AContractUnbuffer as not enough data came back in the read. 

The contract test assumes that stream.read() will return everything, but it could be some buffering problem. Proposed: switch to ReadFully to see if it is a quirk of the read/get or is something actually wrong with the production code."
"add way for s3a to recognise buckets with ""."" in name and switch to path access",13321329,Resolved,Minor,Won't Fix,06/Aug/20 15:59,08/Sep/20 10:39,3.3.0,"# AWS has, historically, allowed buckets with '.' in their name (along with other non-DNS valid chars)
# none of which work with virtual hostname S3 clients -you have to enable path style access
# which we can't do on a per-bucket basis, as the logic there doesn't support buckets with '.' in the name (think about it...)
# and we can't blindly say ""use path access everywhere"", because all buckets created on/after 2020-10-01 won't work that way"
ABFS: Test failure ITestAbfsNetworkStatistics#testAbfsHttpResponseStatistics,13323361,Resolved,Minor,Duplicate,18/Aug/20 13:47,27/May/21 13:21,3.4.0,The test ITestAbfsNetworkStatistics#testAbfsHttpResponseStatistics fails when the property fs.azure.test.appendblob.enabled is set to true
ITestS3ADirectoryPerformance.testListOperations failing,13317364,Resolved,Minor,Fixed,17/Jul/20 09:21,20/Jul/20 15:59,3.4.0,"Because of HADOOP-17022
[INFO] Running org.apache.hadoop.fs.s3a.scale.ITestS3ADirectoryPerformance
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 670.029 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.scale.ITestS3ADirectoryPerformance
[ERROR] testListOperations(org.apache.hadoop.fs.s3a.scale.ITestS3ADirectoryPerformance)  Time elapsed: 44.089 s  <<< FAILURE!
java.lang.AssertionError: object_list_requests starting=166 current=167 diff=1 expected:<2> but was:<1>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.fs.s3a.scale.ITestS3ADirectoryPerformance.testListOperations(ITestS3ADirectoryPerformance.java:117)"
ABFS: Run the integration tests with various combinations of configurations and publish consolidated results,13321368,Resolved,Minor,Fixed,06/Aug/20 19:08,10/Mar/21 18:26,3.3.0,"ADLS Gen 2 supports accounts with and without hierarchical namespace support. ABFS driver supports various authorization mechanisms like OAuth, haredKey, Shared Access Signature. The integration tests need to be executed against accounts with and without hierarchical namespace support using various authorization mechanisms.
Currently the developer has to manually run the tests with different combinations of configurations ex: HNS account with SharedKey and OAuth, NonHNS account with SharedKey etc..
The expectation is to automate these runs with different combinations. This will help the developer to run the integration tests with different variants of configurations automatically. "
ITestS3GuardOutOfBandOperations testListingDelete[auth=false] fails on unversioned bucket,13317207,Resolved,Minor,Won't Fix,16/Jul/20 16:23,27/Feb/21 11:00,3.4.0,"transient failure of 
{code}
[ERROR] Tests run: 24, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 369.68 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations
[ERROR] testListingDelete[auth=false](org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations)  Time elapsed: 19.103 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: No such file or directory: s3a://stevel-ireland/fork-0001/test/dir-e34a122f-a04d-48c3-90c1-9d35427fa939/file-1-e34a122f-a04d-48c3-90c1-9d35427fa939
{code}

config is unguarded codepath, s3a status was not passed down. Test run was really, really slow (8 threads)

hypothesis: test run took so long that a TTL expired and the open operation did a HEAD to s3 even when a s3guard record was found.

"
Fix testCompressorDecompressorWithExeedBufferLimit to cover the intended scenario,13328174,Resolved,Minor,Fixed,18/Sep/20 04:09,19/Sep/20 15:48,,The input data must be greater than internal buffer of Compressor/Decompressor as the test name implies. It must use strategy covering compression/decompression of multiple blocks.
S3AFileSystem.listLocatedStatus(file) does a LIST even with S3Guard,13317174,Resolved,Minor,Won't Fix,16/Jul/20 14:10,16/Nov/20 12:40,3.4.0,"This is minor and we may want to WONTFIX; noticed during work on directory markers.

If you call listLocatedStatus(file) then a LIST call is always made to S3, even when S3Guard is present and has the record to say ""this is a file""

Does this matter enough to fix? 

# The HADOOP-16465 work moved the list before falling back to getFileStatus
# that listing calls s3guard.listChildren(path) to list the children.
# which only returns the chlldren of a path, not a record of the path itself.
# so we get an empty list back, triggering the LIST
# its only after that LIST fails that we fall back to getFileStatus and hence look for the actual file record."
ABFS: Test failure: testFailedRequestWhenCredentialsNotCorrect fails when run with SharedKey,13318860,Resolved,Minor,Fixed,23/Jul/20 07:29,05/Aug/20 17:12,3.3.1,"When authentication is set to SharedKey, below test fails.

 

[ERROR]   ITestGetNameSpaceEnabled.testFailedRequestWhenCredentialsNotCorrect:161 Expecting org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException with text ""Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature."", 403 but got : ""void""

 

This test fails when the newly introduced config ""fs.azure.account.hns.enabled"" is set. This config will avoid network call to check if namespace is enabled, whereas the test expects thsi call to be made. 

 

The assert in test to 403 needs check too. Should ideally be 401."
ABFS: Tests ITestAbfsNetworkStatistics need to be config setting agnostic,13317387,Resolved,Minor,Fixed,17/Jul/20 10:56,05/Aug/20 17:12,3.3.0,"Tess in ITestAbfsNetworkStatistics have asserts to a  static number of network calls made from the start of fileystem instance creation. But this number of calls are dependent on the certain configs settings which allow creation of container or account is HNS enabled to avoid GetAcl call.

 

The tests need to be modified to ensure that count asserts are made for the requests made by the tests alone.

 
{code:java}
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics[ERROR] Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 4.148 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics[ERROR] testAbfsHttpResponseStatistics(org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics)  Time elapsed: 4.148 s  <<< FAILURE!java.lang.AssertionError: Mismatch in get_responses expected:<8> but was:<7> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest.assertAbfsStatistics(AbstractAbfsIntegrationTest.java:445) at org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics.testAbfsHttpResponseStatistics(ITestAbfsNetworkStatistics.java:207) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)
[ERROR] testAbfsHttpSendStatistics(org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics)  Time elapsed: 2.987 s  <<< FAILURE!java.lang.AssertionError: Mismatch in connections_made expected:<6> but was:<5> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest.assertAbfsStatistics(AbstractAbfsIntegrationTest.java:445) at org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics.testAbfsHttpSendStatistics(ITestAbfsNetworkStatistics.java:91) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsInputStreamStatistics
{code}"
s3a: bucket names which aren't parseable hostnames unsupported,13325693,Resolved,Minor,Duplicate,02/Sep/20 08:28,02/Sep/20 14:39,2.7.4,"Hi there,
 I'm using Spark to read some data from S3 and I encountered an error when reading from a bucket that contains a period (e.g. `s3a://okokes-test-v1.1/foo.csv`). I have close to zero Java experience, but I've tried to trace this as well as I can. Apologies for any misunderstanding on my part.

_Edit: the title is a little misleading - buckets can contain dots and s3a will work, but only if these bucket names conform to hostname restrictions - e.g. `s3a://foo.bar/bak.csv` would work, but my case - `okokes-test-v1.1` does not, because `1` is not conform to a top level domain pattern._

Using hadoop-aws:3.2.0, I get the following:
{code:java}
java.lang.NullPointerException: null uri host.
 at java.base/java.util.Objects.requireNonNull(Objects.java:246)
 at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:71)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:470)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:235)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:535)
 ... 47 elided{code}
hadoop-aws:2.7.4 did lead to a similar outcome
{code:java}
java.lang.IllegalArgumentException: The bucketName parameter must be specified.
 at com.amazonaws.services.s3.AmazonS3Client.assertParameterNotNull(AmazonS3Client.java:2816)
 at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1026)
 at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:535)
 ... 47 elided{code}
I investigated the issue a little bit and found buildFSURI to require the host to be not null - [see S3xLoginHelper.java|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java#L70] - but in my case the host is null and the authority part of the URL should be used. When I checked AWS' handling of this case, they seem to be using authority for all s3:// paths - [https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3URI.java#L85].

I verified this URI in a Scala shell (openjdk 1.8.0_252)

 
{code:java}
scala> (new URI(""s3a://okokes-test-v1.1/foo.csv"")).getHost()
val res1: String = null
scala> (new URI(""s3a://okokes-test-v1.1/foo.csv"")).getAuthority()
val res2: String = okokes-test-v1.1
{code}
 

Oh and this is indeed a bucket name. Not only did I create it in the console, but there's also enough documentation on the topic - [https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules]"
Add debug-level logs in Filesystem#close,13327995,Resolved,Minor,Fixed,17/Sep/20 07:57,29/Sep/20 15:09,3.3.0,"HDFS reuses the same cached FileSystem object across the file system. If the client calls FileSystem.close(), closeAllForUgi(), or closeAll() (if it applies to the instance) anywhere in the system it purges the cache of that FS instance, and trying to use the instance results in an IOException: FileSystem closed.

It would be a great help to clients to see where and when a given FS instance was closed. I.e. in close(), closeAllForUgi(), or closeAll(), it would be great to see a DEBUG-level log of
 * calling method name, class, file name/line number
 * FileSystem object's identity hash (FileSystem.close() only)

For the full calling stack, turn on TRACE logging."
Add  ViewFileSystem/InodeTree Mount points Resolution Cache,13325653,Open,Minor,,02/Sep/20 02:06,,,Now the ViewFileSystem walkthrough InodeTree to resolve the passed-in path. This could be optimized by adding a cache of resolution result. It will benefit all kinds of mount points. And it is also very useful to reduce the Regex parse cost of Regex Mount points introduced in HADOOP-15891
ITestS3AEncryptionWithDefaultS3Settings fails if default bucket encryption != KMS,13320202,Resolved,Minor,Fixed,30/Jul/20 14:12,04/Sep/20 14:01,3.3.0,"The test ITestS3AEncryptionWithDefaultS3Settings fails if

* the test run is set up with a KMS key
* the test bucket has a default encryption of AES (maybe also unencrypted)

Proposed: downgrade the test to skip if the default encryption is determined to be something other than S3-KMS"
Support for AWS STSAssumeRoleWithWebIdentitySessionCredentialsProvider based credential provider to support use of IRSA on deployments on AWS EKS Cluster,13321231,Resolved,Minor,Won't Fix,06/Aug/20 05:31,01/Sep/20 14:53,3.3.0,"The latest version of AWS SDK has support to use IRSA for providing credentials to Kubernetes pods which can potentially replace the use of Kube2IAM. For our Apache Spark on Kubernetes use cases, this feature will be useful. The current Hadoop AWS component does support adding custom credential provider but I think if we could add STSAssumeRoleWithWebIdentitySessionCredentialsProvider support to (using roleArn, role session name, web Identity Token File) to the hadoop-aws library, it will be useful for the community as such who use AWS EKS.

[https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/STSAssumeRoleWithWebIdentitySessionCredentialsProvider.html]

[https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/STSAssumeRoleWithWebIdentitySessionCredentialsProvider.Builder.html

] [https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/]"
Fix wrong command line for setting up CentOS 8,13325673,Resolved,Minor,Fixed,02/Sep/20 05:32,02/Sep/20 06:12,,Instruction for setting up CentOS 8 in BUILDING.txt has invalid command line.
Intermittent ITestTerasortOnS3A.test_120_terasort failure,13321330,Open,Minor,,06/Aug/20 16:10,,3.3.0,"[*INFO*] Running org.apache.hadoop.fs.s3a.commit.terasort.*ITestTerasortOnS3A*

[*ERROR*] *Tests* *run: 14*, *Failures: 2*, Errors: 0, *Skipped: 2*, Time elapsed: 110.43 s *<<< FAILURE!* - in org.apache.hadoop.fs.s3a.commit.terasort.*ITestTerasortOnS3A*

[*ERROR*] test_120_terasort[directory](org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A)  Time elapsed: 6.261 s  <<< FAILURE!

java.lang.AssertionError: terasort(s3a://mthakur-data/terasort-directory/sortin, s3a://mthakur-data/terasort-directory/sortout) failed expected:<0> but was:<1>

 at org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A.executeStage(ITestTerasortOnS3A.java:241)

 at org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A.test_120_terasort(ITestTerasortOnS3A.java:291)

 

[*ERROR*] test_120_terasort[magic](org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A)  Time elapsed: 5.962 s  <<< FAILURE!

java.lang.AssertionError: terasort(s3a://mthakur-data/terasort-magic/sortin, s3a://mthakur-data/terasort-magic/sortout) failed expected:<0> but was:<1>

 at org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A.executeStage(ITestTerasortOnS3A.java:241)

 at org.apache.hadoop.fs.s3a.commit.terasort.ITestTerasortOnS3A.test_120_terasort(ITestTerasortOnS3A.java:291)

 "
Fix findbugs warnings in hadoop-tools on branch-2.10,13321915,Resolved,Minor,Fixed,11/Aug/20 03:06,18/Aug/20 04:59,,"{noformat}
M D UC_USELESS_OBJECT UC: Useless object stored in variable keysToUpdateAsFolder of method org.apache.hadoop.fs.azure.NativeAzureFileSystem.mkdirs(Path, FsPermission, boolean)  At NativeAzureFileSystem.java:[line 3013]
M D DLS_DEAD_LOCAL_STORE DLS: Dead store to op in org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.access(Path, FsAction)  At AzureBlobFileSystemStore.java:[line 901]
M B CO_COMPARETO_INCORRECT_FLOATING Co: org.apache.hadoop.mapred.gridmix.InputStriper$1.compare(Map$Entry, Map$Entry) incorrectly handles double value  At InputStriper.java:[line 136]
M V MS_MUTABLE_COLLECTION_PKGPROTECT MS: org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin$DefaultHeapUsageEmulator.heapSpace is a mutable collection which should be package protected  At TotalHeapUsageEmulatorPlugin.java:[line 132]
M D RV_RETURN_VALUE_IGNORED_NO_SIDE_EFFECT RV: Return value of org.codehaus.jackson.map.ObjectMapper.getJsonFactory() ignored, but method has no side effect  At JsonObjectMapperWriter.java:[line 59]
H D RV_RETURN_VALUE_IGNORED_NO_SIDE_EFFECT RV: Return value of new org.apache.hadoop.tools.rumen.datatypes.DefaultDataType(String) ignored, but method has no side effect  At MapReduceJobPropertiesParser.java:[line 212]
{noformat}
"
Add python2 to required package on CentOS 8 for building documentation,13322675,Resolved,Minor,Fixed,14/Aug/20 06:41,14/Aug/20 11:45,,"{{mvn site}} fails without python2.

{noformat}
[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---
/usr/bin/env: ‘python2’: No such file or directory
{noformat}
"
S3A rename operation not the same as HDFS when dest is empty dir,13319538,Open,Minor,,27/Jul/20 12:13,,3.2.1,"When I run the test ITestS3ADeleteManyFiles, I change the [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3ADeleteManyFiles.java#L97]

to 

{code}

fs.mkdirs(finalDir);

{code}

So before rename operator, ""finalParent/final"" has been created.

But after the rename operation,  all the files will be moved from ""srcParent/src"" to ""finalParent/final""

So this is not the same with the HDFS rename operation:

HDFS rename includes the calculation of the destination path. If the destination exists and is a directory, the final destination of the rename becomes the destination + the filename of the source path.
let dest = if (isDir(FS, src) and d != src) :
        d + [filename(src)]
    else :
        d"
DF implementation of CachingGetSpaceUsed makes DFS Used size not correct,13319466,Resolved,Minor,Not A Problem,27/Jul/20 06:24,27/Jul/20 07:08,,"When   we calculate DN's storage used, we add each Volume's used size together and each volume's size comes from it's BP's size. 

When we use DF instead of DU, we know that DF check disk space usage (not disk size of a directory). so when check BP dir path,  What you're actually checking is the corresponding disk directory space. 

 

When we use this with federation, under each volume  may have more than one BP, each BP return it's corresponding disk directory space. 

 

If we have two BP under one volume, we will make DN's storage info's Used size double than real size."
Fix outdated properties of JournalNode when performing rollback,13318410,Resolved,Minor,Duplicate,21/Jul/20 07:11,21/Jul/20 07:13,,"When rollback HDFS cluster, properties in JNStorage won't be refreshed after the storage dir changed. It leads to exceptions when starting namenode.

The exception like:
{code:java}
2020-07-09 19:04:12,810 FATAL [IPC Server handler 105 on 8022] org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: recoverUnfinalizedSegments failed for required journal (JournalAndStream(mgr=QJM to [10.0.118.217:8485, 10.0.117.208:8485, 10.0.118.179:8485], stream=null))
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:
10.0.118.217:8485: Incompatible namespaceID for journal Storage Directory /mnt/vdc-11176G-0/dfs/jn/nameservicetest1: NameNode has nsId 647617129 but storage has nsId 0
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.checkConsistentNamespace(JNStorage.java:236)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.newEpoch(Journal.java:300)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.newEpoch(JournalNodeRpcServer.java:136)
	at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.newEpoch(QJournalProtocolServerSideTranslatorPB.java:133)
	at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25417)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2274)
{code}"
Use RpcMetrics.TIMEUNIT to initialize rpc queueTime and processingTime,13316560,Resolved,Minor,Fixed,13/Jul/20 21:58,14/Jul/20 18:23,,"While making an internal change to use {{TimeUnit.MICROSECONDS}} instead of {{TimeUnit.MILLISECONDS}} for rpc details, we found that we also had to modify this code in DecayRpcScheduler.addResponseTime() to initialize {{queueTime}} and {{processingTime}} with the correct units.
{noformat}
    long queueTime = details.get(Timing.QUEUE, TimeUnit.MILLISECONDS);
    long processingTime = details.get(Timing.PROCESSING, TimeUnit.MILLISECONDS);
{noformat}
If we change these to use {{RpcMetrics.TIMEUNIT}} it is simpler.

We also found one test case in TestRPC that was assuming the units were milliseconds."
Fix typo in Hadoop KMS document,13322190,Resolved,Trivial,Fixed,12/Aug/20 05:27,12/Aug/20 07:11,,"[https://hadoop.apache.org/docs/r3.3.0/hadoop-kms/index.html#HTTP_Kerberos_Principals_Configuration]

bq. In order to be able to access directly a specific KMS instance, the KMS instance must also have Keberos service name with its own hostname. This is required for monitoring and admin purposes.

Keberos -> Kerberos"
Correct spelling errors for separator,13328441,Resolved,Trivial,Fixed,21/Sep/20 02:53,23/Sep/20 06:50,3.3.0,"Many spelling errors for separator, correct them!"
Erasure Coding: Typo in document,13324785,Resolved,Trivial,Fixed,27/Aug/20 07:23,28/Aug/20 12:16,3.3.0,"When review ec document and code, find the typo.
Change ""a erasure code"" to ""an erasure code"""
Fix typos in hadoop-aws documentation,13315580,Resolved,Trivial,Fixed,08/Jul/20 08:50,08/Jul/20 15:06,,There are couple of typos in the hadoop-aws documentation (markdown). I'll open a PR.
S3A: IAMInstanceCredentialsProvider failing: Failed to load credentials from IMDS,13554547,Resolved,Blocker,Fixed,18/Oct/23 10:31,23/Oct/23 13:25,3.4.0,"Failures in impala test VMs using iAM for auth

{code}
Failed to open file as a parquet file: java.net.SocketTimeoutException: re-open s3a://impala-test-uswest2-1/test-warehouse/test_pre_gregorian_date_parquet_2e80ae30.db/hive2_pre_gregorian.parquet at 84 on s3a://impala-test-uswest2-1/test-warehouse/test_pre_gregorian_date_parquet_2e80ae30.db/hive2_pre_gregorian.parquet: org.apache.hadoop.fs.s3a.auth.NoAwsCredentialsException: +: Failed to load credentials from IMDS

{code}
"
Build failure while trying to create apache 3.3.7 release locally.,13553554,Resolved,Critical,Fixed,10/Oct/23 15:43,13/Oct/23 20:18,3.3.6,"{noformat}
[ESC[1;34mINFOESC[m] ESC[1m-------< ESC[0;36morg.apache.hadoop:hadoop-client-check-test-invariantsESC[0;1m >--------ESC[m
[ESC[1;34mINFOESC[m] ESC[1mBuilding Apache Hadoop Client Packaging Invariants for Test 3.3.9-SNAPSHOT [105/111]ESC[m
[ESC[1;34mINFOESC[m] ESC[1m--------------------------------[ pom ]---------------------------------ESC[m
[ESC[1;34mINFOESC[m] 
[ESC[1;34mINFOESC[m] ESC[1m--- ESC[0;32mmaven-enforcer-plugin:3.0.0-M1:enforceESC[m ESC[1m(enforce-banned-dependencies)ESC[m @ ESC[36mhadoop-client-check-test-invariantsESC[0;1m ---ESC[m
[ESC[1;34mINFOESC[m] Adding ignorable dependency: org.apache.hadoop:hadoop-annotations:null
[ESC[1;34mINFOESC[m]   Adding ignore: *
[ESC[1;33mWARNINGESC[m] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:
Duplicate classes found:


  Found in:
    org.apache.hadoop:hadoop-client-minicluster:jar:3.3.9-SNAPSHOT:compile
    org.apache.hadoop:hadoop-client-runtime:jar:3.3.9-SNAPSHOT:compile
  Duplicate classes:
    META-INF/versions/9/module-info.class

{noformat}
CC [~stevel@apache.org]  [~weichu] "
Setup pre-commit CI for Windows 10,13562836,Resolved,Critical,Fixed,24/Dec/23 17:41,02/Jan/24 11:53,3.4.0,"We need to setup a pre-commit CI for validating the Hadoop PRs against Windows 10.

On a sidenote, we've got the nightly Jenkins CI running for Hadoop on Windows 10 - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/."
ABFS: Remove commons IOUtils.close() from AbfsOutputStream,13554387,Open,Critical,,17/Oct/23 09:51,,3.4.0,"
Hive is raising  java.lang.NoSuchMethodError when attempting to run Hive queries against abfs because it's version of commons IO doesn't match that hadoop is built with.

Moving to use Hadoop's IOUtil.cleanup methods rather than commons IOUtils avoids this"
NPE in AWS v2 SDK RetryOnErrorCodeCondition.shouldRetry(),13554228,Resolved,Critical,Fixed,16/Oct/23 11:08,23/Oct/23 13:31,3.4.0,"NPE in error handling code of RetryOnErrorCodeCondition.shouldRetry(); in bundle-2.20.128.jar

This is AWS SDK code; fix needs to go there. 

{code}
Caused by: java.lang.NullPointerException
	at software.amazon.awssdk.awscore.retry.conditions.RetryOnErrorCodeCondition.shouldRetry(RetryOnErrorCodeCondition.java:45) ~[bundle-2.20.128.jar:?]
	at software.amazon.awssdk.core.retry.conditions.OrRetryCondition.lambda$shouldRetry$0(OrRetryCondition.java:46) ~[bundle-2.20.128.jar:?]
	at java.util.stream.MatchOps$1MatchSink.accept(MatchOps.java:90) ~[?:1.8.0_382]
{code}
"
fs.getXattrs(path) for S3FS doesn't have x-amz-server-side-encryption-aws-kms-key-id header.,13561903,Resolved,Major,Fixed,14/Dec/23 20:52,15/May/24 17:41,3.3.6,"Once a path while uploading has been encrypted with SSE-KMS with a key id and then later when we try to read the attributes of the same file, it doesn't contain the key id information as an attribute. should we add it?

 

while cherry-picking please include https://issues.apache.org/jira/browse/HADOOP-19190"
coalesce AWS S3 client proxy settings,13553866,Open,Major,,12/Oct/23 12:50,,3.4.0,"all the code to build up proxy settings for the two clients is nearly identical.

* coalesce into on method to load all the options, another to apply them to the different bullders
* extend TestS3AProxy to test async client too
* test for invalid options, rather than just the good paths.

"
S3A: retry on credential expiry,13559398,Open,Major,,25/Nov/23 13:11,,3.4.0,"Reported in AWS SDK https://github.com/aws/aws-sdk-java-v2/issues/3408

bq. In RetryableStage execute method, the ""AwsCredentails"" does not attempt to renew if it has expired. Therefore, if a method called with the existing credential is expiring soon, the number of retry is less than intended due to the expiration of the credential.

The stack from this report doesn't show any error detail we can use to identify the 400 exception as something we should be retrying on. This could be due to the logging, or it could actually hold. we've have to generate some socket credentials, let them expire and then see how hadoop fs commands failed. Something to do by hand as an STS test to do this is probably slow. *unless we expire all session credentials of a given role?*. Could be good, would be traumatic for other test runs though.

{code}
software.amazon.awssdk.services.s3.model.S3Exception: The provided token has expired. (Service: S3, Status Code: 400, Request ID: 3YWKVBNJPNTXPJX2, Extended Request ID: GkR56xA0r/Ek7zqQdB2ZdP3wqMMhf49HH7hc5N2TAIu47J3HEk6yvSgVNbX7ADuHDy/Irhr2rPQ=)

{code}
"
S3A: Support dynamic region resolution,13558736,Open,Major,,20/Nov/23 17:06,,3.4.0,"AWS v1 SDK used to do dynamic region inference including EC2 metadata. See HADOOP-17771 for that at work with the chain of: sysprops, env vars, EC2 metadata.

I think we *try* to do this, but I'm not sure it is working, maybe related to the region=null vs region="""" options: its very hard to differentiate the two"
S3A: transfer manager not wired up to s3a executor pool,13561134,Open,Major,,08/Dec/23 11:57,,3.4.0,"S3ClientFactory.createS3TransferManager() doesn't use the executor declared in S3ClientCreationParameters.transferManagerExecutor

* method needs to take S3ClientCreationParameters
* and set the transfer manager executor"
ABFS: Enable Footer Read Optimizations with Appropriate Footer Read Buffer Size,13557743,Resolved,Major,Fixed,13/Nov/23 15:55,03/Jan/24 12:50,3.3.6,"Footer Read Optimization was introduced to Hadoop azure in this Jira: https://issues.apache.org/jira/browse/HADOOP-17347
and was kept disabled by default.
This PR is to enable footer reads by default based on the results of analysis performed as below:

In our scale workload analysis, it was found that workloads working with Parquet (or for that matter OCR etc.) have a lot of footer reads. Footer reads here refers to the read operations done by workload to get the metadata of the parquet file which is required to understand where the actual data resides in the parquet.
This whole process takes place in 3 steps:
 # Workload reads the last 8 bytes of parquet file to get the offset and size of the metadata which is present just above these 8 bytes.
 # Using that offset, workload reads the metadata to get the exact offset and length of data which it wants to read.
 # Workload performs the final read operation to get the data it wants to use for its purpose.

Here the first two steps are metadata reads that can be combined into a single footer read. When workload tries to read certain last few bytes of data (let's say this value is footer size), driver will intelligently read some extra bytes above the footer size to cater to the next read which is going to come.

Q. What is the footer size of file?
A: 16KB. Any read request trying to get the data within last 16KB of the file will qualify for whole footer read. This value is enough to cater to all types of files including parquet, OCR, etc.

Q. What is the buffer size to read when reading the footer?
A. Let's call this footer read buffer size. Prior to this PR footer read buffer size was same as read buffer size (default 4MB). It was found that for most of the workload required footer size was only 256KB. i.e. For almost all parquet files metadata for that file was found to be within last 256KBs. Keeping this in mind it does not make sense to read whole buffer length of 4MB as a part of footer read. Moreover, reading larger data than require incur additional costs in terms of server and network latencies. Based on this and extensive experimentation it was observed that footer read buffer size of 512KB is ideal for almost all the workloads running on parquet, OCR, etc.

Following configuration was introduced to configure the footer read buffer size:
{*}fs.azure.footer.read.request.size{*}: default 512 KB.

*Quantitative Stats:* For a workload running on parquet files the number of read requests got reduced by 2.3M down from 20M. That means around 10% reduction in overall TPS."
upgrade avro to 1.11.3 due to CVE,13555487,Open,Major,,25/Oct/23 12:16,,,"[https://nvd.nist.gov/vuln/detail/CVE-2023-39410]

When deserializing untrusted or corrupted data, it is possible for a reader to consume memory beyond the allowed constraints and thus lead to out of memory on the system. This issue affects Java applications using Apache Avro Java SDK up to and including 1.11.2. Users should update to apache-avro version 1.11.3 which addresses this issue."
Release Hadoop 3.4.0,13562871,Resolved,Major,Fixed,25/Dec/23 15:52,19/Mar/24 12:59,3.4.0,"Confirmed features to be included in the release:

- Enhanced functionality for YARN Federation.
- Redesigned resource allocation in YARN Capacity Scheduler
- Optimization of HDFS RBF.
- Introduction of fine-grained global locks for DataNodes.
- Improvements in the stability of HDFS EC, and more.
- Fixes for important CVEs.

*Issues that need to be addressed in hadoop-3.4.0-RC0 version.*

1.  confirm the JIRA target version/fix version is 3.4.0 to ensure that the version setting is correct.
2. confirm the highlight of hadoop-3.4.0.
3. backport branch-3.4.0/branch-3.4.

{code:java}
  HADOOP-19040. mvn site commands fails due to MetricsSystem And MetricsSystemImpl changes. 
  YARN-11634. [Addendum] Speed-up TestTimelineClient. 
  MAPREDUCE-7468. [Addendum] Fix TestMapReduceChildJVM unit tests.
  
  Revert HDFS-16016. BPServiceActor to provide new thread to handle IBR.
{code}



    

"
Parallel Maven Build Support for Apache Hadoop,13561609,Resolved,Major,Fixed,13/Dec/23 02:35,23/Jan/24 06:53,,"The reason for the slow compilation: The Hadoop project has many modules, and the inability to compile them in parallel results in a slow process. For instance, the first compilation of Hadoop might take several hours, and even with local Maven dependencies, a subsequent compilation can still take close to 40 minutes, which is very slow.

How to solve it: Use {{mvn dependency:tree}} and {{maven-to-plantuml}} to investigate the dependency issues that prevent parallel compilation.
 * Investigate the dependencies between project modules.
 * Analyze the dependencies in multi-module Maven projects.
 * Download {{{}maven-to-plantuml{}}}:

 
{{wget [https://github.com/phxql/maven-to-plantuml/releases/download/v1.0/maven-to-plantuml-1.0.jar]}}
 * Generate a dependency tree:

 
{{mvn dependency:tree > dep.txt}}
 * Generate a UML diagram from the dependency tree:

 
{{java -jar maven-to-plantuml.jar --input dep.txt --output dep.puml}}

For more information, visit: [maven-to-plantuml GitHub repository|https://github.com/phxql/maven-to-plantuml/tree/master].

 

*Hadoop Parallel Compilation Submission Logic*
 # Reasons for Parallel Compilation Failure

 * 
 ** In sequential compilation, as modules are compiled one by one in order, there are no errors because the compilation follows the module sequence.
 ** However, in parallel compilation, all modules are compiled simultaneously. The compilation order during multi-module concurrent compilation depends on the inter-module dependencies. If Module A depends on Module B, then Module B will be compiled before Module A. This ensures that the compilation order follows the dependencies between modules.
But when Hadoop compiles in parallel, for example, compiling {{{}hadoop-yarn-project{}}}, the dependencies between modules are correct. The issue arises during the dist package stage. {{dist}} packages all other compiled modules.

*Behavior of {{hadoop-yarn-project}} in Serial Compilation:*
 * 
 ** In serial compilation, it compiles modules in the pom one by one in sequence. After all modules are compiled, it compiles {{{}hadoop-yarn-project{}}}. During the {{prepare-package}} stage, the {{maven-assembly-plugin}} plugin is executed for packaging. All packages are repackaged according to the description in {{{}hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml{}}}.
*Behavior of {{hadoop-yarn-project}} in Parallel Compilation:*

 * 
 ** Parallel compilation compiles modules according to the dependency order among them. If modules do not declare dependencies on each other through {{{}dependency{}}}, they are compiled in parallel. According to the dependency definition in the pom of {{{}hadoop-yarn-project{}}}, the dependencies are compiled first, followed by {{{}hadoop-yarn-project{}}}, executing its {{{}maven-assembly-plugin{}}}.
 ** However, the files needed for packaging in {{hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml}} are not all included in the {{dependency}} of {{{}hadoop-yarn-project{}}}. Therefore, when compiling {{hadoop-yarn-project}} and executing {{{}maven-assembly-plugin{}}}, not all required modules are built yet, leading to errors in parallel compilation.
*Solution:*

 * 
 ** The solution is relatively straightforward: organize all modules from {{{}hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml{}}}, and then declare them as dependencies in the pom of {{{}hadoop-yarn-project{}}}."
S3A region logic to handle vpce and non standard endpoints ,13554191,Resolved,Major,Fixed,16/Oct/23 08:16,05/Sep/24 13:14,3.4.0,"For non standard endpoints such as VPCE the region parsing added in HADOOP-18908 doesn't work. This is expected as that logic is only meant to be used for standard endpoints. 

If you are using a non-standard endpoint, check if a region is also provided, else fail fast. 

Also update documentation to explain to region and endpoint behaviour with SDK V2. "
ITestS3AHugeFilesEncryption failure,13557211,Resolved,Major,Fixed,08/Nov/23 17:17,20/Aug/24 15:23,3.4.0,"test failures for me with a test setup of per-bucket encryption of sse-kms.

suspect (but can't guarantee) HADOOP-18850 may be a factor."
Upgrade kafka to 3.4.0,13556559,Resolved,Major,Fixed,02/Nov/23 16:46,24/May/24 16:44,,Upgrade kafka-clients to 3.4.0 to fix https://nvd.nist.gov/vuln/detail/CVE-2023-25194
Upgrade Guava to 32.0.1 due to CVE-2023-2976,13560949,In Progress,Major,,07/Dec/23 08:51,,,Upgrade Guava to 32.0.1 due to CVE-2023-2976
Java 17 runtime library upgrade prerequisites,13555598,Open,Major,,26/Oct/23 06:56,,,
S3A: make fs.s3a.create.performance an option you can set for the entire bucket,13553580,Resolved,Major,Fixed,10/Oct/23 18:43,13/Feb/24 17:00,3.3.9,"make the fs.s3a.create.performance option something you can set everywhere, rather than just in an openFile() option or under a magic path.

this improves performance on apps like iceberg where filenames are generated with UUIDs in them, so we know there are no overwrites"
Use CRC tables to speed up galoisFieldMultiply in CrcUtil,13561887,Open,Major,,14/Dec/23 18:14,,,"CrcUtil.galoisFieldMultiply(p, q, m) supports multiplying two polynomials p, q modulo any modulus polynomial m over GF(2). Since the method is used for CRC calculations, the modulus polynomial m is restricted to either the GZIP_POLYNOMIAL or the CASTAGNOLI_POLYNOMIAL. We may use CRC tables in PureJavaCrc32/PureJavaCrc32C to speed up the computation."
"S3A: add option ""fs.s3a.optimized.copy.from.local.enabled"" to enable/disable CopyFromLocalOperation",13553381,Resolved,Major,Fixed,09/Oct/23 10:50,07/Dec/23 14:42,3.3.6,"reported failure of CopyFromLocalOperation.getFinalPath() during job submission with s3a declared as cluster fs.

add an emergency option to disable this optimised uploader and revert to the superclass implementation"
S3A: Upgrade AWS SDK version to 2.21.33 for Amazon S3 Express One Zone support,13559872,Resolved,Major,Fixed,29/Nov/23 09:57,29/Nov/23 13:17,3.4.0,"Upgrade SDK version to 2.21.33, which adds S3 Express One Zone support."
S3A to provide full support for S3 Express One Zone,13559882,Resolved,Major,Fixed,29/Nov/23 10:55,01/Dec/23 14:17,3.4.0,"HADOOP-18995 upgrades the SDK version which allows connecting to a s3 express one zone support. 

Complete support needs to be added to address tests that fail with s3 express one zone, additional tests, documentation etc. 

* hadoop-common path capability to indicate that treewalking may encounter missing dirs
* use this in treewalking code in shell, mapreduce FileInputFormat etc to not fail during treewalks
* extra path capability for s3express too.
* tests for this
* anything else

A filesystem can now be probed for inconsistent directory listings through {{fs.hasPathCapability(path, ""fs.capability.directory.listing.inconsistent"")}}

If true, then treewalking code SHOULD NOT report a failure if, when walking into a subdirectory, a list/getFileStatus on that directory raises a FileNotFoundExceptin.
"
LD_LIBRARY_PATH is missing HADOOP_COMMON_LIB_NATIVE_DIR,13560304,Open,Major,,02/Dec/23 09:11,,3.2.4,"When we run a spark job, we find that it cannot load the native library successfully.

We found a difference between hadoop2 and hadoop3.

hadoop2-Spark-System Properties：
|java.library.path|:/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib|

hadoop3-Spark-System Properties：
|java.library.path|:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib|
The key point is：

hadoop2-hadoop-config.sh：
HADOOP_OPTS=""$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH""   <--267
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_LIBRARY_PATH     <--268
 
hadoop3-hadoop-functions.sh:
hadoop_add_param HADOOP_OPTS java.library.path \
""-Djava.library.path=${JAVA_LIBRARY_PATH}""
export LD_LIBRARY_PATH        <--1484
 

At the same time, the hadoop3 will clear all non-whitelisted environment variables.
I'm not sure if it was intentional. But it makes our spark job unable to find the native library on hadoop3. 

Maybe we should modify hadoop-functions.sh(1484) and add LD_LIBRARY_PATH to the default configuration item yarn.nodemanager.env-whitelist."
Filter NaN values from JMX json interface,13555635,Resolved,Major,Fixed,26/Oct/23 09:41,09/Nov/23 09:14,3.4.0,"As we can see in this [Yarn documentation|https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html] beans can represent Float values as NaN. These values will be represented in the JMX response JSON like:
{noformat}
...
""GuaranteedCapacity"": NaN,
...
{noformat}

Based on the [JSON doc|https://www.json.org/] NaN is not a valid JSON token ( however some of the parser libs can handle it ), so not every consumer can parse values like these.

To be able to parse NaN values, a new feature flag should be created.
The new feature will replace the NaN values with 0.0 values.
The feature is default turned off. It can be enabled with the *hadoop.http.jmx.nan-filter.enabled* config."
S3A: Support Authentication through HttpSigner API ,13560691,Resolved,Major,Fixed,05/Dec/23 18:28,11/Jan/24 17:14,3.4.0,"The latest AWS SDK changes how signing works, and for signing S3Express signatures the new {{software.amazon.awssdk.http.auth}} auth mechanism is needed"
AWS SDK v2:  extend support for FIPS endpoints,13558110,Resolved,Major,Fixed,15/Nov/23 18:27,23/Jan/24 11:52,3.4.0,"v1 SDK supported FIPS just by changing the endpoint.

Now we have a new builder setting to use.

* add new  fs.s3a.endpoint.fips option
* pass it down
* test
"
Increase fs.s3a.connection.maximum to 500 to minimize risk of Timeout waiting for connection from pool,13562352,Resolved,Major,Fixed,19/Dec/23 17:09,22/Jan/24 19:07,3.4.0,"Getting errors in jobs which can be fixed by increasing this 
2023-12-14 17:35:56,602 [ERROR] [TezChild] |tez.TezProcessor|: java.lang.RuntimeException: java.io.IOException: org.apache.hadoop.net.ConnectTimeoutException: getFileStatus on s3a://aaa/cc-hive-jzv5y6/warehouse/tablespace/managed/hive/student/delete_delta_0000012_0000012_0001/bucket_00001_0: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:152)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:437)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:297)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:84)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:70)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:70)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:40)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptible"
upgrade avro in hadoop-thirdparty to 1.11.3,13552855,Resolved,Major,Fixed,04/Oct/23 08:57,06/Oct/23 10:14,thirdparty-1.2.0,https://lists.apache.org/thread/wcj1747hvyl7qjhrfr6d6j1l62hvpr5l
Zookeeper SSL/TLS support in HDFS ZKFC,13552662,Resolved,Major,Fixed,02/Oct/23 16:36,23/Oct/23 18:04,3.4.0,"HADOOP-18709 added support for Zookeeper to communicate with SSL/TLS enabled in hadoop-common. With those changes we have the necessary parameters, that we need to set to enable SSL/TLS in a ZK Client.

In YARN-11468 the SSL communication can be set in Yarn, now we need to similar changes in HDFS to enable it correctly. In HDFS ZK Client is used in the Failover Controller. In this improvement we need to create the ZK client with the necessary SSL configs if we enable it, which we can track under a new HDFS config.  "
RPC Metrics : Optimize logic for log slow RPCs,13552846,Resolved,Major,Fixed,04/Oct/23 07:53,25/Oct/23 05:59,3.4.0,"HADOOP-12325 implement a capability where ""slow"" RPCs are logged in NN log.
Current processing logic is the ""slow"" RPCs are to be those whose processing time is outside 3 standard deviation.
However, in practice it is found that many logs of slow rpc are currently output, and sometimes RPCs with a processing time of 1ms are also declared as slow, this is not in line with actual expectations.

Therefore, consider optimize the logic conditions of slow RPC and add a `logSlowRPCThresholdMs` variable to judge whether the current RPCas slow so that the expected slow RPC log can be logger.
for `logSlowRPCThresholdMs`, we can support dynamic refresh to facilitate adjustments based on the actual operating conditions of the hdfs cluster."
Upgrade to jetty 9.4.53,13553915,Resolved,Major,Fixed,12/Oct/23 19:55,29/Oct/23 07:39,3.4.0,"2 CVE fixes in https://github.com/jetty/jetty.project/releases/tag/jetty-9.4.53.v20231009
4 more security fixes in https://github.com/jetty/jetty.project/releases/tag/jetty-9.4.52.v20230823"
Zookeeper SSL/TLS support in ZKDelegationTokenSecretManager and ZKSignerSecretProvider,13554422,Resolved,Major,Fixed,17/Oct/23 13:38,17/Nov/23 09:52,3.4.0,"HADOOP-18709 added support for Zookeeper to communicate with SSL/TLS enabled in hadoop-common. With those changes we have the necessary parameters, that we need to set to enable SSL/TLS in a ZK Client. That change also did changes in ZKCuratorManager, so with that it is easy to set the SSL/TLS, for Yarn it was done in YARN-11468.

In DelegationTokenAuthenticationFilter currently we are using CuratorFrameworkFactory, it'd be good to change it to use ZKCuratorManager and with that we should support SSL/TLS enablement.

*UPDATE*

So as I investigated this a bit more, it wouldn't be so easy to move to using ZKCuratorManager. 
DelegationTokenAuthenticationFilter uses ZK from two places: in ZKDelegationTokenSecretManager and in ZKSignerSecretProvider. In both places it uses CuratorFrameworkFactory, but the attributes and creation differentiates from ZKCuratorManager. 

In ZKDelegationTokenSecretManager it would be easy to add the new config and based on that create ZK with CuratorFrameworkFactory. But ZKSignerSecretProvider is in hadoop-auth module and with my change it would need hadoop-common, so it would introduce circular dependency between modules 'hadoop-auth' and 'hadoop-common'. I'm still working on a straightforward solution. "
NullPointerException in Hadoop Credential Check CLI Command,13561621,Resolved,Major,Fixed,13/Dec/23 05:38,27/Dec/23 12:20,3.3.0,"*Description*: Hadoop's credential check throws {{NullPointerException}} when alias not found.

{code:bash}
hadoop credential check ""fs.gs.proxy.username"" -provider ""jceks://file/usr/lib/hive/conf/hive.jceks"" {code}

Checking aliases for CredentialProvider: jceks://file/usr/lib/hive/conf/hive.jceks
Enter alias password: 

java.lang.NullPointerException
at
org.apache.hadoop.security.alias.CredentialShell$CheckCommand.execute(CredentialShell.java:369)
at org.apache.hadoop.tools.CommandShell.run(CommandShell.java:73)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:529)}}

"
Update the year to 2024,13563262,Resolved,Major,Fixed,31/Dec/23 21:57,01/Jan/24 07:28,3.4.0,Update the year to 2024
S3ARetryHandler to treat SocketExceptions as connectivity failures,13553503,Resolved,Major,Fixed,10/Oct/23 09:46,12/Oct/23 16:49,3.3.6,"i've got a v1 sdk stack trace where a TCP connection reset is breaking a large upload. that should be recoverable with retries.


{code}
com.amazonaws.SdkClientException: Unable to execute HTTP request: Connection reset by peer: Unable to execute HTTP request: Connection reset by peer at...

{code}


proposed:
* S3ARetryPolicy to map SocketException to connectivity failure
* See if we can create a test for this, ideally under the aws sdk.

I'm now unsure about how well we handle these io problems...a quick experiment with the 3.3.5 release shows that the retry policy retries on whatever exception chain has an unknown host for the endpoint. 
"
Upgrade AWS v2 SDK to 2.20.160 and v1 to 1.12.565,13553736,Resolved,Major,Fixed,11/Oct/23 17:57,25/Oct/23 11:12,3.4.0,"Bump up the sdk versions for both...even if we don't ship v1 it helps us qualify releases with newer versions, and means that an upgrade of that alone to branch-3.3 will be in sync."
upgrade netty to 4.1.100 due to CVE,13553751,Resolved,Major,Fixed,11/Oct/23 19:26,25/Oct/23 13:07,3.3.6,"follow up to https://issues.apache.org/jira/browse/HADOOP-18783

https://netty.io/news/2023/10/10/4-1-100-Final.html

security advisory https://github.com/netty/netty/security/advisories/GHSA-xpw8-rcwv-8f8p

""HTTP/2 Rapid Reset Attack - DDoS vector in the HTTP/2 protocol due RST framesHTTP/2 Rapid Reset Attack - DDoS vector in the HTTP/2 protocol due RST frames
"
Upgrade ZooKeeper to 3.7.2,13554483,Resolved,Major,Fixed,18/Oct/23 01:29,19/Oct/23 11:10,3.3.7,"While HADOOP-18613 is proposing to upgrade ZooKeeper to 3.8, it will bring dependency conflicts. Upgrading to ZooKeeper 3.7 could be alternative short-term fix for addressing CVEs."
upgrade maven dependency plugin due to security issue,13555254,Resolved,Major,Fixed,23/Oct/23 23:16,24/Oct/23 11:31,3.4.0,https://github.com/advisories/GHSA-2f88-5hg8-9x2x
Use StandardCharsets.UTF_8 constant,13555841,Resolved,Major,Fixed,27/Oct/23 13:19,20/Nov/23 18:15,3.4.0,"* there are some places in the code that have to check for UnsupportedCharsetException when explicitly using the charset name ""UTF-8""
* using StandardCharsets.UTF_8 is more efficient because the Java libs usually have to look up the charsets when you provide it as String param instead
* also stop using Guava Charsets and use StandardCharsets"
Update plugin for SBOM generation to 2.7.10,13557187,Resolved,Major,Fixed,08/Nov/23 12:46,15/Nov/23 14:28,3.4.0,Update the CycloneDX Maven plugin for SBOM generation to 2.7.10
Fix doc about loading native libraries,13558549,Resolved,Major,Fixed,19/Nov/23 10:48,06/Dec/23 13:25,3.4.0,"When we want load a native library libmyexample.so, the right way is to call 
System.loadLibrary(""myexample"") rather than System.loadLibrary(""libmyexample.so"")."
Upgrade Zookeeper to 3.8.2,13558876,Resolved,Major,Duplicate,21/Nov/23 13:43,17/Dec/23 09:31,3.3.6,
S3A: Upgrade AWS SDK to 2.21.41,13561169,Resolved,Major,Fixed,08/Dec/23 14:40,12/Dec/23 15:16,3.3.7-aws,"sdk 2.21.41 is out and logging now picks up the log4j.properties options. 

move to this ASAP"
Possible ConcurrentModificationException if Exec command fails,13561671,Resolved,Major,Fixed,13/Dec/23 12:32,15/Dec/23 15:39,3.4.0,"{{ConcurrentModificationException}} may happen in:

{code:title=https://github.com/apache/hadoop/blob/19b9e6a97b8faa0eb48e7b1855ab37e6d4b6c2a9/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/util/Exec.java#L64-L82}
  public int run(List<String> command, List<String> output,
      List<String> errors) {
    int retCode = 1;
    ProcessBuilder pb = new ProcessBuilder(command);
    try {
      Process p = pb.start();
      OutputBufferThread stdOut = new OutputBufferThread(p.getInputStream());
      OutputBufferThread stdErr = new OutputBufferThread(p.getErrorStream());
      stdOut.start();
      stdErr.start();
      retCode = p.waitFor();
      if (retCode != 0) {
        mojo.getLog().warn(command + "" failed with error code "" + retCode);
        for (String s : stdErr.getOutput()) {
          mojo.getLog().debug(s);
        }
      }
      stdOut.join();
      stdErr.join();
{code}

due to accessing {{stdErr.getOutput()}} before {{stdErr}} thread stops."
"S3A: add s3guard command ""bucket"" to create buckets",13556267,Resolved,Major,Fixed,31/Oct/23 14:20,05/Dec/23 18:25,3.4.0,"cloudstore has an mkbucket command
https://github.com/steveloughran/cloudstore/blob/main/src/main/site/mkbucket.md

however, its v1 api, has problems with spans and needs rework for v2. Oh, and it has not tests.

* move the command into hadoop-aws
* add a test or two, 
* add a mapper of  409 to bad request, as ""BucketAlreadyOwnedByYouException"" comes in as a 409.

Testing will be tricky...as well as not actually wanting to create new buckets, my test a/c doesn't even have the permission to do so.

Proposed tests
# invalid args must be rejected
# trying to create the current bucket must be rejected. This happens even if you lack the permission to create

*No actual attempts to create a new bucket*
"
Use builder for prefetch CachingBlockManager,13556007,Resolved,Major,Fixed,30/Oct/23 03:55,21/Jan/24 03:54,3.4.0,"Some of the recent changes (HADOOP-18399, HADOOP-18291, HADOOP-18829 etc) have added more params for prefetch CachingBlockManager c'tor to process read/write block requests. They have added too many params and more are likely to be introduced later. We should use builder pattern to pass params.

This would also help consolidating required prefetch params into one single place within S3ACachingInputStream, from scattered locations."
Upgrade grpc jars to v1.53.0 due to CVEs,13553378,Resolved,Major,Fixed,09/Oct/23 10:45,01/Dec/23 04:24,3.3.6,https://mvnrepository.com/artifact/io.grpc/grpc-protobuf
Move oncrpc/portmap from hadoop-nfs to hadoop-common,13558540,Resolved,Major,Fixed,19/Nov/23 06:40,11/Jan/24 14:06,3.4.0,"We want to use udpserver/client for other use cases, rather than only for NFS. One such use case is to export NameNodeHAState for NameNodes via a UDP server. "
use jsr311-compat jar to allow us to use Jackson 2.14.3,13562325,Open,Major,,19/Dec/23 12:24,,,"An alternative to HADOOP-18619

See https://github.com/pjfanning/jsr311-compat"
Remove commons-beanutils dependency from Hadoop 3,13559531,Open,Major,,27/Nov/23 09:48,,,"Hadoop doesn't acually use it, and it pollutes the classpath of dependent projects."
Unable to build Hadoop in Windows Container due to missing of devenv,13562721,Resolved,Major,Not A Problem,22/Dec/23 12:16,25/Dec/23 02:03,3.3.4,"For Windows, [Dockerfile|https://github.com/apache/hadoop/blob/77edca8f0a97668722a6d602aa4d08d1fff06172/dev-support/docker/Dockerfile_windows_10] and [build instructions|https://github.com/apache/hadoop/blob/trunk/BUILDING.txt] are provided for building Hadoop. However, when starting to Maven build Hadoop project in the container, it will fail at calling `devenv` to upgrade VS solutions:
!image-2023-12-22-17-12-45-278.png!

This is caused by [win-vs-upgrade.cmd|https://github.com/apache/hadoop/blob/trunk/dev-support/bin/win-vs-upgrade.cmd]. The script checks whether there's `devenv` command, and if there's not, exit with error.
!image-2023-12-22-17-14-49-935.png!

The script is called during building Hadoop Common project, set in win-native profile of the [POM|https://github.com/apache/hadoop/blob/77edca8f0a97668722a6d602aa4d08d1fff06172/hadoop-common-project/hadoop-common/pom.xml#L903C38-L903C38].
!image-2023-12-22-17-18-37-712.png!

But within the container the command is not available, so it will always fail at this step.
!image-2023-12-22-17-20-24-345.png!

If we manually edit the file, removing the check and the call to devenv. The build will still fail, because current sln file within the code repo is based on VS 2010. Because the VS tools installed is 2019(16), the versions do not match.

!image-2023-12-22-20-08-59-918.png!

I'm not sure if someone has successfully built Hadoop using this Dockerfile before, but currently it doesn't seem to be possible to directly build it just following BUILDING.txt without other change."
Remove keystore parameter requirement from ZookeeperClient ,13561475,Open,Major,,12/Dec/23 00:37,,,"The keystore parameters are not required for ZooKeeper client to establish TLS connection, so we should remove this limitation from the Hadoop client too. The official ZooKeeper client only emits a warning message in the logs, but otherwise proceed. Hadoop should not add additional constraints.

Check it here: [https://github.com/apache/hadoop/blame/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZookeeperClient.java#L256] "
no valid user IDs when importing KEYS in gpg,13561123,Open,Major,,08/Dec/23 10:29,,,"h1. What I did

# Download the KEYS file: https://dlcdn.apache.org/hadoop/common/KEYS
# Import the KEYS file:
{{gpg --import KEYS}}

h1. What happened

There is the error while importing the KEYS file:

{{gpg: key B56E5591: no valid user IDs}}

h1. What is expected

No errors when importing the KEYS file

h1. Observation

The import definitely worked on October 18 2023, but now it doesn't

h1. Environment

h2. Operating System

{{$ cat /etc/os-release}}
{{NAME=""Amazon Linux""}}
{{VERSION=""2""}}
{{ID=""amzn""}}
{{ID_LIKE=""centos rhel fedora""}}
{{VERSION_ID=""2""}}
{{PRETTY_NAME=""Amazon Linux 2""}}
{{ANSI_COLOR=""0;33""}}
{{CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""}}

h2. GPG Version

{{$ gpg --version}}
{{gpg (GnuPG) 2.0.22}}
{{libgcrypt 1.5.3}}
{{Copyright (C) 2013 Free Software Foundation, Inc.}}
{{License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>}}
{{This is free software: you are free to change and redistribute it.}}
{{There is NO WARRANTY, to the extent permitted by law.}}

{{Home: ~/.gnupg}}
{{Supported algorithms:}}
{{Pubkey: RSA, ?, ?, ELG, DSA}}
{{Cipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,}}
{{        CAMELLIA128, CAMELLIA192, CAMELLIA256}}
{{Hash: MD5, SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224}}
{{Compression: Uncompressed, ZIP, ZLIB, BZIP2}}

h2. SHA256 checksum of the KEYS file

{{$ sha256sum KEYS}}
{{807ad6cc5e1fdedf17884a8da9389549e72b9ad5b8e6fc5050382619c7a19ec6  KEYS}}"
S3A: debug logging for http traffic to S3 stores,13560227,Resolved,Major,Not A Problem,01/Dec/23 14:32,08/Dec/23 14:38,3.4.0,"AWS SDK bundle.jar logging doesn't set up right.

{code}
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
{code}

Cloudstore commands have a -debug option to force set this through log4j APIs; this does work. 

Proposed:

* add reflection-based ability to set/query log4j log levels (+tests, obviously)
* add a new log `org.apache.hadoop.fs.s3a.logging.sdk`
* if set to DEBUG, DefaultS3ClientFactory will enable logging on the aws internal/shaded classes

this allows log4j.properties to turn on logging; reflection ensures all is well on other log back-ends and when unshaded aws sdk jars are in use
"
Use thread pool to improve the speed of creating control files in TestDFSIO,13559269,Resolved,Major,Fixed,24/Nov/23 06:33,08/Dec/23 09:18,3.3.6,"When we use TestDFSIO tool to test the throughouts of HDFS clusters, we found it is so slow in the creating controll files stage. 

After refering to the source code, we found that method createControlFile try to create control files serially. It can be improved by using thread pool.

After optimizing, the TestDFSIO tool runs quicker than before.

 "
S3A FileSystem does not correctly list empty directories after HADOOP-17199,13557457,Resolved,Major,Won't Fix,10/Nov/23 08:38,05/Dec/23 10:51,3.2.4,"After HADOOP-17199, s3a filesystem dose not  correctly ls empty directories.   For example. 

Before HADOOP-17199

```

hdfs dfs -ls s3a://testbucket/dir/empty

 

// work right

```

 

After HADOOP-17199

```

hdfs dfs -ls s3a://testbucket/dir/empty

ls: `s3a://testbucket/dir/empty': No such file or directory

 

// wrong ouput

```

 

After Check the code, s3a empty folder don't have prefixes Or objects, it whil throw FIleNotFouldException.

!image-2023-11-10-16-36-53-337.png!  "
Support OpenSSL 3,13559666,Open,Major,,28/Nov/23 08:31,,3.3.6,"Hadoop uses native OpenSSL library provided by linux distribution.

OpenSSL 1.1.1 is deprecated: https://www.openssl.org/blog/blog/2023/03/28/1.1.1-EOL/ 

We need to ensure that Hadoop works with OpenSSL 3."
Snapshotdiff does not detect rename if parent dir removed,13559202,Open,Major,,23/Nov/23 12:39,,3.3.6,"In the example case below, the diff reports the moved folder as a CREATE, where it should be a RENAME.

This means that applications using the diff report (such as DistCP) will have to transfer data where a simple move would suffice.
{code:java}
DistributedFileSystem fs = (DistributedFileSystem) this.fs;

createHdfsFile(new Path(""/level1/level2/file""), ""content"");
fs.allowSnapshot(new Path(""/""));
fs.createSnapshot(new Path(""/""), ""snapshot1"");

fs.rename(new Path(""/level1/level2/""), new Path(""/level2-root""));
fs.delete(new Path(""/level1""), true);
fs.createSnapshot(new Path(""/""), ""snapshot2"");

SnapshotDiffReport snapshotDiff =
        fs.getSnapshotDiffReport(new Path(""/""), ""snapshot1"", ""snapshot2"");
System.out.println(snapshotDiff);
//        Difference between snapshot snapshot1 and snapshot snapshot2 under directory /:
//        M .
//        + ./level2-root
//        - ./level1

// If the delete is commented out:
//        M .
//        M ./level1
//        R ./level1/level2 -> ./level2-root{code}"
Create curator client the same way possibly with the same code for all services,13558395,Open,Major,,17/Nov/23 11:33,,,"Currently ZKSignerSecretProvider and ZKDelegationTokenSecretManager classes are using the same ZookeeperClient class to create their respective Zookeeper connection via a Curator client.
In order to have a standardized way of creating Zookeeper connections, we have two areas to fix:
- YARN and its Curator instantiation
- HDFS ZKFC, which currently uses a Zookeeper client directly, so it should also be migrated to Curator if we want to utilize the common instantiation.

The problem that we would solve is that currently we have 3 places where we create ZK connections, and all of them has its own things, along with some duplications.
We also have the HadoopZookeeperFactory, that I think we do not need. The only thing that is different for this is how we set up SASL permission provider for the connection. I believe that can also be unified, with that the whole Zookeeper connection creation can be pushed down to the recently added code that handles this for ZKDelegationTokenSecretManager and ZKSignerSecretProvider."
Enable service specific keystores and truststores for ZK SSL setup,13558310,Open,Major,,16/Nov/23 22:48,,,"Currently we have the common config properties hadoop.zk.(key|trust)store.(location|password) configuration options.
In HADOOP-18956 a ZKDelegationTokenSecretManager specific option was provided for these configurations, so with that ZKDelegationTokenSecretManager's ZK access can be set in a centralized fashion along with enabling it within ResourceManager, and DFSZKFailoverController.
On the other hand with DTSecretMgr we introduce specific options to be able to specify a separate keystore and truststore to be used.

A good improvement would be to add the truststore/keystore related options to all the components, so that even if the common hadoop.zk.* properties are set, and SSL is enabled, an individual component can have its own separate keystore and truststore set via specific configs, however if there are no specific config specified it can fall back to the common config values."
Replace commons-lang imports with commons-lang3,13558011,Open,Major,,15/Nov/23 07:58,,,"we have a direct dependency on commons-lang3 but still we are using commons-lang at a couple of places, which is a transitive dependency coming from hbase, solr, and couple of other places.

 

good to replace those with lang3, rather than relying on older transitive dependency"
Upgrade hadoop2 docker scripts to latest 2.10.2,13557531,Resolved,Major,Done,10/Nov/23 18:44,14/Nov/23 16:59,,"Apply enhancements from {{docker-hadoop-3}} branch, and upgrade to latest Hadoop 2 release: 2.10.2."
"FsCommand Stat class set the timeZone""UTC"", which is different from the machine's timeZone",13555569,Open,Major,,26/Oct/23 02:02,,,"Using Hadoop version 3.3.4

 

When executing Ls command and Stat command on the same hadoop file, I get two timestamps.

 
{code:java}
hdfs dfs -stat ""modify_time %y, access_time%x"" /path/to/file{code}
 returns:

modify_time {_}*2023-10-17 01:43:05*{_}, access_time _*2023-10-17 01:41:00*_ 

 
{code:java}
hdfs dfs -ls /path/to/file{code}
  returns:

{-}rw{-}rw-r–+     3    user_name     user_group     247400339     _*2023-10-17 09:43*_     /path/to/file

 

these two timestamps has the difference 8hours.

I am in China, the timezone is “UTC+8”， so the timestamp from LS command is correct and timestamp from STAT command is wrong.

 

!image-2023-10-26-10-07-11-637.png!

 "
S3AFileSystem URL encodes twice where Path has trailing /,13553548,Open,Major,,10/Oct/23 15:04,,,"As per the comment at [https://github.com/apache/hadoop/pull/1646#issuecomment-1754221384] ...

due to HADOOP-15430 a Path
{code:java}
s3://my.bucket/my folder/{code}
becomes
{code:java}
s3://my.bucket/my%20folder{code}
which upon HTTP request creation becomes
{code:java}
""GET
...&prefix=my%2520folder{code}
which is a wrong path."
Fix trademark description issue of Apache on website,13554529,Resolved,Major,Fixed,18/Oct/23 09:08,18/Oct/23 10:39,,As title said.
Race condition in ZKDelegationTokenSecretManager creating znode,13553033,Resolved,Major,Fixed,05/Oct/23 16:15,12/Oct/23 15:22,3.3.6,"When multiple nodes come up at the same time, there is a race condition in ZKDelegationTokenSecretManager since the exists and create check do not mean that the znode was created in the meantime. HADOOP-18452 tried to fix this but the issue still exists.

A better fix would be to catch the https://zookeeper.apache.org/doc/r3.9.0/apidocs/zookeeper-server/org/apache/zookeeper/KeeperException.NodeExistsException.html if the create fails when the znode already exists. This would eliminate the race condition.

{code:java}
236 ERROR (jetty-launcher-8-thread-1) [n:127.0.0.1:56203_solr] o.a.s.s.CoreContainerProvider Could not start Solr. Check solr/home property and the logs
          => java.lang.RuntimeException: Could not start class org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager: java.io.IOException: Could not create namespace
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.init(DelegationTokenManager.java:149)
java.lang.RuntimeException: Could not start class org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager: java.io.IOException: Could not create namespace
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.init(DelegationTokenManager.java:149) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.initTokenManager(DelegationTokenAuthenticationHandler.java:163) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.init(DelegationTokenAuthenticationHandler.java:131) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194) ~[hadoop-auth-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.initializeAuthHandler(DelegationTokenAuthenticationFilter.java:215) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.solr.security.hadoop.HadoopAuthFilter.initializeAuthHandler(HadoopAuthFilter.java:124) ~[main/:?]
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180) ~[hadoop-auth-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:181) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.solr.security.hadoop.HadoopAuthFilter.init(HadoopAuthFilter.java:75) ~[main/:?]
	at org.apache.solr.security.hadoop.HadoopAuthPlugin.init(HadoopAuthPlugin.java:135) ~[main/:?]
	at org.apache.solr.core.CoreContainer.initializeAuthenticationPlugin(CoreContainer.java:569) ~[solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.core.CoreContainer.reloadSecurityProperties(CoreContainer.java:1185) ~[solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.core.CoreContainer.loadInternal(CoreContainer.java:854) ~[solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:763) ~[solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.servlet.CoreContainerProvider.createCoreContainer(CoreContainerProvider.java:427) ~[solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.servlet.CoreContainerProvider.init(CoreContainerProvider.java:246) [solr-core-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.embedded.JettySolrRunner$1.lifeCycleStarted(JettySolrRunner.java:405) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.eclipse.jetty.util.component.AbstractLifeCycle.setStarted(AbstractLifeCycle.java:253) [jetty-util-10.0.16.jar:10.0.16]
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:94) [jetty-util-10.0.16.jar:10.0.16]
	at org.apache.solr.embedded.JettySolrRunner.retryOnPortBindFailure(JettySolrRunner.java:614) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.embedded.JettySolrRunner.start(JettySolrRunner.java:552) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.embedded.JettySolrRunner.start(JettySolrRunner.java:523) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.cloud.MiniSolrCloudCluster.startJettySolrRunner(MiniSolrCloudCluster.java:508) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at org.apache.solr.cloud.MiniSolrCloudCluster.lambda$new$0(MiniSolrCloudCluster.java:320) [solr-test-framework-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:294) [solr-solrj-10.0.0-SNAPSHOT.jar:10.0.0-SNAPSHOT a3945a2c3710b1a355abdea7a2e63b5353ad0723 [snapshot build, details omitted]]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: java.io.IOException: Could not create namespace
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.startThreads(ZKDelegationTokenSecretManager.java:275) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.init(DelegationTokenManager.java:146) ~[hadoop-common-3.3.6.jar:?]
	... 28 more
Caused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /solr/security/zkdtsm/ZKDTSMRoot
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:125) ~[zookeeper-3.9.0.jar:3.9.0]
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:53) ~[zookeeper-3.9.0.jar:3.9.0]
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1450) ~[zookeeper-3.9.0.jar:3.9.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl$18.call(CreateBuilderImpl.java:1223) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl$18.call(CreateBuilderImpl.java:1193) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[curator-client-5.2.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1190) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:605) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:595) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:573) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:461) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:391) ~[curator-framework-5.2.0.jar:5.2.0]
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.startThreads(ZKDelegationTokenSecretManager.java:272) ~[hadoop-common-3.3.6.jar:?]
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.init(DelegationTokenManager.java:146) ~[hadoop-common-3.3.6.jar:?]
	... 28 more
{code}
"
Add documentation for building hadoop with docker,13553822,Open,Major,,12/Oct/23 09:09,,,"We have documentation for Single Node setup & stuff, maybe good if we can add documentation to get started via docker:
https://hub.docker.com/r/apache/hadoop"
S3A: remove @deprecated tags where no longer needed,13558724,Open,Minor,,20/Nov/23 15:32,,3.4.0,"{{S3ClientFactory}} is tagged as deprecated ""to be replaced"" when we just changed it instead. probably the same elsewhere so scan for the tag and remove as appropriate"
s3a file rename does double HEAD or LIST on source file/dir,13557345,Open,Minor,,09/Nov/23 14:51,,3.3.6,"going to add this as a v2 issue, though really it's been latent.

when you do a rename() we do a getFileStatus call to get the list/file status..if this is a dir that's a single LIST; a file then it'll be LIST + HEAD.

when the actual copy is kicked off, 
* if the source is a dir, then a duplicate LIST is initiated straight afterwards.
* FIle: there's a second HEAD to get the metadata for the copy

Proposed.
* LIST initiates a full list, and if it returns a list of objects, that list iterator is passed in to rename.
* full result of HEAD preserved and passed down to copyFile()

will cut out one round trip regardless of source type

"
S3A Assume role tests failing against S3Express stores,13560669,Open,Minor,,05/Dec/23 14:45,,3.4.0,"The test suits which assume roles with restricted permissions down paths still fail on S3Express, even after disabling createSession.

This is with a role which *should* work.

Either the role setup is wrong, or there's something special about role configuration for S3Express buckets"
Unknown S3Express bucket raises UnknownHostException rather than NoSuchBucketException; will block for retries,13560240,Open,Minor,,01/Dec/23 15:24,,3.4.0,"When an attempt is made to work with an s3 express bucket which isn't there. the createSession API fails with UnknownHostException

This is actually retried within the sdk, so we are lucky that HADOOP-18889 cut the retry count down. Even so, failures are slow and not very informative.

"
"S3A third party: document ""Certificate doesn't match""",13555526,Open,Minor,,25/Oct/23 16:57,,3.3.6,"
A recurrent problems with third party stores is that the user gets an error message about HTTP certificates
{code}
Unable to execute HTTP request: Certificate for <mybucket.mystore.dev.net> doesn't match any of the subject alternative names: [*.dev.net]

{code}

This is happening because
# the store uses HTTPS and there is an organization certificate
# the store does support virtual hostname access -but it does not match the HTTPS wildcard

Fix: switch to path style access

*add this detail to the third party store doc*

"
ABFS contract-tests with Hadoop-Commons intermittently failing,13556197,Resolved,Minor,Fixed,31/Oct/23 08:13,20/Nov/24 15:37,,"In the merged pr [HADOOP-18869: [ABFS] Fixing Behavior of a File System APIs on root path by anujmodi2021 · Pull Request #6003 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/6003], a config was switched-on: `fs.contract.test.root-tests-enabled`. This enables the root manipulation tests for the filesystem contract.

Now, the execution of contract-tests in abfs works as per executionId integration-test-abfs-parallel-classes of the pom. The tests would work in different jvms, and at a given instance multiple such jvms could be there, depending on ${testsThreadCount}.  The problem is that all the test jvms for contract-test use the same container for test runs which is defined by `fs.contract.test.fs.abfs`. Due to this, one jvm root-contract-runs can influence other jvm's root-contract-runs. This leads to CI failures for hadoop-azure package.

Solution is to run these tests sequentially and separate from other commit/manifest tests."
FileSystem.getFileSystemClass() to log at debug the jar the .class came from,13553722,Resolved,Minor,Fixed,11/Oct/23 15:43,14/Jun/24 18:18,3.3.6,"we want to be able to log the jar the filesystem implementation class, so that we can identify which version of a module the class came from.

this is to help track down problems where different machines in the cluster or the .tar.gz bundle is out of date. "
 Improve UserGroupInformation debug log,13555931,Resolved,Minor,Fixed,29/Oct/23 01:50,14/May/24 19:05,3.3.0,"      Using “new Exception( )” to print the call stack of ""doAs Method "" in the UserGroupInformation class. Using this way will print meaningless Exception information and too many call stacks, This is not conducive to troubleshooting

*example:*

!20231029-122825.jpeg|width=991,height=548!

 

*improved result* :

 

!image-2023-10-29-09-47-56-489.png|width=1099,height=156!

!20231030-143525.jpeg|width=572,height=674!"
S3A credential provider remapping: make extensible,13558425,Resolved,Minor,Fixed,17/Nov/23 14:40,02/Feb/24 17:03,3.4.0,"A new option fs.s3a.aws.credentials.provider.mapping takes a key value pair for automatic mapping of v1 credential providers to v2 credential providers.


h2. Backporting

There's a followup PR to the main patch which *should* be applied, as it hardens the parser.

{code}
HADOOP-18980. Invalid inputs for getTrimmedStringCollectionSplitByEquals (ADDENDUM) (#6546)
{code}
"
Corrections to Hadoop FileSystem API Definition,13559023,Resolved,Minor,Fixed,22/Nov/23 12:18,02/Feb/24 11:54,3.3.6,"I noticed a lot of inconsistencies, typos and informal statements in the ""formal"" FileSystem API definition ([https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/index.html)]

Creating this ticket to link my PR against."
S3A: Add option fs.s3a.classloader.isolation (#6301),13559618,Resolved,Minor,Fixed,27/Nov/23 21:56,05/Feb/24 18:03,3.3.6,"In HADOOP-17372 the S3AFileSystem forces the configuration classloader to be the same as the one that loaded S3AFileSystem. This leads to the impossibility in Spark applications to load third party credentials providers as user jars.


The option fs.s3a.classloader.isolation (default: true) can be set to false to disable s3a classloader isolation;

This can assist in using custom credential providers and other extension points.


"
S3A. Add option fs.s3a.directory.operations.purge.uploads to purge on rename/delete,13555242,Resolved,Minor,Fixed,23/Oct/23 20:50,25/Oct/23 16:40,3.4.0,"On third-party stores without lifecycle rules its possible to accrue many GB of pending multipart uploads, including from
* magic committer jobs where spark driver/MR AM failed before commit/abort
* distcp jobs which timeout and get aborted
* any client code writing datasets which are interrupted before close.

Although there's a purge pending uploads option, that's dangerous because if any fs is instantiated with it, it can destroy in-flight work

otherwise, the ""hadoop s3guard uploads"" command does work but needs scheduling/manual execution

proposed: add a new property {{fs.s3a.directory.operations.purge.uploads}} which will automatically cancel all pending uploads under a path
* delete: everything under the dir
* rename: all under the source dir

This will be done in parallel to the normal operation, but no attempt to post abortMultipartUploads in different threads. The assumption here is that this is rare. And it'll be off by default as in AWS people should have rules for these things.


+ doc (third_party?)
+ add new counter/metric for abort operations, count and duration
+ test to include cost assertions


"
ITestS3GuardTool fails if SSE/DSSE encryption is used,13552605,Resolved,Minor,Fixed,02/Oct/23 06:45,27/Oct/23 20:37,3.3.6,"{code:java}
[ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 25.989 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardTool
[ERROR] testLandsatBucketRequireUnencrypted(org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardTool)  Time elapsed: 0.807 s  <<< ERROR!
46: Bucket s3a://landsat-pds: required encryption is none but actual encryption is DSSE-KMS
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.exitException(S3GuardTool.java:915)
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.badState(S3GuardTool.java:881)
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo.run(S3GuardTool.java:511)
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:283)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:963)
    at org.apache.hadoop.fs.s3a.s3guard.S3GuardToolTestHelper.runS3GuardCommand(S3GuardToolTestHelper.java:147)
    at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardToolTestBase.run(AbstractS3GuardToolTestBase.java:114)
    at org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardTool.testLandsatBucketRequireUnencrypted(ITestS3GuardTool.java:74)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.lang.Thread.run(Thread.java:750)
 {code}
Since landsat requires none encryption, the test should be skipped for any encryption algorithm."
Switch to SPDX identifier for license name,13553131,Resolved,Minor,Fixed,06/Oct/23 12:32,07/Oct/23 17:39,3.3.7,"[https://maven.apache.org/pom.html#Licenses]
""Using an [SPDX identifier|https://spdx.org/licenses/] as the license name is recommended.""

The Apache parent pom is already using this identifier"
S3A: testMultiObjectExceptionFilledIn() assertion error,13554602,Resolved,Minor,Fixed,18/Oct/23 15:36,20/Oct/23 09:13,3.4.0,"Failure in the new test of HADOOP-18939.

I've been fiddling with the sdk upgrade, and only merged HADOOP-18932 after submitting the new pr, so maybe, just maybe, the SDK changed some defaults.

anyway, 

{code}
[ERROR] testMultiObjectExceptionFilledIn(org.apache.hadoop.fs.s3a.impl.TestErrorTranslation)  Time elapsed: 0.026 s  <<< FAILURE!
java.lang.AssertionError: retry policy of MultiObjectException
        at org.junit.Assert.fail(Assert.java:89)
        at org.junit.Assert.assertTrue(Assert.java:42)
        at 
{code}

easily fixed"
S3A:  AbstractS3ACostTest to clear bucket fs.s3a.create.performance flag,13557522,Resolved,Minor,Fixed,10/Nov/23 17:07,21/Nov/23 14:59,3.4.0,"If there's a bucket-specific  fs.s3a.create.performance flag then the create tests can fail as the costs are lower than expected. 

trivial fix: add to the removeBaseAndBucketOverrides list"
S3A: Add option fs.s3a.s3express.create.session to enable/disable CreateSession,13560104,Resolved,Minor,Fixed,30/Nov/23 17:03,07/Dec/23 13:09,3.4.0,"add a way to disable the need to use the createsession call, so as to allow for

* simplifying our role test runs
* benchmarking the performance hit
* troubleshooting IAM permissions



this can also be disabled from the sysprop ""aws.disableS3ExpressAuth"""
Add documentation related to NodeFencer,13553397,Resolved,Minor,Fixed,09/Oct/23 12:24,13/Oct/23 22:35,3.3.4,"In the NodeFencer file, some important comments are missing.
Happens here:
 !screenshot-1.png! 

The guidance instructions for ShellCommandFencer are missing here.
If it is improved, the robustness of the distributed system can be increased."
Modify HBase version in BUILDING.txt,13554393,Resolved,Minor,Fixed,17/Oct/23 10:37,20/Oct/23 07:20,3.4.0,"In current BUILDING.txt document, the version of HBase, which is used by YARN Timeline Service V2 is a bit older than the actual one. Hence, I hereby request to modify this uncertain description in the document."
"AWS SDK v2: add path capability probe ""fs.s3a.capability.aws.v2""",13555639,Resolved,Minor,Fixed,26/Oct/23 10:13,05/Dec/23 18:25,3.4.0,"Add a ""hasPathCapability()"" probe for s3a v2 builds to aid diagnostics -avoids needing to look for specific s3a files on classpath. 

plus bucket-info to enum all capabilities which may be present"
Fix typos in .gitignore,13556689,Resolved,Minor,Fixed,03/Nov/23 12:35,03/Nov/23 23:43,3.3.6,"DS_Store is auto generated by Mac in every opened folder, which is useless but annoying. Not only DS_Store file in the repository root directory should be ignored but DS_Store file in its subfolders.

 "
Allow no-downtime migration of HDFS clusters into secure mode,13557384,Patch Available,Minor,,09/Nov/23 20:13,,,"My employer (HubSpot) recently completed transitioning all of the Hadoop clusters underlying our HBase databases into secure mode. It was important to us that we be able to make this change without impacting the functionality of our SaaS product. To accomplish this, we added some new settings to our fork of Hadoop, and fixed a latent bug (HADOOP-18972). This ticket is my intention to contribute these changes back to the mainline code, so others can benefit. A patch will be incoming.

It was only necessary to change the HDFS code, because other Hadoop components are already able to seamlessly switch into secure mode.

The basic theme of the new functionality is the ability to accept incoming secure connections without requiring them or making them outgoing. Secure mode enablement will then be done in two stages.
 * First, all nodes are given configuration to accept secure connections, and are gracefully rolling-restarted to adopt this new functionality. I'll be adding the new settings to make this stage possible.
 * Second, all nodes are told to require incoming connections be secure, and to make secure outgoing connections, and the settings added in the first stage are removed. Nodes are again rolling-restarted to adopt this functionality. The settings in this final state will look the same as in any secure Hadoop cluster today.

I'll include documentation changes explaining how to do this."
"""Reversed (or previously applied) patch detected"" for hadoop-common-project/hadoop-common/dev-support/jdiff-workaround.patch ",13558758,Open,Minor,,20/Nov/23 22:00,,3.4.0,"tried to compile under Hadoop-common-project and noticed the following. 

trunk: commit f609460bda0c2bd87dd3580158e549e2f34f14d5
{code:java}
$ mvn clean package -Pdist,docs -DskipTests -Dtar
patching file 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java'
Reversed (or previously applied) patch detected!  Assume -R? [y]
1 out of 2 hunks failed--saving rejects to 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java.rej'
patching file 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java'
Reversed (or previously applied) patch detected!  Assume -R? [y]
patching file 'hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestGangliaMetrics.java{code}
also reproducible by 
{code:java}
xinglin@xinglin-mn2 ~/p/h/trunk (trunk)> patch -p1 < hadoop-common-project/hadoop-common/dev-support/jdiff-workaround.patch
patching file 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java'
Reversed (or previously applied) patch detected!  Assume -R? [y]
1 out of 2 hunks failed--saving rejects to 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java.rej'
patching file 'hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java'
Reversed (or previously applied) patch detected!  Assume -R? [y]
patching file 'hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestGangliaMetrics.java'{code}
Build succeeded but the jdiff-workaround patch probably needs a fix."
not a problem,13560176,Resolved,Minor,Abandoned,01/Dec/23 08:31,01/Dec/23 09:42,,
SingleCluster.html instructions are ordered incorrectly,13559559,Open,Minor,,27/Nov/23 12:32,,3.3.6,"I was going over the pesudo distributed cluster instructions at 

https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation

The smoke test instructions direct you to execute a mapreduce job

{noformat}
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
{noformat}

but at this point Yarn hasn't been started, the instructions for that are in the next section.

Switch the YARN setup section and the smoke test section."
Bug in SaslPropertiesResolver allows mutation of internal state,13557783,Patch Available,Minor,,13/Nov/23 19:59,,,"When {{SaslDataTransferServer}} or {{SaslDataTranferClient}} want to get a SASL properties map to do a handshake, they call {{SaslPropertiesResolver#getServerProperties()}} or {{SaslPropertiesResolver#getClientProperties()}}, and they get back a {{Map<String, String>}}. Every call gets the same {{Map}} object back, and then the callers sometimes call [put()|https://github.com/apache/hadoop/blob/rel/release-3.3.6/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java#L385] on it. This means that future users of {{SaslPropertiesResolver}} get back the wrong information.

I propose that {{SaslPropertiesResolver}} should pass a copy of its internal map, so that users can safety modify them.

I discovered this problem in my company's testing environment as we began to enable {{dfs.data.transfer.protection}} on our DataNodes, while our NameNodes were using {{IngressPortBasedResolver}} to give out block tokens with different QOPs depending on the port used. Then our HDFS client applications became unable to read or write to HDFS because they could not find a QOP in common with the DataNodes during SASL handshake. With multiple threads executing SASL handshakes at the same time, the properties map used in {{SaslDataTransferServer}} in a DataNode could be clobbered during usage, since the same map was used by all threads. Also, future clients that do not have a QOP embedded in their block tokens would connect to a server with the wrong SASL properties map. I think that one or both of these issues explains the problem that I saw. I eliminated this unsafety and saw the problem go away."
Clean yum cache after installing which in hadoop2 docker image,13557942,Resolved,Minor,Done,14/Nov/23 17:19,15/Nov/23 08:28,,Minor addendum to HADOOP-18970: clean yum cache after installing {{which}} to reduce image size by ~230MB.
s3a openfile is using readahead rather than async drain threshold to calculate drain threshold,13554536,Open,Minor,,18/Oct/23 09:59,,3.3.6,"we are using the wrong default value for the async drain threshold in   {{org.apache.hadoop.fs.s3a.impl.OpenFileSupport}}


{code}
       builderSupport.getPositiveLong(ASYNC_DRAIN_THRESHOLD,
                defaultReadAhead))

{code}

this means the drain threshold isn't being taken up in random io unless you have a very small readahead"
Add journalnode maintenance node list,13553994,Resolved,Trivial,Abandoned,13/Oct/23 09:19,13/Oct/23 09:23,3.3.6,"* In the case of configuring 3 journal nodes in HDFS, if only 2 journal nodes are available and 1 journal node fails to start due to machine issues, it will result in a long initialization time for the namenode (around 30-40 minutes, depending on the IPC timeout and retry policy configuration). 
* The failed journal node cannot recover immediately, but HDFS can still function in this situation. In our production environment, we encountered this issue and had to reduce the IPC timeout and adjust the retry policy to accelerate the namenode initialization and provide services. 
* I'm wondering if it would be possible to have a journal node maintenance list to speed up the namenode initialization knowing that one journal node cannot provide services in advance?"
Unrecognized SSL message error in LDAPGroupMappings,13599061,Open,Critical,,18/Nov/24 03:11,,3.4.1,"h3. What Happened: 

Got an unrecognized SSL message error instead of the expected LDAP response read timeout when hadoop.security.group.mapping.ldap.ssl is set to true. 
h3. Buggy Code: 

 
{code:java}
try (ServerSocket serverSock = new ServerSocket(0)) { // -> ServerSocket is not configured to accept SSL communication.
  final CountDownLatch finLatch = new CountDownLatch(1);

  final Thread ldapServer = new Thread(new Runnable() {
    @Override
    public void run() {
      try {
        try (Socket clientSock = serverSock.accept()) {
          IOUtils.skipFully(clientSock.getInputStream(), 1);
          clientSock.getOutputStream().write(AUTHENTICATE_SUCCESS_MSG);
          finLatch.await();
        }
      } catch (Exception e) {
        e.printStackTrace();
      }
    }
  });
  ldapServer.start(); {code}
 
h3. Stack Trace: 

 
{code:java}
Expected to find 'LDAP response read timed out, timeout used' but got unexpected exception: javax.naming.CommunicationException: localhost:36143 [Root exception is javax.net.ssl.SSLException: Unsupported or unrecognized SSL message]
        at java.naming/com.sun.jndi.ldap.Connection.<init>(Connection.java:250)
        at java.naming/com.sun.jndi.ldap.LdapClient.<init>(LdapClient.java:137)
        at java.naming/com.sun.jndi.ldap.LdapClient.getInstance(LdapClient.java:1616)
        at java.naming/com.sun.jndi.ldap.LdapCtx.connect(LdapCtx.java:2847)
        at java.naming/com.sun.jndi.ldap.LdapCtx.<init>(LdapCtx.java:348)
        at java.naming/com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxFromUrl(LdapCtxFactory.java:266)
        at java.naming/com.sun.jndi.ldap.LdapCtxFactory.getUsingURL(LdapCtxFactory.java:226)
        at java.naming/com.sun.jndi.ldap.LdapCtxFactory.getUsingURLs(LdapCtxFactory.java:284)
        at java.naming/com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxInstance(LdapCtxFactory.java:185)
        at java.naming/com.sun.jndi.ldap.LdapCtxFactory.getInitialContext(LdapCtxFactory.java:115)
        at java.naming/javax.naming.spi.NamingManager.getInitialContext(NamingManager.java:730)
        at java.naming/javax.naming.InitialContext.getDefaultInitCtx(InitialContext.java:305)
        at java.naming/javax.naming.InitialContext.init(InitialContext.java:236)
        at java.naming/javax.naming.ldap.InitialLdapContext.<init>(InitialLdapContext.java:154)
        at org.apache.hadoop.security.TestLdapGroupsMappingBase$DummyLdapCtxFactory.getInitialContext(TestLdapGroupsMappingBase.java:241)
{code}
h3. How to Reproduce: 

(1) Set hadoop.security.group.mapping.ldap.ssl to true 

(2) Run test: org.apache.hadoop.security.TestLdapGroupsMapping#testLdapReadTimeout
h3. Notes: 

I don't have a patch/fix for this yet, I am working on it. 

 "
Null Pointer Exception in KeyProviderCryptoExtension due to Class Not Found Exception,13599024,Open,Critical,,16/Nov/24 23:29,,3.4.0,"h3. What Happened: 

A null pointer exception occurs in KeyProviderExtension when trying to close a null CryptoCodec object. If supplied with an invalid class name for hadoop.security.crypto.codec.classes.aes.ctr.nopadding getClassbyName throws a ClassNotFound exception and consequently the CryptoCodec object is not created. 
h3. Buggy Code: 
{code:java}
CryptoCodec cc = CryptoCodec.getInstance(keyProvider.getConf()); // -> this does not initialize cc due to a ClassNotFound exception. 
try {
  final byte[] newKey = new byte[encryptionKey.getMaterial().length];
  cc.generateSecureRandom(newKey);
  final byte[] iv = new byte[cc.getCipherSuite().getAlgorithmBlockSize()];
  cc.generateSecureRandom(iv);
  Encryptor encryptor = cc.createEncryptor();
  return generateEncryptedKey(encryptor, encryptionKey, newKey, iv);
} finally {
  cc.close(); // -> this throws a NPE as cc is null
} {code}
h3. Stack Trace: 
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:303)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:513)
        at org.apache.hadoop.crypto.key.TestKeyProviderCryptoExtension.testReencryptEncryptedKeys(TestKeyProviderCryptoExtension.java:229)
 {code}
h3. How to Reproduce: 

(1) Set hadoop.security.crypto.codec.classes.aes.ctr.nopadding to org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec/

(2) Run test: org.apache.hadoop.crypto.key.TestKeyProviderCryptoExtension#testReencryptEncryptedKeys

 "
FastSaslClientFactory failing to initialise due to NPE,13595054,Open,Critical,,11/Oct/24 08:44,,3.3.6,"FastSaslClientFactory is instantiated in both SaslRpcClient and SaslParticipant as new FastSaslClientFactory(null)

In its constructor FastSaslClientFactory loads all the SaslFactories using Sasl.getSaslClientFactories();

Then it iterates through all the loaded factories and calls getMechanismNames method.

The null argument sent in the constructor is sent to the getMechanismNames call.

In some-cases, a loaded factory might not handle this null argument and might throw a NPE.

This can cause the entire FastSaslClientFactory to not instantiate leading to failure while instantiation of SaslRpcClient.

Code Pointer - [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/FastSaslClientFactory.java#L43]

Stack Trace - 
{code:java}
Cause3: java.lang.ExceptionInInitializerError: Exception java.lang.NullPointerException [in thread ""ServiceScheduler:GridforceSfdcLogUploaderProcess""] Cause3-StackTrace:   at org.apache.qpid.client.security.amqplain.AmqPlainSaslClientFactory.getMechanismNames(AmqPlainSaslClientFactory.java:50)  at org.apache.hadoop.security.FastSaslClientFactory.<init>(FastSaslClientFactory.java:47)  at org.apache.hadoop.security.SaslRpcClient.<clinit>(SaslRpcClient.java:106)  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:559)  at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:347)  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:783)  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:779)  at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)  at java.base/javax.security.auth.Subject.doAs(Subject.java:439)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)  at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)  at org.apache.hadoop.ipc.Client.call(Client.java:1457)  at org.apache.hadoop.ipc.Client.call(Client.java:1410)  at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)  at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)  at jdk.proxy2/jdk.proxy2.$Proxy1500.getFileInfo(Unknown Source)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:811)  at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:811)  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)  ... 23 shared with parent` {code}"
Fix ProcessEnvironment ClassCastException in Shell.java,13594866,Resolved,Major,Fixed,10/Oct/24 07:09,03/Mar/25 22:34,3.4.0,"We tried to upgrade Hadoop version from 3.6.6 to 3.4.0 in Apache Hive HIVE-28191. But found exception:
{code:java}
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.ProcessEnvironment$Variable
    at java.lang.ProcessEnvironment$StringEnvironment.toEnvironmentBlock(ProcessEnvironment.java:273) ~[?:1.8.0_221]
    at java.lang.ProcessEnvironment.toEnvironmentBlock(ProcessEnvironment.java:298) ~[?:1.8.0_221]
    at java.lang.ProcessImpl.start(ProcessImpl.java:86) ~[?:1.8.0_221]
    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ~[?:1.8.0_221]
    at org.apache.hadoop.util.Shell.runCommand(Shell.java:998) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.util.Shell.run(Shell.java:959) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1282) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.util.Shell.execCommand(Shell.java:1377) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.util.Shell.execCommand(Shell.java:1359) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1535) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNonNativeIO(RawLocalFileSystem.java:1000) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:991) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:952) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hadoop.hive.ql.exec.Utilities.ensurePathIsWritable(Utilities.java:4954) ~[classes/:?]
    at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:843) ~[classes/:?]
    at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:784) ~[classes/:?]
    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:708) ~[classes/:?]
    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:669) ~[classes/:?]
    at org.apache.hive.service.cli.session.HiveSessionImpl.open(HiveSessionImpl.java:182) ~[classes/:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_221]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_221]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
    at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[classes/:?]
    at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) ~[classes/:?]
    at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[classes/:?]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_221]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_221]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953) ~[hadoop-common-3.4.0.jar:?]
    at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[classes/:?]
    at com.sun.proxy.$Proxy58.open(Unknown Source) ~[?:?]
    at org.apache.hive.service.cli.session.SessionManager.createSession(SessionManager.java:485) ~[classes/:?]
    ... 40 more  {code}
 

After some debugging, i found the  failed  hive tests {{TestRemoteHiveMetastoreWithHttpJwt#testValidJWT}} and {{TestHttpJwtAuthentication#testAuthorizedUser}} are related Hadoop3.4.0 change [HADOOP-17009: Embrace Immutability of Java Collections hadoop#1974|https://github.com/apache/hadoop/pull/1974]
[https://github.com/apache/hadoop/pull/1974/files#diff-372a0d25bcccd88b409a8149949628abd7d3472a1798bebe813e7617b0ef73c7L918-L920]"
VectorIO API to support releasing buffers on failure,13594610,Open,Major,,08/Oct/24 09:31,,3.4.1,"extend for vector IO API with a method that takes a ByteBufferPool implementation rather than just an allocator. This allows for buffers to be returned to the pool when problems occur, before throwing an exception.

The Parquet API is already designed for this"
S3A: Add initial support for analytics-accelerator-s3,13600070,Open,Major,,26/Nov/24 15:31,,3.4.2,"S3 recently released [Analytics Accelerator Library for Amazon S3|https://github.com/awslabs/analytics-accelerator-s3] as an Alpha release, which is an input stream, with an initial goal of improving performance for Apache Spark workloads on Parquet datasets. 

For example, it implements optimisations such as footer prefetching, and so avoids the multiple GETS S3AInputStream currently makes for the footer bytes and PageIndex structures.

The library also tracks columns currently being read by a query using the parquet metadata, and then prefetches these bytes when parquet files with the same schema are opened. 

This ticket tracks the work required for the basic initial integration. There is still more work to be done, such as VectoredIO support etc, which we will identify and follow up with. "
S3A: InputStreams to be created by factory under S3AStore,13600740,Resolved,Major,Fixed,03/Dec/24 14:11,20/Feb/25 10:32,3.4.2,"Migrate S3AInputStream creation into a factory pattern, push down into S3AStore.

Proposed factories
* default: whatever this release has as default
* classic: current S3AInputStream
* prefetch: prefetching
* analytics: new analytics stream
* other: reads a classname from another prop, instantiates.

Also proposed
* stream to implement some stream capability to declare what they are (classic, prefetch, analytics, other). 

h2. Implementation

All callbacks used by the stream also to call directly onto S3AStore.
S3AFileSystem must not be invoked at all (if it is needed: PR is still not ready).
Some interface from Instrumentation will be passed to factory; this shall include a way to create new per-stream 
The factory shall implement org.apache.hadoop.service.Service; S3AStore shall do same and become a subclass of CompositeService. It shall attach the factory as a child, so they can follow the same lifecycle. We shall do the same for anything else that gets pushed down.

Everything related to stream creation must go from s3afs; and creation of the factory itself. This must be done in S3AStore.initialize(). 

As usual, this will complicate mocking. But the streams themselves should not require changes, at least significant ones.

Testing.
* The huge file tests should be tuned so each of the different ones uses a different stream, always.
* use a -Dstream=""factory name"" to choose factory, rather than the -Dprefetch
* if not set, whatever is in auth-keys gets picked up.
"
[ABFS] Implement Backoff and Read Footer metrics using IOStatistics Class,13595690,Resolved,Major,Fixed,16/Oct/24 14:12,25/Feb/25 04:24,3.4.1,"Current Flow: We have implemented metrics collection in ABFS flow. We have created a custom AbfsBackoffMetrics and AbfsReadFooterMetrics class which stores all the metrics on the file system level. Our objective is to move away from the custom class implementation and use IOStatisticsStore to store the metrics which is present in hadoop-common.

Changes Made: This PR contains the changes related to storing metrics related to above mentioned classes in IOStatisticStore which is present in hadoop-common. AbstractAbfsStatisticsSource abstract class is created which is implementing IOStatisticsSource interface. This will store IOStatistics of the child metrics class.

Both AbfsBackoffMetrics and AbfsReadFooterMetrics is inheriting AbstractAbfsStatisticsSource and store the respective metrics in IOStatisticsStore."
S3A: Test failures after CSE support added,13598919,Resolved,Major,Fixed,15/Nov/24 13:04,22/Nov/24 14:00,3.5.0,"Testing hadoop trunk with CSE-KMS configured I get

* 400 error without region set (KMS providing nothing helpful). Proposed: move troubleshooting into encryption.md, cover 400 and this as a possible cuse

* test failures
{code}
[ERROR]   ITestS3AClientSideEncryptionKms>ITestS3AClientSideEncryption.testSizeOfEncryptedObjectFromHeaderWithV1Compatibility:345->ITestS3AClientSideEncryption.assertFileLength:447 [Length of s3a://stevel-london/job-00-fork-0009/test/testSizeOfEncryptedObjectFromHeaderWithV1Compatibility/file status: S3AFileStatus{path=s3a://stevel-london/job-00-fork-0009/test/testSizeOfEncryptedObjectFromHeaderWithV1Compatibility/file; isDirectory=false; length=1024; replication=1; blocksize=33554432; modification_time=1731674289000; access_time=0; owner=stevel; group=stevel; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=""0f343b0931126a20f133d67c2b018a3b"" versionId=JyA1I_OW8osQTS3zWdn_Z0qlQYqBZ_7.] expected:<10[]L> but was:<10[24]L>
[ERROR]   ITestAwsSdkWorkarounds.testNoisyLogging:99 [LOG output does not contain the forbidden text. Has the SDK been fixed?] 
Expecting:
 <"""">
to contain:
 <""The provided S3AsyncClient is an instance of MultipartS3AsyncClient""> 
[ERROR] Errors: 
[ERROR]   ITestUploadRecovery.testCommitOperations:234 » AWSClientIO upload part #1 uplo...
[ERROR]   ITestUploadRecovery.testMagicWriteRecovery[array-commit-true] » AWSClientIO up...
[ERROR]   ITestUploadRecovery.testMagicWriteRecovery[bytebuffer-commit-false] » AWSClientIO
[ERROR]   ITestUploadRecovery.testMagicWriteRecovery[disk-commit-false] » AWSClientIO up...
{code}
"
S3A : Improve Client Side Encryption Documentation,13600152,Resolved,Major,Fixed,27/Nov/24 10:04,04/Dec/24 10:29,3.5.0,"Goal
 # Improve the documentation by adding more details
 # Fix the wrong configuration in the doc"
Hadoop OSS Connector adds support for V4 signatures.,13600684,Resolved,Major,Fixed,03/Dec/24 07:51,18/Feb/25 06:19,,"AliyunOSS is about to adjust its security policy: only V4 signature requests will be supported in the public cloud. Therefore, support for V4 signatures is also required in Hadoop, and V4 signatures will be the default."
Organize JDK version-specific code in IDEA friendly approach,13603089,Resolved,Major,Fixed,24/Dec/24 05:41,13/Feb/25 20:10,,
S3A Analytics-Accelerator: Add IoStatistics support,13602022,Open,Major,,13/Dec/24 10:57,,,"S3A provides InputStream statistics: [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/statistics/S3AInputStreamStatistics.java]

This helps track things like how many bytes were read from a stream etc. 

 

The current integration does not currently implement statistics. To start off with we should identify which of these statistics makes sense for us track in the new stream. Some examples are:

 

1/ bytesRead

2/ readOperationStarted

3/ initiateGetRequest

 

Some of these (1 and 2) are more straightforward, and should not require any changes to analytics-accelerator-s3, but tracking GET requests will require this. 

We should also add tests that make assertions on these statistics. See 
ITestS3APrefetchingInputStream for an example to do this. 

And see https://issues.apache.org/jira/browse/HADOOP-18190 for how this was done on the prefetching stream, and PR: https://github.com/apache/hadoop/pull/4458"
Über-jira: S3A Hadoop 3.4.2 features,13600714,Open,Major,,03/Dec/24 11:29,,3.4.1,Über-jira for stuff we want into 3.4.2 for s3a connector
Bump avro from 1.9.2 to 1.11.4,13596349,Resolved,Major,Fixed,22/Oct/24 15:21,11/Nov/24 15:49,3.4.0,"

* All field access is now via setter/getter methods
* To use Avro to marshal Serializable objects,
  the packages they are in must be declared in the system property {{org.apache.avro.SERIALIZABLE_PACKAGES}}
  
This is required to address
* [CVE-2024-47561|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-47561]
* [CVE-2023-39410|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-39410]

This change is not backwards compatible.

"
AliyunOSS ：Optimize the process by reducing some extra QPS.,13598309,Open,Major,,10/Nov/24 10:14,,,"AliyunOSSConnector has not been updated for a long time. It implements basic functionality,  but its performance is not optimal, especially in terms of QPS requests. It generates a  significant number of invalid QPS, which increases the cost on the storage service side and  also reduces the efficiency on the connector side. "
Support analytics-accelerator-s3 input streams for parquet read performance,13601998,Open,Major,,13/Dec/24 08:11,,3.5.0,"S3 recently released [Analytics Accelerator Library for Amazon S3|https://github.com/awslabs/analytics-accelerator-s3] as an Alpha release, which is an input stream, with an initial goal of improving performance for Apache Spark workloads on Parquet datasets. 

 

This tracks stabilisation work of this integration into S3A."
[JDK17] Remove usage of sun.misc.Signal,13598048,Open,Major,,07/Nov/24 12:11,,,"when we build hadoop in JDK11 runtime based on [https://github.com/apache/hadoop/pull/7085|https://github.com/apache/hadoop/pull/7085,] JDK11 build is failing as flowing due to: {{package sun.misc does not exist}} in.

 

This is because that package sun.misc is not supported after jdk11.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-common: Compilation failure: Compilation failure: 
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[22,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[23,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[44,43] cannot find symbol
[ERROR]   symbol:   class SignalHandler
[ERROR]   location: class org.apache.hadoop.util.SignalLogger
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[46,19] cannot find symbol
[ERROR]   symbol:   class SignalHandler
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[59,24] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[26,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[27,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[42,42] cannot find symbol
[ERROR]   symbol: class SignalHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[71,11] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[126,22] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[48,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[27,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[134,20] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.FastByteComparisons.LexicographicalComparerHolder.UnsafeComparer
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[50,39] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[50,21] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[58,5] method does not override or implement a method from a supertype
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[92,20] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[93,7] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[113,5] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[125,3] method does not override or implement a method from a supertype
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[901,17] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[903,7] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[903,24] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[140,22] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.FastByteComparisons.LexicographicalComparerHolder.UnsafeComparer
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[145,29] cannot find symbol
[ERROR]   symbol: class Unsafe
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-common
{code}
related jira: https://issues.apache.org/jira/browse/HADOOP-19298

 

 

 "
Remove usage of sun.misc.Unsafe,13598720,Open,Major,,14/Nov/24 06:23,,,"when we build hadoop in JDK11 runtime based on [HADOOP-19298|https://github.com/apache/hadoop/pull/7085] , JDK11 build is failing as flowing due to: {{package sun.misc does not exist}} in.

 

This is because that package sun.misc is not supported after jdk11.

 
The purpose of this JIRA is to replace the .misc.Unsafe.

related java class:
{code:java}
NativeIO.java
FastByteComparisons.java
AbstractFuture.java
ShortCircuitShm.java {code}
 
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project hadoop-common: Compilation failure: Compilation failure: 
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[22,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[23,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[44,43] cannot find symbol
[ERROR]   symbol:   class SignalHandler
[ERROR]   location: class org.apache.hadoop.util.SignalLogger
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[46,19] cannot find symbol
[ERROR]   symbol:   class SignalHandler
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[59,24] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[26,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[27,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[42,42] cannot find symbol
[ERROR]   symbol: class SignalHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[71,11] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[126,22] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[48,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[27,16] package sun.misc does not exist
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[134,20] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.FastByteComparisons.LexicographicalComparerHolder.UnsafeComparer
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[50,39] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[50,21] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.util.SignalLogger.Handler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java:[58,5] method does not override or implement a method from a supertype
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[92,20] cannot find symbol
[ERROR]   symbol:   class Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[93,7] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[113,5] cannot find symbol
[ERROR]   symbol:   variable Signal
[ERROR]   location: class org.apache.hadoop.service.launcher.IrqHandler
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java:[125,3] method does not override or implement a method from a supertype
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[901,17] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[903,7] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[903,24] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.nativeio.NativeIO
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[140,22] cannot find symbol
[ERROR]   symbol:   class Unsafe
[ERROR]   location: class org.apache.hadoop.io.FastByteComparisons.LexicographicalComparerHolder.UnsafeComparer
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java:[145,29] cannot find symbol
[ERROR]   symbol: class Unsafe
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-common {code}"
S3A Analytics-Accelerator: Move input stream creation to S3AStore,13602335,Reopened,Major,,17/Dec/24 01:54,,,https://issues.apache.org/jira/browse/HADOOP-19354 tracks work to move InputStream creation to S3AStore. Analytics accelerator must be created here too. 
S3A: Add LeakReporter; use in S3AInputStream,13598074,Resolved,Major,Fixed,07/Nov/24 15:44,15/Nov/24 11:57,3.4.1,"A recurring problem is that applications forget to close their input streams; eventually the HTTP connection runs out.

Having the finalizer close streams during GC will ensure that after a GC the http connections are returned. While this is an improvement on today, it is insufficient
* only happens during GC, so may not fix problem entirely
* doesn't let developers know things are going wrong.
* doesn't let us differentiate well between stream leak and overloaded FS

proposed enhancements then
* collect stack trace in constructor
* log in finalize at warn including path, thread and stack
* have special log for this, so it can be turned off in production (libraries telling end users off for developer errors is simply an annoyance)

h2. Leak Reporting

* the log for  leak reporting is {{org.apache.hadoop.fs.resource.leaks}}
* An error message is reported at WARN, including the file name.
* A stack trace of where the stream was created is reported
  at INFO.
* A best-effort attempt is made to release any active HTTPS
  connection.
* The filesystem IOStatistic stream_leaks is incremented.

The intent is to make it easier to identify where streams
are being opened and not closed -as these consume resources
including often HTTPS connections from the connection pool
of limited size.

It MUST NOT be relied on as a way to clean up open
files/streams automatically; some of the normal actions of
the close() method are omitted.

"
Bump netty to 4.1.116 due to CVE-2024-47535,13598749,Resolved,Major,Fixed,14/Nov/24 12:27,25/Dec/24 20:00,,https://nvd.nist.gov/vuln/detail/CVE-2024-47535
Update the year to 2025,13603492,Resolved,Major,Fixed,31/Dec/24 18:55,01/Jan/25 15:27,,Update the year to 2025
S3A: terasort tests fail with CSE-kMS enabled and london region With Delegation Token Secrets,13603445,Open,Major,,30/Dec/24 17:23,,3.4.2,"terasort test runs failing; job logs show the process yarn spawns is trying to use the default region provider list
{code}
 Unable to load region from any of the providers in the chain software.amazon.awssdk.regions.providers.DefaultAwsRegionProviderChain
{code}

these are spawned processes; all login info and encryption secrets are passed in the marshalled delegation token {{AbstractS3ATokenIdentifier}}, with the encryption stuff in {{EncryptionSecrets}}

This is probably going to need to be extended with information about the KMS region, with this info extracted during instantiation.
"
JDK 17 - java.lang.NoSuchFieldException: modifiers error in hadoop-hdfs and hadoop-yarn module,13602511,Resolved,Major,Duplicate,18/Dec/24 05:20,23/Dec/24 03:36,,"On Compilation of Hadoop over JDK17 getting *java.lang.NoSuchFieldException: modifiers* in following classes : 


1. TestStoragePolicyPermissionSettings

2. TestFileCreation

3. TestAsyncDispatcher "
Install OpenJDk 17 in default ubuntu build container,13602216,Resolved,Major,Fixed,16/Dec/24 07:03,25/Dec/24 08:39,3.4.1,
Add native support for GCS connector,13599350,Open,Major,,20/Nov/24 09:15,,3.5.0,
RPC metrics should be updated correctly when call is defered.,13601582,Resolved,Major,Done,10/Dec/24 15:30,20/Dec/24 02:12,,
Fix setting final field value on Java 17,13602276,Resolved,Major,Fixed,16/Dec/24 14:16,19/Dec/24 00:53,,
"AliyunOSS: When using the OSS Connector, special circumstances can lead to deadlocks in IO threads, causing computation tasks to become stuck",13602523,Open,Major,,18/Dec/24 07:04,,3.3.6,"When using {{AliyunOSSBlockOutputStream}} to read data, the threads waiting to read data and the threads accessing OSS to fetch data are coordinated using a condition lock. We have observed that under certain special circumstances, such as incorrect environment configurations or misconfigurations of third-party dependencies, the threads responsible for fetching data can abnormally terminate. After termination, these threads do not release the locks they hold, leading to indefinite waiting by other threads. From the perspective of the computation engine, this tasks being stalled (but not failing).

 

!https://aone.alibaba-inc.com/v2/api/workitem/adapter/file/url?fileIdentifier=workitem%2Falibaba%2F1026733%2F1733219963897image.png!

We believe that in such situations, it is best for the computation engine to detect the error and exit accordingly, rather than remain indefinitely stuck."
S3A Analytics-Accelerator: Update SDK client creation code ,13602334,Open,Major,,17/Dec/24 01:41,,,"Currently we initialise a new CRT client when analytics-accelerator is enabled. This is done in S3AFileSystem, and is also not configured with S3A configs as used by other clients. 

We should move this creation to S3AStoreImpl, and also use suitable hadoop configs where it makes sense to do so.

Also to use the S3AsynClient, we have temporarily done this.isMultipartUploadEnabled = false, as without this you cannot make ranged GETs. Believe this issue may have been resolved in the later versions of the SDK. We will need to revisit this code."
S3A Analytics-Accelerator: Add audit header support ,13602024,Open,Major,,13/Dec/24 11:12,,,"S3A adds audit information as referrer headers, see [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/auditing.md] for documentation on this. 

These are attached using execution interceptors, see ActiveAuditManagerS3A.
createExecutionInterceptors(). 
 
analyitcs-accelerator-s3 makes the GET requests now but does not update the referrer with GETs. 
 
See LoggingAuditor.attachRangeFromRequest() for how this is done. 
 
When using analytics-accelerator-s3, audit headers from S3A should be sent through. "
Accelerate token negotiation for other similar mechanisms.,13601095,Open,Major,,06/Dec/24 01:46,,,"HADOOP-19227 changed ipc.Server to accelerate token negotiation only for the default mechanism.

In this JIRA, we change to support other similar mechanisms."
Disable releases for apache.snapshots repo,13601391,Resolved,Major,Fixed,09/Dec/24 14:21,16/Dec/24 15:01,,"{code:title=https://github.com/apache/hadoop/blob/aa5fe6f550c8971762c02c292240a7529001e1d8/pom.xml#L54-L59}
  <repositories>
    <repository>
      <id>${distMgmtSnapshotsId}</id>
      <name>${distMgmtSnapshotsName}</name>
      <url>${distMgmtSnapshotsUrl}</url>
    </repository>
{code}

The Apache snapshot repository should be enabled only for snapshots."
OutofBounds Exception due to assumption about buffer size in BlockCompressorStream,13599057,Resolved,Major,Fixed,18/Nov/24 01:53,08/Dec/24 10:44,3.4.1,"h3. What Happened: 

Got an OutofBounds exception when io.compression.codec.snappy.buffersize is set to 7. BlockCompressorStream assumes that the buffer size will always be greater than the compression overhead, and consequently MAX_INPUT_SIZE will always be greater than or equal to 0. 
h3. Buggy Code: 

When io.compression.codec.snappy.buffersize is set to 7, compressionOverhead is 33 and MAX_INPUT_SIZE is -26. 
{code:java}
public BlockCompressorStream(OutputStream out, Compressor compressor, 
                             int bufferSize, int compressionOverhead) {
  super(out, compressor, bufferSize);
  MAX_INPUT_SIZE = bufferSize - compressionOverhead; // -> Assumes bufferSize is always greater than compressionOverhead and MAX_INPUT_SIZE is non-negative. 
} {code}
h3. Stack Trace: 
{code:java}
java.lang.ArrayIndexOutOfBoundsException
        at org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86)
        at org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112) {code}
h3. How to Reproduce: 

(1) Set io.compression.codec.snappy.buffersize to 7

(2) Run test: org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile

 "
Update command usage of appendToFile.,13600972,Resolved,Major,Fixed,05/Dec/24 06:51,05/Dec/24 12:08,,"In HDFS-16716, supported appending on file with new block. FileSystemShell.md should also be updated."
ABFS: [FnsOverBlob] Group Enums in AbfsRestOperationType into categories and Other Code Cleanups,13598470,Open,Major,,12/Nov/24 11:45,,3.4.1,"Currently in AbfsRestOperationType, there are a lot of enums added and some of them corresponds to same operation but going to differentazure storage endpoint.

It makes sense to group these Enums into following Categories:
Global enum: AbfsGlobalRestOperationType
dfs enum: AbfsDFSRestOperationType
blob enum: AbfsBlobRestOperationType

Each of these sub-enums can hold operations used for that particular service endpoint.

For more details:
1. [https://github.com/apache/hadoop/pull/6944#discussion_r1766715478]

2. [https://github.com/apache/hadoop/pull/6944#discussion_r1788170640] "
Support append operation in S3A filesystem for S3 Express One Zone storage class,13599633,Open,Major,,22/Nov/24 05:06,,,"Amazon S3 recently announced support for appending (to the last) to s3 object for s3 express storage class. Refer [1|https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/] and [2|https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-append.html] for more details.

This requires to upgrade to latest aws-sdk-java-v2 version 2.29.19

 "
S3A: fs.s3a.connection.expect.continue controls 100 CONTINUE behavior,13596609,Resolved,Major,Fixed,24/Oct/24 13:58,19/Nov/24 14:40,3.4.1,"New option
{code}
  fs.s3a.connection.expect.continue
{code}

This controls whether or not an PUT request to the S3 store
sets the ""Expect: 100-continue"" header and awaits a 100 CONTINUE
response before uploading any data.

This allows for throttling and other problems to be detected fast.

The default is ""true"" -the header is sent.

It seems like either the AWS SDK or http client libraries underneath don't recognise a dead HTTPS connection when using this.

Leaving as enabled as it can cope with heavy load better (clients can get a 503 and know to back off before uploading any data."
AzureBlobFileSystem.open() should override readVectored() much more efficiently for small reads,13599787,Resolved,Major,Duplicate,23/Nov/24 17:07,23/Nov/24 17:14,,"In hadoop-azure, there are huge performance problems when reading file in a too fragmented way: by reading many small file fragments even with the readVectored() Hadoop API, resulting in distinct Https Requests (=TCP-IP connection established + TLS handshake + requests).
Internally, at lowest level, haddop azure is using class HttpURLConnection from jdk 1.0,  and the ReadAhead Threads do not sufficiently solve all problems.
The hadoop azure implementation of ""readVectored()"" should make a compromise between reading extra ignored data wholes, and establishing too many https connections.

Currently, the class AzureBlobFileSystem#open() does return a default inneficient imlpementation of readVectored:

{code:java}
  private FSDataInputStream open(final Path path,
      final Optional<OpenFileParameters> parameters) throws IOException {
...
      InputStream inputStream = getAbfsStore().openFileForRead(qualifiedPath, parameters, statistics, tracingContext);
      return new FSDataInputStream(inputStream);  // <== FSDataInputStream is not efficiently overriding readVectored() !
 }
{code}


see default implementation of FSDataInpustStream.readVectored:
{code:java}
    public void readVectored(List<? extends FileRange> ranges, IntFunction<ByteBuffer> allocate) throws IOException {
        ((PositionedReadable)this.in).readVectored(ranges, allocate);
    }
{code}

it calls the underlying method from class AbfsInputStream, which is not overriden:
{code:java}
    default void readVectored(List<? extends FileRange> ranges, IntFunction<ByteBuffer> allocate) throws IOException {
        VectoredReadUtils.readVectored(this, ranges, allocate);
    }
{code}


AbfsInputStream should override this method, and accept internally to do less Https calls, with merged range, and ignore some returned data (wholes in the range). 

It is like honouring the parameter of hadoop FSDataInputStream (implements PositionedReadable)
{code:java}
  /**
   * What is the smallest reasonable seek?
   * @return the minimum number of bytes
   */
  default int minSeekForVectorReads() {
    return 4 * 1024;
  }
{code}

Even this 4096 value is very conservative, and should be redined by AbfsFileSystem to be 4Mo or even 8mo.

ask chat gpt: ""on Azure Storage, what is the speed of getting 8Mo of a page block, compared to the time to establish a https tls handshake ?""
The response (untrusted from chat gpt..) says :
HTTPS/TLS Handshake: ~100–300 ms  ... is generally slower than  downloading 8 MB from Page Blob:  on Standard Tier: ~100–200 ms / on Premium Tier: ~30–50 ms

Azure Abfsclient already setup by default a lot of Threads for Prefecth Read Ahead, to prefetch 4Mo of data,  but it is NOT sufficent, and less efficient that simply implementing correctly what is already in Hadoop API : readVectored(). It also have the drawback of reading tons of useless data (past parquet blocks), that are never used.


"
Support user defined auth Callback in SaslRpcServer,13594957,Resolved,Major,Fixed,10/Oct/24 18:12,15/Nov/24 16:59,,"Similar to HDFS-17576, SaslRpcServer should support CustomizedCallbackHandler."
SaslRpcServer.AuthMethod print INFO messages in client side,13599167,Resolved,Major,Fixed,18/Nov/24 19:18,21/Nov/24 16:15,,"After HADOOP-19306, the following INFO messages are printed in client side.  Thanks As [~stevel@apache.org] for reporting it in [this comment|https://github.com/apache/hadoop/pull/7140#issuecomment-2483881941].
{code}
2024-11-18 18:53:37,645 [setup] INFO  security.SaslRpcServer (SaslRpcServer.java:<init>(239)) - AuthMethod SIMPLE: code=80, mechanism=""""
2024-11-18 18:53:37,645 [setup] INFO  security.SaslRpcServer (SaslRpcServer.java:<init>(239)) - AuthMethod KERBEROS: code=81, mechanism=""GSSAPI""
2024-11-18 18:53:37,646 [setup] INFO  security.SaslRpcServer (SaslRpcServer.java:<init>(239)) - AuthMethod DIGEST: code=82, mechanism=""DIGEST-MD5""
2024-11-18 18:53:37,646 [setup] INFO  security.SaslRpcServer (SaslRpcServer.java:<init>(239)) - AuthMethod TOKEN: code=82, mechanism=""DIGEST-MD5""
2024-11-18 18:53:37,646 [setup] INFO  security.SaslRpcServer (SaslRpcServer.java:<init>(239)) - AuthMethod PLAIN: code=83, mechanism=""PLAIN""
{code}"
Backport Mockito Changes to 3.4 Branch.,13598032,In Progress,Major,,07/Nov/24 09:42,,,Backport Mockito Upgrade changes [#6968|https://github.com/apache/hadoop/pull/6968] to 3.4 Branch.
 Follow-up on ResourceManager quit due to ApplicationStateData exceeding znode limit,13596537,Resolved,Major,Duplicate,24/Oct/24 02:44,14/Nov/24 02:12,3.3.6,"In YARN-5006 the issue ""ResourceManager quit due to ApplicationStateData exceeding znode limit"" has been addressed , specifically in the storeApplicationStateInternal() method of the ZKRMStateStore class. However, the methods updateApplicationStateInternal(), storeApplicationAttemptStateInternal(), and updateApplicationAttemptStateInternal() also need to incorporate checks for the ZNode size limit in  ZKRMStateStore class. This will help prevent scenarios where excessive data written to a single ZNode causes the ResourceManager (RM) and ZooKeeper (ZK) services to become unavailable."
Publish hadoop image to GitHub Container Registry,13597545,Resolved,Major,Fixed,03/Nov/24 17:39,07/Nov/24 17:17,,Create GitHub Actions workflow to publish the {{apache/hadoop}} Docker image to GitHub Container Registry.
HTTP web console support Basic Authentication,13598325,Open,Major,,11/Nov/24 03:41,,3.5.0,"hadoop.http.authentication.type support for simple and kerberos, simple too simple. kerberos is too complicated. Expect to support Basic Authentication"
Upgrade hadoop3 docker scripts to 3.4.1,13597028,Resolved,Major,Done,29/Oct/24 08:00,07/Nov/24 19:07,,
ABFS: Add a new API in AzureBlobFileSystem to allow listing with startFrom,13597496,Open,Major,,02/Nov/24 00:49,,,"Currently the API to list from a certain starting point is hidden inside {{{}AzureBlobFileSystemStore{}}}, and is hard to access. It'd be better to surface this up to places like {{AzureBlobFileSystem}} to make it easier to use. 

 

The API is useful in scenarios such as Delta Lake where transaction logs are always indexed sequentially. With this API, the listing in Delta no longer needs to listing the whole {{_delta_log}} directory but only a small subset."
JDK11 build fails with error: release version 1.8 not supported,13596690,Resolved,Major,Workaround,25/Oct/24 07:54,28/Oct/24 06:36,3.5.0,"[JDK11 build|https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java11-linux-x86_64/756/] is failing:

{code:title=https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java11-linux-x86_64/756/artifact/out/patch-compile-root.txt}
Fatal error compiling: error: release version 1.8 not supported
{code}"
ABFS: setConf should also update configuration for AzureBlobFileSystemStore,13597996,Open,Major,,07/Nov/24 02:11,,,Currently the {{AzureBlobFileSystemStore}} inside {{AzureBlobFileSystem}} keeps a separate reference to the input {{{}configuration{}}}. The {{AzureBlobFileSystem.setConf}} only update the conf in itself but not the one in {{AzureBlobFileSystemStore}}
hadoop-rumen is vulnerable to Sonatype CWE611,13597743,Resolved,Major,Not A Problem,05/Nov/24 10:23,05/Nov/24 19:20,3.3.6,"hadoop-rumen is vulnerable to CWE-611: [Improper Restriction of XML External Entity Reference.|https://cwe.mitre.org/data/definitions/611.html]

sonatype-2022-5820

Explanation: The Apache {{hadoop-common}} and {{hadoop-rumen}} packages are vulnerable to XML External Entity (XXE) attacks. The {{readXmlFileToMapWithFileInputStream()}} method in the {{HostsFileReader}} class, the {{parse()}} method in the {{JobConfigurationParser}} class, and the constructor in the {{ParsedConfigFile}} class process malicious external entities by default due to an unsafe XML parser configuration. A remote attacker who can supply or modify the contents of hosts or configuration XML files parsed by these packages can exploit this vulnerability to exfiltrate information, cause a Denial of Service (DoS) condition, or perform other XXE-related attacks.

Root Cause: org/apache/hadoop/tools/rumen/JobConfigurationParser.class, org/apache/hadoop/tools/rumen/ParsedConfigFile.class"
Enable multi-part-upload with client side encryption for async s3 client,13597859,Open,Major,,06/Nov/24 05:36,,,"S3 Client side encryption (cse) is being added as part of https://issues.apache.org/jira/browse/HADOOP-18708.

multi-part-upload with s3 async client is disabled when cse is enabled since it is unavailable. This Jira tracks the effort to enable it one it is available.

 "
[JDK 17] Resolve Http Server error and Http response error in Hadoop Trunk,13594732,Resolved,Major,Duplicate,09/Oct/24 06:18,04/Nov/24 20:06,,"While compiling HADOOP-TRUNK on JDK 17 faced 2 common issues : 

*1.* Unexpected HTTP response: *code=500 != 200* or *code=500 != 307*

*2.* org.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.yarn.webapp.WebAppException: *Error starting http server*"
Use spotbugs instead of findbugs,13595922,Resolved,Major,Duplicate,18/Oct/24 05:47,21/Oct/24 11:12,,"com.google.code.findbugs: findbugs is no longer maintained. Use com.github.spotbugs: spotbugs instead.

This is a follow-on to HADOOP-16870
"
Exclude netty3 from hadoop-hdfs in hadoop-yarn-server-timelineservice-hbase-tests,13596908,Open,Major,,28/Oct/24 09:28,,,
license-maven-plugin latest version also has dependency on commons-collections:3.2.2 which is EOL,13596907,Open,Major,,28/Oct/24 09:24,,,
S3A CopyFromLocalFile operation fails when the source file does not contain file scheme.,13595077,Resolved,Major,Fixed,11/Oct/24 12:09,25/Oct/24 10:12,3.3.6,"When the sourcePath does not contain any file scheme information, S3A CopyFromLocalFile operation fails with the following exception stack trace.
{code:java}
    at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:360)
    at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:222)
    at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:169)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$23(S3AFileSystem.java:4217)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2871)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2890)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:4209)
 {code}
Additionally the failure is seen only when 
{color:#172b4d}*fs.s3a.optimized.copy.from.local.enabled* is enabled (which is by default). This happens only when the local source file is given without any file scheme for example : /tmp/file.txt instead of file:///tmp/file.txt.
{color}
 
{color:#172b4d}The proposal here is to add file scheme to the source if the source path does not contain the same.{color}"
upgrade AWS v1 SDK on branch-3.3.x,13596210,Open,Major,,21/Oct/24 13:20,,3.3.6,"Qualify an SDK upgrade to branch-3.3

must include a fix for HADOOP-19313

This isn't really targeting a 3.3.x release; just keeping up to date and dealing with that noisyness"
[JDK17] Add JPMS options required by Java 17+,13595177,Resolved,Major,Fixed,12/Oct/24 11:16,17/Oct/24 09:25,3.4.0,
Add option to add parent directory of source directories to target directories,13595043,Open,Major,,11/Oct/24 08:07,,3.0.0,"Currently, when we execute the Hadoop distcp  with -update -delete src1/* src2/* dest command to keep the source and target directories exactly the same。 When either -update or -overwrite is specified, the *contents* of the source-directories are copied to target, and not the source directories themselves. 

Consider a copy from /source/first/ and /source/second/ to /target/, where the source paths have the following contents:

hdfs://nn1:8020/source/first/1
hdfs://nn1:8020/source/first/2
hdfs://nn1:8020/source/second/10
hdfs://nn1:8020/source/second/20

distcp2 -update hdfs://nn1:8020/source/first hdfs://nn1:8020/source/second hdfs://nn2:8020/target


would yield the following contents in /target:

hdfs://nn2:8020/target/1
hdfs://nn2:8020/target/2
hdfs://nn2:8020/target/10
hdfs://nn2:8020/target/20

 

But, sometimes, we need to preserve parent directories like this:

hdfs://nn1:8020/target/first/1
hdfs://nn1:8020/target/first/2
hdfs://nn1:8020/target/second/10
hdfs://nn1:8020/target/second/20

 

So, should we introduce an option -preserveParentDir to keep the parent directories to be copied with -update or -overwrite ?

 "
ConcurrentModificationException in HttpReferrerAuditHeader,13594090,Resolved,Major,Fixed,02/Oct/24 17:08,07/Oct/24 16:47,3.4.1,"Surfaced during a test run doing vector iO, where multiple parallel GETs were being issued within the same audit span, just when the header is built by enumerating the attributes.

{code}
      queries = attributes.entrySet().stream()
          .filter(e -> !filter.contains(e.getKey()))
          .map(e -> e.getKey() + ""="" + e.getValue())
          .collect(Collectors.joining(""&""));
{code}

Hypothesis: multiple GET requests are conflicting in updating/reading the header."
Update rat version in the docker build.sh script,13594350,Open,Major,,04/Oct/24 17:49,,3.3.7,"The docker build.sh script uses apache rat 0.15 which is removed from Apache CDN.
https://github.com/apache/hadoop/blob/docker-hadoop-3.4/build.sh#L20

The build in the DockerHub doesn't fail, probably because there's cache. But it fails for me locally.

The latest is 0.16.1. Let's update."
upgrade hadoop thirdparty avro to 1.11.4,13594187,Open,Major,,03/Oct/24 15:10,,thirdparty-1.3.0,"CVE-2024-47561

https://www.cve.org/CVERecord?id=CVE-2024-47561"
JVM GC Metrics supports ZGC pause time and count,13602353,Resolved,Minor,Fixed,17/Dec/24 04:25,25/Dec/24 09:05,3.4.0,"8265136: ZGC: Expose GarbageCollectorMXBeans for both pauses and cycles

[https://bugs.openjdk.org/browse/JDK-8265136]"
Avoid initializing useless HashMap in protocolImplMapArray.,13603245,Resolved,Minor,Fixed,27/Dec/24 02:18,13/Feb/25 21:04,,Avoid initializing useless HashMap in protocolImplMapArray.
S3A: Add config option to skip test with performance mode,13600628,Resolved,Minor,Fixed,02/Dec/24 19:12,15/Jan/25 18:08,3.4.1,"For stores with posix semantics, some tests will fail in performance mode. Add a configuration option to skip running tests in performance mode."
[ABFS] Support Azurite storage emulator,13603268,Open,Minor,,27/Dec/24 08:09,,3.4.1,"In the integration test case, we will start the Azurite using the test container, and we need to access Azurite at another container.
Now, the Azurite emulator uri values is fixed at 127.0.0.1, so at another container, it can't access the Azurite service.

So I want to introduce a new config `fs.azure.storage.emulator.proxy.url`, so that I can config `fs.azure.storage.emulator.proxy.url=http://\{AzuriteIp}`, then it can access the Azurite service.

 

 
{code:java}
private void connectUsingCredentials(String accountName,
StorageCredentials credentials, String containerName)
throws URISyntaxException, StorageException, AzureException {

URI blobEndPoint;
if (isStorageEmulatorAccount(accountName)) {
isStorageEmulator = true;
CloudStorageAccount account =
CloudStorageAccount.getDevelopmentStorageAccount();
storageInteractionLayer.createBlobClient(account);
} else {
blobEndPoint = new URI(getHTTPScheme() + ""://"" + accountName);
storageInteractionLayer.createBlobClient(blobEndPoint, credentials);
}
suppressRetryPolicyInClientIfNeeded();

// Capture the container reference for debugging purposes.
container = storageInteractionLayer.getContainerReference(containerName);
rootDirectory = container.getDirectoryReference("""");

// Can only create container if using account key credentials
canCreateOrModifyContainer = credentials instanceof StorageCredentialsAccountAndKey;
} {code}
 
{code:java}
public static CloudStorageAccount getDevelopmentStorageAccount() {
    try {
        return getDevelopmentStorageAccount(null);
    }
    catch (final URISyntaxException e) {
        // this won't happen since we know the standard development stororage uri is valid.
        return null;
    }
}

public static CloudStorageAccount getDevelopmentStorageAccount(final URI proxyUri) throws URISyntaxException {
    String scheme;
    String host;
    if (proxyUri == null) {
        scheme = ""http"";
        host = ""127.0.0.1"";
    }
    else {
        scheme = proxyUri.getScheme();
        host = proxyUri.getHost();
    }

    StorageCredentials credentials = new StorageCredentialsAccountAndKey(DEVSTORE_ACCOUNT_NAME,
            DEVSTORE_ACCOUNT_KEY);

    URI blobPrimaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_PRIMARY_ENDPOINT_FORMAT, scheme, host,
            ""10000"", DEVSTORE_ACCOUNT_NAME));
    URI queuePrimaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_PRIMARY_ENDPOINT_FORMAT, scheme, host,
            ""10001"", DEVSTORE_ACCOUNT_NAME));
    URI tablePrimaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_PRIMARY_ENDPOINT_FORMAT, scheme, host,
            ""10002"", DEVSTORE_ACCOUNT_NAME));

    URI blobSecondaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_SECONDARY_ENDPOINT_FORMAT, scheme, host,
            ""10000"", DEVSTORE_ACCOUNT_NAME));
    URI queueSecondaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_SECONDARY_ENDPOINT_FORMAT, scheme, host,
            ""10001"", DEVSTORE_ACCOUNT_NAME));
    URI tableSecondaryEndpoint = new URI(String.format(DEVELOPMENT_STORAGE_SECONDARY_ENDPOINT_FORMAT, scheme, host,
            ""10002"", DEVSTORE_ACCOUNT_NAME));

    CloudStorageAccount account = new CloudStorageAccount(credentials, new StorageUri(blobPrimaryEndpoint,
            blobSecondaryEndpoint), new StorageUri(queuePrimaryEndpoint, queueSecondaryEndpoint), new StorageUri(
            tablePrimaryEndpoint, tableSecondaryEndpoint), null /* fileStorageUri */);

    account.isDevStoreAccount = true;

    return account;
} {code}
CloudStorageAccount account = CloudStorageAccount.getDevelopmentStorageAccount(); will using 127.0.0.1 as the `azurite` host. 
 
In fact, here we can pass into a proxy uri by invoke getDevelopmentStorageAccount(final URI proxyUri)
 
 
 
 "
Add fs.hdfs.impl.disable.cache to core-default.xml,13603183,Open,Minor,,26/Dec/24 03:06,,,"We need to add `fs.$SCHEME.impl.disable.cache` configuration entry to core-default.xml.

This is a frequently used configuration entry but end user could not find it in xxx-default.xml"
[ABFS] Add all ABFS metrics related file in one package,13602544,Open,Minor,,18/Dec/24 09:15,,3.4.1,"Currently, all ABFS metrics files are in the service package, which also includes many other files. To improve code readability, we should consolidate all metrics-related files into one common package.
Previous JIRA: [HADOOP-19311] [ABFS] Implement Backoff and Read Footer metrics using IOStatistics Class - ASF JIRA"
Fix error links of huaweicloud in site index.,13602344,Resolved,Minor,Fixed,17/Dec/24 03:20,29/Dec/24 07:58,,Fix links to unexisting pages in site index.
ABFS: Optimizations for Retry Handling and Throttling,13600965,Resolved,Minor,Fixed,05/Dec/24 05:16,17/Dec/24 10:06,3.4.0,"Given the significant improvements on the service side over the past few years, the current client-side retry mechanism, which enforces 5-6 minutes of sleep time with jitter, is now excessive. Therefore, we are realigning the retry and throttling logic thresholds.

Since the service can now handle higher traffic rates, we aim to enhance driver performance by setting the following configurations as default:
 * {*}Client-side throttling (CST){*}: Off
 * *Client Backoff* - 500ms (reduced from 3sec)
 * *Max Backoff* - 25s (reduced from 30sec)
 * *Min Backoff* - 500ms (reduced from 3sec)"
RPC DeferredMetrics bugfix,13601484,Resolved,Minor,Done,10/Dec/24 05:53,10/Dec/24 09:47,3.4.1,
"Move LeakReporter to utils, use more",13600856,Open,Minor,,04/Dec/24 11:12,,3.4.2,"Move LeakReporter of HADOOP-19330 from fs.impl to fs.utils, tag as public/evolving

1. This allows third party apps to track
2. pick up a few more places, e.g local fs output and input streams, s3a block output stream

Actually, we could add it to FSDataInputStream and have it everywhere. be noisy though, potentially

why #2? for unit testing in downstream apps."
ITestS3AContractCreate#testOverwrite*EmptyDirectory fail on ozone when create performance enabled,13600447,Resolved,Minor,Not A Bug,30/Nov/24 09:36,01/Dec/24 21:59,3.4.1,"See [https://github.com/apache/hadoop/pull/6168#issuecomment-1756082091] 
They also fail with Apache Ozone. See https://issues.apache.org/jira/browse/HDDS-11663

Both of these tests expect to throw a FileAlreadyExistsException, while creating a file that overwrites an existing directory.

The following is a step-by-step description of the test.
 # mkdir test/testOverwriteNonEmptyDirectory/         // existing directory
 # get test/testOverwriteNonEmptyDirector                // start to create file to overwrite directory
 # check the file status of test/testOverwriteNonEmptyDirector
 ** If it throws FileNotFound then goes to step 4
 ** check if it is a directory then throw FileAlreadyExistsException              // expect to throw [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L2196-L2199]
 ** check if the overwrite is false then throw FileAlreadyExistsException    
 # put test/testOverwriteNonEmptyDirector

However, in step 3 it actually goes to the first check, and put a new object.

Because S3a is expecting:

Get test/testOverwriteNonEmptyDirector    // does not exist
Get test/testOverwriteNonEmptyDirector/   // exists

And it will still throw an exception but not FileAlreadyExistsException.

 

How to fix:

Maybe we also get the path test test/testOverwriteNonEmptyDirector/ and check if the directory exists."
ViewFileSystem.InnerCache: Replaced ReentrantReadWriteLock with ConcurrentHashMap/putIfAbsent(),13599993,Open,Minor,,26/Nov/24 04:38,,,"Use of ReentrantReadWriteLock  + Map can be replaced/simplified with a ConcurrentHashMap. 

Right now, we are using a ReentrantReadWriteLock to protect concurrent updates to InnerCache.map(). 

As a performance optimization, to reduce acquiring writeLock, we first acquire readLock, check existence of the key and then acquire writeLock, to insert the new key.
{code:java}
    FileSystem get(URI uri, Configuration config) throws IOException {
      Key key = new Key(uri);
      FileSystem fs = null;
      try {
        rwLock.readLock().lock();
        fs = map.get(key);
        if (fs != null) {
          return fs;
        }
      } finally {
        rwLock.readLock().unlock();
      }
      try {
        rwLock.writeLock().lock();
        fs = map.get(key);
        if (fs != null) {
          return fs;
        }
        fs = fsCreator.getNewInstance(uri, config);
        map.put(key, fs);
        return fs;
      } finally {
        rwLock.writeLock().unlock();
      }
    }
{code}

The above function is already available as computeIfAbsent() from ConcurrentHashMap, which atomically checks existence of a key and inserts the new key if not there. This greatly simplifies the code. On the other hand, it should improve performance as well: concurrentHashMap supports finer-grain concurrency vs. a single ReadWrite Lock. 

We can replace the above code with the following one and it is much simpler.
{code:java}
    FileSystem get(URI uri, Configuration config) throws IOException {
      Key key = new Key(uri);

      FileSystem fs = map.computeIfAbsent(key, k -> getNewFileSystem(uri, config));
      if (fs == null) {
        throw new IOException(""Failed to create new FileSystem instance for "" + uri);
      }
      return fs;
{code}


"
AWS SDK deleteObjects() and S3Store.deleteObjects() don't handle 500 failures of individual objects,13600056,Open,Minor,,26/Nov/24 14:50,,3.4.1,"S3Store.deleteObjects() encountered 500 error and didn't recover.

We normally assume that 500 errors are already retried by the SDK so our own retry logic doesn't bother

The root cause is that the 500 errors can surface within the bulk delete.
* The delete POST returns 200, so SDK is happy
* but one of the rows in the request is reports the S3Error ""InternalError"":
{{Code=InternalError, Message=We encountered an internal error. Please try again.)]}}


Proposed.
* bulk delete invoker must map ""InternalError"" to AWSStatus500Exception and throw that.
* Add a retry policy for bulk deletes which considers AWSStatus500Exception as retriable. retry. We currently don't on the assumption that the SDK will retry, which it does for base retries, but clearly not for multiobject delete.
* Maybe also consider possibility that a partial 503 response could be generated? that is: only part of the delete throttled?

{code}

Caused by: org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteException: [S3Error(Key=table/warehouse/tablespace/external/hive/table/-tmp.-ext-10000/file/, Code=InternalError, Message=We encountered an internal error. Please try again.)]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:3186)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeysS3(S3AFileSystem.java:3422)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:3481)
	at org.apache.hadoop.fs.s3a.S3AFileSystem$OperationCallbacksImpl.removeKeys(S3AFileSystem.java:2558)
	at org.apache.hadoop.fs.s3a.impl.RenameOperation.lambda$removeSourceObjects$3(RenameOperation.java:625)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$once$0(Invoker.java:165)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)
  
{code}

"
configurationChangeMonitor is not properly set on HttpServer2,13596826,Open,Minor,,27/Oct/24 01:12,,,"It is related to HADOOP-16524. {{configurationChangeMonitor}} on HttpServer2 is not properly set. When HttpServer#stop() called, it is not gracefully cancelled."
AWS V1 SDK now tells users off for using a deprecated SDK,13596196,Open,Minor,,21/Oct/24 11:50,,3.3.9,"This is for anyone upgrading their v1 SDK to any release after July 2024:

The fix is to set the log to fatal or the sysprop aws.java.v1.disableDeprecationAnnouncement to true

I think I will go with #2 everywhere, and add it to a copy of the AwsWorkarounds class for v1. New warning messages from the SDK telling off end users are not permitted.

{code}
logger.log.com.amazonaws.util.VersionInfoUtils=FATAL
{code}

{code}
WARN util.VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/
You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.
This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.
{code}
"
Fix ZKFailoverController NPE issue due to integer overflow in parseInt when initHM.,13598987,Resolved,Trivial,Fixed,16/Nov/24 01:46,08/Dec/24 10:33,,"h3. *What Happened:* 

A null pointer exception occurs when trying to shutdown healthMonitor. healthMonitor is not initialized if the ha.health-monitor.rpc-timeout.ms is set to 4294967295. The healthMonitor constructor uses parseInt() to parse configuration values and this value overflows for parseInt and throws an exception during healthMonitor initialization. 
h3. *Buggy Code:* 

 
{code:java}
try {
  initRPC();
  initHM(); // -> This throws a java.Lang.NumberFormatException and   healthMonitor is not intialized
  startRPC();
  mainLoop();
} catch (Exception e) {
  LOG.error(""The failover controller encounters runtime error: "", e);
  throw e;
} 

  ...
  
  healthMonitor.shutdown(); // -> NPE if healthMonitor is not intialized
  healthMonitor.join();
} {code}
 

 
{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:266)
        at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:65)
        at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:186)
        at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:182)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:520)
        at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:182)
        at org.apache.hadoop.ha.MiniZKFCCluster$DummyZKFCThread.doWork(MiniZKFCCluster.java:301)
        at org.apache.hadoop.test.MultithreadedTestUtil$TestingThread.run(MultithreadedTestUtil.java:189)
 {code}
h3. *How to Reproduce:*

(1) Set ha.health-monitor.rpc-timeout.ms to 4294967295.  

(2) Run: test: org.apache.hadoop.ha.TestZKFailoverController#testVerifyObserverState"
Mark FutureDataInputStreamBuilderImpl as VisibleForTesting,13600747,Open,Trivial,,03/Dec/24 15:21,,3.4.1,"declare {{org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl}} as
* VisibleForTesting
* Evolving

This is because it is class is the only way to reliably create stub implementations of the builder API. We also know that it's pretty stable, though some methods have been added over time -a change which makes writing a stub implementation on its own really hard.      "
MutableQuantiles.getQuantiles() should be made a static method,13593950,Open,Trivial,,01/Oct/24 22:04,,,"MutableQuantiles.getQuantiles() returns the static member variable QUANTILES, so this method should be a static method too.

https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java#L157"
Update LICENSE for 3.3.1,13374971,Resolved,Blocker,Fixed,26/Apr/21 04:05,24/May/21 13:59,3.3.1,"Before release, do another round of check for the LICENSE file to make sure the dependency versions are updated correctly."
Add back the exceptions removed by HADOOP-17432 for compatibility,13373366,Resolved,Blocker,Fixed,18/Apr/21 08:46,08/May/21 22:34,3.3.1,"As [~ayushsaxena] commented in https://issues.apache.org/jira/browse/HADOOP-17432?focusedCommentId=17324284&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17324284, we need to add back the exceptions for compatibility."
Upgrade Jetty to 9.4.40,13374574,Resolved,Blocker,Fixed,23/Apr/21 07:17,23/Apr/21 10:55,3.3.1,"Jetty 9.4.39 has a critical bug: https://github.com/eclipse/jetty.project/issues/6152
It causes http put failure in HttpFS with SSL enabled.

{noformat}
curl --negotiate -u : ""https://<HttpFS host>:<port>/webhdfs/v1/<output>?op=CREATE&data=true"" -X PUT -H Content-Type:application/octet-stream -T <input>
curl: (55) TCP connection reset by peer
{noformat}"
Explicitly set locale in the Dockerfile,13379495,Resolved,Blocker,Fixed,20/May/21 11:50,21/May/21 16:35,3.3.1,"When producing the RC bits for 3.3.1, the releasedocmaker step failed.

{noformat}
[INFO] --- exec-maven-plugin:1.3.1:exec (releasedocs) @ hadoop-common ---
Traceback (most recent call last):
  File ""/build/source/patchprocess/apache-yetus-0.13.0/bin/../lib/releasedocmaker/releasedocmaker.py"", line 25, in <module>
    releasedocmaker.main()
  File ""/build/source/patchprocess/apache-yetus-0.13.0/lib/releasedocmaker/releasedocmaker/__init__.py"", line 979, in main
    JIRA_BASE_URL)
  File ""/build/source/patchprocess/apache-yetus-0.13.0/lib/releasedocmaker/releasedocmaker/utils.py"", line 199, in write_list
    self.write_key_raw(jira.get_project(), line)
  File ""/build/source/patchprocess/apache-yetus-0.13.0/lib/releasedocmaker/releasedocmaker/utils.py"", line 170, in write_key_raw
    self.base.write(input_string)
UnicodeEncodeError: 'ascii' codec can't encode character '\xdc' in position 71: ordinal not in range(128)

{noformat}

It turns out if the script reads jiras containing ascended characters, it can't write the report.

Inside docker container, the default locale is ""ANSI_X3.4-1968"". Must set it to utf-8 to support special characters.

Curious why it wasn't a problem before.

More details: https://stackoverflow.com/questions/43356982/docker-python-set-utf-8-locale"
[build] fix the Dockerfile for ARM,13379711,Resolved,Blocker,Fixed,21/May/21 09:12,01/Jun/21 01:58,3.3.1,"Running the create-release script for Hadoop 3.3.1 on an ARM machine, docker image fails to build:

{noformat}
    aarch64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Iast27/Include -I/usr/include/python3.6m -c ast27/Parser/acceler.c -o build/temp.linux-aarch64-3.6/ast27/Parser/acceler.o                                                                                                            In file included from ast27/Parser/acceler.c:13:0:                                                                                ast27/Parser/../Include/pgenheaders.h:8:10: fatal error: Python.h: No such file or directory                                       #include ""Python.h""                                                                                                                        ^~~~~~~~~~                                                                                                              compilation terminated.                                                                                                           error: command 'aarch64-linux-gnu-gcc' failed with exit status 1


{noformat}

The missing Python3.h requires python3-dev package: https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory

The PhantomJS binary was built for Xenial, doesn't run after the Dockerfile migrated to Bionic/Focal. Fortunately Bionic/Focal has official PhantomJS packages."
"S3AFS creation fails ""Unable to find a region via the region provider chain.""",13385119,Resolved,Blocker,Fixed,22/Jun/21 09:20,24/Jun/21 16:01,3.3.1,"If you don't have {{fs.s3a.endpoint}} set and lack a region set in
env var {{AWS_REGION_ENV_VAR}}, system property {{aws.region}} or the file  ~/.aws/config
then S3A FS creation fails with  the message
""Unable to find a region via the region provider chain.""

This is caused by the move to the AWS S3 client builder API in HADOOP-13551

This is pretty dramatic and no doubt everyone will be asking ""why didn't you notice this?"",


But in fact there are some reasons.
# when running in EC2, all is well. Meaning our big test runs were all happy.
# if a developer has fs.s3a.endpoint set for the test bucket, all is well.
   Those of us who work with buckets in the ""regions tend to do this, not least because it can save a HEAD request every time an FS is created.
# if you have a region set in ~/.aws/config then all is well

reason #3 is the real surprise and the one which has really caught us out. Even my tests against buckets in usw-2 through central didn't fail because of course I, like my colleagues, have the AWS cli client installed locally. This was sufficient to make the problem go away. It is also why this has been an intermittent problem on test clusters outside AWS infra: it really depended on the VM/docker image whether things worked or not.

h2. Quick Fix: set {{fs.s3a.endpoint}} to {{s3.amazonaws.com}} 

If you have found this JIRA because you are encountering this problem, you can fix it in by explicitly declaring the endpoint in {{core-site.xml}}

{code}
<property>
  <name>fs.s3a.endpoint</name>
  <value>s3.amazonaws.com</value>
</property>
{code}

For Apache Spark, this can be done in {{spark-defaults.conf}}

{code}
spark.hadoop.fs.s3a.endpoint s3.amazonaws.com
{code}

If you know the exact AWS region your data lives in, set the endpoint to be that region's endpoint, and so save an HTTPS request to s3.amazonaws.com every time an S3A Filesystem instance is created.

"
mvn versions:set fails to parse pom.xml,13374772,Resolved,Blocker,Fixed,24/Apr/21 05:14,27/Apr/21 02:16,3.2.3,"Run the following command on trunk
mvn versions:set -DnewVersion=3.4.0

{noformat}
[ERROR] Failed to execute goal org.codehaus.mojo:versions-maven-plugin:2.8.1:set (default-cli) on project hadoop-main: Execution default-cli of goal org.codehaus.mojo:versions-maven-plugin:2.8.1:set failed: Error parsing /Users/weichiu/sandbox/hadoop/hadoop-project/pom.xml: Unexpected character '-' (code 45) (expected a name start character)
[ERROR]  at [row,col {unknown-source}]: [2288,12]
{noformat}
Someone else reported the same issue https://github.com/mojohaus/versions-maven-plugin/issues/429

Maven: 3.3.9 ~ 3.8.1
Affects trunk ~ branch-3.0. (But branch-2.10 is not affected)

We don't use versions-maven-plugin directly, and I couldn't find out when it was updated.


"
"HADOOP-16916 changed interface SASTokenProvider fields, breaking compatibility between 3.3.0 and 3.3.1 ",13378784,Resolved,Blocker,Won't Fix,17/May/21 10:52,19/May/21 02:51,3.3.1,"I understand HADOOP-16730/HADOOP-16916 is specifically made for Ranger, but I am not sure how Ranger consumes this feature. The interface SASTokenProvider has a number of member fields that changed variable names in HADOOP-16916, breaking the compatibility between 3.3.0 and 3.3.1.

As a matter of fact, the feature HADOOP-16730 itself was merged in 3.3.0 not 3.3.1. I just corrected it today.

Raise this jira and mark it as a blocker for 3.3.1. But if this isn't a big deal then we can downgrade, because, well, this feature was not officially in the 3.3.0 release."
Fix license error in GitHub Actions workflow files,13378222,Resolved,Blocker,Fixed,13/May/21 02:35,13/May/21 05:26,,"License header is missing in the following files:
* .github/workflows/build.yml
* .github/workflows/dependency_check.yml"
[thirdparty] Fix the docker image,13378003,Resolved,Blocker,Fixed,12/May/21 05:01,13/May/21 05:14,thirdparty-1.1.0,"The hadoop-thirdparty release script does not work out of box.

(1) Hard code a python dependency version to avoid image build error.
(2) Remove npm which is not used for building hadoop-thirdparty to avoid image build error.
(2) Use GPG 2.2 (LTS) instead of 1.x for signing. GPG 1.x is quite old and is not compatible with GPG 2.x (https://superuser.com/questions/1112673/gpg2-no-secret-key). The build script runs a gpg agent inside docker that shares the gpg keys with the host machine. Most host machines today ships with GPG 2. Ubuntu 18.04 installs gpg 2.2 by default.

(gpg2.1, which is supported by Ubuntu 16.04, is not compatible with gpg 2.2: https://github.com/sobolevn/git-secret/issues/136)"
IOStatistics API in branch-3.3 break compatibility,13375293,Resolved,Critical,Fixed,27/Apr/21 10:15,29/Nov/22 15:37,,"The S3 delegation token feature (3.3.0) added API 
{code:java}
  AmazonS3 createS3Client(URI name,
      String bucket,
      AWSCredentialsProvider credentialSet,
      String userAgentSuffix) throws IOException;
 {code}
However, the IOStatistics API (HADOOP-17271, HADOOP-13551. in 3.3.1) changed it to
{code:java}

  AmazonS3 createS3Client(URI name,
      String bucket,
      AWSCredentialsProvider credentialSet,
      String userAgentSuffix, 
    StatisticsFromAwsSdk statisticsFromAwsSdk) throws IOException; {code}

The API is declared evolving, so we're not supposed to break compat between maintenance releases.

[~stevel@apache.org]"
Checksum type is always forced to be crc32,13372756,Open,Critical,,16/Apr/21 00:22,,,HADOOP-14405 made a change to for non-direct byte buffer input to use native crc32.  In doing so it forces all non-native byte buffers to use crc32.  This is the root cause of the problem in HDFS-14582.
DistCp job fails when AM is killed,13384052,Patch Available,Major,,16/Jun/21 05:36,,,"Job fails as tasks fail with below exception
{code:java}
2021-06-11 18:48:47,047 | ERROR | IPC Server handler 0 on 27101 | Task: attempt_1623387358383_0006_m_000000_1000 - exited : java.io.FileNotFoundException: File does not exist: hdfs://hacluster/staging-dir/dsperf/.staging/_distcp-646531269/fileList.seq
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1637)
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1630)
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1645)
 at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1863)
 at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1886)
 at org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader.initialize(SequenceFileRecordReader.java:54)
 at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:560)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:798)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
 at org.apache.hadoop.mapred.YarnChild$1.run(YarnChild.java:183)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1761)
 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:177)
 | TaskAttemptListenerImpl.java:304{code}"
A divide by zero bug in LoadBalancingKMSClientProvider.java,13379212,Open,Major,,19/May/21 06:52,,,"In the file _kms/LoadBalancingKMSClientProvider.java_, the function _rollNewVersion_ has the following [code|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java#L509-#L516]:

 
{code:java}
@Override
public KeyVersion rollNewVersion(final String name, final byte[] material)
    throws IOException {
    final KeyVersion newVersion = doOp(new ProviderCallable<KeyVersion>() {
      @Override
      public KeyVersion call(KMSClientProvider provider) throws IOException {
        return provider.rollNewVersion(name, material);
      }
    }, nextIdx(), false);
    ...
}
{code}
The function _nextIdx_ uses _providers.length_ as a divisor:
{code:java}
private int nextIdx() {
    while (true) {
      int current = currentIdx.get();
      int next = (current + 1) % providers.length;
      ......
}{code}
However, _providers.length_ may be equal to zero, since the function _doOp_ explicitly checks that and throws an exception when it happens:
{code:java}
private <T> T doOp(ProviderCallable<T> op, int currPos,
      boolean isIdempotent) throws IOException {
    if (providers.length == 0) {
      throw new IOException(""No providers configured !"");
    }
    ...
}
{code}
 

The problem is that when _providers.length_ is 0, a divide by zero problem will happen when computing the argument for _doOp_ (inside the function _nextIdx_) before reaching the protection check above, causing an ArithmeticException.

 

 

 "
[JDK 16] TestRPC#testAuthorization fails by ClassCastException,13370616,Open,Major,,09/Apr/21 09:54,,,"{code}

[ERROR] testAuthorization(org.apache.hadoop.ipc.TestRPC)  Time elapsed: 1.066 s  <<< ERROR!
java.lang.ClassCastException: class java.net.SocketException cannot be cast to class org.apache.hadoop.ipc.RemoteException (java.net.SocketException is in module java.base of loader 'bootstrap'; org.apache.hadoop.ipc.RemoteException is in unnamed module of loader 'app')
	at org.apache.hadoop.ipc.TestRPC.doRPCs(TestRPC.java:591)
	at org.apache.hadoop.ipc.TestRPC.testAuthorization(TestRPC.java:639)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
"
Upgrade Protobuf from 3.7.1 to 3.17.3,13376238,Resolved,Major,Duplicate,02/May/21 01:50,30/Oct/24 07:33,3.3.0,Protobuf 3.17.0 is the latest Protobuf release now.
Release Hadoop 3.3.1,13373446,Resolved,Major,Done,19/Apr/21 04:24,24/Apr/24 22:27,,"File this jira to track the release work of Hadoop 3.3.1

Release dashboard: https://issues.apache.org/jira/secure/Dashboard.jspa?selectPageId=12336122"
Upgrade to hadoop-thirdparty 1.1.1,13380496,Resolved,Major,Duplicate,26/May/21 08:34,24/Apr/24 22:23,3.3.1,
Use hadoop-thirdparty 1.1.1,13381356,Resolved,Major,Fixed,01/Jun/21 01:56,01/Jun/21 03:38,3.3.1,
Remove lock contention in overlay of Configuration,13382902,Open,Major,,09/Jun/21 09:31,,,"The *overlay* field of class *Configuration* is a point of lock contention, which is bad for performance.

E.g.,
{code:java}
$ grep 'waiting to lock <0x00007fa4fc113378>' 17326.jstack | uniq -c
 257 - waiting to lock <0x00007fa4fc113378> (a org.apache.hadoop.conf.Configuration)
{code}
and the thread stack is as follows:
{code:java}
""hconnection-0x66971f6b-shared--pool1-t1060"" #6315 daemon prio=5 os_prio=0 tid=0x00007f5c04018800 nid=0x11f31 waiting for monitor entry [0x00007f567f3f4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.conf.Configuration.getOverlay(Configuration.java:1328)
        - waiting to lock <0x00007fa4fc113378> (a org.apache.hadoop.conf.Configuration)
        at org.apache.hadoop.conf.Configuration.handleDeprecation(Configuration.java:684)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1088)
        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145)
        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1375)
        at org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory.getMetadataPriority(PhoenixRpcSchedulerFactory.java:92)
        at org.apache.hadoop.hbase.ipc.controller.MetadataRpcController.<init>(MetadataRpcController.java:59)
        at org.apache.hadoop.hbase.ipc.controller.ClientRpcControllerFactory.getController(ClientRpcControllerFactory.java:57)
        at org.apache.hadoop.hbase.ipc.controller.ClientRpcControllerFactory.newController(ClientRpcControllerFactory.java:41)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:216)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:365)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:339)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136)
        at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:65)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}"
Abfs directory delete times out on large directory tree w/ Oauth: OperationTimedOut,13377858,Open,Major,,11/May/21 12:15,,3.3.0,"Timeouts surfacing on abfs when a delete of a large directory tree is invoked.
{code}
StatusDescription=Operation could not be completed within the specified time.
ErrorCode=OperationTimedOut
ErrorMessage=Operation could not be completed within the specified time.
{code}

This has surfaced in v1 FileOutputCommitter cleanups, implying the directories created there (many many dirs, no files remaining after the job commit) is sufficient to create the problem.
"
ABFS: ITestAzureBlobFileSystemLease test failure for Appendblob HNS OAuth,13386725,Open,Major,,30/Jun/21 09:03,,3.3.1,"Test testTwoWritersCreateAppendNoInfiniteLease fails with error:

java.io.IOException: Operation failed: ""Value for one of the query parameters specified in the request URI is invalid."""
"Replace Guava Lists usage by Hadoop's own Lists in hadoop-common, hadoop-tools and cloud-storage projects",13381946,Resolved,Major,Fixed,03/Jun/21 16:25,07/Jun/21 04:25,3.4.0,
Update commons-io to 2.8.0,13376512,Resolved,Major,Fixed,04/May/21 08:44,12/May/21 02:04,3.3.1,"https://nvd.nist.gov/vuln/detail/CVE-2021-29425

In Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like ""//../foo"", or ""\\..\foo"", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus ""limited"" path traversal), if the calling code would use the result to construct a path value.

We don't use this API in the Hadoop code, but it's still good to update anyway (we're on 2.5, which is 4 years old)"
Update to Jetty 9.4.39,13369713,Resolved,Major,Fixed,06/Apr/21 07:42,08/Apr/21 06:29,3.3.1,
Remove any rocksdb exclusion code,13369682,Resolved,Major,Fixed,06/Apr/21 03:16,07/Apr/21 04:52,3.4.0,"RocksDB was added as part of Ozone runtime, and we added code to exclude RocksDB from the core runtime. Now that Ozone is a separate project, we should remove anything referencing RocksDB now."
Ignore missing keystore configuration in reloading mechanism ,13374927,Resolved,Major,Fixed,25/Apr/21 15:39,17/May/21 03:01,3.3.1,"When there is no configuration of keystore/truststore location, the reload mechanism should be disabled."
Do not use guava's Files.createTempDir(),13374124,Resolved,Major,Fixed,21/Apr/21 11:59,02/May/21 02:22,3.4.0,
Update wildfly openssl to 1.1.3.Final,13379487,Resolved,Major,Fixed,20/May/21 10:48,27/Jan/23 12:01,3.3.5,"HADOOP-17649 got stalled. IMO we can bump the version to 1.1.3.Final instead, at least, for branch-3.3."
Upgrade JUnit to 4.13.2,13384984,Resolved,Major,Fixed,21/Jun/21 17:24,25/Jun/21 17:21,2.10.2,"JUnit 4.13.1 has a bug that is reported in Junit [issue-1652|https://github.com/junit-team/junit4/issues/1652] _Timeout ThreadGroups should not be destroyed_

After upgrading Junit to 4.13.1 in HADOOP-17602, {{TestBlockRecovery}}  started to fail regularly in branch-3.x and branch-2.10.
While investigating the failure in branch-2.10 HDFS-16072, I found out that the bug is the main reason {{TestBlockRecovery}}  started to fail because the timeout of the Junit would try to close a ThreadGroup that has been already closed which throws the {{java.lang.IllegalThreadStateException}}.

The bug has been fixed in Junit-4.13.2

For branch-3.x, HDFS-15940 did not address the root cause of the problem. Eventually, Splitting the {{TestBlockRecovery}} hid the bug, but the upgrade needs to be done so that the problem does not show up in another unit test."
Fix asf license errors in newly added files by HADOOP-17727,13382662,Resolved,Major,Fixed,08/Jun/21 09:01,09/Jun/21 06:32,3.4.0,
checkcompatibility.py errors out when specifying annotations,13378763,Resolved,Major,Fixed,17/May/21 09:20,18/May/21 03:38,3.3.1,"[https://github.com/apache/hadoop/blob/trunk/dev-support/bin/checkcompatibility.py#L178]
{code:java}
 with file(annotations_path, ""w"") as f: {code}
is not a valid Pythonic code."
SequenceFile.Writer should implement StreamCapabilities,13374720,Resolved,Major,Fixed,23/Apr/21 19:03,04/May/21 09:00,3.3.1,"Following exception is thrown whenever we invoke ProtoMessageWriter.hflush on S3 from Tez, which internally calls org.apache.hadoop.io.SequenceFile$Writer.hflush ->  org.apache.hadoop.fs.FS DataOutputStream.hflush -> S3ABlockOutputStream.hflush which is not implemented and throws java.lang.UnsupportedOperationException. 

bdffe22d96ae [mdc@18060 class=""yarn.YarnUncaughtExceptionHandler"" level=""ERROR"" thread=""HistoryEventHandlingThread""] Thread Thread[HistoryEventHandlingThread, 5,main] threw an Exception.^Mjava.lang.UnsupportedOperationException: S3A streams are not Syncable^M at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.hflush(S3ABlockOutputStream.java:657)^M at org.apache.hadoop.fs.FS DataOutputStream.hflush(FSDataOutputStream.java:136)^M at org.apache.hadoop.io.SequenceFile$Writer.hflush(SequenceFile.java:1367)^M at org.apache.tez.dag.history.logging.proto.ProtoMessageWriter.hflush(ProtoMessageWr iter.java:64)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.finishCurrentDag(ProtoHistoryLoggingService.java:239)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.han dleEvent(ProtoHistoryLoggingService.java:198)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.loop(ProtoHistoryLoggingService.java:153)^M at java.lang.Thread.run(Thread.java:748)^M

In order to fix this issue we should implement StreamCapabilities in SequenceFile.Writer. Also, we should fall back to flush(), if hflush() is not supported. "
WASB : Make metadata checks case insensitive,13373180,Resolved,Major,Fixed,17/Apr/21 06:11,10/Dec/21 05:15,2.7.0,"WASB driver uses meta data on blobs to denote permission, whether its a place holder 0 sized blob for dir etc.
For storage migration users uses Azcopy, it copies the blobs but will cause the metadata keys to get changed to camel case. As per discussion with MSFT Azcopy team, this is a known issue and technical limitation.  This is what Azcopy team explained
""For context, blob metadata is implemented with HTTP headers. They are case insensitive but case preserving. 
There is a known issue with the Go language. The HTTP client that it provides does this case modification to the response headers before we can read the raw values, so the destination metadata keys have a different casing than the source. We’ve reached out to the Go Team in the past but weren’t successful in convincing them to change the behaviour. We don’t have a short term solution right now""

So propose to change the metadata key checks to do case insensitive checks.  May be make case insensitive check configurable with defaults to false for compatibility."
WASB : Support disabling buffered reads in positional reads,13385077,Resolved,Major,Fixed,22/Jun/21 05:58,22/Oct/21 06:19,3.3.2,"This is just like HADOOP-17038

Right now it will do a seek to the position , read and then seek back to the old position. (As per the impl in the super class)
In HBase kind of workloads we rely mostly on short preads. (like 64 KB size by default). So would be ideal to support a pure pos read API which will not even keep the data in a buffer but will only read the required data as what is asked for by the caller. (Not reading ahead more data as per the read size config)

Allow an optional boolean config to be specified while opening file for read using which buffered pread can be disabled.
FutureDataInputStreamBuilder openFile(Path path)"
Remove an invalid comment content in the FileContext class,13375286,Resolved,Major,Fixed,27/Apr/21 09:55,14/Jul/21 07:15,3.4.0,"There is an invalid comment content in the FileContext class:
   /**
    * Protected Static Factory methods for getting a FileContexts
    * that take a AbstractFileSystem as input. To be used for testing.
    */
It doesn't seem to have any effect."
Update clover-maven-plugin version from 3.3.0 to 4.4.1,13385984,Resolved,Major,Fixed,26/Jun/21 11:28,30/Jun/21 01:37,3.3.2,"Update clover-maven-plugin version to 4.4.1, some important changes are as follows:
 * the license key is no longer required to run Clover
 * the Atlassian brand and logos were removed to avoid any trademark violations
 * the 'org.openclover.*' groupId is used for artifacts

The license key is no longer required is the key factor to do this update. So the developer can run and get the code coverage easier."
DistCp: Use Iterator for listing target directory as well,13369466,Resolved,Major,Fixed,04/Apr/21 10:16,23/Apr/21 17:42,3.3.1,"Use iterator for listing target directory as well, when {{-useiterator}} option is specified.

Target is listed when delete option is specified."
[thirdparty] Do not exclude error_prone,13380143,Resolved,Major,Fixed,24/May/21 15:48,26/May/21 05:46,3.3.1,"Building Ozone on Hadoop 3.3.1-RC1, found the following error during compilation:

{noformat}
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-hdds-common: Compilation failure
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] cannot access org.apache.hadoop.thirdparty.com.google.errorprone.annotations.CanIgnoreReturnValue
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] class file for org.apache.hadoop.thirdparty.com.google.errorprone.annotations.CanIgnoreReturnValue not found
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] Consult the following stack trace for details.
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] com.sun.tools.javac.code.Symbol$CompletionFailure: class file for org.apache.hadoop.thirdparty.com.google.errorprone.annotations.CanIgnoreReturnValue not found
18:40:02 2021/05/24 10:40:02 INFO    : [ERROR] -> [Help 1]
{noformat}

The error_prone was excluded by YARN-10195 to avoid dependency divergence with YARN Timeline service. However, now that Hadoop's guava is shaded, we shouldn't have problem with diverging dependencies. So let's add it back."
Remove jaeger document from site index,13378933,Resolved,Major,Fixed,18/May/21 03:10,19/May/21 06:45,thirdparty-1.1.1,
[JDK 15] TestPrintableString fails due to Unicode 13.0 support,13370625,Resolved,Major,Fixed,09/Apr/21 10:29,13/Apr/21 08:11,3.3.1,"After [JDK-8239383|https://bugs.openjdk.java.net/browse/JDK-8239383], Unicode 13.0 is supported and TestPrintableString fails.

U+30000 is actually used in Unicode 13.0: https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_Extension_G
{quote}
[ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.055 s <<< FAILURE! - in org.apache.hadoop.fs.shell.TestPrintableString
[ERROR] testNonPrintableCharacters(org.apache.hadoop.fs.shell.TestPrintableString)  Time elapsed: 0.014 s  <<< FAILURE!
java.lang.AssertionError: 
Should replace unassigned U+30000 and U+DFFFF
Expected: is ""-?-?-""
     but: was ""-𰀀-?-""
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:964)
	at org.apache.hadoop.fs.shell.TestPrintableString.expect(TestPrintableString.java:32)
	at org.apache.hadoop.fs.shell.TestPrintableString.testNonPrintableCharacters(TestPrintableString.java:79)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{quote}"
Update the ubuntu version in the build instruction,13372603,Resolved,Major,Fixed,15/Apr/21 10:55,19/Apr/21 07:45,3.4.0,"In BUILDING.txt
{noformat}
Installing required packages for clean install of Ubuntu 14.04 LTS Desktop:
{noformat}
Ubuntu 14 is already EoL, should be updated to 18 or 20."
Remove appender EventCounter to avoid instantiation ,13373112,Resolved,Major,Fixed,16/Apr/21 19:17,17/Apr/21 11:48,3.4.0,"After removal of EventCounter class, we are not able to bring up HDFS cluster.
{code:java}
log4j:ERROR Could not instantiate class [org.apache.hadoop.log.metrics.EventCounter].
java.lang.ClassNotFoundException: org.apache.hadoop.log.metrics.EventCounter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
	at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
	at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
	at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
	at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
	at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:229)
	at org.apache.hadoop.hdfs.tools.GetConf.<clinit>(GetConf.java:131)
log4j:ERROR Could not instantiate appender named ""EventCounter"".
{code}
We need to clean up log4j.properties to avoid instantiating appender EventCounter."
Use spotbugs-maven-plugin in hadoop-huaweicloud,13375298,Resolved,Major,Fixed,27/Apr/21 10:42,28/Apr/21 01:04,3.4.0,"We replaced Findbugs with Spotbugs in HADOOP-16870. However, we missed hadoop-huaweicloud module there."
Restrict imports from org.apache.curator.shaded,13375901,Resolved,Major,Fixed,29/Apr/21 19:19,03/May/21 18:13,3.3.1,"Once HADOOP-17653 gets in, we should ban ""org.apache.curator.shaded"" imports as discussed on PR#2945. We can use enforcer-rule to restrict imports such that if ever used, mvn build fails.

Thanks for the suggestion [~weichiu] [~aajisaka] [~stevel@apache.org]"
Fix junit deprecation warnings in hadoop-common module,13377010,Resolved,Major,Fixed,06/May/21 10:32,13/May/21 05:23,3.4.0,Fix HADOOP-17684 in hadoop-common module.
Avoid Potential NPE in org.apache.hadoop.fs,13377549,Resolved,Major,Fixed,10/May/21 03:48,12/May/21 15:06,3.4.0,"https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/
 Hello,
Our code analyses found the following potential NPE:

 
{code:java}
  public Path getParent() {
    String path = uri.getPath();
    int lastSlash = path.lastIndexOf('/');
    int start = startPositionWithoutWindowsDrive(path);
    if ((path.length() == start) ||               // empty path
        (lastSlash == start && path.length() == start+1)) { // at root
      return null;
    }
{code}
 
{code:java}
  public FSDataOutputStream createInternal (Path f,
      EnumSet<CreateFlag> flag, FsPermission absolutePermission, int bufferSize,
      short replication, long blockSize, Progressable progress,
      ChecksumOpt checksumOpt, boolean createParent) throws IOException {
    checkPath(f);
    
    // Default impl assumes that permissions do not matter
    // calling the regular create is good enough.
    // FSs that implement permissions should override this.    if (!createParent) { // parent must exist.
      // since this.create makes parent dirs automatically
      // we must throw exception if parent does not exist.
      final FileStatus stat = getFileStatus(f.getParent()); // NPE!
      if (stat == null) {
        throw new FileNotFoundException(""Missing parent:"" + f);
      }
{code}
Full Trace:

1. Return null to caller
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java#L432

2. The return value of function getParent is used as the 1st parameter in function getFileStatus (the return value of function getParent can be null)
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java#L93

3. f is used as the 1st parameter in function checkPath (f can be null)
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java#L127

4. path is passed as the this pointer to function toUri (path can be null)
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java#L369

Commit: f40e3eb0590f85bb42d2471992bf5d524628fdd6"
Remove hardcoded SunX509 usage from SSLFactory,13378545,Resolved,Major,Fixed,15/May/21 00:05,18/May/21 17:12,3.4.0,"In SSLFactory.SSLCERTIFICATE, used by FileBasedKeyStoresFactory and ReloadingX509TrustManager, there is a hardcoded reference to ""SunX509"" which is used to get a KeyManager/TrustManager. This KeyManager type might not be available if using the other JSSE providers, e.g.,  in FIPS deployment.

 
{code:java}
WARN org.apache.hadoop.hdfs.web.URLConnectionFactory: Cannot load customized ssl related configuration. Fall
 back to system-generic settings.
 java.security.NoSuchAlgorithmException: SunX509 KeyManagerFactory not available
 at sun.security.jca.GetInstance.getInstance(GetInstance.java:159)
 at javax.net.ssl.KeyManagerFactory.getInstance(KeyManagerFactory.java:137)
 at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:186)
 at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:187)
 at org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50)
 at org.apache.hadoop.hdfs.web.URLConnectionFactory.getSSLConnectionConfiguration(URLConnectionFactory.java:100)
 at org.apache.hadoop.hdfs.web.URLConnectionFactory.newDefaultURLConnectionFactory(URLConnectionFactory.java:79)
{code}
This ticket is opened to use the DefaultAlgorithm defined by Java system property: 

ssl.KeyManagerFactory.algorithm and ssl.TrustManagerFactory.algorithm.

 "
ExitUtil#halt info log should log HaltException,13378634,Resolved,Major,Fixed,16/May/21 07:07,22/May/21 10:14,3.4.0,ExitUtil#halt with non-zero exit status code provides info log with incorrect no of placeholders. We should log HaltException with the log.
Replace Guava Sets usage by Hadoop's own Sets in hadoop-hdfs-project,13379563,Resolved,Major,Fixed,20/May/21 18:01,25/May/21 01:56,3.4.0,
Replace Guava Sets usage by Hadoop's own Sets in hadoop-yarn-project,13379567,Resolved,Major,Fixed,20/May/21 18:12,25/May/21 09:11,3.4.0,
Replace Guava Sets usage by Hadoop's own Sets in hadoop-mapreduce-project,13379568,Resolved,Major,Fixed,20/May/21 18:12,25/May/21 01:08,3.4.0,
Keep restrict-imports-enforcer-rule for Guava Sets in hadoop-main pom,13380314,Resolved,Major,Fixed,25/May/21 10:59,26/May/21 08:15,3.4.0,"Now that all sub-tasks to remove dependency on Guava Sets are completed, we should move restrict-imports-enforcer-rule for Guava Sets import in hadoop-main pom and remove from individual project poms."
Remove lock contention in SelectorPool of SocketIOWithTimeout,13382376,Resolved,Major,Fixed,07/Jun/21 04:54,07/Jul/21 02:07,,"*SelectorPool* in hadoop-common/src/main/java/org/apache/hadoop/net/*SocketIOWithTimeout.java* is a point of lock contention.

For example: 
{code:java}
$ grep 'waiting to lock <0x00007f7d94006d90>' 63692.jstack | uniq -c
 1005 - waiting to lock <0x00007f7d94006d90> (a org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool)
{code}
and the thread stack is as follows:
{code:java}
""IPC Client (324579982) connection to /100.10.6.10:60020 from user_00"" #14139 daemon prio=5 os_prio=0 tid=0x00007f7374039000 nid=0x85cc waiting for monitor entry [0x00007f6f45939000]
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.get(SocketIOWithTimeout.java:390)
 - waiting to lock <0x00007f7d94006d90> (a org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool)
 at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:325)
 at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
 at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
 at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
 at java.io.FilterInputStream.read(FilterInputStream.java:133)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
 - locked <0x00007fa818caf258> (a java.io.BufferedInputStream)
 at java.io.DataInputStream.readInt(DataInputStream.java:387)
 at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.readResponse(RpcClientImpl.java:967)
 at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.run(RpcClientImpl.java:568)

{code}
We should remove the lock contention."
S3AInputStream read does not re-open the input stream on the second read retry attempt,13384133,Resolved,Major,Fixed,16/Jun/21 10:19,29/Jun/21 17:11,3.3.1,"*Bug description:*

The read method in S3AInputStream has this following behaviour when an IOException happening during the read:
 * {{reopen and read quickly}}: The client after failing in the first attempt of {{read}}, will reopen the stream and try reading again without {{sleep}}.

 * {{reopen and wait for fixed duration}}: The client after failing in the attempt of {{read}}, will reopen the stream, sleep for {{fs.s3a.retry.interval}} milliseconds (defaults to 500 ms), and then try reading from the stream.

While doing the {{reopen and read quickly}} process, the subsequent read will be retried without reopening the input stream in case of the second failure happened. This leads to some of the bytes read being skipped which results to corrupt/less data than required. 

 

*Scenario to reproduce:*
 * Execute S3AInputStream `read()` or `read(b, off, len)`.
 * The read failed and throws `Connection Reset` exception after reading some data.
 * The InputStream is re-opened and another `read()` or `read(b, off, len)` is executed
 * The read failed for the second time and throws `Connection Reset` exception after reading some data.
 * The InputStream is not re-opened and another `read()` or `read(b, off, len)` is executed after sleep
 * The read succeed, but it skips the first few bytes that has already been read on the second failure.

 

*Proposed fix:*

[https://github.com/apache/hadoop/pull/3109]

Added the test that reproduces the issue along with the fix"
HADOOP-17065 breaks compatibility between 3.3.0 and 3.3.1,13378756,Open,Major,,17/May/21 09:03,,3.3.1,"AzureBlobFileSystemStore is a Public, Evolving class, by contract can't break compatibility between maintenance releases.

HADOOP-17065 changed its constructor signature from 
{noformat}
  public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme,
                                  Configuration configuration,
                                  AbfsCounters abfsCounters) throws IOException {
{noformat}
to
{noformat}
  public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme,
                                  Configuration configuration,
                                  AbfsCounters abfsCounters) throws IOException {
{noformat}
between 3.3.0 and 3.3.1.

cc: [~mehakmeetSingh], [~tmarquardt]"
Update wildfly openssl to 2.1.3.Final,13373449,Open,Major,,19/Apr/21 04:54,,,"https://nvd.nist.gov/vuln/detail/CVE-2020-25644

A memory leak flaw was found in WildFly OpenSSL in versions prior to 1.1.3.Final, where it removes an HTTP session. It may allow the attacker to cause OOM leading to a denial of service. The highest threat from this vulnerability is to system availability."
DistCp with snapshot diff should support file systems supporting getSnapshotDiffReport ,13379531,Resolved,Major,Duplicate,20/May/21 14:35,23/Jul/23 01:03,,{{distcp -diff}} currently supports only DistributedFileSystem and WebHdfsFileSystem. Other file system could be used if getSnapshotDiffReport is supported.
The java agent args should be passed as the VM arguments,13385983,Resolved,Major,Pending Closed,26/Jun/21 10:43,13/Jul/23 02:53,,"Some java agents or maven plugins want works well, must pass the agent arguments as the VM arguments, such as JaCoCo etc.

PS. Use the JaCoCo to generate the code coverage data and put into SonarQube.


{code:java}
mvn clean org.jacoco:jacoco-maven-plugin:prepare-agent package -Dmaven.test.failure.ignore=true org.jacoco:jacoco-maven-plugin:report surefire-report:report
{code}

"
S3A to add option fs.s3a.endpoint.region to set AWS region,13378789,Resolved,Major,Fixed,17/May/21 11:21,24/May/21 12:14,,"Currently, AWS region is either constructed via the endpoint URL, by making an assumption that the 2nd component after delimiter ""."" is the region in endpoint URL, which doesn't work for private links and sets the default to us-east-1 thus causing authorization issue w.r.t the private link.

The option fs.s3a.endpoint.region allows this to be explicitly set

h2. how to set the s3 region on older hadoop releases

For anyone who needs to set the signing region on older versions of the s3a client *you do not need this festure*. instead just provide a custom endpoint to region mapping json file

# Download the default region mapping file [awssdk_config_default.json|https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/resources/com/amazonaws/internal/config/awssdk_config_default.json]
# Add a new regular expression to map the endpoint/hostname to the target region
# Save the file as {{/etc/hadoop/conf/awssdk_config_override.json}}
# verify basic hadop fs -ls commands work
# copy to the rest of the cluster.
# There should be no need to restart any services
"
Remove lock contention in REGISTRY of Configuration,13382819,Open,Major,,09/Jun/21 02:37,,,"Every Configuration instance is put into *Configuration#REGISTRY* by its constructor. This operation is guard by Configuration.class.

REGISTRY is a *WeakHashMap*, which should be replaced by *ConcurrentHashMap*."
"Port HADOOP-17079, HADOOP-17505  to branch-3.3",13375191,Reopened,Major,,27/Apr/21 02:31,,3.3.1,
Improve error message for token providers in ABFS,13379741,Resolved,Major,Fixed,21/May/21 11:07,04/May/22 10:41,3.3.0,"It would be good to improve error messages for token providers in ABFS. Currently, when a configuration key is not found or mistyped, the error is not very clear on what went wrong. It would be good to indicate that the key was required but not found in Hadoop configuration when creating a token provider.

For example, when running the following code:
{code:java}
import org.apache.hadoop.conf._
import org.apache.hadoop.fs._

val conf = new Configuration()
conf.set(""fs.azure.account.auth.type"", ""OAuth"")
conf.set(""fs.azure.account.oauth.provider.type"", ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
conf.set(""fs.azure.account.oauth2.client.id"", ""my-client-id"")
// conf.set(""fs.azure.account.oauth2.client.secret.my-account.dfs.core.windows.net"", ""my-secret"")
conf.set(""fs.azure.account.oauth2.client.endpoint"", ""my-endpoint"")

val path = new Path(""abfss://container@my-account.dfs.core.windows.net/"")
val fs = path.getFileSystem(conf)
fs.getFileStatus(path){code}
The following exception is thrown:
{code:java}
TokenAccessProviderException: Unable to load OAuth token provider class.
...
Caused by: UncheckedExecutionException: java.lang.NullPointerException: clientSecret
...
Caused by: NullPointerException: clientSecret {code}
which does not tell what configuration key was not loaded.

 

IMHO, it would be good if the exception was something like this:
{code:java}
TokenAccessProviderException: Unable to load OAuth token provider class.
...
Caused by: ConfigurationPropertyNotFoundException: Configuration property fs.azure.account.oauth2.client.secret not found. {code}"
Problem in installation of Hadoop 3.2 with docket-  libc-bin  bug ,13378907,Open,Major,,18/May/21 00:01,,3.2.2,"Hi, 

I got the following bug while installing Hadoop 3.2.2 with docker:

Processing triggers for libc-bin (2.23-0ubuntu11.2) ...
WARN engine npm@7.13.0: wanted: \{""node"":"">=10""} (current: \{""node"":""4.2.6"",""npm"":""3.5.2""})
WARN engine npm@7.13.0: wanted: \{""node"":"">=10""} (current: \{""node"":""4.2.6"",""npm"":""3.5.2""})
/usr/local/lib
`-- (empty)

npm ERR! Linux 4.15.0-29-generic
npm ERR! argv ""/usr/bin/nodejs"" ""/usr/bin/npm"" ""install"" ""npm@latest"" ""-g""
npm ERR! node v4.2.6
npm ERR! npm v3.5.2
npm ERR! path /usr/local/lib/node_modules/.staging/@npmcli/ci-detect-c7bf9552
npm ERR! code ENOENT
npm ERR! errno -2
npm ERR! syscall rename

npm ERR! enoent ENOENT: no such file or directory, rename '/usr/local/lib/node_modules/.staging/@npmcli/ci-detect-c7bf9552' -> '/usr/local/lib/node_modules/npm/node_modules/@npmcli/ci-detect'
npm ERR! enoent ENOENT: no such file or directory, rename '/usr/local/lib/node_modules/.staging/@npmcli/ci-detect-c7bf9552' -> '/usr/local/lib/node_modules/npm/node_modules/@npmcli/ci-detect'
npm ERR! enoent This is most likely not a problem with npm itself
npm ERR! enoent and is related to npm not being able to find a file.
npm ERR! enoent

npm ERR! Please include the following file with any support request:
npm ERR! /root/npm-debug.log
npm ERR! code 1

 

 

What can I do?"
Set locale for Centos 7 and 8,13381381,Open,Major,,01/Jun/21 06:22,,3.4.0,The locale is set for Ubuntu Focal - https://github.com/apache/hadoop/blob/a234d00c1ce57427202d4c9587f891ec0164d10c/dev-support/docker/Dockerfile#L52-L54. The locale needs to be set for Centos 7 and 8 as well.
adls test suite TestAdlContractGetFileStatusLive failing with no assertJ on the classpath,13378145,Open,Major,,12/May/21 17:18,,3.3.1,"Reported on PR #2482: https://github.com/apache/hadoop/pull/2842 ; CNFE on assertJ assertions in adls test runs. 

Cause will be HADOOP-17281, which added the asserts to the existing fs contract test. We need to mark assertJ as an export of the hadoop-common suite, or work out why hadoop-azuredatalake isn't picking itup"
Upgrade aws-java-sdk to 1.11.1026,13380582,Resolved,Major,Fixed,26/May/21 15:56,31/May/21 20:52,3.3.0,"Upgrade the AWS SDK. Apparently the shaded netty jar has some CVEs, and even though the AWS codepaths don't seem vulnerable, it's still causing scan tools to warn"
ABFS: Support FileStatus input to OpenFileWithOptions() via OpenFileParameters,13376509,Resolved,Major,Fixed,04/May/21 08:29,27/Apr/22 18:25,,"ABFS open methods require certain information (contentLength, eTag, etc) to  to create an InputStream for the file at the given path. This information is retrieved via a GetFileStatus request to backend.

However, client applications may often have access to the FileStatus prior to invoking the open API. Providing this FileStatus to the driver through the OpenFileParameters argument of openFileWithOptions() can help avoid the call to Store for FileStatus.

This PR adds handling for the FileStatus instance (if any) provided via the OpenFileParameters argument."
Fails to build using Maven 3.8.1,13373736,Resolved,Major,Fixed,20/Apr/21 02:44,26/Apr/21 06:49,3.4.0,"The latest Maven (3.8.1) errors out when building Hadoop (tried trunk)
{noformat}
[ERROR] Failed to execute goal on project hadoop-yarn-applications-catalog-webapp: Could not resolve dependencies for project org.apache.hadoop:hadoop-yarn-applications-catalog-webapp:war:3.4.0-SNAPSHOT: Failed to collect dependencies at org.apache.solr:solr-core:jar:7.7.0 -> org.restlet.jee:org.restlet:jar:2.3.0: Failed to read artifact descriptor for org.restlet.jee:org.restlet:jar:2.3.0: Could not transfer artifact org.restlet.jee:org.restlet:pom:2.3.0 from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [maven-restlet (http://maven.restlet.org, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, disabled)] -> [Help 1]
{noformat}
According to [https://maven.apache.org/docs/3.8.1/release-notes.html#how-to-fix-when-i-get-a-http-repository-blocked] we need to update our Maven repo.

 

Maven 3.6.3 is good.

 

(For what is worth, I used my company's mirror to bypass this error. Not sure what is a good fix for Hadoop itself)"
ABFS: Use Unique File Paths in Tests,13384345,Resolved,Major,Fixed,17/Jun/21 12:02,01/Mar/22 13:50,3.3.1,"Many of ABFS driver tests use common names for file paths (e.g., ""/testfile""). This poses a risk of errors during parallel test runs when static variables (such as those for monitoring stats) affected by file paths are introduced.

Using unique test file names will avoid possible errors arising from shared resources during parallel runs."
Bump json-smart to 2.4.2 and nimbus-jose-jwt to 9.8 due to CVEs,13371140,Resolved,Major,Fixed,11/Apr/21 15:27,16/Apr/21 07:24,3.2.1,"Please upgrade the json-smart dependency to the latest version available.

Currently hadoop-auth is using version 2.3. Fortify scan picked up a security issue with this version. Please upgrade to the latest version. 

Thanks!

 "
Update guava to 30.1.1-jre,13373448,Resolved,Major,Fixed,19/Apr/21 04:44,26/Apr/21 04:10,,The latest guava version is 30.1.1-jre. Let's bump the version.
ADLS client can throw an IOException when it should throw an InterruptedIOException,13381978,Open,Major,,03/Jun/21 18:26,,,The Azure client sometimes throws an IOException with an InterruptedException cause which can be converted to an InterruptedIOException. This is important for downstream consumers that rely on an InterruptedIOException to gracefully close.
Über-JIRA: abfs phase III: Hadoop 3.3.2 features & fixes,13380771,Resolved,Major,Done,27/May/21 13:16,07/Jan/22 14:25,3.3.1,Wrap up JIRA for Hadoop 3.4/3.3.2 changes - successor to HADOOP-15763
Old JQuery version causing security concerns,13386877,Resolved,Major,Duplicate,30/Jun/21 19:35,16/Dec/21 06:11,3.3.0,"These fixes are required for Hadoop 3.3.0. 

Can you please update the following jqueries for the UI, they are causing security and vulnerabilities concerns: 

URL : http://web-address:8088/static/jquery/jquery-3.4.1.min.js Installed version : 3.4.1 Fixed version : 3.5.0 or latest

URL : http://web-address:8080/static/jquery-1.12.4.min.js  Installed version : 1.12.4 Fixed version : 3.5.0 or latest

These also extend to Spark-on-Yarn cluster. I hope I'm not messing up with my files paths!

 

Thank you"
Use profile hbase-2.0 by default and update hbase version,13375059,Open,Major,,26/Apr/21 12:47,,3.4.0,"We currently use hbase1 profile by default (for those who aren't aware, the YARN timeline service uses HBase as the underlying storage). There isn't much development activity in HBase 1.x and 2.x is production ready. I think it's time to switch to hbase 2 by default.

 

The HBase 2 version being used is 2.0.2. We should use the more recent versions.  (e.g. 2.2/2.3/2.4) (And update hbase 1 version as well)"
CI for Centos 8,13386096,Resolved,Major,Fixed,27/Jun/21 14:28,01/Jul/21 05:06,3.4.0,Need to run CI for Centos 8 platform to ensure that further changes are stable on this platform.
CI for Debian 10,13384611,Resolved,Major,Fixed,19/Jun/21 03:30,23/Jun/21 17:02,3.4.0,Need to setup CI for Debian 10. We need to also ensure it runs only if there are any changes to C++ files. Running it for all the PRs would be redundant.
Add Dockerfile for Debian 10,13379729,Resolved,Major,Fixed,21/May/21 10:08,17/Jun/21 17:15,3.4.0,Adding a Dockerfile for building on Debian 10 since there are a lot of users in the community using this distro.
Modularize docker images,13379923,Resolved,Major,Fixed,22/May/21 12:39,08/Jun/21 03:11,3.4.0,We're now creating the *Dockerfile*s for different platforms. We need a way to manage the packages in a clean way as maintaining the packages for all the different environments becomes cumbersome.
Dockerfile for building on Centos 8,13378062,Resolved,Major,Fixed,12/May/21 09:49,13/May/21 16:29,3.4.0,Need to add a Dockerfile for building on Centos 8 since some folks in the community are using it.
Dockerfile for building on Centos 7,13376105,Resolved,Major,Fixed,30/Apr/21 16:24,10/May/21 17:06,3.4.0,Need to add a Dockerfile for building on Centos 7 since some folks in the community are using it.
Update apache/hadoop:3 docker image to 3.3.1 release,13379424,Resolved,Major,Done,20/May/21 03:46,05/Nov/21 19:49,,"After the release passes the vote, update apache/hadoop:3 docker image by pointing it to 3.3.1 release bits."
"Backport to branch-3.2 HADOOP-17371, HADOOP-17621, HADOOP-17625 to update Jetty to 9.4.39",13370328,Resolved,Major,Fixed,08/Apr/21 09:25,20/Apr/21 03:43,3.2.3,
Avoid using zookeeper deprecated API and classes,13385310,Resolved,Major,Duplicate,23/Jun/21 05:45,03/Sep/21 20:19,3.3.1,"In latest version of zookeeper some internal classes are removed which is used in hadoop test code, for example ServerCnxnFactoryAccessor."
OBSFileSystem should support Snapshot operations,13379334,Open,Major,,19/May/21 17:47,,,"OBSFileSystem should support Snapshot operation like other files system.

CC: [~zhongjun] [~iwasakims] [~pbacsko]"
Distcp is unable to determine region with S3 PrivateLink endpoints,13375443,Resolved,Major,Fixed,27/Apr/21 20:53,27/Aug/21 11:27,2.9.1,"[root@ip-10-101-83-42 bin]# export AWS_REGION=us-east-2

[root@ip-10-101-83-42 bin]# hadoop distcp -Dfs.s3a.aws.credentials.provider=""org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider"" \

> -Dfs.s3a.access.key=""${AWS_ACCESS_KEY_ID}"" \

> -Dfs.s3a.secret.key=""${AWS_SECRET_ACCESS_KEY}"" \

> -Dfs.s3a.session.token=""${AWS_SESSION_TOKEN}"" \

> -Dfs.s3a.path.style.access=true \

> -Dfs.s3a.connection.ssl.enabled=true \

> -Dfs.s3a.attempts.maximum=1 \

> -Dfs.s3a.endpoint=bucket.vpce-123456-4jiz2sq4-us-east-2b.s3.us-east-2.vpce.amazonaws.com \

> hdfs:///user/root/ s3a://emr-tls-test/

21/04/27 10:02:13 INFO tools.OptionsParser: parseChunkSize: blocksperchunk false

21/04/27 10:02:15 WARN s3a.S3AFileSystem: Client: Amazon S3 error 400: 400 Bad Request; Bad Request (retryable)

 

x-amz-bucket-region=us-east-2

 

com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 0M1323C7AYJY315J; S3 Extended Request ID: vNyJdHN3bQ4g2w1sy2bvzoYLk+JGjMM4dQeSeDtQAdNk63ucXwOUWfoXZq1ZG8XQbnOclpc9Oi8=; Proxy: null), S3 Extended Request ID: vNyJdHN3bQ4g2w1sy2bvzoYLk+JGjMM4dQeSeDtQAdNk63ucXwOUWfoXZq1ZG8XQbnOclpc9Oi8=

 

It seems the request was rejected due to Malformed Header. The client was not sending the right region for request signing."
Fail to build hadoop-common from source on Fedora,13379050,Open,Major,,18/May/21 13:08,,,"Dear I tried to build hadoop from source with a vanilla fedora 34

{code:bash}
dnf group install -y ""Development Tools"" \
 && dnf install -y java-1.8.0-openjdk-devel fuse-devel snappy-java snappy-devel jansson-devel protobuf zlib-devel libzstd-devel \
                   maven-1:3.6.3 cmake gcc-c++ ant protobuf-compiler protobuf-java slf4j 

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-0.fc34.x86_64/
export MAVEN_OPTS=""-Xms2048M -Xmx4096M""
export PATH=""/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-0.fc34.x86_64/bin/:$PATH""
export CC=/usr/bin/gcc
export CXX=/usr/bin/g++

curl -LO https://apache.mediamirrors.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2-src.tar.gz
tar xf hadoop-3.2.2-src.tar.gz && cd hadoop-3.2.2-src
mvn package -Pdist,native -Drequire.snappy=true  -DskipTests -Dtar
{code}

But I have this error

{code:java}
    at org.apache.hadoop.maven.plugin.cmakebuilder.CompileMojo.runMake (CompileMojo.java:229)                                                                                                                                                                                                   
    at org.apache.hadoop.maven.plugin.cmakebuilder.CompileMojo.execute (CompileMojo.java:98)                                                                                                                                                                                                    
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)                                                                                                                                                                                       
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)                                                                                                                                                                                                         
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)                                                                                                                                                                                                         
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)                                                                                                                                                                                                         
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)                                                                                                                                                                                
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)                                                                                                                                                                                 
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)                                                                                                                                                                   
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)                                                                                                                                                                                                 
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)                                                                                                                                                                                                                          
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)                                                                                                                                                                                                                          
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)                                                                                                                                                                                                                            
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)                                                                                                                                                                                                                                
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)                                                                                                                                                                                                                                 
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)                                                                                                                                                                                                                                   
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)                                                                                                                                                                                                                             
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)                                                                                                                                                                                                           
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)                                                                                                                                                                                                   
    at java.lang.reflect.Method.invoke (Method.java:498)                                                                                                                                                                                                                                        
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)                                                                                                                                                                                                     
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)                                                                                                                                                                                                             
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)                                                                                                                                                                                                   
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)                                                                                                                                                                                                               
[ERROR]                                                                                                                                                                                                                                                                                         
[ERROR]                                                                                                                                                                                                                                                                                         
[ERROR] For more information about the errors and possible solutions, please read the following articles:                                                                                                                                                                                       
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException                                                                                                                                                                                                        
[ERROR]                                                                                                                                                                                                                                                                                         
[ERROR] After correcting the problems, you can resume the build with the command                                                                                                                                                                                                                
[ERROR]   mvn <args> -rf :hadoop-common                                                             
{code}

If we take a look to cmake log
I have:


{code:c}
gmake[1]: *** [CMakeFiles/cmTC_82239.dir/build.make:106: cmTC_82239] Error 1                                                                                                                                                                                                                    
gmake[1]: Leaving directory '/hadoop-3.2.2-src/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeTmp'                                                                                                                                                                           
gmake: *** [Makefile:140: cmTC_82239/fast] Error 2                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                
Source file was:                                                                                                                                                                                                                                                                                
#include <pthread.h>                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                
static void* test_func(void* data)                                                                                                                                                                                                                                                              
{                                                                                                                                                                                                                                                                                               
  return data;                                                                                                                                                                                                                                                                                  
}                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                
int main(void)                                                                                                                                                                                                                                                                                  
{                                                                                                                                                                                                                                                                                               
  pthread_t thread;                                                                                                                                                                                                                                                                             
  pthread_create(&thread, NULL, test_func, NULL);                                                                                                                                                                                                                                               
  pthread_detach(thread);                                                                                                                                                                                                                                                                       
  pthread_cancel(thread);                                                                                                                                                                                                                                                                       
  pthread_join(thread, NULL);                                                                                                                                                                                                                                                                   
  pthread_atfork(NULL, NULL, NULL);                                                                                                                                                                                                                                                             
  pthread_exit(NULL);                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                
  return 0;                                                                                                                                                                                                                                                                                     
}                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                
Determining if the function pthread_create exists in the pthreads failed with the following output:                                                                                                                                                                                             
Change Dir: /hadoop-3.2.2-src/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeTmp                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                
Run Build Command(s):/usr/bin/gmake cmTC_2b9ff/fast && /usr/bin/gmake  -f CMakeFiles/cmTC_2b9ff.dir/build.make CMakeFiles/cmTC_2b9ff.dir/build                                                                                                                                                  
gmake[1]: Entering directory '/hadoop-3.2.2-src/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeTmp'                                                                                                                                                                          
Building C object CMakeFiles/cmTC_2b9ff.dir/CheckFunctionExists.c.o                                                                                                                                                                                                                             
/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create -o CMakeFiles/cmTC_2b9ff.dir/CheckFunctionExists.c.o -c /usr/share/cmake/Modules/CheckFunctionExists.c                                                                                                                                     
Linking C executable cmTC_2b9ff                                                                                                                                                                                                                                                                 
/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_2b9ff.dir/link.txt --verbose=1                                                                                                                                                                                                              
/usr/bin/cc  -DCHECK_FUNCTION_EXISTS=pthread_create -rdynamic CMakeFiles/cmTC_2b9ff.dir/CheckFunctionExists.c.o -o cmTC_2b9ff  -lpthreads                                                                                                                                                       
/usr/bin/ld: cannot find -lpthreads                                                                                                                                                                                                                                                             
collect2: error: ld returned 1 exit status                                                                                                                                                                                                                                                      
gmake[1]: *** [CMakeFiles/cmTC_2b9ff.dir/build.make:106: cmTC_2b9ff] Error 1                                                                                                                                                                                                                    
gmake[1]: Leaving directory '/hadoop-3.2.2-src/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeTmp'                                                                                                                                                                           
gmake: *** [Makefile:140: cmTC_2b9ff/fast] Error 2 
{code}

and if I create the C code shown above and build with `-lpthread` that works fine

So as I use cmake 3.19.7, is it an old cmake bug ?


Thanks for your help


best regards"
EOF reached error reading ORC file on S3A,13382937,Resolved,Major,Duplicate,09/Jun/21 11:53,02/Jul/21 12:56,3.2.0,"Hi I am trying to do some transformation using Spark 3.1.1-Hadoop 3.2 on K8s and using s3a

I have around 700 GB of data to read and around 200 executors (5 vCore and 30G each).

Its able to read most of the files in problematic stage (Scan orc => Filter => Project) but is failing with few files at the end with below error.  The size of the file mentioned in error is around 140 MB and all other files are of similar size.

I am able to read and rewrite the specific file mentioned which suggest the file is not corrupted.

Let me know if further information is required

 
{code:java}
java.io.IOException: Error reading file: s3a://<bucket-with-prefix>/part-00001-5e22a873-82a5-4781-9eb9-473b483396bd.c000.zlib.orcjava.io.IOException: Error reading file: s3a://<bucket-with-prefix>/part-00001-5e22a873-82a5-4781-9eb9-473b483396bd.c000.zlib.orc at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1331) at org.apache.orc.mapreduce.OrcMapreduceRecordReader.ensureBatch(OrcMapreduceRecordReader.java:78) at org.apache.orc.mapreduce.OrcMapreduceRecordReader.nextKeyValue(OrcMapreduceRecordReader.java:96) at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source)Caused by: java.io.EOFException: End of file reached before reading fully. at org.apache.hadoop.fs.s3a.S3AInputStream.readFully(S3AInputStream.java:702) at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111) at org.apache.orc.impl.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:566) at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readFileData(RecordReaderUtils.java:285) at org.apache.orc.impl.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:1237) at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1105) at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1256) at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1291) at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1327) ... 20 more
{code}
 

 "
Remove JavaScript package from Docker environment,13385551,Resolved,Major,Fixed,24/Jun/21 05:52,07/Jul/21 08:50,,"As described in the [README of yarn-ui|https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/README.md], required javascript modules are automatically pulled by frontend-maven-plugin. We can leverage them for local testing too.

While hadoop-yarn-ui and hadoop-yarn-applications-catalog-webapp is using node.js, the version of node.js does not match. JavaScript related packages of the docker environment is not sure to work.

* https://github.com/apache/hadoop/blob/fdef2b4ccacb8753aac0f5625505181c9b4dc154/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/pom.xml#L170-L212
* https://github.com/apache/hadoop/blob/fdef2b4ccacb8753aac0f5625505181c9b4dc154/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml#L264-L290"
bytesRead FS statistic showing twice the correct value in S3A,13385380,Resolved,Major,Fixed,23/Jun/21 10:07,02/Jul/21 13:14,,"S3A ""bytes read"" statistic is being incremented twice. Firstly while reading in S3AInputStream and then in merge() of S3AInstrumentation when S3AInputStream is closed.

This makes ""bytes read"" statistic equal to sum of stream_read_bytes and stream_read_total_bytes."
mvn install -DskipTests failed about hadoop@3.3.1,13386726,Resolved,Major,Cannot Reproduce,30/Jun/21 09:16,01/Jul/21 02:12,3.3.1,"{panel:title=mvn install -DskipTests failed about hadoop@3.3.1}
mvn install -DskipTests failed
{panel}
[root@centos8 spack-src]# mvn -version
Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: /home/spack/spack/opt/spack/linux-centos8-aarch64/gcc-8.4.1/maven-3.6.3-bnannw7m3zq6axy7nmovnlhjawdy6pzt
Java version: 1.8.0_191, vendor: AdoptOpenJdk, runtime: /home/spack/spack/opt/spack/linux-centos8-aarch64/gcc-8.4.1/openjdk-1.8.0_191-b12-fidptihybskgklbjoo4lagkacm6n6lod/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""4.18.0-240.22.1.el8_3.aarch64"", arch: ""aarch64"", family: ""unix""
[root@centos8 spack-src]# java -version
openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.191-b12, mixed mode)
[root@centos8 spack-src]#mvn install -DskipTests
……
[INFO]
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-yarn-applications-catalog-webapp ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /root/6/hadoop-trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/test-dir
[INFO] Executed tasks
[INFO]
[INFO] --- maven-dependency-plugin:3.0.2:copy (default) @ hadoop-yarn-applications-catalog-webapp ---
[INFO]
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (banned-illegal-imports) @ hadoop-yarn-applications-catalog-webapp ---
[INFO]
[INFO] --- replacer:1.5.3:replace (replace-generated-sources) @ hadoop-yarn-applications-catalog-webapp ---
[INFO] Skipping
[INFO]
[INFO] --- replacer:1.5.3:replace (replace-sources) @ hadoop-yarn-applications-catalog-webapp ---
[INFO] Skipping
[INFO]
[INFO] --- replacer:1.5.3:replace (replace-guava) @ hadoop-yarn-applications-catalog-webapp ---
[INFO] Replacement run on 22 files.
[INFO]
[INFO] --- frontend-maven-plugin:1.11.2:install-node-and-yarn (install node and yarn) @ hadoop-yarn-applications-catalog-webapp ---
[INFO] Installing node version v8.11.3
[INFO] Downloading https://nodejs.org/dist/v8.11.3/node-v8.11.3-linux-arm64.tar.gz to /root/.m2/repository/com/github/eirslett/node/8.11.3/node-8.11.3-linux-arm64.tar.gz
[INFO] No proxies configured
[INFO] No proxy was configured, downloading directly
Jun 30, 2021 4:56:30 PM org.apache.http.impl.execchain.RetryExec execute
INFO: I/O exception (java.net.SocketException) caught when processing request to {s}->https://nodejs.org:443: Network is unreachable (connect failed)
Jun 30, 2021 4:56:30 PM org.apache.http.impl.execchain.RetryExec execute
INFO: Retrying request to {s}->https://nodejs.org:443
Jun 30, 2021 5:00:56 PM org.apache.http.impl.execchain.RetryExec execute
INFO: I/O exception (java.net.SocketException) caught when processing request to {s}->https://nodejs.org:443: Network is unreachable (connect failed)
Jun 30, 2021 5:00:56 PM org.apache.http.impl.execchain.RetryExec execute
INFO: Retrying request to {s}->https://nodejs.org:443
Jun 30, 2021 5:05:22 PM org.apache.http.impl.execchain.RetryExec execute
INFO: I/O exception (java.net.SocketException) caught when processing request to {s}->https://nodejs.org:443: Network is unreachable (connect failed)
Jun 30, 2021 5:05:22 PM org.apache.http.impl.execchain.RetryExec execute
INFO: Retrying request to {s}->https://nodejs.org:443
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Main 3.4.0-SNAPSHOT:
[INFO]
[INFO] Apache Hadoop Main ................................. SUCCESS [  1.394 s]
[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  1.139 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [  1.138 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  1.608 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.117 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.144 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  3.554 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.875 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [  3.514 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  1.086 s]
[INFO] Apache Hadoop Common ............................... SUCCESS [ 32.955 s]
[INFO] Apache Hadoop NFS .................................. SUCCESS [  1.536 s]
[INFO] Apache Hadoop KMS .................................. SUCCESS [  1.799 s]
[INFO] Apache Hadoop Registry ............................. SUCCESS [  1.746 s]
[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.068 s]
[INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 35.859 s]
[INFO] Apache Hadoop HDFS ................................. SUCCESS [ 41.663 s]
[INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [  0.482 s]
[INFO] Apache Hadoop HttpFS ............................... SUCCESS [  2.553 s]
[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  1.263 s]
[INFO] Apache Hadoop YARN ................................. SUCCESS [  0.071 s]
[INFO] Apache Hadoop YARN API ............................. SUCCESS [ 16.292 s]
[INFO] Apache Hadoop YARN Common .......................... SUCCESS [ 12.975 s]
[INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.067 s]
[INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  7.767 s]
[INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  2.587 s]
[INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  1.853 s]
[INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  1.426 s]
[INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 26.982 s]
[INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 10.825 s]
[INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.875 s]
[INFO] Apache Hadoop YARN Client .......................... SUCCESS [  3.608 s]
[INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.457 s]
[INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  6.434 s]
[INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  4.141 s]
[INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.973 s]
[INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  4.802 s]
[INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  2.877 s]
[INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  6.246 s]
[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  3.060 s]
[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.293 s]
[INFO] Apache Hadoop Federation Balance ................... SUCCESS [  2.601 s]
[INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [  7.814 s]
[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.069 s]
[INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  1.363 s]
[INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  1.484 s]
[INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.061 s]
[INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  2.148 s]
[INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  2.850 s]
[INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.064 s]
[INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SUCCESS [  2.887 s]
[INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  2.953 s]
[INFO] Apache Hadoop YARN Router .......................... SUCCESS [  2.013 s]
[INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  1.233 s]
[INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.060 s]
[INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  1.584 s]
[INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  2.965 s]
[INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.066 s]
[INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  3.973 s]
[INFO] Apache Hadoop YARN Services API .................... SUCCESS [  1.708 s]
[INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.059 s]
[INFO] Apache Hadoop YARN Application Catalog Webapp ...... FAILURE [18:02 min]
[INFO] Apache Hadoop YARN Application Catalog Docker Image  SKIPPED
[INFO] Apache Hadoop YARN Application MaWo ................ SKIPPED
[INFO] Apache Hadoop YARN Application MaWo Core ........... SKIPPED
[INFO] Apache Hadoop YARN Site ............................ SKIPPED
[INFO] Apache Hadoop YARN Registry ........................ SKIPPED
[INFO] Apache Hadoop YARN UI .............................. SKIPPED
[INFO] Apache Hadoop YARN CSI ............................. SKIPPED
[INFO] Apache Hadoop YARN Project ......................... SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SKIPPED
[INFO] Apache Hadoop MapReduce NativeTask ................. SKIPPED
[INFO] Apache Hadoop MapReduce Uploader ................... SKIPPED
[INFO] Apache Hadoop MapReduce Examples ................... SKIPPED
[INFO] Apache Hadoop MapReduce ............................ SKIPPED
[INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED
[INFO] Apache Hadoop Client Aggregator .................... SKIPPED
[INFO] Apache Hadoop Dynamometer Workload Simulator ....... SKIPPED
[INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SKIPPED
[INFO] Apache Hadoop Dynamometer Block Listing Generator .. SKIPPED
[INFO] Apache Hadoop Dynamometer Dist ..................... SKIPPED
[INFO] Apache Hadoop Dynamometer .......................... SKIPPED
[INFO] Apache Hadoop Archives ............................. SKIPPED
[INFO] Apache Hadoop Archive Logs ......................... SKIPPED
[INFO] Apache Hadoop Rumen ................................ SKIPPED
[INFO] Apache Hadoop Gridmix .............................. SKIPPED
[INFO] Apache Hadoop Data Join ............................ SKIPPED
[INFO] Apache Hadoop Extras ............................... SKIPPED
[INFO] Apache Hadoop Pipes ................................ SKIPPED
[INFO] Apache Hadoop OpenStack support .................... SKIPPED
[INFO] Apache Hadoop Amazon Web Services support .......... SKIPPED
[INFO] Apache Hadoop Kafka Library support ................ SKIPPED
[INFO] Apache Hadoop Azure support ........................ SKIPPED
[INFO] Apache Hadoop Aliyun OSS support ................... SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED
[INFO] Apache Hadoop Resource Estimator Service ........... SKIPPED
[INFO] Apache Hadoop Azure Data Lake support .............. SKIPPED
[INFO] Apache Hadoop Image Generation Tool ................ SKIPPED
[INFO] Apache Hadoop Tools Dist ........................... SKIPPED
[INFO] Apache Hadoop Tools ................................ SKIPPED
[INFO] Apache Hadoop Client API ........................... SKIPPED
[INFO] Apache Hadoop Client Runtime ....................... SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED
[INFO] Apache Hadoop Client Test Minicluster .............. SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED
[INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED
[INFO] Apache Hadoop Distribution ......................... SKIPPED
[INFO] Apache Hadoop Client Modules ....................... SKIPPED
[INFO] Apache Hadoop Tencent COS Support .................. SKIPPED
[INFO] Apache Hadoop OBS support .......................... SKIPPED
[INFO] Apache Hadoop Cloud Storage ........................ SKIPPED
[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  22:55 min
[INFO] Finished at: 2021-06-30T17:09:48+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.2:install-node-and-yarn (install node and yarn) on project hadoop-yarn-applications-catalog-webapp: Could not download Node.js: Could not download https://nodejs.org/dist/v8.11.3/node-v8.11.3-linux-arm64.tar.gz: Network is unreachable (connect failed) -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-yarn-applications-catalog-webapp

----
Can you help me find out what caused it and how to solve it?

"
wasb: delete() should have timeout option,13385268,Open,Major,,22/Jun/21 21:33,,3.2.0,The delete() API in AzureBlobFileSystem could potentially stuck when trying to delete a infinitely lease blob file/directory. We hope that there is a timeout option for this API and the delete() could throw an timeoutException when specified timeout limit is reached.
branch-2.10 daily build fails to pull latest changes,13383996,Resolved,Major,Duplicate,15/Jun/21 17:32,23/Jun/21 01:15,2.10.1,"I noticed that the build for branch-2.10 failed to pull the latest changes for the last few days.

CC: [~aajisaka], [~tasanuma], [~Jim_Brennan]

https://ci-hadoop.apache.org/job/hadoop-qbt-branch-2.10-java7-linux-x86_64/329/console

{code:bash}
Started by timer
Running as SYSTEM
Building remotely on H20 (Hadoop) in workspace /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64
The recommended git tool is: NONE
No credentials specified
Cloning the remote Git repository
Using shallow clone with depth 10
Avoid fetching tags
Cloning repository https://github.com/apache/hadoop
ERROR: Failed to clean the workspace
jenkins.util.io.CompositeIOException: Unable to delete '/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir'. Tried 3 times (of a maximum of 3) waiting 0.1 sec between attempts. (Discarded 1 additional exceptions)
	at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:90)
	at hudson.Util.deleteContentsRecursive(Util.java:262)
	at hudson.Util.deleteContentsRecursive(Util.java:251)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:743)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:161)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:154)
	at hudson.remoting.UserRequest.perform(UserRequest.java:211)
	at hudson.remoting.UserRequest.perform(UserRequest.java:54)
	at hudson.remoting.Request$2.run(Request.java:375)
	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:73)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.nio.file.AccessDeniedException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current
		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
		at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
		at java.nio.file.Files.newDirectoryStream(Files.java:457)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:224)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
	Suppressed: java.nio.file.AccessDeniedException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current
		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
		at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
		at java.nio.file.Files.newDirectoryStream(Files.java:457)
		at java.nio.file.Files.list(Files.java:3451)
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:247)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1 with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 33 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 33 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 31 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 31 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 29 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 29 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3 with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 27 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 27 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 25 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 25 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 23 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 23 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 21 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 21 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 19 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 19 more
ERROR: Error cloning remote repo 'origin'
hudson.plugins.git.GitException: Failed to delete workspace
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:746)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:161)
	at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:154)
	at hudson.remoting.UserRequest.perform(UserRequest.java:211)
	at hudson.remoting.UserRequest.perform(UserRequest.java:54)
	at hudson.remoting.Request$2.run(Request.java:375)
	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:73)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: hudson.remoting.Channel$CallSiteStackTrace: Remote call to H20
		at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1800)
		at hudson.remoting.UserRequest$ExceptionResponse.retrieve(UserRequest.java:357)
		at hudson.remoting.Channel.call(Channel.java:1001)
		at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.execute(RemoteGitImpl.java:146)
		at sun.reflect.GeneratedMethodAccessor1536.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.invoke(RemoteGitImpl.java:132)
		at com.sun.proxy.$Proxy172.execute(Unknown Source)
		at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1224)
		at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1302)
		at hudson.scm.SCM.checkout(SCM.java:505)
		at hudson.model.AbstractProject.checkout(AbstractProject.java:1206)
		at hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:637)
		at jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)
		at hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:509)
		at hudson.model.Run.execute(Run.java:1907)
		at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)
		at hudson.model.ResourceController.execute(ResourceController.java:97)
		at hudson.model.Executor.run(Executor.java:429)
Caused by: jenkins.util.io.CompositeIOException: Unable to delete '/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir'. Tried 3 times (of a maximum of 3) waiting 0.1 sec between attempts. (Discarded 1 additional exceptions)
	at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:90)
	at hudson.Util.deleteContentsRecursive(Util.java:262)
	at hudson.Util.deleteContentsRecursive(Util.java:251)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:743)
	... 10 more
	Suppressed: java.nio.file.AccessDeniedException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current
		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
		at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
		at java.nio.file.Files.newDirectoryStream(Files.java:457)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:224)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
	Suppressed: java.nio.file.AccessDeniedException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current
		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
		at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
		at java.nio.file.Files.newDirectoryStream(Files.java:457)
		at java.nio.file.Files.list(Files.java:3451)
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:247)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1 with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1/current]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 33 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 33 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data/data1]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 31 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 31 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs/data]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 29 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 29 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3 with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3/dfs]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 27 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 27 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data/3]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 25 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 25 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test/data]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 23 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 23 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target/test]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 21 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 21 more
	Suppressed: jenkins.util.io.CompositeIOException: Unable to remove directory /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs with directory contents: [/home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs/target]
		at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:250)
		at jenkins.util.io.PathRemover.tryRemoveFile(PathRemover.java:205)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:216)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.tryRemoveRecursive(PathRemover.java:215)
		at jenkins.util.io.PathRemover.tryRemoveDirectoryContents(PathRemover.java:226)
		at jenkins.util.io.PathRemover.forceRemoveDirectoryContents(PathRemover.java:87)
		... 13 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:237)
			... 19 more
		Suppressed: java.nio.file.DirectoryNotEmptyException: /home/jenkins/jenkins-home/workspace/hadoop-qbt-branch-2.10-java7-linux-x86_64/sourcedir/hadoop-hdfs-project/hadoop-hdfs
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
			at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
			at java.nio.file.Files.deleteIfExists(Files.java:1165)
			at jenkins.util.io.PathRemover.removeOrMakeRemovableThenRemove(PathRemover.java:241)
			... 19 more
ERROR: Error cloning remote repo 'origin'
Archiving artifacts
Recording test results
ERROR: Step ?Publish JUnit test result report? failed: No test report files were found. Configuration error?
Email was triggered for: Always
Sending email for trigger: Always
Sending email to: common-dev@hadoop.apache.org hdfs-dev@hadoop.apache.org mapreduce-dev@hadoop.apache.org yarn-dev@hadoop.apache.org
Finished: FAILURE
REST API
CloudBees CI Client Master 2.263.4.2-rolling
{code}
"
Remove Hadoop 3.1.4 from the download page,13383256,Resolved,Major,Done,10/Jun/21 17:56,21/Jun/21 06:29,,"Since Hadoop 3.1.x is EoL, 3.1.4 should be removed from https://hadoop.apache.org/releases.html."
Service authorization adds ip-based user authentication,13381429,Open,Major,,01/Jun/21 10:36,,3.4.0,"At present, the authentication of the user and IP of the service RPC is independent and global. And most of our needs are to allow some users of specific IPs, or some users of specific network segments.
This strategy is used in our internal HDFS, and I want to contribute this feature to the community."
Delete hadoop.ssl.enabled and dfs.https.enable from docs and core-default.xml,13383348,Resolved,Major,Fixed,11/Jun/21 08:12,17/Jun/21 01:02,,"{{hadoop.ssl.enabled}} and {{dfs.https.enable}} were removed from hadoop-3 by HADOOP-10348. They are not deprecated.
https://github.com/apache/hadoop/commit/8b196816d8de638aaab0294249e560303fd92b7a

Let's remove them from docs and core-default.xml."
Increase precommit job timeout from 20 hours to 24 hours.,13383092,Resolved,Major,Fixed,10/Jun/21 05:28,14/Jun/21 01:24,,"If QA runs for the whole project, it may not be finished within 20 hours. I suggest extending the timeout to 1day.
* https://github.com/apache/hadoop/pull/3049 : 17 hours. (finished)
* https://github.com/apache/hadoop/pull/3087 : 20 hours. (timeout)"
NPE and excessive warnings after HADOOP-17728,13383219,Open,Major,,10/Jun/21 14:55,,3.4.0,"I'm noticing these warnings and NPE's when just running a simple pi test on a one node cluster:
{noformat}
2021-06-09 21:51:12,334 WARN  [org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner] fs.FileSystem (FileSystem.java:run(4025)) - Exception in the cleaner thread but it will continue to run
java.lang.NullPointerException
	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:4020)
	at java.lang.Thread.run(Thread.java:748){noformat}
This appears to be due to [HADOOP-17728].
I'm not sure I understand why that change was made?  Wasn't it by design that the remove should wait until something is queued?
[~kaifeiYi] can you please investigate?
"
Replace usage of Hashtable and ConcurrentHashMap.contains(Object),13383642,Open,Major,,13/Jun/21 22:42,,,"The Hadoop project and its subprojects use in a few places [{{Hashtable.contains(Object)}}|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Hashtable.html#contains(java.lang.Object)] (or the inherited {{Properties.contains(Object)}}) and [{{ConcurrentHashMap.contains(Object)}}|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ConcurrentHashMap.html#contains(java.lang.Object)]. They originate from the old {{Hashtable}} class. The {{java.util.Map}} interface added {{containsValue(Object)}}, which behaves exactly the same, but has the major advantage that its name makes it more clear what the method actually does.

In fact some usage of {{contains(Object)}} by Hadoop is erroneous and results in incorrect behavior, it seems the author should have used {{containsKey(Object)}} instead.

Affected:
- [{{Configuration.setDeprecatedProperties()}}|https://github.com/apache/hadoop/blob/35e4c31fff9946d35a3543481585031558e9f5d5/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L692]
Seems to be a bug, should use {{containsKey(Object)}}; looks like this was also reported in HADOOP-15468
- {{DFSClient.isDeadNode(...)}}: [here|https://github.com/apache/hadoop/blob/35e4c31fff9946d35a3543481585031558e9f5d5/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#L3400] and [here|https://github.com/apache/hadoop/blob/35e4c31fff9946d35a3543481585031558e9f5d5/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#L3404]
Not sure if correct usage, should use either {{containsKey(Object)}} or {{containsValue(Object)}}
- [{{RestClientBindings.bind(...)}}|https://github.com/apache/hadoop/blob/35e4c31fff9946d35a3543481585031558e9f5d5/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/RestClientBindings.java#L152]
Not sure if correct usage, should use either {{containsKey(Object)}} or {{containsValue(Object)}}
- [{{TestableFederationClientInterceptor.registerBadSubCluster(...)}}|https://github.com/apache/hadoop/blob/35e4c31fff9946d35a3543481585031558e9f5d5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestableFederationClientInterceptor.java#L113]
Likely a bug, should use {{containsKey(Object)}}; also detected by [LGTM / CodeQL|https://lgtm.com/projects/g/apache/hadoop/snapshot/0c9c811fe5bfda548596b99d7c111994bf66a66b/files/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestableFederationClientInterceptor.java?sort=name&dir=ASC&mode=heatmap#L113]
"
Reduce lock contention in properties of Configuration,13383106,Open,Major,,10/Jun/21 07:39,,,"The *properties* field of class *Configuration* is guard by _synchronized_, and its type java.util.Property is a sub class of java.util.*Hashtable*, which methods are all synchronized. These are bad for multithread performance.

The properties field should be replaced by a *HashMap<String, String>* with a *ReentrantReadWriteLock*, then all _get()_ related operations can be parallel guard by ReadLock, and _set/load_ operations can be guard by WriteLock. This is better for performance.

 "
Add a publish section to the .asf.yaml file,13369680,Resolved,Major,Fixed,06/Apr/21 02:23,07/Apr/21 02:34,,
"Backport to branch-3.1 HADOOP-17371, HADOOP-17621, HADOOP-17625 to update Jetty to 9.4.39",13373739,Resolved,Major,Won't Fix,20/Apr/21 03:44,10/Jun/21 08:01,3.2.3,
Reduce lock contention in org.apache.hadoop.conf.Configuration,13382815,Open,Major,,09/Jun/21 02:05,,,"There are many locks in class *Configuration.*

These locks are bad for performance.

Some locks can be removed."
Upgrade azure-storage library from v8 to v12,13382020,Open,Major,,04/Jun/21 00:55,,,"I was using the Hadoop Azure to download azure storage blobs. I found the hadoop-azure still using the v8 sdk. Do we have a plan to upgrade it to v12?

 

I tested the download speed using the v8 and v12 sdk. Using v8 sdk, the speed of downloading a blob can reach 900Mbit/s, using v12 sdk, the speed can reach almost 4GBit/s. So we might can upgrade the azure-storage sdk for getting better performance."
Release hadoop-thirdparty 1.1.1,13380437,Resolved,Major,Done,26/May/21 01:31,01/Jun/21 01:57,thirdparty-1.1.1,Release hadoop-thirdparty 1.1.1 to address HADOOP-17730.
ChecksumFileSystem does not filter .crc files in listStatusIterator,13379098,Resolved,Major,Not A Bug,18/May/21 16:28,27/May/21 06:18,,"The listStatusIterator does not filter the .crc files. It has to be overridden and implemented in ChecksumFileSystem. This affects all filesystems derived from ChecksumFileSystem, the most notable is LocalFileSystem which is used by default for file://."
"IPC client is stuck, waiting for connections map to be empty",13380076,Open,Major,,24/May/21 11:27,,2.10.1,"In our cluster we have observed when close command is issued to Datanode, It is stuck indefinitely waiting for connections map to be empty for some reason the map is not empty even after the connections have closed. [code link|https://github.com/apache/hadoop/blob/branch-2.10.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L1360]. 

Instead of infinite wait a timed wait (based on retries or timeout) would enable to shutdown the datanode in timely manner.

{code:java}
for (Connection conn : connections.values()) {
      conn.interrupt();
      conn.interruptConnectingThread();
    }
    
    // wait until all connections are closed
    while (!connections.isEmpty()) {
      try {
        Thread.sleep(STOP_SLEEP_TIME_MS);
      } catch (InterruptedException e) {
      }
    }
{code}"
Release Hadoop-Thirdparty 1.1.0,13374752,Resolved,Major,Done,24/Apr/21 00:35,20/May/21 03:49,,
Run jacc to check for Hadoop's Java API compat,13375232,Resolved,Major,Done,27/Apr/21 05:54,20/May/21 03:48,3.3.1,Run Java API Compliance Checker as part of release process to ensure compatibility.
abfs: Unable to use OAuth authentication at storage account level if the default authn type is Custom,13378080,Resolved,Major,Duplicate,12/May/21 11:43,19/May/21 09:52,3.3.0,"If we set the default auth type as Custom and then decided to use OAuth type for some select storage accounts then the fs initialization for storage accounts with Oauth type authn fails.

Steps to recreate
{code:java}
conf.set(""fs.abfss.impl"", ""org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem"")
conf.set(""fs.azure.account.auth.type"", ""Custom"")
conf.set(""fs.azure.account.oauth.provider.type"", ""xxx.yyy.zzz.ADTokenAdaptee"")
conf.set(""fs.azure.account.auth.type.abctest.dfs.core.windows.net"", ""OAuth"")
conf.set(""fs.azure.account.oauth.provider.type.abctest.dfs.core.windows.net"",
  ""org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider"")

val fs = FileSystem.get(
  new URI(""abfss://conatinerxyz@abctest.dfs.core.windows.net/arion-scribe-de-dev""),
  conf)
{code}
Error: java.lang.RuntimeException: class xxx.yyy.zzz.ADTokenAdaptee not org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider

Cause:

In [AbfsConfiguration. getTokenProvider|https://github.com/apache/hadoop/blob/aa96f1871bfd858f9bac59cf2a81ec470da649af/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java#L540] , after evaluating the auth type as OAuth, the program proceeds to get the implementing class using property `fs.azure.account.auth.type.abctest.dfs.core.windows.net`,  while doing so the first [step|https://github.com/apache/hadoop/blob/aa96f1871bfd858f9bac59cf2a81ec470da649af/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java#L321] is to get the default auth class (`fs.azure.account.oauth.provider.type`), which in our case is Custom. Here the problem is Default Auth class is CustomTokenProviderAdaptee implementation and not implementing  AccessTokenProvider.class, hence the program would fail.

proposed solution:
 In the getClass function in AbfsConfiguration, we split the logic and not use the default value property
{code:java}
public <U> Class<? extends U> getClass(String name, Class<? extends U> defaultValue, Class<U> xface) {

    Class<? extends U> klass = rawConfig.getClass(accountConf(name),
            null, xface);
    
    if(klass!=null){
        return klass;
    }else{
        return rawConfig.getClass(name, defaultValue, xface);
    }
}
{code}
 "
Use SSLFactory to initialize KeyManagerFactory for LdapGroupsMapping ,13379118,Open,Major,,18/May/21 18:13,,,"This was found when working on HADOOP-17699. The special handling for IBM JDK is not handled in LdapGroupsMapping class, that will fail in IBM JDK environment. "
Update Hadoop-thirdparty LICENSEs and NOTICEs,13378218,Resolved,Major,Fixed,13/May/21 01:23,13/May/21 05:11,thirdparty-1.1.0,
Avoid potential NPE by using Path#getParentPath API in hadoop-huaweicloud,13377135,Resolved,Major,Fixed,07/May/21 03:54,12/May/21 01:36,,"Hello,
Our code analyses found a following potential NPE:

 
{code:java}
public Path getParent() {
    String path = uri.getPath();
    int lastSlash = path.lastIndexOf('/');
    int start = startPositionWithoutWindowsDrive(path);
    if ((path.length() == start) ||               // empty path
        (lastSlash == start && path.length() == start+1)) { // at root
      return null; <--- Null returned
    }
{code}
 
{code:java}
private static void getDirectories(final String key, final String sourceKey,
      final Set<String> directories) {
    Path p = new Path(key);
    Path sourcePath = new Path(sourceKey);
    // directory must add first
    if (key.endsWith(""/"") && p.compareTo(sourcePath) > 0) {
      directories.add(p.toString());
    }
    while (p.compareTo(sourcePath) > 0) {
      p = p.getParent(); <--- NPE
      if (p.isRoot() || p.compareTo(sourcePath) == 0) {
        break;
      }
{code}
Given a root path, it will lead to NPE at method getDirectories

 

Full trace:

 

1. Return null to caller
[https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java#L432]

2. Function getParent executes and returns
[https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-cloud-storage-project/hadoop-huaweicloud/src/main/java/org/apache/hadoop/fs/obs/OBSObjectBucketUtils.java#L875]

3. The return value of function getParent is passed as the this pointer to function isRoot (the return value of function getParent can be null)
[https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-cloud-storage-project/hadoop-huaweicloud/src/main/java/org/apache/hadoop/fs/obs/OBSObjectBucketUtils.java#L876]

Commit: f40e3eb0590f85bb42d2471992bf5d524628fdd6"
Potenail NPE in org.apache.hadoop.classification.tools,13377548,Open,Major,,10/May/21 03:28,,,"Hello,
Our code analyses found the following potential NPE:
{code:java}
  private static Object process(Object obj, Class<?> type) {
    if (obj == null) {
      return null;
    }{code}
{code:java}
  public static boolean start(RootDoc root) {
    System.out.println(
        ExcludePrivateAnnotationsStandardDoclet.class.getSimpleName());
    RootDoc excludedDoc = RootDocProcessor.process(root);
    if (excludedDoc.specifiedPackages().length == 0) { // NPE
      return true;
    }
{code}
 

Full Trace:

 

1. Return null to caller
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/RootDocProcessor.java#L61

2. Return the return value of function process to caller
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/RootDocProcessor.java#L56

3. Function process executes and stores the return value to excludedDoc (excludedDoc can be null)
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java#L41

4. excludedDoc is passed as the this pointer to function com.sun.javadoc.RootDoc.specifiedPackages (excludedDoc can be null)
https://github.com/apache/hadoop/blob/f40e3eb0590f85bb42d2471992bf5d524628fdd6/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java#L42

Commit: f40e3eb0590f85bb42d2471992bf5d524628fdd6"
Fix javac warnings introduced by JUnit 4.13.1,13377004,Open,Major,,06/May/21 09:35,,,"HADOOP-17602 updated JUnit to 4.13.1, and Assert.assertThat is deprecated. It can be replaced with AssertJ (Assertions.assertThat)."
LdapGroupsMapping$LdapSslSocketFactory ClassNotFoundException,13375806,Resolved,Major,Fixed,29/Apr/21 10:25,04/May/21 14:52,3.2.2,"Using LdapGroupsMapping with SSL enabled causes ClassNotFoundException when it is called through native threads, such as Apache Impala does.

When a thread is attached to the VM, the currentThread's context classloader is null, so when jndi internally tries to use the current thread's context classloader to load the socket factory implementation, the Class.forname(String, boolean, ClassLoader) method gets a null as the loader  uses the bootstrap classloader.
 Meanwhile the LdapGroupsMapping class and the SslSocketFactory defined in it is loaded by the application classloader from its classpath.

As the bootstrap classloader does not have hadoop-common in its classpath, when a native thread tries to use/load the LdapGroupsMapping class it can't because the bootstrap loader can't load anything from hadoop-common. The correct solution seems to be to set the currentThread's context classloader to the classloader of LdapGroupsMapping class before initializing the jndi internals, and then reset to the original value after, with that we can ensure that the behaviour of other things does not change, but this failure can be avoided as well.

Attached the complete stacktrace to this Jira."
Add precommit check for Hadoop Thirdparty repository,13374993,Resolved,Major,Fixed,26/Apr/21 07:19,02/May/21 02:26,,"Not a blocker for thirparty-1.1.0 release, but it would help a lot if we have a precommit check for the hadoop-thirdparty. HBase thirdparty is a good example.

 

Reference: HBASE-24899 and [https://ci-hadoop.apache.org/view/HBase/job/HBase/job/HBase-Thirdparty-PreCommit/configure]"
Remove hadoop-shaded-jaeger from hadoop-thirdparty 1.1.0 release,13374753,Resolved,Major,Fixed,24/Apr/21 00:39,27/Apr/21 06:57,,"The hadoop-shaded-jaeger was created to produce a shaded jaegertracer artifact used by the core Hadoop code. But the core Hadoop side isn't ready to consume it, and the version we shade is too old, need update.

So regardless, we can't release as is, therefore I propose to remove this jar from hadoop-thirdparty 1.1.0 for now."
Update dependency-check-maven version,13374754,Resolved,Major,Fixed,24/Apr/21 00:41,26/Apr/21 03:54,thirdparty-1.1.0,The version of org.owasp:dependency-check-maven we use (1.4.3) is too old. The command (mvn dependency-check:aggregate) doesn't even run. Update to the latest version.
Hadoop Customed Counter,13374822,Open,Major,,24/Apr/21 14:12,,,"I have run a hadoop job and set multiple reducers in the reduce phase, then I have customize a counter to sum the results of multiple reducers. However, the data type of the result in each reducer is double, while the increment method of conuter only supports the long data type. If the conversion from double to long is forced, the precision will be lost. How can I solve this problem?"
abfs incremental listing to support many active listings,13374409,Open,Major,,22/Apr/21 13:23,,3.3.1,"Each incremental iterator submits an async fetcher operation into the JVM's common ForkJoin thread pool, which defaults to # of cores -1., unless set iin ""java.util.concurrent.ForkJoinPool.common.parallelism"";

Given the LIST calls are going to be blocking, this may puts a limit on the performance of listing if you have many threads executing list requests, e.g spark workers.

Reviewing the code, the maximum number of list operations which can collect results will be limited to the #of cores -the others are going to block until the lists have been processed.

Which may also means: if you have multiple incremental iterators in the same thread (e.g. treewalking) there's a risk that you could actually deadlock. 

I'm not convinced this will happen, as once each listing has reached the end of its directory or there are 10 pages in the result queue, the submitted operation will complete.

But: we need a test for this. Is there any public abfs store with many, many objects we could use as a source for listings, similar to the AWS landsat repo we (ab)use for such purposes in the s3a ITests?
"
"Restrict the ""-skipTrash"" param for accidentally deletes data",13372868,Open,Major,,16/Apr/21 08:20,,,"Suppose the user tries to delete the data from CLI with the ""-skipTrash"" param but by mistake, he deleted a couple of directories but actually, that directory user want to retain then their is no way to retrieve the delete data.

It will be good to have a confirm message like: ""Skip the trash for the hdfs:///dri1/file.txt files? (Y or N)"" ro we can completely disable the ""-skipTrash"" param.

 "
Backport HADOOP-17608 (Fix TestKMS failure) to branch-3.2,13373441,Resolved,Major,Done,19/Apr/21 03:17,21/Apr/21 00:58,,I see the same test failures in branch-3.2. File this jira to track the fix.
Please upgrade the log4j dependency to log4j2,13371128,Resolved,Major,Duplicate,11/Apr/21 13:29,12/Apr/21 03:53,3.3.0,"The log4j dependency being use by hadoop-common is currently version 1.2.17. Our fortify scan picked up a couple of issues with this dependency. Please upgrade it to the latest version of log4j2 dependencies:


<dependency>
 <groupId>org.apache.logging.log4j</groupId>
 <artifactId>log4j-api</artifactId>
 <version>2.14.1</version>
</dependency>

<dependency>
 <groupId>org.apache.logging.log4j</groupId>
 <artifactId>log4j-core</artifactId>
 <version>2.14.1</version>
</dependency>

 

The slf4j dependency will need to be updated as well after you upgrade log4j to log4j2.

 

 "
hadoop-auth to remove jetty-server dependency,13369483,Resolved,Major,Fixed,04/Apr/21 13:00,07/Apr/21 05:46,3.3.1,"HADOOP-17371 bumped Jetty version to 9.4.35. As part of that change, we included jetty-server in order to use a Jetty specific API to deal with a behavior change introduced by the version bump.

However, that broke applications like Solr [1]. Although the exact root cause isn't clear, it is suspected that Jetty forbids loading Jetty-server classes in the web applications. More details is in [https://portail.capsana.ca/doc/9.4.5.v20170502/jetty-classloading.html]

This JIRA aims to use J2EE compatible API to achieve the same behavior without introducing jetty-server.

[1]
{noformat}
2021-03-25 00:36:05.705 WARN  (qtp2144665602-17) [   ] o.e.j.s.HttpChannel /solr/admin/metrics => java.lang.NoClassDefFoundError: org/eclipse/
jetty/server/Response
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:632)
java.lang.NoClassDefFoundError: org/eclipse/jetty/server/Response
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:632) ~[hadoop-auth-3.1.1.7
.2.9.0-153.jar:?]
        at org.apache.solr.security.HadoopAuthFilter.doFilter(HadoopAuthFilter.java:103) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1.7.2.9.0-153 f
9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.apache.solr.security.HadoopAuthPlugin.doAuthenticate(HadoopAuthPlugin.java:250) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1.7.2.9.0
-153 f9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.apache.solr.security.AuthenticationPlugin.authenticate(AuthenticationPlugin.java:88) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1.7.
2.9.0-153 f9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.apache.solr.servlet.SolrDispatchFilter.authenticateRequest(SolrDispatchFilter.java:499) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1
.7.2.9.0-153 f9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:400) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1.7.2.9.0-15
3 f9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:346) ~[solr-core-8.4.1.7.2.9.0-153.jar:8.4.1.7.2.9.0-15
3 f9f290366013b29e885bdb9e62ad582c5fa4f29c - jenkins - 2021-03-24 18:58:11]
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1630) ~[jetty-servlet-9.4.31.v20200723.jar:9.4.31
.v20200723]
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567) ~[jetty-servlet-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602) ~[jetty-security-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v2020072
3]
        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200
723]
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v2020
0723]
        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200
723]
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v2020
0723]
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v202007
23]
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507) ~[jetty-servlet-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:191) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.rewrite.handler.RewriteHandler.handle(RewriteHandler.java:322) ~[jetty-rewrite-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.Server.handle(Server.java:501) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273) ~[jetty-server-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:540) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:395) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:161) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) ~[jetty-io-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336) ~[jetty-util-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313) ~[jetty-util-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171) ~[jetty-util-9.4.31.v20200723.jar:9.4.31.
v20200723]
        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135) ~[jetty-util-9.4.31.v20200723.jar:9.4.31.v20
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806) [jetty-util-9.4.31.v20200723.jar:9.4.31.v20200723]
        at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938) [jetty-util-9.4.31.v20200723.jar:9.4.31.v20200723]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]
Caused by: java.lang.ClassNotFoundException: org.eclipse.jetty.server.Response
        at org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:565) ~[jetty-webapp-9.4.31.v20200723.jar:9.4.31.v20200723]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_232]
        ... 46 more
{noformat}"
s3a listing IOStatistics to count #of entries returned per LIST call,13382096,Open,Minor,,04/Jun/21 09:21,,3.3.1,"
Extend IOStatistics tracking for the S3A Listing operations to count the #of object entries returned in a LIST call. This would be interesting when working with versioned buckets, to see what the actual page size of a listing is, so understand how it behaves there, and maybe get some estimate of how many old versions/tombstones are being skipped. 

MUST: add to S3AFS stats, SHOULD add to Listing"
TestS3AGetFileStatus:testNotFound() to use intercept(),13384923,Open,Minor,,21/Jun/21 14:08,,3.3.1,"The test TestS3AGetFileStatus:testNotFound() uses a JUnit rule of an ExpectedException, which is now deprecated. It should move to intercept()

This could be time to clean up that entire little test suite with a move to AssertJ asserts."
ABFS: Remove the workaround used for Http PATCH,13372607,Open,Minor,,15/Apr/21 11:07,,3.3.0,"JDK7 does not support PATCH, so to to achieve the same as a workaround we use PUT and specify the real method in the X-Http-Method-Override header.
This needs to be changed. Since we are using java 8."
ABFS: transient failure of TestAzureBlobFileSystemFileStatus.testLastModifiedTime,13373044,Open,Minor,,16/Apr/21 15:03,,3.3.1,"saw a transient failure of TestAzureBlobFileSystemFileStatus.testLastModifiedTime during a parallel (threads=8) test run against UK-west

{code}
java.lang.AssertionError: lastModifiedTime should be before createEndTime
{code}

assumption: the times are in fact equal, though the fact the assert doesn't include the values makes this hard to guarantee"
ABFS IsNamespaceEnabled shouldn't use/log exceptions as default execution path,13374090,Open,Minor,,21/Apr/21 09:59,,3.3.1,"abfs isNameSpaceEnabled() call seems to raise then swallow TrileanConversionExceptions during the default execution path, logging the whole stack at debug

this is inefficient, noisy and distracting, as now there are stack traces to ignore when debugging things.

Better: treat Trilean.UKNOWN as exactly that and react accordingly"
ABFS: rename ListResultSchemaTest to TestListResultSchema so maven tests run it,13372677,Open,Minor,,15/Apr/21 15:54,,3.3.1,"The test {{ListResultSchemaTest}} from  HADOOP-17086 won't actually be run in maven test runs because the Test- needs to come as a prefix.

Fix: rename"
ABFS: ITestAbfsStreamStatistics TestAbfsStreamOps fail with append blob on HNS account,13379479,Open,Minor,,20/May/21 10:02,,,ITestAbfsStreamStatistics -TestAbfsStreamOps fail with append blob on HNS account
ABFS: Improve test scripts,13384788,Patch Available,Minor,,21/Jun/21 07:19,,3.3.0,"Current test run scripts need manual update across all combinations in runTests.sh for account name and is working off a single azure-auth-keys.xml file. While having to test across accounts that span various geo, the config file grows big and also needs a manual change for configs such as fs.contract.test.[abfs/abfss] which has to be uniquely set. To use the script across various combinations, dev to be aware of the names of all the combinations defined in runTests.sh as well.

 

These concerns are addressed in the new version of the scripts."
ABFS: delete call sets Socket timeout lesser than query timeout leading to failures,13377439,Open,Minor,,08/May/21 14:23,,3.3.0,"ABFS Driver sets Socket timeout to 30 seconds and query timeout to 90 seconds. The client  will fail with SocketTimeoutException when the delete path has huge number of dirs/files before the actual query timeout. The socket timeout has to be greater than query timeout value. And it is good to have this timeout configurable to avoid failures when delete call takes more than the hardcoded configuration.

{code}
21/03/26 09:24:00 DEBUG services.AbfsClient: First execution of REST operation - DeletePath
.........
21/03/26 09:24:30 DEBUG services.AbfsClient: HttpRequestFailure: 0,,cid=bf4e4d0b,rid=,sent=0,recv=0,DELETE,https://prabhuAbfs.dfs.core.windows.net/general/output/_temporary?timeout=90&recursive=true
java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
        at java.net.SocketInputStream.read(SocketInputStream.java:171)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at org.wildfly.openssl.OpenSSLSocket.read(OpenSSLSocket.java:423)
        at org.wildfly.openssl.OpenSSLInputStream.read(OpenSSLInputStream.java:41)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:743)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1593)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1498)
        at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)
        at org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation.processResponse(AbfsHttpOperation.java:303)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:192)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)
        at org.apache.hadoop.fs.azurebfs.services.AbfsClient.deletePath(AbfsClient.java:462)
        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.delete(AzureBlobFileSystemStore.java:558)
        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.delete(AzureBlobFileSystem.java:339)
        at org.apache.hadoop.fs.shell.Delete$Rm.processPath(Delete.java:121)
        at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:391)
{code}


"
Avoid usage of deprecated IOUtils#cleanup API,13369484,Resolved,Minor,Fixed,04/Apr/21 13:08,06/Apr/21 05:19,3.3.1,We can replace usage of deprecated API IOUtils#cleanup() with IOUtils#cleanupWithLogger().
ABFS: Append blob tests with non HNS accounts fail,13379477,Resolved,Minor,Fixed,20/May/21 10:00,12/Jul/21 06:22,3.3.2,"Append blob tests with non HNS accounts fail.
 # The script to run the tests should ensure that append blob tests with non HNS account don't execute
 # Should have proper documentation mentioning that append blob is allowed only for HNS accounts"
Fix test failures in org.apache.hadoop.fs.azure.ITestOutputStreamSemantics,13373379,Resolved,Minor,Fixed,18/Apr/21 11:21,13/Jun/21 17:38,3.3.1,"Failures after HADOOP-13327
PageBlob and Compacting BlockBlob having only hflush and hsync capability. The test wrongly assert capability DROPBEHIND, READAHEAD, UNBUFFER"
Improve the log for The DecayRpcScheduler ,13371247,Resolved,Minor,Fixed,12/Apr/21 10:02,10/May/21 06:16,3.4.0,Improve the log for The DecayRpcScheduler to make use of the SELF4j logger factory
Keep restrict-imports-enforcer-rule for Guava Lists in hadoop-main pom,13382869,Resolved,Minor,Fixed,09/Jun/21 07:43,11/Jun/21 03:17,3.4.0,
compatibility table in directory_markers.md doesn't render right,13381990,Resolved,Minor,Fixed,03/Jun/21 19:44,15/Mar/23 17:12,3.3.1,"
The compatibility table in the directory markers doc doesn't render right; looks like what is meant to be the top and bottom of the table are being interpreted as horizontal lines, which confuses the maven site code

Renders ~OK on github
https://github.com/apache/hadoop/blob/branch-3.3.1/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/directory_markers.md"
Lock File System Creator Semaphore Uninterruptibly,13386624,Resolved,Minor,Fixed,30/Jun/21 00:58,11/Oct/22 12:26,3.3.1,"{{FileSystem}} ""creator permits"" are acquired in an interruptable way.  This changed the behavior of the call because previous callers were handling the IOException as a critical error.  An interrupt was handled in the typical {{InterruptedException}} way.  Lastly, there was no documentation of this new event so again, callers are not prepared.

Restore the previous behavior and lock the semaphore Uninterruptibly."
Configuration ${env.VAR:-FALLBACK} should eval FALLBACK when restrictSystemProps=true ,13370666,Resolved,Minor,Fixed,09/Apr/21 13:36,08/Jun/21 21:13,3.3.0,"When configuration reads in resources with a restricted parser, it skips evaluaging system ${env. } vars. But it also skips evaluating fallbacks

As a result, a property like {{fs.s3a.buffer.dir}}

{code}
${env.LOCAL_DIRS:-${hadoop.tmp.dir}} ends up evaluating as ${env.LOCAL_DIRS:-${hadoop.tmp.dir}}
{code}

It should instead fall back to the ""env var unset"" option of ${hadoop.tmp.dir}. This allows for configs (like for s3a buffer dirs) which are usable in restricted mode as well as unrestricted deployments."
Fix issue of the StatisticsDataReferenceCleaner cleanUp,13379780,Resolved,Minor,Invalid,21/May/21 14:03,17/Sep/21 15:54,3.2.1,"Cleaner thread will be blocked if we remove reference from ReferenceQueue unless the `queue.enqueue` called.
----
    As shown below, We call ReferenceQueue.remove() now while cleanUp, Call chain as follow：

                         *StatisticsDataReferenceCleaner#queue.remove()  ->  ReferenceQueue.remove(0)  -> lock.wait(0)*

    But, lock.notifyAll is called when queue.enqueue only, so Cleaner thread will be blocked.

 

ThreadDump:
{code:java}
""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f7afc088800 nid=0x2119 in Object.wait() [0x00007f7b00230000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000000c00c2f58> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
        - locked <0x00000000c00c2f58> (a java.lang.ref.Reference$Lock)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153){code}"
Distcp contract test is really slow with ABFS and S3A; timing out,13370520,Resolved,Minor,Fixed,09/Apr/21 02:35,02/Aug/21 12:33,3.4.0,"The test case testDistCpWithIterator in AbstractContractDistCpTest is consistently timing out.

 "
DistCp: distcp fail when renaming within ftp filesystem,13381677,Reopened,Minor,,02/Jun/21 11:13,,3.3.0,"When I copy file from a ftp filesystem to other ftp filesystem by distCp, there are two problems. Firstly,  the tmp target path can't be found in the exception information. After my test, it was caused by the comma. Then when the uri of target path with schema, host, port can't be rename because of the ioexception.
{code:java}
// exception information
java.io.FileNotFoundException: Source path ftp://hadoop336.photo.163.org:21/ndi-0111/56598489-3de3-4585-8426-c03435640481/.distcp.tmp.attempt_local344388648_0001_m_000000_0.1622634094600 does not existjava.io.FileNotFoundException: Source path ftp://hadoop336.photo.163.org:21/ndi-0111/56598489-3de3-4585-8426-c03435640481/.distcp.tmp.attempt_local344388648_0001_m_000000_0.1622634094600 does not exist at org.apache.hadoop.fs.ftp.FTPFileSystem.rename(FTPFileSystem.java:668) at org.apache.hadoop.fs.ftp.FTPFileSystem.rename(FTPFileSystem.java:626) at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.promoteTmpToTarget(RetriableFileCopyCommand.java:220) at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:155) at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:115) at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87) at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:258)

{code}

When I update the tempFileName from '.distcp.tmp.XXX' in  org.apache.hadoop.tools.mapred.RetriableFileCopyCommand#getTempFile to 'distcp.tmp.XXX', the question one is gone.

The targetPath can be renamed when only the path of uri exists in org.apache.hadoop.tools.mapred.RetriableFileCopyCommand#doCopy."
mvn test failed about hadoop@3.2.1,13386730,Resolved,Minor,Works for Me,30/Jun/21 09:36,06/Aug/21 11:08,3.2.1,"{panel:title=mvn test failed about hadoop@3.2.1}
mvn test failed
{panel}
[root@localhost spack-src]# mvn -version
Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: /home/all_spack_env/spack/opt/spack/linux-centos8-aarch64/gcc-8.4.1/maven-3.6.3-fpgpwvz7es5yiaz2tez2pnlilrcatuvg
Java version: 1.8.0_191, vendor: AdoptOpenJdk, runtime: /home/all_spack_env/spack/opt/spack/linux-centos8-aarch64/gcc-8.4.1/openjdk-1.8.0_191-b12-fidptihybskgklbjoo4lagkacm6n6lod/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: ""linux"", version: ""4.18.0-80.el8.aarch64"", arch: ""aarch64"", family: ""unix""
[root@localhost spack-src]# java -version
openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.191-b12, mixed mode)
[root@localhost spack-src]# mvn test
……
[INFO] Running org.apache.hadoop.tools.TestCommandShell
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.111 s - in org.apache.hadoop.tools.TestCommandShell
[INFO]
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   TestFSMainOperationsLocalFileSystem>FSMainOperationsBaseTest.testGlobStatusThrowsExceptionForUnreadableDir:643 Should throw IOException
[ERROR]   TestFSMainOperationsLocalFileSystem>FSMainOperationsBaseTest.testListStatusThrowsExceptionForUnreadableDir:288 Should throw IOException
[ERROR]   TestFileUtil.testFailFullyDelete:446->validateAndSetWritablePermissions:422 The directory xSubDir *should* not have been deleted. expected:<true> but was:<false>
[ERROR]   TestFileUtil.testFailFullyDeleteContents:525->validateAndSetWritablePermissions:422 The directory xSubDir *should* not have been deleted. expected:<true> but was:<false>
[ERROR]   TestFileUtil.testGetDU:571
[ERROR]   TestFsShellCopy.testPutSrcDirNoPerm:627->shellRun:80 expected:<1> but was:<0>
[ERROR]   TestFsShellCopy.testPutSrcFileNoPerm:652->shellRun:80 expected:<1> but was:<0>
[ERROR]   TestLocalDirAllocator.test0:140->validateTempDirCreation:109 Checking for build/test/temp/RELATIVE1 in build/test/temp/RELATIVE0/block995011826146306285.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.test0:140->validateTempDirCreation:109 Checking for /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE1 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE0/block792666236482175348.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.test0:141->validateTempDirCreation:109 Checking for file:/home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED1 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED0/block5124616846677903649.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testROBufferDirAndRWBufferDir:162->validateTempDirCreation:109 Checking for build/test/temp/RELATIVE2 in build/test/temp/RELATIVE1/block1176062344115776027.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testROBufferDirAndRWBufferDir:163->validateTempDirCreation:109 Checking for /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE2 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE1/block3514694215643608527.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testROBufferDirAndRWBufferDir:163->validateTempDirCreation:109 Checking for file:/home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED2 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED1/block883026101475466701.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testRWBufferDirBecomesRO:219->validateTempDirCreation:109 Checking for build/test/temp/RELATIVE3 in build/test/temp/RELATIVE4/block2198073115547564040.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testRWBufferDirBecomesRO:219->validateTempDirCreation:109 Checking for /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE3 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE4/block4187087898130713397.tmp - FAILED!
[ERROR]   TestLocalDirAllocator.testRWBufferDirBecomesRO:219->validateTempDirCreation:109 Checking for file:/home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED3 in /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED4/block7779116721351278125.tmp - FAILED!
[ERROR]   TestLocalFileSystem.testReportChecksumFailure:390
[ERROR]   TestPathData.testGlobThrowsExceptionForUnreadableDir:230 Should throw IOException
[ERROR]   TestFSMainOperationsLocalFileSystem>FSMainOperationsBaseTest.testGlobStatusThrowsExceptionForUnreadableDir:643 Should throw IOException
[ERROR]   TestFSMainOperationsLocalFileSystem>FSMainOperationsBaseTest.testListStatusThrowsExceptionForUnreadableDir:288 Should throw IOException
[ERROR]   TestSharedFileDescriptorFactory.testDirectoryFallbacks:103
[ERROR]   TestRollingFileSystemSinkWithLocal.testFailedWrite:117 No exception was generated while writing metrics even though the target directory was not writable
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notListable:131->TestDiskChecker._checkDirs:164 checkDir success, expected failure
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notListable_local:200->checkDirs:40 call to checkDir() succeeded.
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notReadable:121->TestDiskChecker._checkDirs:164 checkDir success, expected failure
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notReadable_local:190->checkDirs:40 call to checkDir() succeeded.
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notWritable:126->TestDiskChecker._checkDirs:164 checkDir success, expected failure
[ERROR]   TestBasicDiskValidator>TestDiskChecker.testCheckDir_notWritable_local:195->checkDirs:40 call to checkDir() succeeded.
[ERROR]   TestDiskChecker.testCheckDir_notListable:131->_checkDirs:164 checkDir success, expected failure
[ERROR]   TestDiskChecker.testCheckDir_notListable_local:200->checkDirs:210 checkDir success, expected failure
[ERROR]   TestDiskChecker.testCheckDir_notReadable:121->_checkDirs:164 checkDir success, expected failure
[ERROR]   TestDiskChecker.testCheckDir_notReadable_local:190->checkDirs:210 checkDir success, expected failure
[ERROR]   TestDiskChecker.testCheckDir_notWritable:126->_checkDirs:164 checkDir success, expected failure
[ERROR]   TestDiskChecker.testCheckDir_notWritable_local:195->checkDirs:210 checkDir success, expected failure
[ERROR]   TestReadWriteDiskValidator.testCheckFailures:127 Disk check should fail.
[ERROR] Errors:
[ERROR]   TestNativeIO.testMultiThreadedStat:249 ? Execution java.lang.IllegalArgumentEx...
[ERROR]   TestNativeIO.testStat:183->doStatTest:205 ? IllegalArgument length != 10(unixS...
[ERROR]   TestRPCCallBenchmark.testBenchmarkWithProto:30->Object.wait:-2 ?  test timed o...
[ERROR]   TestShell.testEnvVarsWithInheritance:159->testEnvHelper:180 ? StringIndexOutOfBounds
[INFO]
[ERROR] Tests run: 4192, Failures: 35, Errors: 4, Skipped: 252
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Common 3.2.1:
[INFO]
[INFO] Apache Hadoop Common ............................... FAILURE [24:57 min]
[INFO] Apache Hadoop NFS .................................. SKIPPED
[INFO] Apache Hadoop KMS .................................. SKIPPED
[INFO] Apache Hadoop Common Project ....................... SKIPPED
[INFO] Apache Hadoop HDFS Client .......................... SKIPPED
[INFO] Apache Hadoop HDFS ................................. SKIPPED
[INFO] Apache Hadoop HDFS Native Client ................... SKIPPED
[INFO] Apache Hadoop HttpFS ............................... SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................. SKIPPED
[INFO] Apache Hadoop HDFS-RBF ............................. SKIPPED
[INFO] Apache Hadoop HDFS Project ......................... SKIPPED
[INFO] Apache Hadoop YARN ................................. SKIPPED
[INFO] Apache Hadoop YARN API ............................. SKIPPED
[INFO] Apache Hadoop YARN Common .......................... SKIPPED
[INFO] Apache Hadoop YARN Registry ........................ SKIPPED
[INFO] Apache Hadoop YARN Server .......................... SKIPPED
[INFO] Apache Hadoop YARN Server Common ................... SKIPPED
[INFO] Apache Hadoop YARN NodeManager ..................... SKIPPED
[INFO] Apache Hadoop YARN Web Proxy ....................... SKIPPED
[INFO] Apache Hadoop YARN ApplicationHistoryService ....... SKIPPED
[INFO] Apache Hadoop YARN Timeline Service ................ SKIPPED
[INFO] Apache Hadoop YARN ResourceManager ................. SKIPPED
[INFO] Apache Hadoop YARN Server Tests .................... SKIPPED
[INFO] Apache Hadoop YARN Client .......................... SKIPPED
[INFO] Apache Hadoop YARN SharedCacheManager .............. SKIPPED
[INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Backend ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Common .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Client .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Servers ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase tests ..... SKIPPED
[INFO] Apache Hadoop YARN Router .......................... SKIPPED
[INFO] Apache Hadoop YARN Applications .................... SKIPPED
[INFO] Apache Hadoop YARN DistributedShell ................ SKIPPED
[INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SKIPPED
[INFO] Apache Hadoop MapReduce Client ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Core ....................... SKIPPED
[INFO] Apache Hadoop MapReduce Common ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Shuffle .................... SKIPPED
[INFO] Apache Hadoop MapReduce App ........................ SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer .............. SKIPPED
[INFO] Apache Hadoop MapReduce JobClient .................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ......................... SKIPPED
[INFO] Apache Hadoop YARN Services ........................ SKIPPED
[INFO] Apache Hadoop YARN Services Core ................... SKIPPED
[INFO] Apache Hadoop YARN Services API .................... SKIPPED
[INFO] Apache Hadoop Image Generation Tool ................ SKIPPED
[INFO] Yet Another Learning Platform ...................... SKIPPED
[INFO] Apache Hadoop YARN Site ............................ SKIPPED
[INFO] Apache Hadoop YARN UI .............................. SKIPPED
[INFO] Apache Hadoop YARN Project ......................... SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SKIPPED
[INFO] Apache Hadoop MapReduce NativeTask ................. SKIPPED
[INFO] Apache Hadoop MapReduce Uploader ................... SKIPPED
[INFO] Apache Hadoop MapReduce Examples ................... SKIPPED
[INFO] Apache Hadoop MapReduce ............................ SKIPPED
[INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED
[INFO] Apache Hadoop Distributed Copy ..................... SKIPPED
[INFO] Apache Hadoop Archives ............................. SKIPPED
[INFO] Apache Hadoop Archive Logs ......................... SKIPPED
[INFO] Apache Hadoop Rumen ................................ SKIPPED
[INFO] Apache Hadoop Gridmix .............................. SKIPPED
[INFO] Apache Hadoop Data Join ............................ SKIPPED
[INFO] Apache Hadoop Extras ............................... SKIPPED
[INFO] Apache Hadoop Pipes ................................ SKIPPED
[INFO] Apache Hadoop OpenStack support .................... SKIPPED
[INFO] Apache Hadoop Amazon Web Services support .......... SKIPPED
[INFO] Apache Hadoop Kafka Library support ................ SKIPPED
[INFO] Apache Hadoop Azure support ........................ SKIPPED
[INFO] Apache Hadoop Aliyun OSS support ................... SKIPPED
[INFO] Apache Hadoop Client Aggregator .................... SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED
[INFO] Apache Hadoop Resource Estimator Service ........... SKIPPED
[INFO] Apache Hadoop Azure Data Lake support .............. SKIPPED
[INFO] Apache Hadoop Tools Dist ........................... SKIPPED
[INFO] Apache Hadoop Tools ................................ SKIPPED
[INFO] Apache Hadoop Client API ........................... SKIPPED
[INFO] Apache Hadoop Client Runtime ....................... SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED
[INFO] Apache Hadoop Client Test Minicluster .............. SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED
[INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED
[INFO] Apache Hadoop Distribution ......................... SKIPPED
[INFO] Apache Hadoop Client Modules ....................... SKIPPED
[INFO] Apache Hadoop Cloud Storage ........................ SKIPPED
[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  24:59 min
[INFO] Finished at: 2021-06-30T17:24:03+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-common: There are test failures.
[ERROR]
[ERROR] Please refer to /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-common: There are test failures.

Please refer to /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/target/surefire-reports for the individual test results.
Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures.

Please refer to /home/all_spack_env/spack_stage/root/spack-stage-hadoop-3.2.1-xvpobktnlicqhfzwbkriy4cick5tpsab/spack-src/hadoop-common-project/hadoop-common/target/surefire-reports for the individual test results.
Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
    at org.apache.maven.plugin.surefire.SurefireHelper.throwException (SurefireHelper.java:271)
    at org.apache.maven.plugin.surefire.SurefireHelper.reportExecution (SurefireHelper.java:159)
    at org.apache.maven.plugin.surefire.SurefirePlugin.handleSummary (SurefirePlugin.java:362)
    at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked (AbstractSurefireMojo.java:1007)
    at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute (AbstractSurefireMojo.java:837)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[ERROR]
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

----
Can you help me find out what caused it and how to solve it?

"
"ABFS: testBlobBackCompatibility, testRandomRead & WasbAbfsCompatibility tests fail when triggered with default configs",13379475,Resolved,Minor,Fixed,20/May/21 09:56,12/Jul/21 06:24,,"testBlobBackCompatibility, testRandomRead & WasbAbfsCompatibility tests fail when triggered with default configs as http is not enabled on gen2 accounts by default.

 

Options to fix it:

tests' config should enforce https by default 

or the tests should be modified not execute http requests

 "
Path.suffix() raises an exception when the path is in the root dir,13381969,Open,Minor,,03/Jun/21 17:53,,3.3.1,"Root cause of SPARK-34298.

If you have a Path (/something) and call suffix on it.

{code}
 new Path(""/something"").suffix(""else"")
{code}
you see an error because the path doesn't have a parent

{code}
Exception in thread ""main"" java.lang.IllegalArgumentException: Can not create a Path from an empty string
at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)[....]
at org.apache.hadoop.fs.Path.suffix(Path.java:446)
{code}"
abfs test failure ITestSmallWriteOptimization.testSmallWriteOptimization,13380776,Resolved,Minor,Cannot Reproduce,27/May/21 13:28,27/May/21 13:34,3.4.0,"(transient) failure of ITestSmallWriteOptimization.testSmallWriteOptimization; Azure cardiff over a long-haul link

This is on my manifest committer branch -but I've not done any changes to input streams there

{code}
[ERROR] Failures: 
[ERROR]   ITestSmallWriteOptimization.testSmallWriteOptimization:324->formulateSmallWriteTestAppendPattern:437->assertOpStats:499->AbstractAbfsIntegrationTest.assertAbfsStatistics:453->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 Mismatch in connections_made expected:<7> but was:<8>
{code}
"
S3AFS and ABFS to log IOStats at DEBUG mode or optionally at INFO level in close(),13375230,Resolved,Minor,Fixed,27/Apr/21 05:49,24/May/21 12:15,,Adding a property to log IOStats in close() for S3AFS and ABFS at DEBUG level by default or optionally at INFO level. 
FilterFileSystem to implement and forward getIOStatistics(),13379311,Open,Minor,,19/May/21 14:12,,3.3.1,FilterFileSystem should forward getIOStatistics() to nested FS if it implements the interface too. This will allow callers to get those statistics.
HDFS rebalance commands,13378733,Resolved,Minor,Invalid,17/May/21 07:36,18/May/21 00:05,,"Team ,

Having 3 node cluster and one of the node 100% disk utilized . when i am trying to run rebalance commands by cli using (hdfs balancer -source 192.168.x.x) and even from ambari its not releasing space . Could you please  help me on this "
Improve rendering of XML configuration files,13378461,Open,Minor,,14/May/21 10:45,,,XML configuration files could be rendered more readable in web browsers by adding some CSS to the XSL. Cf. [Nutch 1.18 properties file|https://nutch.apache.org/apidocs/apidocs-1.18/resources/nutch-default.xml] and [configuration.xsl|https://github.com/apache/nutch/blob/master/conf/configuration.xsl].
"Received ""Permission denied"" error while copying files through NFS Gateway",13376504,Open,Minor,,04/May/21 07:59,,,"In this case, while the file was copying through NFS gateway, the below permission error reports.
2021-03-29 05:19:13,389 INFO org.apache.hadoop.security.ShellBasedIdMapping: Update cache now
2021-03-29 05:19:13,399 WARN org.apache.hadoop.security.ShellBasedIdMapping: Can't find user name for uid 5999. Use default user name nobody
2021-03-29 05:19:13,412 INFO org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: Error writing to fileId 10131842296 at offset 3254190080 and length 32768
org.apache.hadoop.security.AccessControlException: Permission denied: user=nobody, access=EXECUTE, inode=""/xxx"":test:test:drwxrwx---

There is a map file exist, but never got changed.

Between Update Cache and Clear, some race conditions cause the unavailability of the user/group mapping from cache?"
Fix release change log url error,13374872,Resolved,Minor,Fixed,25/Apr/21 07:55,26/Apr/21 03:49,3.2.2,Web site release changelog url 404
ITestWasbUriAndConfiguration.testCanonicalServiceName() failing now mockaccount exists,13373102,Resolved,Minor,Fixed,16/Apr/21 18:10,20/Apr/21 16:25,3.2.1,"The test ITestWasbUriAndConfiguration.testCanonicalServiceName() is failing in its intercept

[ERROR]   ITestWasbUriAndConfiguration.testCanonicalServiceName:656 Expected a java.lang.IllegalArgumentException to be thrown, but got the result: : ""20.38.122.132:0""

Root cause is: the mock account in AzureBlobStorageTestAccount.MOCK_ACCOUNT_NAME is ""mockAccount.blob.core.windows.net"" and *someone has created that account*

This means it resolves
 nslookup mockAccount.blob.core.windows.net
Server:		172.18.64.15
Address:	172.18.64.15#53

Non-authoritative answer:
mockAccount.blob.core.windows.net	canonical name = blob.dsm08prdstr02a.store.core.windows.net.
Name:	blob.dsm08prdstr02a.store.core.windows.net
Address: 20.38.122.132
"
Replace Sets#newHashSet() and newTreeSet() with constructors directly,13379763,In Progress,Trivial,,21/May/21 12:57,,3.3.5," 

As part of removing guava dependencies  HADOOP-17115, HADOOP-17721, HADOOP-17722 and HADOOP-17720 are fixed,

Currently the code call util function to create HashSet and TreeSet in the repo . These function calls dont have much importance as it is calling internally new HashSet<> / new TreeSet<> from java.utils 

This task is to clean up all the function calls to create sets which is redundant "
Remove useless property hadoop.assemblies.version in pom file,13374854,Resolved,Trivial,Fixed,25/Apr/21 03:25,20/May/21 01:51,3.3.1,"The property hadoop.assemblies.version in hadoop-project/pom.xml is not used anywhere.

 

Remove it to avoid confusions. (As a matter of fact, this property was not updated in branch-3.3 and it remains to be 3.3.0-SNAPSHOT)"
Fix for Javac Error of Unchecked Cast,13531248,Open,Blocker,,03/Apr/23 11:14,,,"This task is to fix javac unchecked cast error on yetus build.

Yetus checks on PR are failing at javac step with the error:
*""hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/TokenUtils.java:43:53:[unchecked] unchecked cast""*

This is blocking other PRs in the branches 3.3.3, 3.3.4, 3.3.2
This task will fix this error/warning and unblock other PRs.

Blocked PRs:
1. [Backport HADOOP-18546: ABFS: Disable purging list of in progress reads in abfs stream closed in 3.3.2 by anujmodi2021 · Pull Request #5529 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/5529]
2. [Backport HADOOP-18546: ABFS: Disable purging list of in progress reads in abfs stream closed in 3.3.3 by anujmodi2021 · Pull Request #5530 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/5530]
3. [Backport HADOOP-18546: ABFS: Disable purging list of in progress reads in abfs stream closed in 3.3.4 by anujmodi2021 · Pull Request #5531 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/5531]"
Remove mysql-connector-java,13539348,Resolved,Blocker,Fixed,09/Jun/23 01:04,12/Jun/23 22:33,,"While preparing for 3.3.6 RC, I realized the mysql-connector-java dependency added by HADOOP-18535 is GPL licensed.


Source: https://github.com/mysql/mysql-connector-j/blob/release/8.0/LICENSE 
See legal discussion at LEGAL-423.

I looked at the original jira and github PR and I don't think the license issue was noticed. 

Is it possible to get rid of the mysql connector dependency? As far as I can tell the dependency is very limited.

If not, I guess I'll have to revert the commits for now."
AWS v2 SDK: stabilise dependencies with rest of hadoop libraries,13536540,Resolved,Blocker,Not A Problem,17/May/23 10:31,24/Aug/23 17:23,," aws v2 sdk dependencies need to 

# be in sync with rest of hadoop
# not include anything coming in from hadoop common
# not export in hadoop-cloud-storage stuff from hadoop-common.

currently it is pulling in a version of jackson cbor whose consistency with hadoop's import is simply luck, joda time is also there

In an ideal world all this should be shaded: we cannot have the AWS sdk dictate what jackson version we ship with, given the history of downstream problems there.

{code}
[INFO] +- com.amazonaws:aws-java-sdk-core:jar:1.12.316:compile
[INFO] |  +- software.amazon.ion:ion-java:jar:1.0.2:compile
[INFO] |  +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.12.7:compile
[INFO] |  \- joda-time:joda-time:jar:2.8.1:compile
[INFO] +- software.amazon.awssdk:bundle:jar:2.19.12:compile
[INFO] |  \- software.amazon.eventstream:eventstream:jar:1.0.1:compile
[INFO] +- software.amazon.awssdk.crt:aws-crt:jar:0.21.0:compile

{code}
"
3.3.6 Release NOTICE and LICENSE file update,13539347,Open,Blocker,,09/Jun/23 00:34,,3.3.6,"As far as I can tell looking at hadoop-project/pom.xml the only difference between 3.3.5 and 3.3.6 from a dependency point of view is mysql connector (HADOOP-18535) derby (HADOOP-18535, HADOOP-18693).

Json-smart, snakeyaml and jetty, jettison are updated in LICENSE-binary already. grizzly was used in test scope only so its removal doesn't matter."
Hadoop build depends on archives.apache.org,13541622,Resolved,Critical,Fixed,27/Jun/23 18:38,19/Aug/24 10:53,3.3.6,"Several times throughout Hadoop's source, the ASF archive is referenced, including part of the build that downloads Yetus.

Building a release from source should not require access to the ASF archives, as that contributes to end users being subject to throttling and blocking by INFRA, for ""abuse"" of the archives, even though they are merely building a current ASF release from source. This is particularly problematic for downstream packagers who must build from Hadoop's source, or for CI/CD situations that depend on Hadoop's source, and particularly problematic for those end users behind a NAT gateway, because even if Hadoop's use of the archive is modest, it adds up for multiple users.

The build should be modified, so that it does not require access to fixed versions in the archives (or should work with the upstream of those dependent projects to publish their releases elsewhere, for routine consumptions). In the interim, the source could be updated to point to the current dependency versions available on downloads.apache.org."
"[JDK-17] Failed unit tests , with Java 17 runtime and compiled Java 8",13533778,Open,Critical,,24/Apr/23 07:21,,,"Compiled Hadoop - Hadoop branch 3.3.3

mvn clean install - DskipTests=True

Java_Home ->  points to Java-8

maven version - 3.8.8 (Quite latest)

 

Ran various whole test suit on my private cloud environment -  

Changed Java_Home to   Java-17 

 

mvn surefire:test 

 

Out of 22k tests - 2.5 k tests failed ."
Create qbt.sh symlink on Windows,13535269,Resolved,Critical,Fixed,06/May/23 12:29,08/May/23 16:56,3.4.0,"The hadoop-common project fails when mvnsite is built while running the shelldocs plugin on Windows 10 -

{code}
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.1:exec (shelldocs) on project hadoop-common: Command execution failed. Process exited with an error: 1 (Exit value: 1) -> [Help 1]
{code}

This being the reason -

{code}
[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---
tar: apache-yetus-0.14.0/lib/precommit/qbt.sh: Cannot create symlink to 'test-patch.sh': No such file or directory
tar: Exiting with failure status due to previous errors
ERROR: apache-yetus-0.14.0-bin.tar.gz is corrupt. Investigate and then remove /c/hadoop/patchprocess to try again.
{code}

The apache-yetus-0.14.0 tarball contains a symlink *qbt.sh*. Unzipping this tarball fails to create the qbt.sh symlink since the creation of symlink is limited to Admin or when the developer mode is enabled.

The solution here is to use the *ln* command to create the symlink and move it to the required target location"
Fix incorrect output path in javadoc build phase,13537522,Resolved,Critical,Fixed,24/May/23 17:45,26/Jun/23 22:52,3.4.0,"The javadoc build phase fails with the following error -

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc-no-fork (default-cli) on project hadoop-common: An error has occurred in Javadoc report generation: Unable to write 'options' temporary file for command execution: H:\hadoop-common-project\hadoop-common\target\site\H:\hadoop-common-project\hadoop-common\target\api\options (The filename, directory name, or volume label syntax is incorrect) -> [Help 1]
{code}

As called out by the error message the path *H:\hadoop-common-project\hadoop-common\target\site\H:\hadoop-common-project\hadoop-common\target\api\options* is invalid.

The culprit being - https://github.com/apache/hadoop/blob/e9740cb17aef157a615dc36ae08cd224ce1672f0/hadoop-project-dist/pom.xml#L109

{code}
<reportOutputDirectory>${project.build.directory}/site</reportOutputDirectory>
<destDir>${project.build.directory}/api</destDir>
{code}

As per the [docs from maven-javadoc-plugin|https://maven.apache.org/plugins/maven-javadoc-plugin/examples/output-configuration.html], *destDir* attribute's value gets appended to that of *reportOutputDirectory*. This implies that *destDir* must be a relative path, although not called out in the documentation. Since this isn't the case here,
# In Linux, this yields an unintended path (albeit a valid one) and doesn't fail.
# In Windows, it yields an incorrect path and thus fails since there's a colon ( : ) for the drive letter in the middle of the incorrectly concatenated path -
H:\hadoop-common-project\hadoop-common\target\site\H {color:red}*:*{color} \hadoop-common-project\hadoop-common\target\api\options

Thus, fixing this would fix the build failure on Windows and put the docs in the appropriate directory in Linux."
Open file fails with NumberFormatException for S3AFileSystem,13534466,Resolved,Critical,Fixed,28/Apr/23 13:13,16/May/23 12:42,3.3.5,"Saw the trace for Hive-Iceberg, was using the old client, doesn't happen once I upgraded.
{noformat}
Caused by: java.lang.NumberFormatException: For input string: ""5783.0""
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.base/java.lang.Long.parseLong(Long.java:692)
	at java.base/java.lang.Long.parseLong(Long.java:817)
	at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1601)
	at org.apache.hadoop.fs.s3a.impl.OpenFileSupport.prepareToOpenFile(OpenFileSupport.java:262)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.openFileWithOptions(S3AFileSystem.java:5219)
	at org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4753)
	at org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:196)
	at org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:101)
	at org.apache.iceberg.avro.AvroIterable.getMetadata(AvroIterable.java:66){noformat}"
Fix mvnsite on Windows 10,13535003,Resolved,Critical,Fixed,04/May/23 14:17,05/May/23 20:09,3.4.0,"The mvnsite step fails to build on Windows 10 due to the following error -

[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3.1:exec (shelldocs) on project hadoop-common: Command execution failed. Cannot run program ""C:\hadoop\hadoop-common-project\hadoop-common\..\..\dev-support\bin\shelldocs"" (in directory ""C:\hadoop\hadoop-common-project\hadoop-common\src\site\markdown""): CreateProcess error=193, %1 is not a valid Win32 application -> [Help 1]

shelldocs is a bash script which Windows can't execute natively. Thus, we need to run this through bash on Windows 10."
AWS SDK V2 - Implement CSE,13533219,Resolved,Major,Fixed,19/Apr/23 12:46,17/Nov/24 04:25,3.4.0,"S3 Encryption client for SDK V2 is now available, so add client side encryption back in. 

When backporting, follow up with
*  HADOOP-19336
* HADOOP-19349"
"Change fs.s3a.directory.marker.retention to ""keep""",13537523,Resolved,Major,Fixed,24/May/23 17:53,08/Jun/23 11:22,3.3.5,"Change the default value of ""fs.s3a.directory.marker.retention"" to keep; update docs to match.

maybe include with HADOOP-17802 so we don't blow up with fewer markers being created."
zk 3.3.6 adds in slf-log4j dependencies to hadoop-common,13541809,Resolved,Major,Fixed,28/Jun/23 14:19,10/Dec/24 14:21,3.3.9,"seeing this building against the hadoop aws v2 + branch-33 feature branch, so if it's been fixed in the main please close.

My downstream code is printing errors in test run because of conflicting slf4j appenders. the root cause appears to be that zk 3.3.6 is pulling in log4j rather than relog4j; both end up on the classpath downstream for much pain.

proposed: exclude it"
Client.Connection#updateAddress needs to ensure that address is resolved before updating,13532051,Resolved,Major,Fixed,10/Apr/23 13:47,09/Dec/24 03:22,3.3.5,"When Client.Connection#setupConnection encounters an IOException, it will try to update the server address. ([HADOOP-18365|https://issues.apache.org/jira/browse/HADOOP-18365])

When the address is re-parsed, it may be an unresolved address (UnknownHostException), which causes Client.Connection#setupConnection to fail to reconnect.
{code:java}
while (true) {
  try {
    if (server.isUnresolved()) { {code}
Especially when DN is connected to NN, BPServiceActor#bpNamenode is only initialized once, which causes DN to never connect to NN before restarting."
S3A: Test failures with CSE enabled,13540749,Resolved,Major,Cannot Reproduce,20/Jun/23 12:44,03/Dec/24 14:13,,"The following tests fail when run hadoop-aws suite is run with CSE enabled:

 

{{ITestS3APrefetchingInputStream.testRandomReadLargeFile}}
{{ITestS3APrefetchingInputStream.testReadLargeFileFully}}
{{ITestS3APrefetchingInputStream.testReadLargeFileFullyLazySeek}}
{{ITestS3ARequesterPays.testRequesterPaysOptionSuccess}}
{{ITestAssumeRole.testReadOnlyOperations }}
{{ITestPartialRenamesDeletes.testRenameParentPathNotWriteable}}
{{ITestPartialRenamesDeletes.testRenameParentPathNotWriteable}}
{{ITestS3GuardTool.testLandsatBucketRequireUnencrypted}}

 

Most of these are because they're using landsat data which is not encrypted, so trying to read with a CSE will fail. These tests should be skipped if using CSE.

 "
ITestS3ABlockOutputArray failure with IO File name too long,13536711,Open,Major,,18/May/23 08:50,,,"On an EC2 instance, the following tests are failing:

 

{{{}ITestS3ABlockOutputArray.testDiskBlockCreate{}}}{{{}ITestS3ABlockOutputByteBuffer>ITestS3ABlockOutputArray.testDiskBlockCreate{}}}{{{}ITestS3ABlockOutputDisk>ITestS3ABlockOutputArray.testDiskBlockCreate{}}}

 

with the error IO File name too long. 

 

The tests create a file with a 1024 char file name and rely on File.createTempFile() to truncate the file name to < OS limit. 

 

Stack trace:

{{Java.io.IOException: File name too long}}
{{    at java.io.UnixFileSystem.createFileExclusively(Native Method)}}
{{    at java.io.File.createTempFile(File.java:2063)}}
{{    at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:1377)}}
{{    at org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:829)}}
{{    at org.apache.hadoop.fs.s3a.ITestS3ABlockOutputArray.testDiskBlockCreate(ITestS3ABlockOutputArray.java:114)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)}}
{{    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)}}"
[ABFS][Backoff-Optimization] Have a Static retry policy for connection timeout failures,13538585,Resolved,Major,Fixed,02/Jun/23 11:35,16/May/24 15:47,3.3.4,"Today when a request fails with connection timeout, it falls back into the loop for exponential retry. Unlike Azure Storage, there are no guarantees of success on exponentially retried request or recommendations for ideal retry policies for Azure network or any other general failures. Faster failure and retry might be more beneficial for such generic connection timeout failures. 

This PR introduces a new Static Retry Policy which will currently be used only for Connection Timeout failures. It means all the requests failing with Connection Timeout errors will be retried after a constant retry(sleep) interval independent of how many times that request has failed. Max Retry Count check will still be in place.

Following Configurations will be introduced in the change:
 # ""fs.azure.static.retry.for.connection.timeout.enabled"" - default: true, true: static retry will be used for CT, false: Exponential retry will be used.
 # ""fs.azure.static.retry.interval"" - default: 1000ms.

This also introduces a new field in x-ms-client-request-id only for the requests that are being retried after connection timeout failure. New filed will tell what retry policy was used to get the sleep interval before making this request.
Header ""x-ms-client-request-id "" right now has only the retryCount and retryReason this particular API call is. For ex:  :eb06d8f6-5693-461b-b63c-5858fa7655e6:29cb0d19-2b68-4409-bc35-cb7160b90dd8:::CF:1_CT.

Moving ahead for retryReason ""CT"" it will have retry policy abbreviation as well.
For ex:  :eb06d8f6-5693-461b-b63c-5858fa7655e6:29cb0d19-2b68-4409-bc35-cb7160b90dd8:::CF:1_CT_E."
Optimise S3A delete objects when multiObjectDelete is disabled,13534168,Open,Major,,26/Apr/23 14:29,,3.3.6,"Currently, for doing a bulk delete in S3A, we rely on multiObjectDelete call, but when this property is disabled we delete one key at a time. We can optimize this scenario by adding parallelism."
upgrade nimbus jwt jar due to issues in its embedded shaded json-smart code,13533280,Resolved,Major,Fixed,19/Apr/23 19:19,22/Apr/23 08:32,3.4.0,https://github.com/apache/hadoop/pull/5549#issuecomment-1515174820
ABFS backReference passed down to streams to avoid GC closing the FS.,13541108,Resolved,Major,Fixed,23/Jun/23 04:03,12/Jul/23 05:04,,"Applications using AzureBlobFileSystem to create the AbfsOutputStream can use the AbfsOutputStream for the purpose of writing, however, the OutputStream doesn't hold any reference to the fs instance that created it, which can make the FS instance eligible for GC, when this occurs, AzureblobFileSystem's `finalize()` method gets called which in turn closes the FS, and in turn call the close for AzureBlobFileSystemStore, which uses the same Threadpool that is used by the AbfsOutputStream. This leads to the closing of the thread pool while the writing is happening in the background and leads to hanging while writing.

 

*Solution:*
Pass a backreference of AzureBlobFileSystem into AzureBlobFileSystemStore and AbfsOutputStream as well.

 

Same should be done for AbfsInputStream as well."
Upgrade guava in hadoop-thirdparty to 31.1-jre,13534158,Resolved,Major,Fixed,26/Apr/23 13:59,02/May/23 12:59,thirdparty-1.2.0,Upgrade guava in hadoop-thirdparty to 31.1-jre to be in sync with the downstream project Tez
Bump jettison from 1.5.3 to 1.5.4 in /hadoop-project,13531092,Resolved,Major,Fixed,01/Apr/23 19:16,22/Apr/23 10:50,3.3.9,"PR from github depandabot 
https://github.com/apache/hadoop/pull/5502

Mentions CVE: https://nvd.nist.gov/vuln/detail/CVE-2023-1436

Creating ticket for tracking."
Add a CallerContext getter on the Schedulable interface,13531765,Resolved,Major,Fixed,06/Apr/23 17:41,20/Apr/23 17:12,3.3.6,"We would like to add a default *{color:#00875a}CallerContext{color}* getter on the *{color:#00875a}Schedulable{color}* interface
{code:java}
default public CallerContext getCallerContext() {
  return null;  
} {code}
and then override it on the *{color:#00875a}i{color}{color:#00875a}{*}pc/{*}Server.Call{color}* class
{code:java}
@Override
public CallerContext getCallerContext() {  
  return this.callerContext;
} {code}
to expose the already existing *{color:#00875a}callerContext{color}* field.

 

This change will help us access the *{color:#00875a}CallerContext{color}* on an Apache Ozone *{color:#00875a}IdentityProvider{color}* implementation.

On Ozone side the *{color:#00875a}FairCallQueue{color}* doesn't work with the Ozone S3G, because all users are masked under a special S3G user and there is no impersonation. Therefore, the FCQ reads only 1 user and becomes ineffective. We can use the *{color:#00875a}CallerContext{color}* field to store the current user and access it on the Ozone {*}{color:#00875a}IdentityProvider{color}{*}.

 

This is a presentation with the proposed approach.

[https://docs.google.com/presentation/d/1iChpCz_qf-LXiPyvotpOGiZ31yEUyxAdU4RhWMKo0c0/edit#slide=id.p]"
ABFS should exclude incompatible credential providers,13532880,Resolved,Major,Fixed,17/Apr/23 12:17,24/Apr/23 14:55,3.3.5,"The DelegationTokenManager in AzureBlobFileSystem.initialize() gets the untouched configuration which may contain a credentialProviderPath config with incompatible credential providers (e.g.: jceks stored on abfs). This results in an error:

{quote}
Caused by: org.apache.hadoop.fs.PathIOException: `jceks://abfs@a@b.c.d/tmp/a.jceks': Recursive load of credential provider; if loading a JCEKS file, this means that the filesystem connector is trying to load the same file
{quote}

{code}
        this.delegationTokenManager = abfsConfiguration.getDelegationTokenManager();
        delegationTokenManager.bind(getUri(), configuration);
{code}

The abfsConfiguration excludes the incompatible credential providers already.

Reproduction steps:
{code}
export HADOOP_ROOT_LOGGER=DEBUG,console
hdfs dfs -rm -r -skipTrash /user/qa/sort_input; hadoop jar hadoop-mapreduce-examples.jar randomwriter ""-Dmapreduce.randomwriter.totalbytes=100"" ""-Dhadoop.security.credential.provider.path=jceks://abfs@a@b.c.d/tmp/a.jceks"" /user/qa/sort_input 
{code}

Error:
{code}
...
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager.bind(AbfsDelegationTokenManager.java:96)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:224)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3452)
    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:162)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3557)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3504)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:522)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    at org.apache.hadoop.security.alias.KeyStoreProvider.initFileSystem(KeyStoreProvider.java:84)
    at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:85)
    at org.apache.hadoop.security.alias.KeyStoreProvider.<init>(KeyStoreProvider.java:49)
    at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:42)
    at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:35)
    at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:68)
    at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:91)
    at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:2450)
    at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:2388)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBClient.getTruststorePassword(AbfsIDBClient.java:104)
    at org.apache.knox.gateway.cloud.idbroker.AbstractIDBClient.initializeAsFullIDBClient(AbstractIDBClient.java:860)
    at org.apache.knox.gateway.cloud.idbroker.AbstractIDBClient.<init>(AbstractIDBClient.java:139)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBClient.<init>(AbfsIDBClient.java:74)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBIntegration.getClient(AbfsIDBIntegration.java:287)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBIntegration.serviceStart(AbfsIDBIntegration.java:240)
    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBIntegration.fromDelegationTokenManager(AbfsIDBIntegration.java:205)
    at org.apache.knox.gateway.cloud.idbroker.abfs.AbfsIDBDelegationTokenManager.bind(AbfsIDBDelegationTokenManager.java:66)
    at org.apache.hadoop.fs.azurebfs.extensions.ExtensionHelper.bind(ExtensionHelper.java:54)
    at org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager.bind(AbfsDelegationTokenManager.java:96)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:224)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3452)
    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:162)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3557)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3504)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:522)
    at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.getRollOverLogMaxSize(LogAggregationIndexedFileController.java:1164)
    at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initInternal(LogAggregationIndexedFileController.java:149)
    at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController.initialize(LogAggregationFileController.java:138)
    at org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory.<init>(LogAggregationFileControllerFactory.java:77)
    at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.addLogAggregationDelegationToken(YarnClientImpl.java:405)
    at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:321)
    at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:303)
    at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:331)
    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:252)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1576)
    at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1573)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1573)
    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1594)
    at org.apache.hadoop.examples.RandomWriter.run(RandomWriter.java:282)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)
    at org.apache.hadoop.examples.RandomWriter.main(RandomWriter.java:293)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
    at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
    at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
Caused by: org.apache.hadoop.fs.PathIOException: `jceks://abfs@a@b.c.d/tmp/a.jceks': Recursive load of credential provider; if loading a JCEKS file, this means that the filesystem connector is trying to load the same file
{code}"
Add curator based ZooKeeper communication support over SSL/TLS into the common library,13533250,Resolved,Major,Fixed,19/Apr/23 15:00,04/Jun/23 18:41,3.4.0,"With HADOOP-16579 the ZooKeeper client is capable of securing communication with SSL. 

To follow the convention introduced in HADOOP-14741, proposing to add to the core-default.xml the following configurations, as the groundwork for the components to enable encrypted communication between the individual components and ZooKeeper:
 * hadoop.zk.ssl.keystore.location
 * hadoop.zk.ssl.keystore.password
 * hadoop.zk.ssl.truststore.location
 * hadoop.zk.ssl.truststore.password

These parameters along with the component-specific ssl.client.enable option (e.g. yarn.zookeeper.ssl.client.enable) should be passed to the ZKCuratorManager to build the CuratorFramework. The ZKCuratorManager needs a new overloaded start() method to build the encrypted communication.
 * The secured ZK Client uses Netty, hence the dependency is included in the pom.xml. Added netty-handler and netty-transport-native-epoll dependency to the pom.xml based on ZOOKEEPER-3494 - ""No need to depend on netty-all (SSL)"".
 * The change was exclusively tested with the unit test, which is a kind of integration test, as a ZK Server was brought up and the communication tested between the client and the server.
 * This code change is in the common code base and there is no component calling it yet. Once YARN-11468 - ""Zookeeper SSL/TLS support"" is implemented, we can test it in a real cluster environment."
Avoid cross-platform build for irrelevant Dockerfile changes,13534568,Resolved,Major,Fixed,30/Apr/23 19:28,01/May/23 16:38,3.4.0,"Currently, when one of the Dockerfiles (located at https://github.com/apache/hadoop/tree/trunk/dev-support/docker) changes, all the platform builds are run.
For example, a change to Dockerfile_debian_10 would trigger a run for Centos 7, which isn't relevant.
This leads to unnecessary delays in PR validation. We should thus limit the changes specific to the platform for the case where the corresponding Dockerfile changes."
Install Python 3 for Windows 10 docker image,13536969,Resolved,Major,Fixed,21/May/23 07:52,21/May/23 15:45,3.4.0,"Currently, mvnsite build phase fails due to the following error -

{code}
[INFO] ------------------< org.apache.hadoop:hadoop-common >-------------------
[INFO] Building Apache Hadoop Common 3.4.0-SNAPSHOT                    [11/114]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-common ---
[INFO] Deleting C:\hadoop\hadoop-common-project\hadoop-common\target
[INFO] Deleting C:\hadoop\hadoop-common-project\hadoop-common\src\site\markdown (includes = [UnixShellAPI.md], excludes = [])
[INFO] Deleting C:\hadoop\hadoop-common-project\hadoop-common\src\site\resources (includes = [configuration.xsl, core-default.xml], excludes = [])
[INFO] 
[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---
tar: apache-yetus-0.14.0/lib/precommit/qbt.sh: Cannot create symlink to 'test-patch.sh': No such file or directory
tar: Exiting with failure status due to previous errors
/usr/bin/env: 'python3': No such file or directory
{code}

Thus, we need to install Python 3 in the Windows 10 Hadoop builder docker image to fix this."
AWS SDK V2 - ITestS3AHugeFilesNoMultipart failure,13537168,Resolved,Major,Duplicate,22/May/23 13:14,29/Aug/23 15:28,3.4.0,"ITestS3AHugeFilesNoMultipart fails with

java.lang.AssertionError: Expected a org.apache.hadoop.fs.s3a.api.UnsupportedRequestException to be thrown, but got the result: : true

Happens because the transfer manager currently does not do any MPU when used with the Java async client, so the UnsupportedRequestException never gets thrown. "
CachingBlockManager to use AtomicBoolean for closed flag,13538280,Resolved,Major,Fixed,31/May/23 14:12,27/Jun/23 12:43,3.3.9,"the {{CachingBlockManager}} uses the boolean field {{closed)) in various operations, including a do/while loop. to ensure the flag is correctly updated across threads, it needs to move to an atomic boolean.

"
S3A Committer only finalizes the commits in a single thread,13538392,Resolved,Major,Fixed,01/Jun/23 10:48,19/Jul/23 11:34,3.3.5,"S3A Committer is being bottle-necked on the driver when finalizing the commits. It seems like only a single thread is being used to finalize the commit. In the experiment we are saving 36,000 files ending committing for almost 2 hours each file taking 0.1 - 0.5 seconds while all the executors stay idle while the driver commits. I have attached the driver log snippets to support this theory in comparison to spark 3.4.0.

The most likely reason is the usage of [ThreadPoolExecutor]([https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html]) in 
[https://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/impl/CommitContext.java#L239-L244]
where an LinkedBlockingQueue is used with a corePoolSize of 0 which ends up with a single thread and no new threads created.

From the documentation


{code:java}
Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed.{code}

Magic Committer spark 3.5.0-SNAPSHOT
{code:java}
2023-05-26 15:35:04,852 DEBUG impl.CommitContext: creating thread pool of size 32
2023-05-26 15:35:04,922 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s) for resource profile id: 0.
2023-05-26 15:35:07,910 INFO commit.AbstractS3ACommitter: Starting: committing the output of 36000 task(s)
2023-05-26 15:35:07,914 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448
e-8022-31e0f7457354/00/task_202305261454174742717183892533843_0031_m_000000.pendingset
2023-05-26 15:35:07,941 DEBUG files.PersistentCommitData: Reading commit data from file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448e-8022-31e0f7457354/00/
task_202305261454174742717183892533843_0031_m_000000.pendingset
2023-05-26 15:35:08,036 DEBUG impl.CommitContext: creating thread pool of size 32
2023-05-26 15:35:08,037 DEBUG impl.CommitOperations: Committing single commit SinglePendingCommit
{version=2, uri='s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5- 448e-8022-31e0f7457354/00/tasks/attempt_202305261454174742717183892533843_0031_m_000000_1920855/__base/rawRequestType=imp_dsp/part-00000-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', destination='<PATH>/date=20230520/rawRequestType=imp_dsp/part-00000-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', uploadId='SCp78J9aYOmnPwrtfd5a.Q6B9Zu6olQw3eZcpyt.W.BCu6.M6fz54nlTe2ZYUDicSskFgPocsjlVXAbfiPn3Xu 26MzXNdWcg5j_kBGg9iWpeoWh4K21gt7bbKetML95MXAck15yP.VGFeOleoAspUg--', created=1685113555232, saved=1685113555232, size=110636173, date='Fri May 26 15:05:55 UTC 2023', jobId='b9167b1a-35a5-448e-8022-31e0f7457354', taskId='attemp t_202305261454174742717183892533843_0031_m_000000_1920855', notes='', etags=[64b9d14a89890cf7f931ff05074f6b6a,7a8c22d95d85736806205ad0f84094a5,82f55a1b0595b38420c6b55cb2763a46,212a3105afcf4a9045730f885a622e0a,b563ec91932a66498 e8e9d717b625961,0425b7ae17a8a2af80734f0adf4debe9,496e9fbc08a583657fd29779dcd848d6,72bff33d090699baacb6a6171760a675,b2c6b9b76256416d304392bfbabca382,1a8f63d960cf50589b83b407ddfc77c7,d79cf49f16cd61fb00faf67034268529,e3b7ea2bd0ff f2e20a89eccd0ebbeb4a,8edc254a94902fee2884350b4a89f223,f0858853a4542b562c6973e28ee40c19]}
2023-05-26 15:35:08,037 INFO impl.CommitOperations: Starting: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00000-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet s
ize 110636173
2023-05-26 15:35:08,198 DEBUG impl.CommitOperations: Successful commit of file length 110636173
2023-05-26 15:35:08,199 INFO impl.CommitOperations: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00000-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet size 110636
173: duration 0:00.162s
2023-05-26 15:35:08,208 INFO commit.AbstractS3ACommitter: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448e-8022-31e
0f7457354/00/task_202305261454174742717183892533843_0031_m_000000.pendingset: duration 0:00.294s
2023-05-26 15:35:08,208 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448
e-8022-31e0f7457354/00/task_202305261454174742717183892533843_0031_m_000001.pendingset
2023-05-26 15:35:08,208 DEBUG files.PersistentCommitData: Reading commit data from file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448e-8022-31e0f7457354/00/
task_202305261454174742717183892533843_0031_m_000001.pendingset
2023-05-26 15:35:08,232 DEBUG impl.CommitOperations: Committing single commit SinglePendingCommit
{version=2, uri='s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5- 448e-8022-31e0f7457354/00/tasks/attempt_202305261454174742717183892533843_0031_m_000001_1920856/__base/rawRequestType=imp_dsp/part-00001-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', destination='<PATH>/date=20230520/rawRequestType=imp_dsp/part-00001-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', uploadId='I.5Kns.n5JKLbupr5.AWbzApL9YDGhQyI438WQT3JXUjevkQS8u1aQwQhkVbwBoxnFXItCbKJCS8EwPdtoijtc UFrjU0eU3pNvt7lwcYRJ0iklLwXexHbYMpZq3pDj0udwsHCq0trnqyq2Ee.o.prw--', created=1685113548923, saved=1685113548923, size=110570244, date='Fri May 26 15:05:48 UTC 2023', jobId='b9167b1a-35a5-448e-8022-31e0f7457354', taskId='attemp t_202305261454174742717183892533843_0031_m_000001_1920856', notes='', etags=[835dd18d4ce995c22c1a84de5cd26fd8,c1f2acd3360b2f94ce90ef0d80edd7e5,208787fce146d917dbc50b1cf4adff4c,a66b228379201a0a278061b096dc2faf,58134b7ae228f0d2a 6879ec53a3f35c1,e847a7614639d3f1a43a1b3090b603e1,2d8eac2399238e6d356fe0bdfafd076e,e0c3a964ee2a3c270f67e7e4a7791440,84b05192e896e41e1d16aa363f0eb48b,c3cf764e4361da4573b8dfa6361e4174,78b145f682946c7e18750758da3578cd,d9cd438899d2 0522668b9cd7f61cc098,e8b3944929f17d927ed7aef31800bdbf,52d279a75e9382ead94fe4d9b81c3bf9]}
2023-05-26 15:35:08,232 INFO impl.CommitOperations: Starting: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00001-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet s
ize 110570244
2023-05-26 15:35:08,438 DEBUG impl.CommitOperations: Successful commit of file length 110570244
2023-05-26 15:35:08,438 INFO impl.CommitOperations: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00001-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet size 110570
244: duration 0:00.206s
2023-05-26 15:35:08,448 INFO commit.AbstractS3ACommitter: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448e-8022-31e
0f7457354/00/task_202305261454174742717183892533843_0031_m_000001.pendingset: duration 0:00.240s
2023-05-26 15:35:08,448 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448
e-8022-31e0f7457354/00/task_202305261454174742717183892533843_0031_m_000002.pendingset
2023-05-26 15:35:08,448 DEBUG files.PersistentCommitData: Reading commit data from file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5-448e-8022-31e0f7457354/00/
task_202305261454174742717183892533843_0031_m_000002.pendingset
2023-05-26 15:35:08,489 DEBUG impl.CommitOperations: Committing single commit SinglePendingCommit
{version=2, uri='s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b9167b1a-35a5- 448e-8022-31e0f7457354/00/tasks/attempt_202305261454174742717183892533843_0031_m_000002_1920857/__base/rawRequestType=imp_dsp/part-00002-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', destination='<PATH>/date=20230520/rawRequestType=imp_dsp/part-00002-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet', uploadId='AB982jhG0EAfnSxXWphX7R.QfqrkS2ipP6APqJZYGlqDl335WX4X2xn81wy7l2i5NBQrvG.eVcQgYFudk51wi5 EMDJRNVz__SEWyF0xTncpIHpaC5xn8La.AY8gzCHQrAfRJ83nErQhe4Idmhcs04w--', created=1685113553072, saved=1685113553072, size=110136058, date='Fri May 26 15:05:53 UTC 2023', jobId='b9167b1a-35a5-448e-8022-31e0f7457354', taskId='attemp t_202305261454174742717183892533843_0031_m_000002_1920857', notes='', etags=[f043628528fc9bdd6db4107fd175eb1d,11e36b4bde35bdb1d2e160ffd5fcc3f6,5105cb3ebee1b27cfec66f7682679316,1a85e5c9477e855e38f7bd4e81bed28e,7c57360865e12c19b 3125cbfb8c786f8,59d928629e7afff4c4c038a1dadd0ce7,bfcedee2f51b45c20a097fbbae85a48f,7e3ff2665714d8cdc6ec20134856eb4c,0117e5c47f94a9ff4d8d0eafd1f6b76e,3c4f27e69544e4cc5297c56413231639,a32cd6422aaae63bdcbaafa7400ab889,1d5d07f0a0b2 58773993a456bc7ec7ee,a220e786aa36f643edc330a184e23d88,e19a85ab3accaa4dc697e7dfbf5d5325]}
2023-05-26 15:35:08,489 INFO impl.CommitOperations: Starting: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00002-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet s
ize 110136058
2023-05-26 15:35:08,646 DEBUG impl.CommitOperations: Successful commit of file length 110136058
2023-05-26 15:35:08,646 INFO impl.CommitOperations: Committing file <PATH>/date=20230520/rawRequestType=imp_dsp/part-00002-b196e04e-c87b-43e1-97e0-7ad34ef0bedf.c000.zstd.parquet size 110136058: duration 0:00.157s
{code}
Magic Committer spark 3.4.0

 
{code:java}
2023-05-24 09:47:19,906 INFO yarn.YarnAllocator: Driver requested a total number of 524 executor(s) for resource profile id: 0.
2023-05-24 09:47:23,064 INFO commit.AbstractS3ACommitter: Starting: committing the output of 36000 task(s)
2023-05-24 09:47:23,069 DEBUG commit.AbstractS3ACommitter: Task committer attempt_202305240933298436217226765687147_0000_m_000000_0: creating thread pool of size 32
2023-05-24 09:47:23,074 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,074 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000000.pendingset
2023-05-24 09:47:23,075 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,075 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000001.pendingset
2023-05-24 09:47:23,075 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,075 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000002.pendingset
2023-05-24 09:47:23,075 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,075 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000003.pendingset
2023-05-24 09:47:23,075 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,075 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000004.pendingset
2023-05-24 09:47:23,076 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,076 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000005.pendingset
2023-05-24 09:47:23,076 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,076 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000006.pendingset
2023-05-24 09:47:23,076 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7a
e3c5785060/task_20230524093329924571364925494992_0031_m_000004.pendingset
2023-05-24 09:47:23,076 INFO commit.AbstractS3ACommitter: Starting: Loading and committing files in pendingset s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic
/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000007.pendingset
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7a
e3c5785060/task_20230524093329924571364925494992_0031_m_000003.pendingset
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7a
e3c5785060/task_20230524093329924571364925494992_0031_m_000000.pendingset
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7a
e3c5785060/task_20230524093329924571364925494992_0031_m_000002.pendingset
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7a
e3c5785060/task_20230524093329924571364925494992_0031_m_000001.pendingset
2023-05-24 09:47:23,078 DEBUG commit.Tasks: Executing task
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000005.pendingset
2023-05-24 09:47:23,076 DEBUG files.PendingSet: Reading pending commits in file s3://<BUCKET_NAME>/<PATH>/date=20230520/__magic/job-b15cffa2-2764-45f6-883b-7ae3c5785060/task_20230524093329924571364925494992_0031_m_000007.pendingset{code}"
Add .vscode to gitignore,13540501,Resolved,Major,Fixed,18/Jun/23 03:44,18/Jun/23 06:09,3.4.0,Add .vscode directory to gitignore to avoid stages it when using vsode as IDE.
upgrade netty to 4.1.94 due to CVE,13541236,Resolved,Major,Fixed,24/Jun/23 09:24,02/Jul/23 08:38,3.4.0,https://github.com/advisories/GHSA-6mjq-h674-j845
Delete path directly when it can not be parsed in trash,13539798,Resolved,Major,Fixed,13/Jun/23 07:35,16/Jul/23 04:21,3.4.0,"If we move path to trash dir directly rather than use delete API or rm command, when 

invoke deleteCheckpoint method, it will catch ParseException and ignore deleting the path. It will never be deleted, so we should do something to prevent or monitor it.

Some logs are listed below.

 
{code:java}
WARN org.apache.hadoop.fs.TrashPolicyDefault: Unexpected item in trash: /user/de_eight/.Trash/college_geek_job_recall_als_modelres_5_2_6.del. Ignoring.

WARN org.apache.hadoop.fs.TrashPolicyDefault: Unexpected item in trash: /user/de_eight/.Trash/college_geek_addf_vector. Ignoring.
 {code}
 

 "
Upgrade Apache Derby from 10.10.2.0 to 10.14.2.0 due to CVEs,13531853,Resolved,Major,Fixed,07/Apr/23 11:06,13/Apr/23 17:25,3.4.0,"[https://github.com/advisories/GHSA-wr69-g62g-2r9h]

[https://github.com/advisories/GHSA-42xw-p62x-hwcf]

[https://github.com/apache/hadoop/pull/5427]

Only seems to be used in test scope but it would be nice to silence the dependabot warnings by merging the PR. 

 

 "
upgrade to snappy-java 1.1.10.1 due to CVEs,13541235,Resolved,Major,Fixed,24/Jun/23 09:17,27/Jun/23 12:39,3.4.0,see https://mvnrepository.com/artifact/org.xerial.snappy/snappy-java
Upgrade to jetty 9.4.51 due to cve,13533286,Resolved,Major,Fixed,19/Apr/23 20:44,23/Apr/23 19:33,,https://github.com/advisories/GHSA-qw69-rqj8-6qw8
s3a prefetch cache blocks should be accessed by RW locks,13535878,Resolved,Major,Fixed,11/May/23 21:24,09/Jun/23 17:05,3.3.6,"In order to implement LRU or LFU based cache removal policies for s3a prefetched cache blocks, it is important for all cache reader threads to acquire read lock and similarly cache file removal mechanism (fs close or cache eviction) to acquire write lock before accessing the files.

As we maintain the block entries in an in-memory map, we should be able to introduce read-write lock per cache file entry, we don't need coarse-grained lock shared by all entries.

 

This is a prerequisite to HADOOP-18291."
openFile builder new optLong() methods break hbase-filesystem,13538142,Resolved,Major,Fixed,30/May/23 17:23,01/Jun/23 13:58,3.3.6,"The new methods of HADOOP-18724 break builds of hbase filesystem because HBASE-26483 added an implementation of the interface, which,
because it lacks the complete methods, doesn't compile any more.

It also marked the (deprecated) methods as final in the base implementation, which is also a source of pain."
Upgrade aws-java-sdk to 1.12.367+,13539411,Resolved,Major,Fixed,09/Jun/23 10:50,14/Jun/23 20:13,3.3.5,"
aws sdk bundle < 1.12.367 uses a vulnerable versions of netty which is pulling in high severity CVE and creating unhappiness in security scans, even if s3a doesn't use that lib. 

The safe version for netty is netty:4.1.86.Final and this is used by aws-java-adk:1.12.367+"
Make DistCp Split files hidden to prevent interfering with query engines ,13537761,Open,Major,,26/May/23 07:22,,,"As of today, the distcp split files are not hidden, and hence could interfere with query engines. Making them hidden. 

 

Contributing a fix to it."
AWS SDK V2 - sigv2 support,13537148,Resolved,Major,Won't Fix,22/May/23 10:39,10/Oct/23 18:36,3.4.0,"AWS SDK V2 does not support sigV2 signing. However, the S3 client supports configurable signers so a custom sigV2 signer can be implemented and configured. "
build/release a shaded version of the AWS SDK,13538412,Resolved,Major,Won't Fix,01/Jun/23 12:42,28/Sep/23 09:52,3.4.0,"we need a shaded version of the AWS SDK to stay in control of our dependencies.

Either we somehow get the AWS dev team to do it, or we do it ourselves in the hadoop-thirdparty repository.

Doing it ourselves would actually give us a bit more control as we have the possibility of making a lighter weight release. It would reduce the control of people downstream though, as they wouldn't be able to ""just"" drop in a new JAR
"
convert declarations of AWS v1 SDK EnvironmentVariableCredentialsProvider to v2 version,13536573,Resolved,Major,Duplicate,17/May/23 13:44,27/Jul/23 14:39,,"As I play with the v2 sdk I've cut the v1 sdk from the tools/lib and now I'm getting stack traces about missing class EnvironmentVariableCredentialsProvider

{code}
java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class com.amazonaws.auth.EnvironmentVariableCredentialsProvider not found
        at org.apache.hadoop.fs.s3a.auth.AwsCredentialListProvider.loadAWSProviderClasses(AwsCredentialListProvider.java:128)
        at org.apache.hadoop.fs.s3a.auth.AwsCredentialListProvider.buildAWSProviderList(AwsCredentialListProvider.java:167)
        at org.apache.hadoop.fs.s3a.auth.AwsCredentialListProvider.createAWSCredentialProviderSet(AwsCredentialListProvider.java:102)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:945)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:597)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3595)
        at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:171)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3696)
        at org.apache.hadoop.fs.FileSystem$Cache.getUnique(FileSystem.java:3653)
        at org.apache.hadoop.fs.FileSystem.newInstance(FileSystem.java:608)
        at org.apache.hadoop.fs.store.diag.StoreDiag.executeFileSystemOperations(StoreDiag.java:753)
 
this provider is listed in the fs.s3a.aws.credentials.provider chain

{code}
<property>
  <name>fs.s3a.aws.credentials.provider</name>
  <value>
    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,
    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,
    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider
  </value>
...

{code}

Ideally we should remove all v1 dependencies, explicitly (here) and implicitly.

maybe for the env vars, we should consider adding our own env var provider which could go into branch-3.3 *now* as a single patch, which can then be cherrypicked by anyone who wants it in older releases; the v2 version will work with the new api
"
remove use of com.google.common.base.Charsets;,13540044,Open,Major,,14/Jun/23 12:32,,3.3.9,"just noticed there's references to com.google.common.base.Charsets /thirdparty equivalent just for Charsets.UTF_8 references.

we can cut these and remove one more guava version pain point"
Add OptimizedS3AMagicCommitter For Zero Rename Commits to S3 Endpoints,13540582,Resolved,Major,Won't Fix,19/Jun/23 08:45,13/Jul/23 05:37,,"The goal is to add a new S3A committer named *OptimizedS3AMagicCommitter* which is an another type of S3 Magic committer but with a better performance by taking in few tradeoffs.

The following are the differences in MagicCommitter vs OptimizedMagicCommitter

 
||Operation||Magic Committer||*OptimizedS3AMagicCommitter*||
|commitTask    |1. Lists all {{.pending}} files in its attempt directory.
 
2. The contents are loaded into a list of single pending uploads.
 
3. Saved to a {{.pendingset}} file in the job attempt directory.|1. Lists all {{.pending}} files in its attempt directory
 
2. The contents are loaded into a list of single pending uploads.
 
3. For each pending upload, commit operation is called (complete multiPartUpload)|
|commitJob|1. Loads all {{.pendingset}} files in its job attempt directory
 
2. Then every pending commit in the job will be committed.
 
3. ""SUCCESS"" marker is created (if config is enabled)
 
4. ""__magic"" directory is cleaned up.|1. ""SUCCESS"" marker is created (if config is enabled)
 
2.  ""__magic"" directory is cleaned up.|

 

*Performance Benefits :-*
 # The primary performance boost due to distributed complete multiPartUpload call being made in the taskAttempts(Task containers/Executors) rather than a single job driver. In case of MagicCommitter it is O(files/threads).
 # It also saves a couple of S3 calls needed to PUT the ""{{{}.pendingset{}}}"" files and READ call to read them in the Job Driver.

 

*TradeOffs :-*

The tradeoffs are similar to the one in FileOutputCommitter V2 version. Users migrating from FileOutputCommitter V2 to OptimizedS3AMagicCommitter will no see behavioral change as such
 # During execution, intermediate data becomes visible after commitTask operation
 # On a failure, all output must be deleted and the job needs to be restarted.

 

*Performance Benchmark :-*

Cluster : c4.8x large (ec2-instance)
Instance : 1 (primary) + 5 (core)
Data Size : 3TB Partitioned(TPC-DS store_sales data)
Engine     : Apache Spark 3.3.1 / Hadoop 3.3.3

Query: The following query inserts around 3000+ files into the table directory (ran for 3 iterations)
{code:java}
insert into <table> select ss_quantity from store_sales; {code}
||Committer||Iteration 1||Iteration 2||Iteration 3||
|Magic|126|127|122|
|OptimizedMagic|50|51|58|

So on an average, OptimizedMagicCommitter was *~2.3x* faster as compared to MagicCommitter.

 

_*Note: Unlike MagicCommitter , OptimizedMagicCommitter is not suitable for all the cases where in user requires the guarantees of file not being visible in failure scenarios. Given the performance benefit, user can may choose to use this if they don't require any guarantees or have some mechanism to clean up the data before retrying.*_

 "
Parallelize concatenation of distcp chunks of separate files in CopyCommitter,13535695,Open,Major,,10/May/23 20:30,,3.3.6,"While copying a folder containing large files consisting of multiple distcp chunks, copy committer synchronously picks chunks of each file and concatenates them. This part can be improved by parallelizing the concatenation of distcp chunks of separate files. We are able to save 2-3 minutes while copying a folder of 100 GB containing 20 files of 5GB size with this improvement.

Contributing a patch for this."
"Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error",13535498,Open,Major,,09/May/23 09:27,,,"Fix '-Dbundle.pmdk' does not take effect, checknative pmdk shows error

before fix:

!image-2023-05-10-14-33-49-601.png!

after fix:

!image-2023-05-10-14-34-02-922.png|width=583,height=32!"
Improve hadoop-function.sh#status script,13540874,Resolved,Major,Fixed,21/Jun/23 09:34,03/Jul/23 15:47,3.4.0,"When we use yarn --daemon status router/nodemanager, I found that no information can be displayed. I checked the script and modified the script of hadoop-functions.sh#status to increase the output information.

yarn --daemon status router

- If the service is started
router is running as process 47626.

- If the service is stop
router is stopped."
Release 3.3.6,13539468,Resolved,Major,Done,09/Jun/23 17:07,26/Jun/23 21:31,3.3.5,"* Move out all incomplete jiras
* Branching
* Unit test and verifications
* License check
* Produce signed artifacts and source tarball."
fs.azure.buffer.dir to be under Yarn container path on yarn applications,13539430,Resolved,Major,Fixed,09/Jun/23 12:42,30/Jun/23 06:31,3.3.5,replicate HADOOP-17386 for azure so things clean up better long lived yarn clusters
Spark History Server 3.3.1 fails to starts with Hadoop 3.3.x,13537425,Open,Major,,24/May/23 04:20,,,"When Spark History Server tries to start with Hadoop 3.3.4 (Happens only in Kerberos scenarios), it fails to do so with the following exception: 
{code:java}
23/05/23 03:14:15 ERROR HistoryServer [main]: Failed to bind HistoryServer

java.lang.IllegalStateException: class org.apache.hadoop.security.authentication.server.AuthenticationFilter is not a javax.servlet.Filter

        at org.sparkproject.jetty.servlet.FilterHolder.doStart(FilterHolder.java:103) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:749) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) ~[?:1.8.0_372]

        at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742) ~[?:1.8.0_372]

        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647) ~[?:1.8.0_372]

        at org.sparkproject.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.ui.ServerInfo.addHandler(JettyUtils.scala:491) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.ui.WebUI.$anonfun$bind$3(WebUI.scala:154) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.ui.WebUI.$anonfun$bind$3$adapted(WebUI.scala:154) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]

        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]

        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]

        at org.apache.spark.ui.WebUI.bind(WebUI.scala:154) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.deploy.history.HistoryServer.bind(HistoryServer.scala:164) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.deploy.history.HistoryServer$.main(HistoryServer.scala:310) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT]

        at org.apache.spark.deploy.history.HistoryServer.main(HistoryServer.scala) ~[spark-core_2.12-3.3.1.5.1-SNAPSHOT.jar:3.3.1.5.1-SNAPSHOT] {code}
 "
3.3.6 Dependency vulnerability check,13539474,Resolved,Major,Done,09/Jun/23 17:56,26/Jun/23 21:31,3.3.6,
Upgrade hadoop3 docker scripts to use 3.3.6,13539730,Resolved,Major,Fixed,12/Jun/23 20:42,26/Jun/23 16:13,,Similar to what was done in HADOOP-18681
Add tags 3.3/3.3.5 to the official Hadoop docker image,13540960,Resolved,Major,Fixed,21/Jun/23 23:29,22/Jun/23 15:22,,"Our official docker repository has just two tags for the images: 2 and 3.

Shall we add more tags to the images? For example Hadoop 3 has two release lines: 3.2 and 3.3. And probably even maintenance release tags: 3.3.5, 3.3.6. This way, downstream users will not have surprises when we make a new release."
Update jackson2 version from 2.12.7.1 to 2.15.0,13540738,Open,Major,,20/Jun/23 10:31,,,"can the jackson2 version in hadoop-project be updated from 2.12.7.1 to 2.15.*

This is to rectify the following vulnerability

[https://github.com/FasterXML/jackson-core/pull/827]

https://github.com/apache/hadoop/blob/trunk/hadoop-project/pom.xml#L72"
Add instrumentation access check for /logs servlet ,13540547,Open,Major,,19/Jun/23 04:33,,,"Currently AdminAuthorizedServlet is configured for /logs which would always check if user has admin access. Just like other servelets, log should also have instrumentation check before the admin access check. "
Configuration.get is slow,13537167,Open,Major,,22/May/23 13:11,,3.3.5,`Configuration.get` is slow mainly because of the overhead of `handleDeprecation` and eager creation of `overlay` even when null.
Integrating Apache Hadoop into OSS-Fuzz,13539695,Open,Major,,12/Jun/23 16:09,,,"Hi all,

We have prepared the [initial integration|https://github.com/google/oss-fuzz/pull/10511] of Apache Hadoop into [Google OSS-Fuzz|https://github.com/google/oss-fuzz] which will provide more security for your project.

 

*Why do you need Fuzzing?*
The Code Intelligence JVM fuzzer [Jazzer|https://github.com/CodeIntelligenceTesting/jazzer] has already found [hundreds of bugs|https://github.com/CodeIntelligenceTesting/jazzer/blob/main/docs/findings.md] in open source projects including for example [OpenJDK|https://nvd.nist.gov/vuln/detail/CVE-2022-21360], [Protobuf|https://nvd.nist.gov/vuln/detail/CVE-2021-22569] or [jsoup|https://github.com/jhy/jsoup/security/advisories/GHSA-m72m-mhq2-9p6c]. Fuzzing proved to be very effective having no false positives. It provides a crashing input which helps you to reproduce and debug any finding easily. The integration of your project into the OSS-Fuzz platform will enable continuous fuzzing of your project by [Jazzer|https://github.com/CodeIntelligenceTesting/jazzer].

 

*What do you need to do?*
The integration requires the maintainer or one established project committer to deal with the bug reports.

You need to create or provide one email address that is associated with a google account as per [here|https://google.github.io/oss-fuzz/getting-started/accepting-new-projects/]. When a bug is found, you will receive an email that will provide you with access to ClusterFuzz, crash reports, code coverage reports and fuzzer statistics. More than 1 person can be included.

 

*How can Code Intelligence support you?*
We will continue to add more fuzz targets to improve code coverage over time. Furthermore, we are permanently enhancing fuzzing technologies by developing new fuzzers and bug detectors.

 

Please let me know if you have any questions regarding fuzzing or the OSS-Fuzz integration."
Incorporate Qiniu Cloud Kodo File System Implementation,13539366,Open,Major,,09/Jun/23 04:41,,3.3.9,"Qiniu Kodo is a self-developed unstructured data storage management platform by Qiniu Cloud Storage that supports center and edge storage. The platform has been verified by a large number of users for many years and has been widely used in various scenarios of massive data management. It is widely used in many cloud service users in China, but currently in the Apache Hadoop project, there is a lack of a solution that supports Kodo through Hadoop/Spark directly.

The purpose of this project is to integrate Kodo into Hadoop/Spark projects, so that users can operate Kodo through the API of Hadoop/Spark without additional learning costs."
Fix transient failure of ITestS3APrefetchingInputStream#testRandomReadLargeFile,13532397,Resolved,Major,Fixed,12/Apr/23 18:51,02/May/23 14:48,3.4.0,"intermittent failure of test; caller in uk, no VPN involved. does not always surface

{code}
testRandomReadLargeFile(org.apache.hadoop.fs.s3a.ITestS3APrefetchingInputStream)  Time elapsed: 19.913 s  <<< FAILURE!
org.junit.ComparisonFailure: [Gauge named stream_read_blocks_in_cache with expected value 3] expected:<[3]L> but was:<[2]L>

{code}
"
Backport S3A prefetching stream to branch-3.3,13532679,Resolved,Major,Fixed,14/Apr/23 16:12,28/Apr/23 11:46,3.3.9,"Jira to cover work for backporting the prefetching into branch-3.3

I'm thinking of the best way to do do this. Proposed

# new feature branch
# cherrypick from trunk all current changes for it
# squash into one single pr, the backport change
# merge that into branch-3.3
# then for all the outstanding issues add them separately to trunk and branch-3.3"
S3A ITestS3ABucketExistence access point test failure,13532396,Resolved,Major,Fixed,12/Apr/23 18:44,17/Apr/23 09:21,3.3.9,"this is inevitably me having some config option in the way; just need to find out what and clear it for the test case so the probes are to the same region/endpont as the mock bucket

{code}
[ERROR] testAccessPointProbingV2(org.apache.hadoop.fs.s3a.ITestS3ABucketExistence)  Time elapsed: 1.748 s  <<< ERROR!
java.lang.IllegalArgumentException: The region field of the ARN being passed as a bucket parameter to an S3 operation does not match the region the client was configured with. Provided region: 'eu-west-1'; client region: 'eu-west-2'.
        at com.amazonaws.services.s3.AmazonS3Client.validateIsTrue(AmazonS3Client.java:6588)


{code}
"
Cannot write to Azure Datalake Gen2 (abfs/abfss) after Spark 3.1.2,13533213,Resolved,Major,Workaround,19/Apr/23 12:29,26/May/23 15:50,3.3.2,"Hello,

I have an issue with Spark 3.3.2 & Spark 3.4.0 to write into Azure Data Lake Storage Gen2 (abfs/abfss scheme). I've got the following errors:
{code:java}
warn 13:12:47.554: StdErr from Kernel Process 23/04/19 13:12:47 ERROR FileFormatWriter: Aborting job 6a75949c-1473-4445-b8ab-d125be3f0f21.org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (myhost executor driver): org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for datablock-0001-    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.createTmpFileForWrite(DataBlocks.java:980)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.create(DataBlocks.java:960)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.createBlockIfNeeded(AbfsOutputStream.java:262)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.<init>(AbfsOutputStream.java:173)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:580)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:301)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:480)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)    at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)    at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)    at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)    at org.apache.spark.scheduler.Task.run(Task.scala:139)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)
Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)    at scala.Option.foreach(Option.scala:407)    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)    at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)    at py4j.Gateway.invoke(Gateway.java:282)    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)    at py4j.commands.CallCommand.execute(CallCommand.java:79)    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)    at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for datablock-0001-    at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165)    at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.createTmpFileForWrite(DataBlocks.java:980)    at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.create(DataBlocks.java:960)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.createBlockIfNeeded(AbfsOutputStream.java:262)    at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.<init>(AbfsOutputStream.java:173)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:580)    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:301)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)    at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)    at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:480)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)    at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)    at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)    at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)    at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)    at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)    at org.apache.spark.scheduler.Task.run(Task.scala:139)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    ... 1 more {code}
Before that, I was able to write into Azure Data Lake Storage with Spark 3.1.2 with hadoop-azure 3.2.1 without encountering this error.

Here's what I have tried but with no success:
 * Spark 3.3.2 with hadoop-azure 3.3.2
 * Spark 3.3.2 with hadoop-azure 3.3.5
 * Spark 3.4.0 with hadoop-azure 3.3.4
 * Spark 3.4.0 with hadoop-azure 3.3.5

Regards,

PS: I have posted the issues on Spark too https://issues.apache.org/jira/browse/SPARK-43188"
S3AFileSystem doesn't consistently handle prefixes that are both files and directories between versions,13537526,Resolved,Major,Won't Fix,24/May/23 18:36,25/May/23 14:23,3.3.4,"We have a prefix structure where the prefix Spark reads is both a file and a directory. So s3://a/b is the file we are trying to read, but s3://a/b/c is also a file. In 3.2.1, listStatuses identifies a/b as a File, but a change in 3.3.4 now identifies a/b as a directory and tries to read a/b/c instead of a/b.

When s3GetFileStatus is called on the path with StatusProbeEnum HEAD, the path does return as ""File"". However innerListStatus first assumes that any prefix that is ""nonempty"" is a directory; it only calls s3GetFileStatus on empty directories and on listObjects results of the prefix.

Wonder if this is known/if there are any suggestions to get around this without changing the prefix structure?

"
Backport HADOOP-18671 to branch-2.10.x,13535542,Resolved,Major,Not A Problem,09/May/23 17:06,16/May/23 22:11,,
Error message when multipart disabled and buffering is array not useful,13535688,Open,Major,,10/May/23 18:50,,3.3.9,"testing branch-3.3 on a bucket with array buffering, I get a fairly useless error message.
Problem here is that AbstractCommitITest forces fs.s3a.fast.upload.buffer to array and somehow multipart is disabled.

# error text needs fixing (move to %s)
# s3a fs should fail in init if the threshold is isn't allowed for the store type
# then determine why the test is failing, and fix.


The tests do work in trunk, so something in branch-3.3 is stopping this


Also 

* correct the name of the constant STORE_CAPABILITY_DIRECTORY_MARKER_MULTIPART_UPLOAD_ENABLED to match the actual string ""fs.s3a.capability.multipart.uploads.enabled""; accidental copy there

* capability FS_MULTIPART_UPLOADER must declare itself as disabled when multipart is off
    

{code}

[ERROR] testRevertCommit(org.apache.hadoop.fs.s3a.commit.ITestCommitOperations)  Time elapsed: 0.688 s  <<< ERROR!
java.lang.IllegalArgumentException: Invalid block size: %d [-1]
        at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:205)
        at org.apache.hadoop.fs.s3a.S3ADataBlocks$ArrayBlockFactory.create(S3ADataBlocks.java:397)
        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:235)
        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:217)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCreateFile(S3AFileSystem.java:1887)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$create$7(S3AFileSystem.java:1789)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2479)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2498)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1788)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1210)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1091)
        at org.apache.hadoop.fs.contract.ContractTestUtils.createFile(ContractTestUtils.java:650)
        at org.apache.hadoop.fs.contract.ContractTestUtils.touch(ContractTestUtils.java:686)
        at org.apache.hadoop.fs.s3a.commit.ITestCommitOperations.testRevertCommit(ITestCommitOperations.java:543)
{code}


"
Backport HADOOP-18671 to branch-3.2,13535033,Resolved,Major,Done,04/May/23 16:52,09/May/23 00:08,,
Backport HADOOP-18671 to branch-3.3,13535032,Resolved,Major,Done,04/May/23 16:52,08/May/23 22:21,,
"Support a ""permissive"" mode for secure clusters to allow ""simple"" auth clients",13532811,Open,Major,,17/Apr/23 02:25,,2.10.3,"Similar to HBASE-14700, would like to add support for Secure Server to fallback to simple auth for non-secure clients.



Secure Hadoop to support a permissive mode to allow mixed secure and insecure clients. This allows clients to be incrementally migrated over to a secure configuration. To enable clients to continue to connect using SIMPLE authentication when the cluster is configured for security, set ""hadoop.ipc.server.fallback-to-simple-auth-allowed"" equal to ""true"" in hdfs-site.xml. NOTE: This setting should ONLY be used as a temporary measure while converting clients over to secure authentication. It MUST BE DISABLED for secure operation."
Support VFSFileSystem based on apache commons vfs,13535230,Open,Major,,06/May/23 01:25,,3.4.0,"Common vfs supports multiple types of storage, different archives, and a variety of compression algorithms

 

simple demo:  [https://github.com/melin/spark-jobserver/blob/master/jobserver-extensions/src/test/scala/com/github/melin/jobserver/extensions/sql/]

spark.read.option(""header"", ""true"")
      .csv(""vfs://tgz:[ftp://fcftp:fcftp@172.18.1.52/csv.tar.gz!/csv]"").show()
spark.read.option(""header"", ""true"")
.csv(""vfs://tgz:s3://BxiljVd5YZa3mRUn:3Mq9dsmdMbN1JipE1TlOF7OuDkuYBYpe@cdh1:9300/demo-bucket/csv.tar.gz!/csv"").show()
 
spark.read.option(""header"", ""true"")
.csv(""vfs://tgz:sftp:///test:test2023@172.18.5.46:22/ftpdata/csv.tar.gz!/csv"").show()

 

[~prabhujoseph]  "
Add detail logs if distcp checksum mismatch,13534310,Open,Major,,27/Apr/23 11:10,,,"We encountered some errors of mismatch checksum during Distcp jobs. It took us some time to figure out that checksum type is different.

Adding error logs shall help us to figure out such problems faster."
ABFS handles nonexistent/non-auth accounts badly,13534833,Open,Major,,03/May/23 13:17,,3.3.5,"Testing storediag with a nonexistent account/bucket raises interesting failures modes

# DNS failures are retried, rather than fail fast
# a container which exists but you can't access fails with the wrong exception


{code}
org.apache.hadoop.fs.FileAlreadyExistsException: Operation failed: ""This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint."", 409, HEAD, https://somewhere.dfs.core.windows

{code}


Proposed
# failfast on host not found, escalate to an account not found
# recognise and translate that does not support BlobStorageEvents 

side issue: is that failure leaking secrets about another a/c? or is it always raised if not authed
"
Publish test jar for hadoop-aws project ,13534121,Open,Major,,26/Apr/23 09:59,,3.3.5,"At $dayJob, we are interested in consuming test util code for hadoop-aws. We can add a maven goal to the pom.xml for generating test jar as well "
upgrade snakeyaml to 2.0 (fixes CVE-2022-1471),13534114,Resolved,Major,Duplicate,26/Apr/23 08:33,26/Apr/23 08:37,,https://bitbucket.org/snakeyaml/snakeyaml/wiki/Changes
User in staticPriorities cost also shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost.,13531807,Open,Major,,07/Apr/23 01:43,,,"After HADOOP-17165, we can avoid to restrict some important user who has many request.

In HADOOP-17346, the solution is similar to HADOOP-17165, but the user is service user. And another improvement in HADOOP-17346 is that we can avoid backoff by response time. 

HADOOP-17280 solve the problem ""Service-user cost shouldn't be accumulated to totalDecayedCallCost and totalRawCallCost."" for HADOOP-17165. HADOOP-17346 also fix it.

I think some code is redundancy, we should reconstruct. "
Generic Build Improvements,13532457,Open,Major,,13/Apr/23 07:35,,,"Some proposed build changes.
 * Add  {{surefire.failIfNoSpecifiedTests}} as false in POM, else it fails if the test specified in -Dtest isn't there in that module, creates problem when you plan to run multiple tests across multiple sub-projects from the root of the project. (https://maven.apache.org/surefire/maven-surefire-plugin/test-mojo.html#failifnospecifiedtests)

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0:test (default-test) on project hadoop-build-tools: No tests matching pattern ""TestServiceInterruptHandling"" were executed! (Set -Dsurefire.failIfNoSpecifiedTests=false to ignore this error.)
{noformat}
 * Disable Concurrent builds: Folks push multiple commits in 5-10 mins while pre-commit is running already, so good to discourage this.

 * Add threshold to number of builds per day, saves resources for genuine PR's against someone pushing multiple commits. (This & the above one: Copied idea from Hive)

 * Leverage Github Actions to delegate some of the tasks to them, so a bit of parallel execution and might save time, may be explore pushing JDK-11 related stuff to Github Actions (We don't run tests as of now for both JDK-11 & 8, tests are for 8 only in precommit)"
Remove ozone mailing lists from Hadoop website,13532552,Resolved,Major,Fixed,13/Apr/23 18:23,14/Apr/23 20:06,,"Ozone is its own TLP. Drop its information from Hadoop website.

https://hadoop.apache.org/mailing_lists.html
"
InvalidProtocolBufferException caused by JDK 11 < 11.0.18 AES-CTR cipher state corruption with AVX-512 bug,13532401,Open,Major,,12/Apr/23 19:43,,,"This serves as a PSA for a JDK bug. Not really a bug in Hadoop / HDFS. Symptom/Workaround/Solution detailed below.

[~relek] identified [JDK-8292158|https://bugs.openjdk.org/browse/JDK-8292158] (backported to JDK 11 in [JDK-8295297|https://bugs.openjdk.org/browse/JDK-8295297]) causes HDFS clients to fail with InvalidProtocolBufferException due to corrupted protobuf message in Hadoop RPC request when all of the below conditions are met:

1. The host is capable of AVX-512 instruction sets
2. AVX-512 is enabled in JVM. This should be enabled by default on AVX-512 capable hosts, equivalent to specifying JVM arg {{-XX:UseAVX=3}}
3. Hadoop native library (e.g. libhadoop.so) is not available. So the HDFS client falls back using Hotspot JVM's {{aesctr_encrypt}} implementation for AES/CTR/NoPadding.
4. Client uses JDK 11. And OpenJDK version < 11.0.18

As a result, the client could print messages like these:

{code:title=Symptoms on the HDFS client}
2023-02-21 15:21:44,380 WARN org.apache.hadoop.hdfs.DFSClient: Connection failure: Failed to connect to <HOST/IP:PORT> for file /tmp/.cloudera_health_monitoring_canary_files/.canary_file_2023_02_21-15_21_25.b6788e89894a61b5 for block BP-1836197545-10.125.248.11-1672668423261:blk_1073935111_194857:com.google.protobuf.InvalidProtocolBufferException: Protocol message tag had invalid wire type.
com.google.protobuf.InvalidProtocolBufferException: Protocol message tag had invalid wire type.

2023-02-21 15:21:44,378 WARN org.apache.hadoop.hdfs.DFSClient: Connection failure: Failed to connect to <HOST/IP:PORT> for file /tmp/.cloudera_health_monitoring_canary_files/.canary_file_2023_02_21-15_21_25.b6788e89894a61b5 for block BP-1836197545-<IP>-1672668423261:blk_1073935111_194857:com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.
com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.

2023-02-21 15:06:55,530 WARN org.apache.hadoop.hdfs.DFSClient: Connection failure: Failed to connect to <HOST/IP:PORT> for file /tmp/.cloudera_health_monitoring_canary_files/.canary_file_2023_02_21-15_06_55.b4a633a8bde014aa for block BP-1836197545-<IP>-1672668423261:blk_1073935025_194771:com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field. This could mean either than the input has been truncated or that an embedded message misreported its own length.
com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field. This could mean either than the input has been truncated or that an embedded message misreported its own length.
{code}

The error message might mislead devs/users into thinking this is a Hadoop Common or HDFS bug (while it is a JDK bug in this case).


{color:red}Solutions:{color}
1. As a workaround, append {{-XX:UseAVX=2}} to client JVM args; or
2. Upgrade to OpenJDK >= 11.0.18.


I might post a repro test case for this, or find a way in the code to prompt the user that this could be the potential issue (need to upgrade JDK 11) when it occurs."
ITestS3ACopyFromLocalFile: AuditFailureException,13542137,Open,Minor,,30/Jun/23 17:11,,3.3.9,"test failure in AuditFailureException 


{code}
java.nio.file.AccessDeniedException: s3a://stevel-london/fork-0007/test/source5589796147222327638: org.apache.hadoop.fs.s3a.audit.AuditFailureException: 92cd5c39-d723-4c9c-add8-635f15196a2e-00000003 unaudited operation executing a request outside an audit span {object_list_request 'fork-0007/test/source5589796147222327638/' size=2, mutating=false}

{code}

the stack trace shows something is happening in an executor so maybe the context hasn't got in"
ITestS3AFileSystemStatistic failure on mvn verify,13541566,Open,Minor,,27/Jun/23 12:51,,,"But succeeds when running by itself. Most likely fs might be reused between tests and we count the fsStats.getBytesRead()

```

[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.393 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic
[ERROR] testBytesReadWithStream(org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic)  Time elapsed: 2.392 s  <<< FAILURE!
java.lang.AssertionError: Mismatch in number of FS bytes read by InputStreams expected:<2048> but was:<69944985>
    at org.junit.Assert.fail(Assert.java:89)
    at org.junit.Assert.failNotEquals(Assert.java:835)
    at org.junit.Assert.assertEquals(Assert.java:647)
    at org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic.testBytesReadWithStream(ITestS3AFileSystemStatistic.java:72)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

```"
Add RPC metrics for response time,13532496,Resolved,Minor,Fixed,13/Apr/23 13:20,21/Apr/23 19:37,3.4.0,
Improve S3ABlockOutputStream recovery,13532928,Reopened,Minor,,17/Apr/23 17:45,,,"If an application crashes during an S3ABlockOutputStream upload, it's possible to complete the upload if fast.upload.buffer is set to disk by uploading the s3ablock file with putObject as the final part of the multipart upload. If the application has multiple uploads running in parallel though and they're on the same part number when the application fails, then there is no way to determine which file belongs to which object, and recovery of either upload is impossible.

If the temporary file name for disk buffering included the s3 key, then every partial upload would be recoverable.

h3. Important disclaimer

This change does not directly add the Syncable semantics which applications that require {{Syncable.hsync()}} to only return after all pending data has been durably written to the destination path. S3 is not a filesystem and this change does not make it so.

What is does do is assist anyone trying to implement some post-crash recovery process which
# interrogates s3 to identofy pending uploads to a specific path and get a list of uploaded blocks yet to be committed
# scans the local fs.s3a.buffer dir directories to identify in-progress-write blocks for the same target destination. That is those which were being uploaded, queued for uploaded and the single ""new data being written to"" block for an output stream
# uploads all those pending blocks
# generates a new POST to complete a multipart upload with all the blocks in the correct order

All this patch does is ensure the buffered block filenames include the final path and block ID, to aid in identify which blocks need to be uploaded and what order. 

h2. warning
causes HADOOP-18744 -always include the relevant fix when backporting"
Fix several maven build warnings,13534078,Resolved,Minor,Fixed,26/Apr/23 03:29,11/Jun/23 06:09,3.4.0,"{code}
[WARNING] 'build.plugins.plugin.version' for org.cyclonedx:cyclonedx-maven-plugin is missing.
{code}

{code}
[WARNING] Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword
{code}

{code}
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:findbugs-maven-plugin is missing
{code}

{code}
[WARNING] Parameter 'requiresOnline' is unknown for plugin 'exec-maven-plugin:1.3.1:exec (pre-dist)'
{code}

{code}
[WARNING] Parameter 'destDir' (user property 'destDir') is deprecated: No reason given
{code}

{code}
[WARNING] Parameter 'tasks' is deprecated: Use target instead
[WARNING] Parameter tasks is deprecated, use target instead
{code}

{code}
[WARNING] Parameter 'systemProperties' is deprecated: Use systemPropertyVariables instead.
{code}"
S3A: reject multipart copy requests when disabled,13532349,Resolved,Minor,Fixed,12/Apr/23 13:27,27/Apr/23 10:01,3.4.0,"follow-on to HADOOP-18637 and support for huge file uploads with stores which don't support MPU.

* prevent use of API against any s3 store when disabled, using logging auditor to reject it
* tests to verify rename of huge files still works (by setting large part size)"
Upgrade maven-shade-plugin to 3.4.1,13540163,Resolved,Minor,Fixed,15/Jun/23 08:29,27/Jun/23 14:04,3.4.0,"Errors were reported while shading some of the upgraded dependencies due to their inclination towards later java versions e.g
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.3.0:shade (default) on project hadoop-client-runtime: Error creating shaded jar: Problem shading JAR /home/rohit/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.15.0/jackson-core-2.15.0.jar entry META-INF/versions/19/com/fasterxml/jackson/core/io/doubleparser/FastDoubleSwar.class: java.lang.IllegalArgumentException: Unsupported class file major version 63 {code}
Version 3.4.1 seems to have fixed such issues."
Add debug log for getting details of tokenKindMap,13533641,Resolved,Minor,Fixed,22/Apr/23 15:01,29/Apr/23 12:06,3.3.4,"Currently there is no way to know what is store in tokenKindMap. There are also no debug logs . I think we should add gettokenKindMap method , so that caller can see the content of the tokenKindMap . It will really help to debug . 
{code:java}
while (tokenIdentifiers.hasNext()) {
          try {
            TokenIdentifier id = tokenIdentifiers.next();
            *tokenKindMap.put(id.getKind(), id.getClass());*
          } catch (ServiceConfigurationError | LinkageError e) {
            // failure to load a token implementation
            // log at debug and continue.
            LOG.debug(""Failed to load token identifier implementation"", e);
          }
        }
      }
{code}"
AWS SDK v2  code tuning,13536276,Resolved,Minor,Duplicate,15/May/23 14:24,23/Aug/23 10:35,3.4.0,"tuning of the v2 sdk code prior to merge;

{code}
* 

hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AwsCredentialListProvider.java
L184 access denied exception. add test for this?

AWSClientConfig
TODO: Don't think you can set a socket factory for the netty client.


cloudstore: add the new paths
import software.amazon.awssdk.http.apache.ApacheHttpClient;
import software.amazon.awssdk.thirdparty.org.apache.http.conn.ssl.SSLConnectionSocketFactory;  
oftware.amazon.awssdk.services.s3.model.HeadBucketResponse;

hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java
+add test for getHeaders(/) to see what comes back

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java
L128 use explicit region constant rather than inline string

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AConfiguration.java
L552: use intercept()

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEndpointRegion.java
L75: just throw the exception again
L87, L90, use constants

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java
L44 move o.a.h. imports into ""real"" hadoop block; include the sets one too

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AProxy.java
is new ssl.proxy  setting consistent with what this pr does

hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestSessionDelegationInFileystem.java
L335 TODO open, getObjectMetadata(""/"")

+cut hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentS3ClientFactory.java

{code}
"
Support LZ4 compressionLevel,13541978,Open,Minor,,29/Jun/23 17:35,,,"The Hadoop's LZ4 compression codec now depends on lz4-java. There two type of compressor in lz4, `fastCompressor` and `highCompressor`. The default lz4 compressor in hadoop is fastCompressor, we also can use highCompressor by using config `IO_COMPRESSION_CODEC_LZ4_USELZ4HC_DEFAULT`

 

When we want to use highCompressor in hadoop, we only can use the default compression level, which is level 9. while highCompressor in lz4-java supports compression level from 1 to 17.

 

I think we can set a configuration to let users to choose different compresssionLevel for lz4 highCompressor."
Make it possible to disable ITestS3ATemporaryCredentials,13541567,Resolved,Minor,Done,27/Jun/23 12:54,28/Jun/23 20:14,,Could be done by setting the endpoint to a special value
BlockManager#addStoredBlock should log storage id when AddBlockResult is REPLACED,13539824,Resolved,Minor,Abandoned,13/Jun/23 10:13,13/Jun/23 10:17,,
S3ABlockOutputStream doesn't collect stats on multipart block uploads,13532398,Resolved,Minor,Duplicate,12/Apr/23 18:58,27/Apr/23 10:02,3.3.5,"assertion failures in HADOOP-18695 show that S3ABlockOutputStream isn't collecting stats on how long the individual post calls of multipart uploads: duration or number.
that is despite some counters in the IOStatistics of the stream set up to do this and merge to the FS in close()

Looks like the simplest fix is to instrument the FS to do collect these, rather than in-stream. in stream would be best for context stats"
add rpc metrics for response time,13532421,Reopened,Minor,,13/Apr/23 02:06,,,Rpc ResponseTime record the time to encode and send response. This should be added in the rpc metrics.
Move CodecPool getCompressor/getDecompressor logs to DEBUG,13534034,Open,Trivial,,25/Apr/23 16:48,,3.3.6,"The ""Got brand new compressor|decompressor"" logs in CodecPool[0] can be quite noisy when reading thousands of blocks and aren't that illuminating for the end user. I'd like to propose moving them from log.info to log.debug if there's no objection.

 

[0] https://github.com/apache/hadoop/blob/b737869e01fe3334b948a38fe3835e48873bf3a6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CodecPool.java#L149-L195"
Remove ozone from hadoop dev support,13542096,Resolved,Trivial,Fixed,30/Jun/23 12:39,02/Jul/23 07:24,3.4.0,Remove ozone at dev support script which has its own TLP now.
Wrong StringUtils.join() called in AbstractContractRootDirectoryTest,13533562,Resolved,Trivial,Fixed,21/Apr/23 13:56,24/Apr/23 13:50,3.3.6,"In the following call to {{StringUtils.join()}}, {{""\n""}} is treated as an element, not as a separator:

{code:title=https://github.com/apache/hadoop/blob/964c1902c8054dfe13c787222a12fb0daf1aaab9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractRootDirectoryTest.java#L199-L201}
    assertEquals(""listStatus on empty root-directory returned found: ""
        + join(""\n"", rootListStatus),
        0, rootListStatus.length);
{code}

Expected output:

{code}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.711 s <<< FAILURE! - in org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRootDirectory
[ERROR] testListEmptyRootDirectory(org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRootDirectory)  Time elapsed: 0.011 s  <<< FAILURE!
java.lang.AssertionError: listStatus on empty root-directory returned found: HdfsLocatedFileStatus{path=hdfs://localhost:37225/test; isDirectory=true; ...} expected:<0> but was:<1>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testListEmptyRootDirectory(AbstractContractRootDirectoryTest.java:199)
{code}

Actual output:

{code}
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.708 s <<< FAILURE! - in org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRootDirectory
[ERROR] testListEmptyRootDirectory(org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRootDirectory)  Time elapsed: 0.011 s  <<< FAILURE!
java.lang.AssertionError: 
listStatus on empty root-directory returned found: 
[Lorg.apache.hadoop.fs.FileStatus;@73344c46 expected:<0> but was:<1>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testListEmptyRootDirectory(AbstractContractRootDirectoryTest.java:199)
{code}"
Fix WriteOperations.listMultipartUploads function description,13534737,Resolved,Trivial,Fixed,02/May/23 19:04,04/May/23 18:40,3.3.2,"{code:java}
   /**
-   * Abort multipart uploads under a path: limited to the first
-   * few hundred.
-   * @param prefix prefix for uploads to abort
-   * @return a count of aborts
-   * @throws IOException trouble; FileNotFoundExceptions are swallowed.
+   * List in-progress multipart uploads under a path
+   * @param prefix prefix for uploads to list
+   * @return a list of in-progress multipart uploads
+   * @throws IOException on problems
    */
   List<MultipartUpload> listMultipartUploads(String prefix) {code}"
WeakReferenceMap creation warning log doesn't format correctly,13539636,Open,Trivial,,12/Jun/23 09:40,,3.3.5,"{{WeakReferenceMap.create(K key)}} logs once at warning level if a reference to a key was lost during creation.



{code}
referenceLostDuringCreation.warn(""reference to %s lost during creation"", key);
{code}
but it uses %s as the insertion point for the key, when it should be {}.

this means the log, if it ever surfaces, won't include the key. 

i don't think this is a very important issue, but it exists.
"
cmake CompileMojo resolution,13539820,Open,Trivial,,13/Jun/23 09:52,,3.3.5,class {{org.apache.hadoop.maven.plugin.cmakebuilder.CompileMojo}} isn't doing its native path resolution properly -this may break the build.
Bump netty to the latest 4.1.61,13368769,Resolved,Blocker,Fixed,31/Mar/21 04:23,05/Apr/21 00:36,3.2.3,"For more details: https://netty.io/news/2021/03/09/4-1-60-Final.html

Actually, just yesterday there's a new version 4.1.61. https://netty.io/news/2021/03/30/4-1-61-Final.html"
Build failure due to python2.7 deprecation by pip,13354617,Resolved,Blocker,Fixed,26/Jan/21 07:54,26/Jan/21 14:07,3.4.0,"The latest version of pip has deprecated its support for python 2.7 - [pip install on Python 2 fails with SyntaxError: sys.stderr.write(f""ERROR: \{exc}"") · Issue #9500 · pypa/pip (github.com)|https://github.com/pypa/pip/issues/9500]

We encounter the following error while building Hadoop -
{code:java}
11:24:56  [0m[91mTraceback (most recent call last):
11:24:56    File ""get-pip.py"", line 24226, in <module>
11:24:56  [0m[91m    main()
11:24:56    File ""get-pip.py"", line 199, in main
11:24:56      bootstrap(tmpdir=tmpdir)
11:24:56    File ""get-pip.py"", line 82, in bootstrap
11:24:56      from pip._internal.cli.main import main as pip_entry_point
11:24:56    File ""/tmp/tmpDXDtcP/pip.zip/pip/_internal/cli/main.py"", line 60
11:24:56      sys.stderr.write(f""ERROR: {exc}"")
11:24:56                                     ^
11:24:56  SyntaxError: invalid syntax {code}
Link : [https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2567/16/consoleFull]"
[branch-2.10] Docker image build fails due to the removal of openjdk-7-jdk package,13363436,Resolved,Blocker,Fixed,10/Mar/21 06:57,11/Mar/21 12:57,2.10.1,"openjdk-7-jdk has been removed from the openjdk ppa at the end of February 2021.

[https://launchpad.net/~openjdk-r/+archive/ubuntu/ppa/+packages?field.name_filter=&field.status_filter=superseded&field.series_filter=xenial]"
[ERROR] unknown os.arch: riscv -> [Help 1] org.apache.maven.MavenExecutionException: unknown os.arch: riscv,13354017,Open,Blocker,,22/Jan/21 18:40,,,"I a have a remote access to [SiFive's U54-MC |https://www.sifive.com/cores/u54-mc]board.

[Here is |https://gist.github.com/advancedwebdeveloper/dbaaf3c8e1b9bf26d240ec4997dad815]the full log.

Since I have no exact list of JNI features/Maven artifacts, to consider - it is not clear from which of those I should start porting."
DistCp: Reduce memory usage on copying huge directories,13358863,Resolved,Critical,Fixed,16/Feb/21 13:04,27/Mar/21 04:01,3.3.1,"Presently distCp, uses the producer-consumer kind of setup while building the listing, the input queue and output queue are both unbounded, thus the listStatus grows quite huge.

Rel Code Part :

https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java#L635

This goes on bredth-first traversal kind of stuff(uses queue instead of earlier stack), so if you have files at lower depth, it will like open up the entire tree and the start processing...."
Where is hadoop-user-functions.sh.examples ?,13362092,Resolved,Critical,Invalid,03/Mar/21 12:51,03/Mar/21 13:20,3.1.3,"in UnixShellGuide page [https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-common/UnixShellGuide.html]

it is mentioned that 

{{Examples of function replacement are in the ??{{hadoop-user-functions.sh.examples}}?? file.}}

I've searched through whole Hadoop directory and source code, but there is no trace of this file except:

[hadoop-common-project/hadoop-common/src/site/markdown/UnixShellGuide.md]

which only mentions ??Examples of function replacement are in the `hadoop-user-functions.sh.examples` file.??

 "
Update Dockerfile to use Focal,13351659,Resolved,Major,Fixed,12/Jan/21 06:56,26/Jan/21 05:16,3.4.0,"Referring to the current Dockerfile, it seems like the toolchain provided by Ubuntu Bionic isn't on track with the versions of the libraries that are needed. Thus, we are separately installing Boost and other libraries (gcc-9 and CMake 3.19 needed by HDFS-15740). All these won't be necessary if we upgrade to Focal as these library versions are part of the Focal toolchain itself."
S3A doesn't calculate Content-MD5 on uploads,13355037,Open,Major,,27/Jan/21 22:19,,,"Hadoop doesn't specify the Content-MD5 of an object when uploading it to an S3 Bucket. This prevents uploads to buckets with Object Lock, that require the Content-MD5 to be specified.

 
{code:java}
com.amazonaws.services.s3.model.AmazonS3Exception: Content-MD5 HTTP header is required for Put Part requests with Object Lock parameters (Service: Amazon S3; Status Code: 400; Error Code: InvalidRequest; Request ID: ****************; S3 Extended Request ID: ****************************************************************************; Proxy: null), S3 Extended Request ID: ****************************************************************************
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5248)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5195)
	at com.amazonaws.services.s3.AmazonS3Client.doUploadPart(AmazonS3Client.java:3768)
	at com.amazonaws.services.s3.AmazonS3Client.uploadPart(AmazonS3Client.java:3753)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.uploadPart(S3AFileSystem.java:2230)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$uploadPart$8(WriteOperationHelper.java:558)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)
	... 15 more{code}
 

Similar to https://issues.apache.org/jira/browse/JCLOUDS-1549

Related to https://issues.apache.org/jira/browse/HADOOP-13076"
Make SM4 support optional for OpenSSL native code,13368284,Resolved,Major,Fixed,29/Mar/21 05:20,08/Aug/24 12:06,3.4.0,openssl-devel-1.1.1g provided by CentOS 8 does not work after HDFS-15098 because the SM4 is not enabled on the openssl package. We should not force users to install OpenSSL from source code even if they do not use SM4 feature.
Upgrade guice to 4.2.3,13348637,Resolved,Major,Fixed,01/Jan/21 00:17,25/Jan/21 09:01,3.4.0,"Upgrade guice to 4.2.3 to fix compatibility issue:
{noformat}
Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.inject.util.Types.collectionOf(Ljava/lang/reflect/Type;)Ljava/lang/reflect/ParameterizedType;
» at com.google.inject.multibindings.Multibinder.collectionOfProvidersOf(Multibinder.java:202)
» at com.google.inject.multibindings.Multibinder$RealMultibinder.<init>(Multibinder.java:283)
» at com.google.inject.multibindings.Multibinder$RealMultibinder.<init>(Multibinder.java:258)
» at com.google.inject.multibindings.Multibinder.newRealSetBinder(Multibinder.java:178)
» at com.google.inject.multibindings.Multibinder.newSetBinder(Multibinder.java:150)
» at org.apache.druid.guice.LifecycleModule.getEagerBinder(LifecycleModule.java:115)
» at org.apache.druid.guice.LifecycleModule.configure(LifecycleModule.java:121)
» at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)
» at com.google.inject.spi.Elements.getElements(Elements.java:110)
» at com.google.inject.util.Modules$OverrideModule.configure(Modules.java:177)
» at com.google.inject.AbstractModule.configure(AbstractModule.java:62)
» at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)
» at com.google.inject.spi.Elements.getElements(Elements.java:110)
» at com.google.inject.util.Modules$OverrideModule.configure(Modules.java:177)
» at com.google.inject.AbstractModule.configure(AbstractModule.java:62)
» at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)
» at com.google.inject.spi.Elements.getElements(Elements.java:110)
» at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)
» at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)
» at com.google.inject.Guice.createInjector(Guice.java:96)
» at com.google.inject.Guice.createInjector(Guice.java:73)
» at com.google.inject.Guice.createInjector(Guice.java:62)
» at org.apache.druid.initialization.Initialization.makeInjectorWithModules(Initialization.java:431)
» at org.apache.druid.cli.GuiceRunnable.makeInjector(GuiceRunnable.java:69)
» at org.apache.druid.cli.ServerRunnable.run(ServerRunnable.java:58)
» at org.apache.druid.cli.Main.main(Main.java:113)
{noformat}"
ABFS: Introduce Lease Operations with Append to provide single writer semantics,13365213,Open,Major,,15/Mar/21 10:59,,,"The lease operations will be introduced as part of Append, Flush to ensure the single writer semantics.

 

Details:

Acquire Lease will be introduced in Create, Auto-Renew, Acquire will be added to Append & Release, Auto-Renew, Acquire in Flush.

 

Duration the creation of the file the lease will be acquired, as part of appends the lease will be auto-renewed & the lease can be released as part of flush.

 

By default the lease duration will be of 60 seconds.

""fs.azure.write.enforcelease"" & ""fs.azure.write.lease.duration"" two configs will be introduced."
"ABFS test setup failing ""The specified filesystem already exists""",13360127,Open,Major,,22/Feb/21 14:54,,3.3.1,"been hitting this in parallel test runs. Even though FS Creation is meant to catch and swallow exceptions of code {{AzureServiceErrorCode.FILE_SYSTEM_ALREADY_EXISTS}}, this doesn't happen because the exception is translated into FileAlreadyExistsException before that catch/swallow"
Pass exception to the caller in AbfsClient#appendSASTokenToQuery,13357748,Open,Major,,09/Feb/21 11:53,,3.3.0,"{code:java}
Failed to acquire a SAS token for get-status on <path> due to java.lang.NullPointerException
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.appendSASTokenToQuery(AbfsClient.java:677)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.appendSASTokenToQuery(AbfsClient.java:643)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:419)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:652)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:470)
	at org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:458)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:371)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.footerFileMetaData$1(ParquetFileFormat.scala:370)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:374)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:645)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:445)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1289)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:451)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748){code}
Exact reason for NPE is not propagated here.

 

CC [~stevel@apache.org] [~mehakmeetSingh] "
Use Slf4jRequestLog for HttpRequestLog,13358270,Resolved,Major,Fixed,12/Feb/21 05:38,10/Mar/22 02:16,3.4.0,"Now the log file name and retain days in HttpRequestLog is configured from log4j.properties. It can be configured from Configuration instead to drop the usage of log4j API.

==== Update ====

Use jetty's Slf4jRequestLogWriter, we can remove the code dependency on log appenders."
public interface GroupMappingServiceProvider needs default impl for getGroupsSet() ,13355351,Resolved,Major,Fixed,29/Jan/21 06:03,21/Apr/21 19:35,3.4.0,"HADOOP-17079 added ""GroupMappingServiceProvider#getGroupsSet()"" interface.

But since this is a public interface, it will break compilation of existing implementations in downstreams.

Consider adding a default implementation in the interface to avoid such failures."
Incorrect representation of RESPONSE for Get Key Version in KMS index.md.vm file,13368833,Resolved,Major,Fixed,31/Mar/21 09:50,07/Apr/21 18:22,3.3.1,"Format of RESPONSE of Get Key Versions in KMS index.md.vm is incorrect

[https://hadoop.apache.org/docs/r3.1.1/hadoop-kms/index.html#Get_Key_Versions]

Attached the outputs of the commands for reference"
CryptoInputStream#close() should be synchronized,13365004,Resolved,Major,Fixed,13/Mar/21 18:25,06/Apr/21 12:40,3.3.5,"org.apache.hadoop.crypto.CryptoInputStream.close() - when 2 threads try to close the stream second thread, fails with error.

This operation should be synchronized to avoid multiple threads to perform the close operation concurrently.

 !image-2021-03-13-23-56-18-865.png|thumbnail! "
Kinit with keytab should not display the keytab file's full path in any logs,13365000,Resolved,Major,Fixed,13/Mar/21 18:09,02/Apr/21 04:36,3.3.1," The keytab is sensitive information, and the full path should not be printed in the log"
DelegationTokenAuthenticator prints token information,13368386,Resolved,Major,Fixed,29/Mar/21 14:30,02/Apr/21 04:30,3.3.1,"Resource Manager logs print token information , as this is sensitive information it must be exempted from being printed "
Improve UGI debug log to help troubleshooting TokenCache related issues,13363950,Resolved,Major,Fixed,11/Mar/21 22:04,17/Mar/21 17:57,3.2.0,"We have seen some issues around TokenCache getDelegationToken failures even though the UGI already has a valid token. The tricky part is the token map is keyed by the canonical service name, which can be different from the actual service field in the token, e.g. KMS token in HA case. The current UGI log dumps all the tokens but not the keys of the token map. This ticket is opened to include the complete token map information in the debug log.  

"
Upgrade org.codehaus.woodstox:stax2-api to 4.2.1,13364648,Resolved,Major,Fixed,12/Mar/21 13:06,13/Mar/21 10:00,2.10.2,Upgrade stax2-api to 4.2.1
Fix reference to LOG is ambiguous after HADOOP-17482,13363996,Resolved,Major,Fixed,12/Mar/21 03:05,12/Mar/21 19:28,3.4.0,"HADOOP-17482 changes to have two slf4j LOG instances for FileSystem.class.  This seems to breaks the Hadoop CI/Jenkins as some tests using this LOG directly are hitting the ambiguity issue between two slf4j Logger instances to the same FileSystem.class failed the build. This ticket is opened to fix those tests to unblock CI. 

 
{code:java}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2762/src/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java:[1424,25] error: reference to LOG is ambiguous
[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2762/src/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithKMS.java:[102,25] error: reference to LOG is ambiguous
[INFO] 2 errors 
{code}"
Fix compilation error of OBSFileSystem in trunk,13363499,Resolved,Major,Fixed,10/Mar/21 10:07,10/Mar/21 13:25,3.4.0,"{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-huaweicloud: Compilation failure
[ERROR] /home/centos/srcs/hadoop/hadoop-cloud-storage-project/hadoop-huaweicloud/src/main/java/org/apache/hadoop/fs/obs/OBSFileSystem.java:[396,58] incompatible types: org.apache.hadoop.util.BlockingThreadPoolExecutorService cannot be converted to org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListeningExecutorService
{noformat}"
Hadoop prints sensitive Cookie information.,13356015,Resolved,Major,Fixed,01/Feb/21 17:59,24/Feb/21 09:53,3.3.0,"org.apache.hadoop.security.authentication.client.AuthenticatedURL.AuthCookieHandler#setAuthCookie - prints cookie information in log. Any sensitive infomation in Cookies will be logged, which needs to be avaided.

LOG.trace(""Setting token value to {} ({})"", authCookie, oldCookie);"
Checkstyle IllegalImport does not catch guava imports,13356539,Resolved,Major,Fixed,03/Feb/21 17:02,05/Feb/21 07:47,3.4.0,"Although YARN-10352 introduces {{guava iterator import}}, it was committed to trunk without checkstyle errors.

According to [IllegalImportCheck#setIllegalPkgs |https://github.com/checkstyle/checkstyle/blob/master/src/main/java/com/puppycrawl/tools/checkstyle/checks/imports/IllegalImportCheck.java], the packages regex should be the prefix of the package. The code automatically append {{\.*}} to the regex.

CC: [~aajisaka]
 "
Upgrade maven-site-plugin to 3.11.0,13361241,Resolved,Major,Fixed,26/Feb/21 18:17,21/Apr/22 13:25,3.3.5,"After upgrading maven-site-plugin, error messages will be more detailed and it will help debugging.
 
Ref: https://issues.apache.org/jira/browse/YARN-10656?focusedCommentId=17291846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17291846"
DistCp: Expose the JobId for applications executing through run method,13366067,Resolved,Major,Fixed,18/Mar/21 12:24,19/Mar/21 09:11,3.2.3,"Presently the job id is set in the JobConf, and If an application submits using execute method, it can fetch the job id, but if it goes through the run method flow, There isn't a way to get the job id."
Add thread-level IOStatistics Context,13351135,Resolved,Major,Fixed,08/Jan/21 17:48,27/Jul/22 10:23,3.3.1,"For effective reporting of the iostatistics of individual worker threads, we need a thread-level context which IO components update.

* this contact needs to be passed in two background thread forming work on behalf of a task.
* IO Components (streams, iterators, filesystems) need to update this context statistics as they perform work
* Without double counting anything.

I imagine a ThreadLocal IOStatisticContext which will be updated in the FileSystem API Calls. This context MUST be passed into the background threads used by a task, so that IO is correctly aggregated.

I don't want streams, listIterators &c to do the updating as there is more risk of double counting. However, we need to see their statistics if we want to know things like ""bytes discarded in backwards seeks"". And I don't want to be updating a shared context object on every read() call.
If all we want is store IO (HEAD, GET, DELETE, list performance etc) then the FS is sufficient. 
If we do want the stream-specific detail, then I propose
* caching the context in the constructor
* updating it only in close() or unbuffer() (as we do from S3AInputStream to S3AInstrumenation)
* excluding those we know the FS already collects.


h3. important
when backporting, please follow with HADOOP-18373


"
Add an Audit plugin point for S3A auditing/context,13356217,Resolved,Major,Fixed,02/Feb/21 14:49,25/May/21 12:03,3.3.1,"Add a way for auditing tools to correlate S3 object calls with Hadoop FS API calls.

Initially just to log/forward to an auditing service.

Later: let us attach them as parameters in S3 requests, such as opentrace headeers or (my initial idea: http referrer header -where it will get into the log)

Challenges
* ensuring the audit span is created for every public entry point. That will have to include those used in s3guard tools, some defacto public APIs
* and not re-entered for active spans. s3A code must not call back into the FS API points
* Propagation across worker threads


Documentation Links

* Using: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/auditing.md
* architecture: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/auditing_architecture.md
"
Upgrade ant to 1.10.9,13356960,Resolved,Major,Fixed,05/Feb/21 07:35,10/Feb/21 23:51,3.3.1,Upgrade Apache Ant to fix CVE-2020-1945 and CVE-2020-11979
Remove EventCounter and Log counters from JVM Metrics,13358265,Resolved,Major,Fixed,12/Feb/21 05:06,15/Apr/21 09:06,3.4.0,EventCount is using Log4J 1.x API. We need to remove it to drop Log4J 1.x.
Mark KeyProvider as Stable,13360479,Resolved,Major,Fixed,24/Feb/21 02:45,30/Aug/21 00:56,3.4.0,"Now, o.a.h.crypto.key.KeyProvider.java is marked Public and Unstable. I think the class is very stable, and it should be annotated as Stable."
Apply YETUS-1102 to re-enable GitHub comments,13363184,Resolved,Major,Fixed,09/Mar/21 07:08,11/Mar/21 14:27,3.3.1,"Yetus 0.13.0 enabled updating GitHub status instead of commenting the report, however, the report comments are still useful for some cases. Let's apply YETUS-1102 to re-enable the comments."
Upgrade com.fasterxml.woodstox:woodstox-core for security reasons,13363422,Resolved,Major,Fixed,10/Mar/21 05:48,11/Mar/21 20:51,3.3.1,"Due to security concerns (CVE: sonatype-2018-0624), we should bump up woodstox-core to 5.3.0."
Replace GitHub App Token with GitHub OAuth token,13364032,Resolved,Major,Fixed,12/Mar/21 07:23,12/Mar/21 09:06,3.3.1,"GitHub App Token expires within 1 hour, so Yetus fails to write GitHub comments in most cases."
Correct timestamp format in the docs for the touch command,13364232,Resolved,Major,Fixed,12/Mar/21 11:20,14/Mar/21 10:11,3.4.0,"The touch command was added by HADOOP-9214, but the usage instructions and docs have never aligned with the code.

The code uses a timestamp format of:

{code}
 new SimpleDateFormat(""yyyyMMdd:HHmmss"");
{code}

But the docs indicate it should be yyyyMMddHHmmss.

While I believe the format in the docs is better, tools or applications may be relying on the old format now, so I suggest we simply update the docs and usage, and improve the error message if the wrong format is used."
Fix TestKMS failure,13368254,Resolved,Major,Fixed,29/Mar/21 01:51,12/Apr/21 03:59,3.3.1,"[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/460/artifact/out/patch-unit-hadoop-common-project_hadoop-kms.txt]

The following https tests are flaky:
 * testStartStopHttpsPseudo
 * testStartStopHttpsKerberos
 * testDelegationTokensOpsHttpsPseudo

{noformat}
[ERROR] testStartStopHttpsPseudo(org.apache.hadoop.crypto.key.kms.server.TestKMS)  Time elapsed: 1.354 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$1.call(TestKMS.java:553)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$1.call(TestKMS.java:534)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.runServer(TestKMS.java:258)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.runServer(TestKMS.java:235)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.runServer(TestKMS.java:230)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStop(TestKMS.java:534)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo(TestKMS.java:634){noformat}"
Upgrade Zookeeper to 3.6.3 and Curator to 5.2.0,13368667,Resolved,Major,Fixed,30/Mar/21 14:52,10/May/23 03:21,3.4.0,"Let's upgrade Zookeeper and Curator to 3.6.3 and 5.2.0 respectively.

Curator 5.2 also supports Zookeeper 3.5 servers."
Change ipc.client.rpc-timeout.ms from 0 to 120000 by default to avoid potential hang,13361408,Resolved,Major,Fixed,28/Feb/21 06:25,06/Mar/21 13:28,3.2.2,"    We are doing some systematic fault injection testing in Hadoop-3.2.2 and when we try to run a client (e.g., `bin/hdfs dfs -ls /`) to our HDFS cluster (1 NameNode, 2 DataNodes), the client gets stuck forever. After some investigation, we believe that it’s a bug in `hadoop.ipc.Client` because the read method of `hadoop.ipc.Client$Connection$PingInputStream` keeps swallowing `java.net.SocketTimeoutException` due to the mistaken usage of the `rpcTimeout` configuration in the `handleTimeout` method.

*Reproduction*

    Start HDFS with the default configuration. Then execute a client (we used the command `bin/hdfs dfs -ls /` in the terminal). While HDFS is trying to accept the client’s socket, inject a socket error (java.net.SocketException or java.io.IOException), specifically at line 1402 (line 1403 or 1404 will also work).

    We prepare the scripts for reproduction in a gist ([https://gist.github.com/functioner/08bcd86491b8ff32860eafda8c140e24]).

*Diagnosis*

    When the NameNode tries to accept a client’s socket, basically there are 4 steps:
 # accept the socket (line 1400)
 # configure the socket (line 1402-1404)
 # make the socket a Reader (after line 1404)
 # swallow the possible IOException in line 1350

{code:java}
//hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java

    public void run() {
      while (running) {
        SelectionKey key = null;
        try {
          getSelector().select();
          Iterator<SelectionKey> iter = getSelector().selectedKeys().iterator();
          while (iter.hasNext()) {
            key = iter.next();
            iter.remove();
            try {
              if (key.isValid()) {
                if (key.isAcceptable())
                  doAccept(key);
              }
            } catch (IOException e) {                         // line 1350
            }
            key = null;
          }
        } catch (OutOfMemoryError e) {
          // ...
        } catch (Exception e) {
          // ...
        }
      }
    }

    void doAccept(SelectionKey key) throws InterruptedException, IOException, 
        OutOfMemoryError {
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) {           // line 1400

        channel.configureBlocking(false);                     // line 1402
        channel.socket().setTcpNoDelay(tcpNoDelay);           // line 1403
        channel.socket().setKeepAlive(true);                  // line 1404
        
        Reader reader = getReader();
        Connection c = connectionManager.register(channel,
            this.listenPort, this.isOnAuxiliaryPort);
        // If the connectionManager can't take it, close the connection.
        if (c == null) {
          if (channel.isOpen()) {
            IOUtils.cleanup(null, channel);
          }
          connectionManager.droppedConnections.getAndIncrement();
          continue;
        }
        key.attach(c);  // so closeCurrentConnection can get the object
        reader.addConnection(c);
      }
    }
{code}
    When a SocketException occurs in line 1402 (or 1403 or 1404), the server.accept() in line 1400 has finished, so we expect the following behavior:
 # The server (NameNode) accepts this connection but it will basically write nothing to this connection because it’s not added as a Reader data structure.
 # The client is aware that the connection has been established, and tries to read and write in this connection. After some time threshold, the client finds that it can’t read anything from this connection and exits with some exception or error.

    However, we do not observe behavior 2. The client just gets stuck forever (>10min). We re-examine the default configuration in [https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/core-default.xml] and we believe that the client should be able to time out in 60 seconds, according to the parameter `ipc.client.rpc-timeout.ms`, `ipc.client.ping` and `ipc.ping.interval`.

    We find where the client gets stuck from the jstack dump:
{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007f5554019800 nid=0x4312 in Object.wait() [0x00007f555d62e000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x000000071cc19ff0> (a org.apache.hadoop.ipc.Client$Call)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1551)
        - locked <0x000000071cc19ff0> (a org.apache.hadoop.ipc.Client$Call)
        at org.apache.hadoop.ipc.Client.call(Client.java:1508)
        at org.apache.hadoop.ipc.Client.call(Client.java:1405)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
        at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:910)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        - locked <0x000000071cb435b8> (a org.apache.hadoop.io.retry.RetryInvocationHandler$Call)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1671)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1602)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1599)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1614)
        at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)
        at org.apache.hadoop.fs.Globber.doGlob(Globber.java:294)
        at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
        at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2050)
        at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:353)
        at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:250)
        at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233)
        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:104)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:390)
{code}
    According to org.apache.hadoop.ipc.Client.call(Client.java:1508), the runtime value of timeout in org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1551) is -1, meaning that it waits forever. The only way of notifying it is within the callComplete method of Client$Call (Client.java:367). To invoke callComplete, there are only 2 methods in Client$Call: setException and setRpcResponse.

    Our expectation is that after the timeout in the client happens, the setException will be finally invoked. By some analysis, we will explain the workflow of dealing with this timeout issue and point out where the bug is.

    The setException method should be invoked by the Client$Connection thread:
{code:java}
    @Override
    public void run() {
      // ...
      try {
        while (waitForWork()) {                       // line 1086
          receiveRpcResponse();                       // line 1087
        }
      } catch (Throwable t) {
        // ...
        markClosed(new IOException(""Error reading responses"", t));
      }
      close();                                        // line 1097
      // ...
    }    private void receiveRpcResponse() {
      // ...
      try {
        ByteBuffer bb = ipcStreams.readResponse();    // line 1191
        // ...
      } catch (IOException e) {
        markClosed(e);                                // line 1235
      }
    }
{code}
    When the timeout happens, the correct workflow is:
 # Before the I/O, the run method invokes receiveRpcResponse (line 1087) and then invokes readResponse (line 1191)
 # After timeout, the readResponse invocation (line 1191) throws java.net.SocketTimeoutException
 # This exception is caught by markClosed (line 1235) and handled
 # The waitForWork (line 1086) returns false due to markClosed’s handling
 # The close method (line 1097) gets run, and finally invokes the setException method, which will unlock the org.apache.hadoop.fs.FsShell.main thread that we currently get stuck in.

    The bug is within step 2. We confirmed that this Client$Connection thread is running the readResponse invocation (line 1191) forever, without throwing any exception. We have the jstack dump of this Client$Connection thread:
{code:java}
""IPC Client (1390869998) connection to /127.0.0.1:9000 from whz"" #16 daemon prio=5 os_prio=0 tid=0x00007f555551e000 nid=0x432b run
nable [0x00007f5524126000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        - locked <0x000000071ce57d28> (a sun.nio.ch.Util$3)
        - locked <0x000000071ce57ca0> (a java.util.Collections$UnmodifiableSet)
        - locked <0x000000071ce578d8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:336)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        - locked <0x000000071ce85708> (a java.io.BufferedInputStream)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:562)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1881)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1191)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1087)
{code}
    By investigating each level of this stack trace, we confirmed that there should be a java.net.SocketTimeoutException thrown by org.apache.hadoop.net.SocketIOWithTimeout.doIO ([https://github.com/apache/hadoop/blob/rel/release-3.2.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketIOWithTimeout.java#L164]), but this exception is swallowed by org.apache.hadoop.ipc.Client$Connection$PingInputStream.read:
{code:java}
      public int read() throws IOException {
        int waiting = 0;
        do {
          try {
            return super.read();                // appear in stack trace
          } catch (SocketTimeoutException e) {
            waiting += soTimeout;
            handleTimeout(e, waiting);          // line 565
          }
        } while (true);                         // line 567
      }
{code}
    The handleTimeout invocation (line 565) should throw this SocketTimeoutException again. Otherwise the infinite loop indicated by line 567 will run forever.

    However, the handleTimeout method fails to enter the branch of throwing exception:
{code:java}
     /* Process timeout exception
       * if the connection is not going to be closed or 
       * the RPC is not timed out yet, send a ping.
       */
      private void handleTimeout(SocketTimeoutException e, int waiting)
          throws IOException {
        if (shouldCloseConnection.get() || !running.get() ||
            (0 < rpcTimeout && rpcTimeout <= waiting)) {            // line 545
          throw e;
        } else {
          sendPing();                                               // line 548
        }
      }
{code}
    It goes to line 548 and sends PING. PING can be sent successfully because the connection is currently not broken. However, in the correct behavior, line 545 is evaluated to be true and the exception can be thrown again.

    Line 545 is evaluated to be false, because the rpcTimeout variable (default value: 0) (`ipc.client.rpc-timeout.ms` in the default configuration [https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/core-default.xml] ) seems to be used in an incorrect way in this function (handleTimeout). According to the explanation of `ipc.client.rpc-timeout.ms` in the default configuration [https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/core-default.xml], “If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval.”

    The rpcTimeout is used correctly in the constructor of Client$Connection class:
{code:java}
    Connection(ConnectionId remoteId, int serviceClass,
        Consumer<Connection> removeMethod) {
      // ...
      if (rpcTimeout > 0) {
        // effective rpc timeout is rounded up to multiple of pingInterval
        // if pingInterval < rpcTimeout.
        this.soTimeout = (doPing && pingInterval < rpcTimeout) ?
            pingInterval : rpcTimeout;
      } else {
        this.soTimeout = pingInterval;
      }
      // ...
    }
{code}
    We have confirmed that, in the handleException method, if we use this.soTimeout variable prepared by this constructor, then this bug is fixed.

*Fix*

    We propose that we should modify the line 545 of the handleException method of the Client$Connection class. The value of rpcTimeout should be used in the way that the constructor of Client$Connection deals with it."
Replace LogCapturer with mock,13358264,Open,Major,,12/Feb/21 05:02,,,"LogCapturer uses Log4J1 API, and it should be removed. Mockito can be used instead for capturing logs."
Support log4j2 API in GenericTestUtils.setLogLevel,13358267,Open,Major,,12/Feb/21 05:27,,,"GenericTestUtils.setLogLevel depends on Log4J 1.x API, should be updated to use Log4J 2.x API."
Support Custom Endpoint For Hadoop Azure working with Storage Emulator,13365047,Patch Available,Major,,14/Mar/21 11:47,,3.3.0,"When using the Hadoop Azure library to write files using the wasb:// schema, and using the configuration for Hadoop Azure storage emulator (fs.azure.storage.emulator.account.name), The client assumes that the emulator will run on localhost with the default port.
When writing ITs and using Test Containers, the emulator will run on a custom endpoint but there is no way to tell that for the client.

The problem is in AzureNativeFileSystemStore.java, in connectUsingCredentials method,
which runs:

CloudStorageAccount account = CloudStorageAccount.getDevelopmentStorageAccount()

Which will use the default localhost address.

The fix is easy (And I already fixed it and tested it on my own environment, and I will PR the fix)

CloudStorageAccount.getDevelopmentStorageAccounts has already an overload that accepts URI for a custom endpont, and we can use it.




 "
Understanding Netty versions and upgrading them (three findings in Hadoop we could upgrade?),13361627,Open,Major,,01/Mar/21 13:50,,,"Hi everyone, have been raising a few JIRAs recently related to dependencies in Flink and Hadoop, and for Hadoop I have noticed the following versions of Netty in use. I'm wondering if we can work to upgrade these (potentially all to the same version) to remediate any CVEs we have. 

 

Here's what the Twistlock container scan picked up (so, this is Flink with Hadoop 3.3.1 snapshot, which I've scanned), so any thoughts or upgrade ideas would be most welcome.

 

""version"": ""3.10.6.Final""
 ""name"": ""io.netty_netty""

""path"": ""/opt/flink/lib/flink-shaded-hadoop-3-uber-3.3.1-SNAPSHOT-10.0.jar""

 

""version"": ""4.1.50.Final""
""name"": ""io.netty_netty-all""

""path"": ""/opt/flink/lib/flink-shaded-hadoop-3-uber-3.3.1-SNAPSHOT-10.0.jar""

 

""version"": ""4.1.42.Final""
""name"": ""io.netty_netty-codec""

""path"": ""/opt/flink/lib/flink-shaded-hadoop-3-uber-3.3.1-SNAPSHOT-10.0.jar""

 

The latest 4.1 Netty I see is

 {{[https://mvnrepository.com/artifact/io.netty/netty-all/4.1.59.Final]}}

 

which may help with the above findings (assume things are all compatible!), thanks

 "
Update Bouncy Castle to 1.68 or later,13362199,Resolved,Major,Fixed,04/Mar/21 01:59,17/Oct/22 17:28,3.3.1,"-Bouncy Castle 1.60 has Hash Collision Vulnerability. Let's update to 1.68.-

Bouncy Castle 1.60 has the following vulnerabilities. Let's update to 1.68.
 * [https://nvd.nist.gov/vuln/detail/CVE-2020-26939]
 * [https://nvd.nist.gov/vuln/detail/CVE-2020-28052]
 * [https://nvd.nist.gov/vuln/detail/CVE-2020-15522]

for anyone backporting this, note that recent bouncy castle jars are incompatible with older versions of asm.jar, and so older versions of spark."
s3a magic committer may commit more data,13364096,Resolved,Major,Fixed,12/Mar/21 10:07,24/Feb/23 15:51,3.2.0,"s3a magic committer isRecoverySupported() is false, so will restart all task after application master restart for am jvm crashed, leaving pendingset in magic path not to clear. pendingset name format is jobAttemptPath + taskAttemptID.getTaskID() + "".pendingset"", and jobAttemptPath is actually jobIdPath not JobAttemptIdPath in s3a magic committer. These pendingset files are overwritted by new task commit.

But if in new am attempt, a speculative task overcomes origin task, so pendingset file in last attempt may be hold for job commit, the data for commit is wrong"
Enable shelldoc check in GitHub PR,13364038,Open,Major,,12/Mar/21 07:37,,,"After HADOOP-17570, we can enable shelldoc check again because the commit hash of Yetus includes YETUS-1099."
[s3a] Disable bucket existence check - set fs.s3a.bucket.probe to 0,13349159,Resolved,Major,Fixed,05/Jan/21 10:38,05/Jan/21 14:48,3.3.0,"Set the value of fs.s3a.bucket.probe to 0 by default.
Bucket existence checks are done in the initialization phase of the S3AFileSystem. It's not required to run this check: the operation itself will fail if the bucket does not exist instead of the check.

Some points on why do we want to set this to 0:
* When it's set to 0, bucket existence checks won't be done during initialization thus making it faster.
* Avoid the additional one or two requests on the bucket root, so the user does not need rights to read or list that folder."
FileSystem.close() to optionally log IOStats; save to local dir,13361449,Resolved,Major,Won't Fix,28/Feb/21 16:14,27/Jul/22 13:35,3.3.1,"We could save the IOStats to a local temp dir as JSON (the snapshot is designed to be serializable, even has a test), with a unique name (iostats-stevel-s3a-bucket1-timestamp-random#.json ... etc). 

We can collect these (Rajesh can, anyway), and then
* look for load on a specific bucket
* look what happened at a specific time

The best bit: the IOStatisticsSnapshot aggregates counters, min/max/mean, so you could merge iostats-*-s3a-bucket1-*.json to get the IOStats of all principals working with a given bucket

This will be local, so low cost, low cost enough we could turn it on in production. All that's needed is collection of the stats from the local hosts (or they write to a shared mounted volume)
We will need some ""hadoop iostats merge"" command to take multiple files and merge them all together; print to screen or save to a new file. Straightforward as all the load and merge code is present.


Needs
* logging in FS.close
* new iostats CLI + docs, tests
* extend IOStatisticsSnapshot with list of <string, string> options for use in annotating saved logs (hostname, principal, jobID, ...). Don't know how to merge these.

If we are going to add a new context map to the IOStatisticsSnapshot then we MUST update it before 3.3.1 ships so as to avoid breaking the serialization format on the next release, especially the java one. "
IOStatistics Phase II,13352266,Open,Major,,14/Jan/21 14:04,,3.3.1,"Continue IOStatistics development with goals of

* Easy adoption in applications
* better instrumentation in hadoop codebase (distcp?)
* more stats in abfs and s3a connectors

A key has to be a thread level context for statistics so that app code doesn't have to explicitly ask for the stats for each worker thread. Instead 

filesystem components update the context stats as well as thread stats (when?) and then apps can pick up.

* need to manage performance by minimising inefficient lookups, lock acquisition etc on what should be memory-only ops (read()), (write()),
* and for duration tracking, cut down on calls to System.currentTime() so that only 1 should be made per operation, 
* need to propagate the context into worker threads

Target uses

* Impala 
* Spark via SPARK-29397 
* S3A committers
* Iceberg.

I have a WiP Parquet branch too, to see what can be done there. This shows up how the thread context is needed as its unworkable to build up your own stats shapshot. Even if you collect it for listX and stream reads, it doesn't include FS operations (e.g. rename()) and you need to rework all your methods to pass the stats collector around
"
Building native code fails on Fedora 33,13362882,Resolved,Major,Fixed,07/Mar/21 23:13,15/Apr/21 12:34,,"I tried to build native code on Fedora 33, in which glibc 2.32 is installed by default, but it failed with the following error.
{code:java}
$ cat /etc/redhat-release 
Fedora release 33 (Thirty Three)
$ sudo dnf info --installed glibc
Installed Packages
Name         : glibc
Version      : 2.32
Release      : 1.fc33
Architecture : x86_64
Size         : 17 M
Source       : glibc-2.32-1.fc33.src.rpm
Repository   : @System
From repo    : anaconda
Summary      : The GNU libc libraries
URL          : http://www.gnu.org/software/glibc/
License      : LGPLv2+ and LGPLv2+ with exceptions and GPLv2+ and GPLv2+ with exceptions and BSD and Inner-Net and ISC and Public Domain and GFDL
Description  : The glibc package contains standard libraries which are used by
             : multiple programs on the system. In order to save disk space and
             : memory, as well as to make upgrading easier, common system code is
             : kept in one place and shared between programs. This particular package
             : contains the most important sets of shared libraries: the standard C
             : library and the standard math library. Without these two libraries, a
             : Linux system will not function.

$ mvn clean compile -Pnative

...

[INFO] Running make -j 1 VERBOSE=1
[WARNING] /usr/bin/cmake -S/home/vagrant/hadoop/hadoop-common-project/hadoop-common/src -B/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native --check-build-system CMakeFiles/Makefile.cmake 0
[WARNING] /usr/bin/cmake -E cmake_progress_start /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native//CMakeFiles/progress.marks
[WARNING] make  -f CMakeFiles/Makefile2 all
[WARNING] make[1]: Entering directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] make  -f CMakeFiles/hadoop_static.dir/build.make CMakeFiles/hadoop_static.dir/depend
[WARNING] make[2]: Entering directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] cd /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native && /usr/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native /home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles/hadoop_static.dir/DependInfo.cmake --color=
[WARNING] Dependee ""/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles/hadoop_static.dir/DependInfo.cmake"" is newer than depender ""/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles/hadoop_static.dir/depend.internal"".
[WARNING] Dependee ""/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeDirectoryInformation.cmake"" is newer than depender ""/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/CMakeFiles/hadoop_static.dir/depend.internal"".
[WARNING] Scanning dependencies of target hadoop_static
[WARNING] make[2]: Leaving directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] make  -f CMakeFiles/hadoop_static.dir/build.make CMakeFiles/hadoop_static.dir/build
[WARNING] make[2]: Entering directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] [  2%] Building C object CMakeFiles/hadoop_static.dir/main/native/src/exception.c.o
[WARNING] /usr/bin/cc  -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native/javah -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/src -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/src -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native -I/usr/lib/jvm/java-1.8.0/include -I/usr/lib/jvm/java-1.8.0/include/linux -I/home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util -g -O2 -Wall -pthread -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCE -std=gnu99 -o CMakeFiles/hadoop_static.dir/main/native/src/exception.c.o -c /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c
[WARNING] make[2]: Leaving directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] make[1]: Leaving directory '/home/vagrant/hadoop/hadoop-common-project/hadoop-common/target/native'
[WARNING] /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c: In function ‘terror’:
[WARNING] /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c:118:34: error: ‘sys_nerr’ undeclared (first use in this function)
[WARNING]   118 |   if ((errnum < 0) || (errnum >= sys_nerr)) {
[WARNING]       |                                  ^~~~~~~~
[WARNING] /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c:118:34: note: each undeclared identifier is reported only once for each function it appears in
[WARNING] /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c:121:10: error: ‘sys_errlist’ undeclared (first use in this function)
[WARNING]   121 |   return sys_errlist[errnum];
[WARNING]       |          ^~~~~~~~~~~
[WARNING] /home/vagrant/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/exception.c:123:1: warning: control reaches end of non-void function [-Wreturn-type]
[WARNING]   123 | }
[WARNING]       | ^
[WARNING] make[2]: *** [CMakeFiles/hadoop_static.dir/build.make:82: CMakeFiles/hadoop_static.dir/main/native/src/exception.c.o] Error 1
[WARNING] make[1]: *** [CMakeFiles/Makefile2:99: CMakeFiles/hadoop_static.dir/all] Error 2
[WARNING] make: *** [Makefile:103: all] Error 2
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Main 3.4.0-SNAPSHOT:
[INFO] 
[INFO] Apache Hadoop Main ................................. SUCCESS [ 21.786 s]
[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  2.888 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 12.919 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  3.877 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.567 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.782 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [03:33 min]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  4.756 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [03:10 min]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.905 s]
[INFO] Apache Hadoop Common ............................... FAILURE [01:49 min]
[INFO] Apache Hadoop NFS .................................. SKIPPED

...

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  09:49 min
[INFO] Finished at: 2021-03-06T15:29:00Z
[INFO] ------------------------------------------------------------------------
{code}
[According to the release note of glibc 2.32|https://sourceware.org/pipermail/libc-announce/2020/000029.html], these symbols has been removed since that version.
{quote}The deprecated symbols sys_errlist, _sys_errlist, sys_nerr, and _sys_nerr are no longer available to newly linked binaries, and their declarations have been removed from from <stdio.h>.
{quote}"
Not closing an SFTP File System instance prevents JVM from exiting. ,13358503,Resolved,Major,Fixed,13/Feb/21 21:23,23/Feb/21 20:01,3.2.0,SFTP file system leverages a connection pool which is not closed when a file system instance gets closed preventing a JVM from exiting as every SFTP connection runs in a separate non-daemon thread.
hadoop-huaweicloud and hadoop-cloud-storage to remove log4j as transitive dependency,13365829,Resolved,Major,Fixed,17/Mar/21 12:58,24/Jan/22 12:06,3.4.0,"Dependencies of hadoop-cloud-storage show that hadoop-huaweicloud is pulling in logj4. 

it should not/must not, at least, not if the huaweicloud can live without it. 

* A version of log4j 2.,2 on the CP is only going to complicate lives
* once we can move onto it ourselves we need to be in control of versions

[INFO] \- org.apache.hadoop:hadoop-huaweicloud:jar:3.4.0-SNAPSHOT:compile
[INFO]    \- com.huaweicloud:esdk-obs-java:jar:3.20.4.2:compile
[INFO]       +- com.jamesmurty.utils:java-xmlbuilder:jar:1.2:compile
[INFO]       +- com.squareup.okhttp3:okhttp:jar:3.14.2:compile
[INFO]       +- org.apache.logging.log4j:log4j-core:jar:2.12.0:compile
[INFO]       \- org.apache.logging.log4j:log4j-api:jar:2.12.0:compile"
S3A to downgraded to unguarded if the DDB table cannot be found,13357601,Resolved,Major,Won't Fix,08/Feb/21 18:12,30/Sep/21 10:58,3.1.4,"when an FNFE is raised in s3guard init, catch and downgrade to NullMetastore"
intermittent ITestS3AInconsistency.testGetFileStatus failure.,13350592,Resolved,Major,Duplicate,06/Jan/21 06:45,18/Jan/22 15:29,3.3.1,"{code}
[*ERROR*] *Tests* *run: 3*, *Failures: 1*, Errors: 0, Skipped: 0, Time elapsed: 30.944 s *<<< FAILURE!* - in org.apache.hadoop.fs.s3a.*ITestS3AInconsistency*

[*ERROR*] testGetFileStatus(org.apache.hadoop.fs.s3a.ITestS3AInconsistency)  Time elapsed: 6.471 s  <<< FAILURE!

java.lang.AssertionError: getFileStatus should fail due to delayed visibility.

 at org.junit.Assert.fail(Assert.java:88)

 at org.apache.hadoop.fs.s3a.ITestS3AInconsistency.testGetFileStatus(ITestS3AInconsistency.java:114)

 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

 at java.lang.reflect.Method.invoke(Method.java:498)

 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)

 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)

 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)

 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)

 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)

 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)

 at java.util.concurrent.FutureTask.run(FutureTask.java:266)

 at java.lang.Thread.run(Thread.java:748)
{code}"
abfs listLocatedStatus to support incremental/async page fetching,13354368,Resolved,Major,Fixed,25/Jan/21 10:39,07/Jan/22 14:21,3.3.0,"listLocatedStatus uses listStatus[] then serves up as an iterator.

Once HADOOP-17475 is in, it can do the list incrementally with async prefetch.

Although not used much in the Hadoop codebase, the LocationStatusFetcher class is used during query planning across applications"
Über-jira: S3A Hadoop 3.3.2 features,13362348,Resolved,Major,Done,04/Mar/21 17:29,05/Jan/22 17:05,3.3.1,
Upgrade Jackson databind to 2.10.5.1,13359270,Resolved,Major,Fixed,18/Feb/21 10:51,22/Feb/21 08:35,3.2.2,"Hey everyone, we've done a container scan of Hadoop 3.2.2 we are using to build a shaded version of a Flink uber jar with, and noticed several apparent problems that are primarily related to com.faster.xml.jackson.core_jackson-databind.

 

Specifically the report claims version 2.4.0 of the library is used (am not sure about this part personally so I may be mistaken) and the fix suggestion I see is to move up to either 2.10.5.1, 2.9.10.8, 2.6.7.4 as appropriate.

 

I believe 2.10.3 is actually what's currently in use based on [https://github.com/apache/hadoop/blob/4cf35315838a6e65f87ed64aaa8f1d31594c7fcd/hadoop-project/pom.xml#L75|https://github.com/apache/hadoop/blob/4cf35315838a6e65f87ed64aaa8f1d31594c7fcd/hadoop-project/pom.xml#L75.]

 

Hopefully not a far-reaching change as I know changing dependencies can sometimes have a big knock-on effect, anyway - figured I'd report it incase someone plans to work on it.

 

Again do note that this is using a scan of an image built for Flink 1.11.3, but using Hadoop so it has a bunch of the same classes in, and I do believe that in Flink itself, the version of Jackson pulled in does not have the same problems, thus my thinking it is related to the Hadoop dependencies.

Thanks!"
port UGI#getGroupsSet optimizations into 2.10,13353809,In Progress,Major,,21/Jan/21 17:33,,2.10.1,"HADOOP-17079 introduced an optimization adding a UGI#getGroupsSet and use Set#contains() instead of List#contains() to speed up large group look up while minimize List->Set conversions in Groups#getGroups() call.

This ticket is to port the changes into branch-2.10.

 

CC: [~Jim_Brennan], [~xyao]"
S3Guard import can OOM on large imports,13361828,Resolved,Major,Won't Fix,02/Mar/21 10:39,18/Oct/21 13:56,3.3.1,"I know I'm closing ~all S3Guard issues as wontfix, but this is pressing so I'm going to do it anyway

S3guard import of directory tree containing many, many files will OOM. Looking at the code this is going to be because

* import tool builds a map of all dirs imported, which as the comments note ""superfluous for DDB"". - *cut*
* DDB AncestorState tracks files as well as dirs, purely as a safety check to make sure current op doesn't somehow write a file entry above a dir entry in the same operation

We've been running S3Guard for a long time, and condition #2 has never arisen.

Propose: don't store filenames there, so memory consumption goes from O(files + dirs) to O(dirs)

Code straightforward, can't think of any tests"
Upgrade Jackson databind in branch-2.10 to 2.9.10.7,13367290,Resolved,Major,Fixed,24/Mar/21 16:19,13/Apr/21 00:43,,"Two known vulnerabilities found in Jackson-databind

[CVE-2021-20190|https://nvd.nist.gov/vuln/detail/CVE-2021-20190] high severity
[CVE-2020-25649|https://nvd.nist.gov/vuln/detail/CVE-2020-25649] high severity"
Create hadoop-compression module,13351206,Open,Major,,09/Jan/21 06:22,,,"We added lz4-java, snappy-java dependencies to replace native libs. As per the suggestion from the review comments, we better add a hadoop module to have these extra dependencies, to avoid messing up the dependencies of user application.

"
ABFS: Partially obfuscate SAS object IDs in Logs,13368849,Resolved,Major,Fixed,31/Mar/21 10:59,09/Sep/21 13:04,3.3.1,"Delegation SAS tokens are created using various parameters for specifying details such as permissions and validity. The requests are logged, along with values of all the query parameters. This change will partially mask values logged for the following object IDs representing the security principal: skoid, saoid, suoid"
Add a MkdirOperation for chained S3 operations during mkdir,13354713,Resolved,Major,Duplicate,26/Jan/21 16:18,02/Aug/21 15:11,3.3.1,"
on S3A mkdirs is implemented as a walk up the tree looking for any parent dir, exiting fast when found; failing if a file is found.

Proposed: pull this out into its own MkDir operation with the goals of efficiency, track duration and isolate/test

* specific callbacks to getFileStatus and  createFakeDirectory(key);
* Move from simple getFileStatus call to probes assuming dir exists before looking for a file: HEAD + path /, LIST path,
* the probe for a file  HEAD path will only succeed in a failure mode where higher cost is a detail

I think I'd also like to start looking at what we can do with some context tracking/telemetry where something provides an operation ID and ability to attach this to the requests"
ABFS : add high performance listStatusIterator,13352965,Resolved,Major,Fixed,18/Jan/21 04:34,04/Feb/21 13:50,3.4.0,"The ABFS connector now implements listStatusIterator() with
asynchronous prefetching of the next page(s) of results.
For listing large directories this can provide tangible speedups.

If for any reason this needs to be disabled, set
fs.azure.enable.abfslistiterator to false."
Mapred/YARN job fails due to kms-dt can't be found in cache with LoadBalancingKMSClientProvider + Kerberos,13362877,Resolved,Major,Not A Bug,07/Mar/21 21:45,12/Jul/21 01:25,3.2.2,"I deployed Hadoop 3.2.2 cluster with KMS in HA using LoadBalancingKMSClientProvider with Kerberos authentication. KMS instances are configured with ZooKeeper for storing the shared secret.

I have created an encryption key and an encryption zone in `/test` directory and executed `randomtextwriter` from mapreduce examples passing it a sub-directory in the encryption zone:
{code:java}
hadoop jar hadoop-mapreduce-examples-3.2.2.jar randomtextwriter /test/randomtextwriter
{code}
Unfortunately the job keeps failing with errors like:
{code:java}
java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615146155993, maxDate=1615750955993, sequenceNumber=1, masterKeyId=2) can't be found in cache
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:363)
	at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:532)
	at org.apache.hadoop.hdfs.HdfsKMSUtil.decryptEncryptedDataEncryptionKey(HdfsKMSUtil.java:212)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:972)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:952)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:536)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:530)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:544)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:471)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1125)
	at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1168)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:285)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:542)
	at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getSequenceWriter(SequenceFileOutputFormat.java:64)
	at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getRecordWriter(SequenceFileOutputFormat.java:75)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:659)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:779)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615146155993, maxDate=1615750955993, sequenceNumber=1, masterKeyId=2) can't be found in cache
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:154)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:592)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:540)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:833)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:356)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:352)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:174)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:352)
{code}
 

I've injected a few logs on my own and it seems that the client gets 403 on ""decrypt"" request:
{code:java}
2021-03-07 21:26:23,009 INFO [main] org.apache.hadoop.hdfs.HdfsKMSUtil: DD: decrypting encrypted data encryption key
2021-03-07 21:26:23,012 INFO [main] org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: DD: decryptEncryptedKey called
2021-03-07 21:26:23,012 INFO [main] org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: DD: trying out all providers providers.length=2
2021-03-07 21:26:23,012 INFO [main] org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: DD: Trying out provider=0, i=0
2021-03-07 21:26:23,028 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: Current UGI: packer (auth:SIMPLE)
2021-03-07 21:26:23,028 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: mapreduce.job, Service: 10.9.4.227:38684, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@6b5966e1)
2021-03-07 21:26:23,028 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: HDFS_DELEGATION_TOKEN, Service: 10.9.4.140:8020, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:sacluster, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: HDFS_DELEGATION_TOKEN, Service: 10.9.4.175:8020, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: Login UGI: packer (auth:SIMPLE)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: mapreduce.job, Service: 10.9.4.227:38684, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@65e61854)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:sacluster, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42)
2021-03-07 21:26:23,029 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: +token:Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:26:23,030 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: Searching for KMS delegation token in user packer (auth:SIMPLE)'s credentials
2021-03-07 21:26:23,030 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: selected by alias=10.9.4.175:16000 token=Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:26:23,031 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:packer (auth:SIMPLE) from:org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:506)
2021-03-07 21:26:23,037 DEBUG [main] org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL: Connecting to url http://node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt with token  as null
2021-03-07 21:26:23,038 DEBUG [main] org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL: Token not set, looking for delegation token. Creds:[Kind: mapreduce.job, Service: 10.9.4.227:38684, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@674c583e), Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:sacluster, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42), Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)], size:3
2021-03-07 21:26:23,039 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: Looking for delegation token. creds: [Kind: mapreduce.job, Service: 10.9.4.227:38684, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@25f7391e), Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:sacluster, Ident: (token for packer: HDFS_DELEGATION_TOKEN owner=packer/node-10-9-4-175.bdcluster@SOME_REALM, renewer=packer, realUser=, issueDate=1615152335661, maxDate=1615757135661, sequenceNumber=23, masterKeyId=42), Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)]
2021-03-07 21:26:23,039 DEBUG [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: selected by alias=10.9.4.175:16000 token=Kind: kms-dt, Service: kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms, Ident: (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:26:23,039 DEBUG [main] org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator: Authenticated from delegation token. url=http://node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt, token=
2021-03-07 21:26:23,057 INFO [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: DD: calling decrypt key at http://node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt
2021-03-07 21:26:27,325 INFO [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: DD: Got response to url=http://node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt, code=403, message=Forbidden
2021-03-07 21:26:27,326 INFO [main] org.apache.hadoop.crypto.key.kms.KMSClientProvider: DD: Validating response
2021-03-07 21:26:27,346 ERROR [main] org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: DD: Re-throwing: 
org.apache.hadoop.security.authentication.client.AuthenticationException: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2) can't be found in cache
{code}
and the exception is thrown from: 
{code:java}
org.apache.hadoop.crypto.key.kms.KMSClientProvider#call(java.net.HttpURLConnection, java.lang.Object, int, java.lang.Class<T>, int)
   ...
   LOG.info(""DD: Validating response"");
   HttpExceptionUtils.validateResponse(conn, expectedResponse);
   LOG.info(""DD: Response passed validation"");
   ...{code}
It seems that the delegation token is not shared between both KMS instances and when the request hits the KMS instance that does not have the delegation token it responds with `AuthenticationException`, from:
{code:java}
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler#authenticate
{code}
which makes the client not retry the request with the other KMS instance.

 

Here are a few relevant lines from the failing KMS log:
{code:java}
2021-03-07 21:28:18,823 DEBUG AuthenticationFilter - Request [http://node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt] triggering authentication. handler: class org.apache.hadoop.security.token.delegation
.web.KerberosDelegationTokenAuthenticationHandler
2021-03-07 21:28:18,824 DEBUG DelegationTokenAuthenticationHandler - Authenticating with dt param: IAAGcGFja2VyBnBhY2tlcgCKAXgOlNA2igF4MqFUNgECFNcBZ7fbjrLRO4-ekukipzAQdh1DBmttcy1kdEhrbXM6Ly9odHRwQG5vZGUtMTAtOS00LTE3NS5iZGNsdXN0ZXI7bm9kZS0xMC05LTQtMTQwLmJk
Y2x1c3RlcjoxNjAwMC9rbXM
2021-03-07 21:28:18,824 DEBUG ManagedSelector - Destroyed SocketChannelEndPoint@343bb301{/10.9.4.140:52258<->/10.9.4.175:16000,CLOSED,fill=-,flush=-,to=705/1000}{io=0/0,kio=-1,kro=-1}->HttpConnection@4ff601bb[p=HttpParser{s=CLOSED,0 of -1},g=HttpGenerator
@26bc00ff{s=START}]=>HttpChannelOverHttp@11f7d1d3{r=1,c=false,c=false/false,a=IDLE,uri=null,age=0}
2021-03-07 21:28:18,827 DEBUG AbstractDelegationTokenSecretManager - DD: Looking for token id=(kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:28:18,827 DEBUG HttpConnection - HttpConnection@4ff601bb::SocketChannelEndPoint@343bb301{/10.9.4.140:52258<->/10.9.4.175:16000,CLOSED,fill=-,flush=-,to=705/1000}{io=0/0,kio=-1,kro=-1}->HttpConnection@4ff601bb[p=HttpParser{s=CLOSED,0 of -1},g
=HttpGenerator@26bc00ff{s=START}]=>HttpChannelOverHttp@11f7d1d3{r=1,c=false,c=false/false,a=IDLE,uri=null,age=0} parsed false HttpParser{s=CLOSED,0 of -1}
2021-03-07 21:28:18,828 DEBUG HttpChannel - sendResponse info=null content=DirectByteBuffer@7daa9fd5[p=0,l=413,c=32768,r=413]={<<<{\n  ""RemoteExcept...xception""\n  }\n}>>>z-6-ZywrRKw"",\n   ...hdfs_root_dir_k} complete=true committing=true callback=Blocker
@119c9686{null}
2021-03-07 21:28:18,829 DEBUG HttpChannel - COMMIT for /kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek on HttpChannelOverHttp@47d8dfbe{r=1,c=true,c=false/false,a=DISPATCHED,uri=//node-10-9-4-175.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_di
r_key%400/_eek?eek_op=decrypt,age=53}
403 null HTTP/1.1
Date: Sun, 07 Mar 2021 21:28:18 GMT
Cache-Control: no-cache
Expires: Sun, 07 Mar 2021 21:28:18 GMT
Date: Sun, 07 Mar 2021 21:28:18 GMT
Pragma: no-cache
Content-Type: application/json
X-Content-Type-Options: nosniff
X-XSS-Protection: 1; mode=block

{code}
and here are corresponding lines from succeeding KMS log:
{code:java}
2021-03-07 21:27:43,639 DEBUG AuthenticationFilter - Request [http://node-10-9-4-140.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt] triggering authentication. handler: class org.apache.hadoop.security.token.delegation
.web.KerberosDelegationTokenAuthenticationHandler
2021-03-07 21:27:43,640 DEBUG DelegationTokenAuthenticationHandler - Authenticating with dt param: IAAGcGFja2VyBnBhY2tlcgCKAXgOlNA2igF4MqFUNgECFNcBZ7fbjrLRO4-ekukipzAQdh1DBmttcy1kdEhrbXM6Ly9odHRwQG5vZGUtMTAtOS00LTE3NS5iZGNsdXN0ZXI7bm9kZS0xMC05LTQtMTQwLmJk
Y2x1c3RlcjoxNjAwMC9rbXM
2021-03-07 21:27:43,648 DEBUG AbstractDelegationTokenSecretManager - DD: Looking for token id=(kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:27:43,648 DEBUG AbstractDelegationTokenSecretManager - DD: token id=(kms-dt owner=packer, renewer=packer, realUser=, issueDate=1615152336950, maxDate=1615757136950, sequenceNumber=1, masterKeyId=2)
2021-03-07 21:27:43,668 DEBUG AuthenticationFilter - Request [http://node-10-9-4-140.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt] user [packer] authenticated
2021-03-07 21:27:43,668 DEBUG ServletHandler - call filter MDCFilter@5a7fe64f==org.apache.hadoop.crypto.key.kms.server.KMSMDCFilter,inst=true,async=false
2021-03-07 21:27:43,687 DEBUG ServletHandler - call servlet webservices-driver@79e80ea3==com.sun.jersey.spi.container.servlet.ServletContainer,jsp=null,order=1,inst=true,async=false
2021-03-07 21:27:44,252 DEBUG IdleTimeout - SocketChannelEndPoint@78a5b977{/10.9.4.227:34860<->/10.9.4.140:16000,OPEN,fill=-,flush=-,to=1039/1000}{io=0/0,kio=0,kro=1}->HttpConnection@4ef7005[p=HttpParser{s=CONTENT,0 of 122},g=HttpGenerator@51540fee{s=STAR
T}]=>HttpChannelOverHttp@661cde57{r=1,c=false,c=false/false,a=DISPATCHED,uri=//node-10-9-4-140.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt,age=758} idle timeout check, elapsed: 1039 ms, remaining: -39 ms
2021-03-07 21:27:44,260 DEBUG IdleTimeout - SocketChannelEndPoint@78a5b977{/10.9.4.227:34860<->/10.9.4.140:16000,OPEN,fill=-,flush=-,to=1041/1000}{io=0/0,kio=0,kro=1}->HttpConnection@4ef7005[p=HttpParser{s=CONTENT,0 of 122},g=HttpGenerator@51540fee{s=START}]=>HttpChannelOverHttp@661cde57{r=1,c=false,c=false/false,a=DISPATCHED,uri=//node-10-9-4-140.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt,age=759} idle timeout expired
2021-03-07 21:27:44,264 DEBUG FillInterest - onFail FillInterest@51ba8df5{null}
java.util.concurrent.TimeoutException: Idle timeout expired: 1039/1000 ms
...
2021-03-07 21:27:44,527 DEBUG KMSACLs - User: [packer], OpType: DECRYPT_EEK, KeyName: dotdata_hdfs_root_dir_key Result: true
2021-03-07 21:27:44,607 DEBUG PerformanceAdvisory - Crypto codec org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec is not available.
2021-03-07 21:27:44,652 DEBUG PerformanceAdvisory - Using crypto codec org.apache.hadoop.crypto.JceAesCtrCryptoCodec.
2021-03-07 21:27:44,768 DEBUG HttpChannel - sendResponse info=null content=DirectByteBuffer@2d84038a[p=0,l=107,c=32768,r=107]={<<<{\n  ""material"" : ...nName"" : ""EK""\n}>>>""><doc xmlns:jers...hdfs_root_dir_k} complete=true committing=true callback=Blocker@1e52a824{null}
2021-03-07 21:27:44,768 DEBUG HttpChannel - COMMIT for /kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek on HttpChannelOverHttp@661cde57{r=1,c=true,c=false/false,a=DISPATCHED,uri=//node-10-9-4-140.bdcluster:16000/kms/v1/keyversion/dotdata_hdfs_root_dir_key%400/_eek?eek_op=decrypt,age=1274}
200 OK HTTP/1.1
Date: Sun, 07 Mar 2021 21:27:43 GMT
Cache-Control: no-cache
Expires: Sun, 07 Mar 2021 21:27:43 GMT
Date: Sun, 07 Mar 2021 21:27:43 GMT
Pragma: no-cache
Content-Type: application/json
X-Content-Type-Options: nosniff
X-XSS-Protection: 1; mode=block{code}
When I shutdown one of the KMS instances before launching the job then the job succeeds.

I thought it might have something to do with https://issues.apache.org/jira/browse/HADOOP-16199 so I tried the same setup with 3.3.0 but, unfortunately, with the same result.

I also run exactly the same job on CDH 5.16.1, which is my current deployment and which I am considering to replace with 3.2.2. The job did succeed on CDH 5.16.1.

I can provide more logs if that is needed, the issue is deterministic in my environment.

 

Here is configuration of one of the KMS instances (they are both identical - down to the host part of the KMS principal):

 
{code:java}
<?xml version=""1.0"" encoding=""UTF-8""?>
<configuration>
    <property>
        <name>hadoop.kms.key.provider.uri</name>
        <value>jceks://file@/opt/sa/kms/kms.keystore</value>
    </property>
    <property>
        <name>hadoop.kms.http.port</name>
        <value>16000</value>
    </property>
    <property>
        <name>hadoop.kms.proxyuser.packer.users</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.kms.proxyuser.packer.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.kms.proxyuser.packer.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.kms.authentication.type</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.kms.authentication.kerberos.keytab</name>
        <value>/tmp/keytab</value>
    </property>
    <property>
        <name>hadoop.kms.authentication.kerberos.principal</name>
        <value>HTTP/node-10-9-4-175.bdcluster@SOME_REALM</value>
    </property>
    <property>
        <name>hadoop.kms.authentication.kerberos.name.rules</name>
        <value>DEFAULT</value>
   </property>
  <property>
    <name>hadoop.kms.authentication.signer.secret.provider</name>
    <value>zookeeper</value>
  </property>
  <property>
    <name>hadoop.kms.authentication.signer.secret.provider.zookeeper.path</name>
    <value>/hadoop-kms/hadoop-auth-signature-secret</value>
  </property>
  <property>
    <name>hadoop.kms.authentication.signer.secret.provider.zookeeper.connection.string</name>
    <value>node-10-9-4-175.bdcluster:2181,node-10-9-4-227.bdcluster:2181,node-10-9-4-140.bdcluster:2181</value>
  </property>
</configuration>
{code}
 and here is how KMS is configured in `core-site.xml` in my cluster:
{code:java}
<property>
  <name>hadoop.security.key.provider.path</name>
  <value>kms://http@node-10-9-4-175.bdcluster;node-10-9-4-140.bdcluster:16000/kms</value>
</property>
{code}"
ABFS: Change default Readahead Queue Depth from num(processors) to const,13366703,Resolved,Major,Fixed,22/Mar/21 11:05,10/Jul/21 09:40,3.3.1,"The default value of readahead queue depth is currently set to the number of available processors. However, this can result in one inputstream instance consuming more processor time. To ensure equal thread allocation during read for all inputstreams created in a session, we change the default readahead queue depth to a constant (2)."
Upgrade JUnit to 4.13.1,13367296,Resolved,Major,Fixed,24/Mar/21 16:36,25/Mar/21 11:31,,"A reported vulnerability reported in JUnit4.7-4.13.
The JUnit4 test rule [TemporaryFolder on unix-like systems does not limit access to created files|https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp]"
Magic committer to downgrade abort in cleanup if list uploads fails with access denied,13360937,Resolved,Major,Fixed,25/Feb/21 11:21,12/Jun/21 16:47,3.3.0,"If the caller doesn't have ""s3:ListBucketMultipartUploads"" permissions on a bucket, then magic committer cleanup fails. 
{code}
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:247)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:112)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:315)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)
	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:311)
	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:286)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listMultipartUploads(S3AFileSystem.java:4549)
	at org.apache.hadoop.fs.s3a.commit.CommitOperations.listPendingUploadsUnderPath(CommitOperations.java:361)
	at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.abortPendingUploadsInCleanup(AbstractS3ACommitter.java:671)
	at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.cleanup(AbstractS3ACommitter.java:770)
{code}
it should just swallow this, given it's best effort "
property 'ssl.server.keystore.location' has not been set in the ssl configuration file,13361160,Resolved,Major,Not A Problem,26/Feb/21 11:01,10/Jun/21 08:02,2.8.5,"I trying to install hadoop cluster HA , but datanode does not start properly; I get this errror:

2021-02-23 17:13:26,934 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: java.security.GeneralSecurityException: The property 'ssl.server.keystore.location' has not been set in the ssl configuration file.
at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:199)
at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:905)
at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1303)
at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:481)
at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2609)
at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
Caused by: java.security.GeneralSecurityException: The property 'ssl.server.keystore.location' has not been set in the ssl configuration file.
at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:152)
at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:148)
at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:197)
... 8 more

But in my ssl-server.xml i correctly set this property:

<property>
<name>ssl.server.keystore.location</name>
<value>/data/hadoop/server.jks</value>
<description>Keystore to be used by clients like distcp. Must be
specified.
</description>
</property>

<property>
<name>ssl.server.keystore.password</name>
<value>xxxx</value>
<description>Optional. Default value is """".
</description>
</property>

<property>
<name>ssl.server.keystore.keypassword</name>
<value>xxxxx</value>
<description>Optional. Default value is """".
</description>
</property>

<property>
<name>ssl.server.keystore.type</name>
<value>jks</value>
<description>Optional. The keystore file format, default value is ""jks"".
</description>
</property>

Do you have any suggestion to solve this problem?
my hadoop version is: 2.8.5
java version: 8
SO: centos 7"
Fix logging typo in ShutdownHookManager,13355043,Resolved,Major,Fixed,27/Jan/21 23:05,31/Jan/21 12:13,,"Three log messages in {{ShutdownHookManager}} have a typo, saying ""ShutdownHookManger"". Should be ""ShutdownHookManager"""
Allow ProtobufRpcEngine to be extensible,13366807,Resolved,Major,Fixed,22/Mar/21 19:33,17/May/21 07:38,,"The ProtobufRpcEngine class doesn't allow for new RpcEngine implementations to extend some of its inner classes (e.g. Invoker and Server.ProtoBufRpcInvoker). Also, some of its methods are long enough such that overriding them would result in a lot of code duplication (e.g. Invoker#invoke and Server.ProtoBufRpcInvoker#call).

When implementing a new RpcEngine, it would be helpful to reuse most of the code already in ProtobufRpcEngine. This would allow new fields to be added to the RPC header or message with minimal code changes."
skip-dir option is not processed by Yetus,13361677,Resolved,Major,Fixed,01/Mar/21 18:55,09/Mar/21 02:12,,"Running test patch locally does not work anymore after the Yetus upgrade


{code:bash}
dev-support/bin/test-patch --plugins=""maven,checkstyle"" --test-parallel=true patch-file.patch
{code}

Error is 

{code:bash}
Testing  patch on trunk.
ERROR: Unprocessed flag(s): --skip-dir

    environment {
        SOURCEDIR = 'src'
        // will also need to change notification section below
        PATCHDIR = 'out'
        DOCKERFILE = ""${SOURCEDIR}/dev-support/docker/Dockerfile""
        YETUS='yetus'
        // Branch or tag name.  Yetus release tags are 'rel/X.Y.Z'
        YETUS_VERSION='rel/0.13.0'
/skip-
                        # URL for user-side presentation in reports and such to our artifacts

 _____     _ _                _
|  ___|_ _(_) |_   _ _ __ ___| |
| |_ / _` | | | | | | '__/ _ \ |
|  _| (_| | | | |_| | | |  __/_|
|_|  \__,_|_|_|\__,_|_|  \___(_)



| Vote |    Subsystem |  Runtime   | Comment
============================================================================
|  -1  |       yetus  |   0m 05s   | Unprocessed flag(s): --skip-dir
{code}

It seems that the ""{{--skip-dir}}"" option supported Yetus release prior to 0.11."
Replace currentTimeMillis with monotonicNow in elapsed time,13351164,Resolved,Major,Won't Fix,08/Jan/21 20:39,04/Jun/21 21:53,,"I noticed that there is a widespread incorrect usage of {{System.currentTimeMillis()}}  throughout the hadoop code.

For example:

{code:java}
// Some comments here
long start = System.currentTimeMillis();
while (System.currentTimeMillis() - start < timeout) {
  // Do something
}
{code}

Elapsed time should be measured using `monotonicNow()`."
Decommission S3Guard,13357600,Resolved,Major,Duplicate,08/Feb/21 18:10,04/Jun/21 09:32,3.3.0,"S3Guard is obsolete in functionality and performance. Remove, carefully.

Goals: 
* make it easy for the 3.3.1 release to turn off s3guard
* remove it from the code for ease of maintenance
* strip down the tests
* be happy"
ITestAssumeRole.testAssumeRoleBadInnerAuth failure,13353110,Resolved,Major,Fixed,18/Jan/21 16:13,24/Mar/21 17:00,3.3.0,"failure of test {{ITestAssumeRole.testAssumeRoleBadInnerAuth}} where a failure was expected, but the error text was wrong.

Either STS has changed its error text or something is changing where the failure happens.

Given the nature of the test, it may be simplest to keep the expectation of an FS init faiure, but remove the text match"
Collect more S3A IOStatistics,13352271,Resolved,Major,Fixed,14/Jan/21 14:23,25/May/21 14:47,,"collect more stats of 
* how long FS API calls are taking
* what is going wrong

API
* Duration of rename(), delete() other common API Calls.
* openFile: how long the open operation is taking, split of open and openfile
* time to initialize FS instance (maybe better in FileSystem.cache?)
* create: Time to complete the create checks
* finishedWrite: time to execute finishedWrite operations (which may include bulk deletes)

Failure tracking: what failures are we seeing as counters
Interrupts, connections, auth failures:

This would be done in S3ARetryPolicy with an IOStatisticsStore passed in."
refactoring the signature of S3ClientFactory breaks hboss compile,13354693,Resolved,Major,Won't Fix,26/Jan/21 15:26,25/Mar/21 10:27,3.3.1,...restore removed S3ClientFactory method and call it as default by new one
Change String.getBytes() to DFSUtil.string2Bytes(String) to avoid Unsupported Encoding Exception,13368074,Open,Major,,27/Mar/21 09:24,,,"Hello,
I found that DFSUtil.string2Bytes(String) can be used here instead of String.getBytes(). Otherwise, the API String.getBytes() may cause potential risk of UnsupportedEncodingException since the behavior of this method when the string cannot be encoded in the default charset is unspecified. One recommended API is DFSUtil.string2Bytes(String) which provides more control over the encoding process and can avoid this exception. "
ABFS: Toggle Store Mkdirs request overwrite parameter,13361087,Resolved,Major,Fixed,26/Feb/21 03:31,14/Mar/21 08:07,3.3.1,"The call to mkdirs with overwrite set to true results in an additional call to set properties (LMT update, etc) at the backend, which is not required for the HDFS scenario. Moreover, mkdirs on an existing file path returns success. This PR provides an option to set the overwrite parameter to false, and ensures that mkdirs on a file throws an exception."
Upgrade hadoop default version in hadoop.tools.dynamometer.TestDynamometerInfra,13352400,Resolved,Major,Duplicate,15/Jan/21 04:13,07/May/21 08:19,,"Currently hadoop.tools.dynamometer.TestDynamometerInfra in trunk is failed.

{code}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.531 s <<< FAILURE! - in org.apache.hadoop.tools.dynamometer.TestDynamometerInfra
[ERROR] org.apache.hadoop.tools.dynamometer.TestDynamometerInfra  Time elapsed: 0.53 s  <<< ERROR!
java.io.FileNotFoundException: http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz
        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1923)
        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1523)
        at org.apache.commons.io.FileUtils.copyURLToFile(FileUtils.java:1506)
        at org.apache.hadoop.tools.dynamometer.DynoInfraUtils.fetchHadoopTarball(DynoInfraUtils.java:151)
        at org.apache.hadoop.tools.dynamometer.TestDynamometerInfra.setupClass(TestDynamometerInfra.java:176)
{code}"
ABFS to collect IOStatistics,13352272,Resolved,Major,Fixed,14/Jan/21 14:24,24/Apr/21 17:00,,"

Add stats collection to ABFS FS operations, especially
* create
* open
* delete
* rename
* getFilesStatus
* list
* attribute get/set"
ABFS: ITestAzureBlobFileSystemCheckAccess test failure if test doesn't have oauth keys,13359331,Resolved,Major,Fixed,18/Feb/21 13:52,24/Apr/21 17:32,3.3.1,"If the test isn't set up with oauth tokens, then ITestAzureBlobFileSystemCheckAccess raises illegal argument exception

# Should this downgrade to skip?
# if not:  someone needs to document how to get get oauth tokens for azure storage as the instructions documenting my last previous successful event are out of date with the latest Azure UI."
ABFS: Disable throttling update for auth failures,13363789,Resolved,Major,Fixed,11/Mar/21 09:21,16/Apr/21 05:19,3.3.1,"Throttling metrics are updated post the execution of each request. Failures related to fetching access tokens and signing requests do not occur at the Store. Hence, such operations should not contribute to the measured Store failures, and are therefore excluded from the metric update for throttling."
Distcp parallel file copy breaks the modification time,13368401,Patch Available,Major,,29/Mar/21 15:11,,,"The commit HADOOP-11794. Enable distcp to copy blocks in parallel. (bf3fb585aaf2b179836e139c041fc87920a3c886) broke the modification time of large files.

 

In CopyCommitter.java inside concatFileChunks Filesystem.concat is called which changes the modification time therefore the modification times of files copeid by distcp will not match the source files. However this only occurs for large enough files, which are copied by splitting them up by distcp.

In concatFileChunks before calling concat extract the modification time and apply that to the concatenated result-file after the concat. (probably best -after- before the rename())."
Log not flushed fully when daemon shutdown,13368684,Resolved,Major,Fixed,30/Mar/21 16:26,07/Apr/21 04:21,2.7.2,"When server generating large amount of logs and gets stopped, doesnt print all the logs. Need to call LogManager.shutdown(); to flush all the pending log to be written before shutdown."
Fix TestLogLevel,13363841,Open,Major,,11/Mar/21 13:05,,,"Found when fixing HADOOP-17572: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2755/2/testReport/org.apache.hadoop.log/TestLogLevel/testLogLevelByHttp/
{noformat}
Expected to find 'Unrecognized SSL message' but got unexpected exception:javax.net.ssl.SSLException: Unsupported or unrecognized SSL message
 at sun.security.ssl.SSLSocketInputRecord.handleUnknownRecord(SSLSocketInputRecord.java:448)
 at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:184)
 at sun.security.ssl.SSLTransport.decode(SSLTransport.java:108)
 at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1143)
 at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1054)
 at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:394)
 at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
 at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)
 at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:167)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:347)
 at org.apache.hadoop.log.LogLevel$CLI.connect(LogLevel.java:271)
 at org.apache.hadoop.log.LogLevel$CLI.process(LogLevel.java:293)
 at org.apache.hadoop.log.LogLevel$CLI.doGetLevel(LogLevel.java:234)
 at org.apache.hadoop.log.LogLevel$CLI.sendLogLevelRequest(LogLevel.java:127)
 at org.apache.hadoop.log.LogLevel$CLI.run(LogLevel.java:110)
 at org.apache.hadoop.log.TestLogLevel.getLevel(TestLogLevel.java:301)
 at org.apache.hadoop.log.TestLogLevel.access$000(TestLogLevel.java:63)
 at org.apache.hadoop.log.TestLogLevel$1.call(TestLogLevel.java:279)
 at org.apache.hadoop.log.TestLogLevel$1.call(TestLogLevel.java:275)
 at org.apache.hadoop.security.authentication.KerberosTestUtils$1.run(KerberosTestUtils.java:102)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.authentication.KerberosTestUtils.doAs(KerberosTestUtils.java:99)
 at org.apache.hadoop.security.authentication.KerberosTestUtils.doAsClient(KerberosTestUtils.java:115)
 at org.apache.hadoop.log.TestLogLevel.testDynamicLogLevel(TestLogLevel.java:275)
 at org.apache.hadoop.log.TestLogLevel.testDynamicLogLevel(TestLogLevel.java:234)
 at org.apache.hadoop.log.TestLogLevel.testLogLevelByHttp(TestLogLevel.java:354)
{noformat}"
Upgrade tomcat-embed-core to 7.0.108,13367329,Resolved,Major,Fixed,24/Mar/21 18:27,12/Apr/21 17:12,,"[CVE-2021-25329|https://nvd.nist.gov/vuln/detail/CVE-2021-25329] critical severity.
Impact: [CVE-2020-9494|https://nvd.nist.gov/vuln/detail/CVE-2020-9494]
7.0.0-7.0.107 are all affected by the vulnerability.
"
Some tests in TestBlockRecovery are consistently failing,13368805,Resolved,Major,Duplicate,31/Mar/21 07:34,31/Mar/21 07:36,3.4.0,"Some long running tests in TestBlockRecovery are consistently failing. Also, TestBlockRecovery is huge with so many tests, we should refactor some of long running and race condition specific tests to separate class."
TFileDumper creates malformed hex sample output,13368121,Open,Major,,27/Mar/21 17:38,,3.2.2,"{{org.apache.hadoop.io.file.tfile.TFileDumper}} creates malformed hex output:
{code}
out.print(""0X"");
for (int j = 0; j < sampleLen; ++j) {
  byte b = key[i];
  out.printf(""%X"", b);
}
{code}
It should use {{""%02X""}} (zero-pad and use width 2) as formatting pattern; otherwise bytes < 16 will only have on hex char and for example both {{1, 0}} and {{16}} would have the result {{""10""}}."
VersionInfoMojo.byteArrayToString(...) creates malformed hex strings,13368118,Open,Major,,27/Mar/21 17:20,,3.2.2,"{{org.apache.hadoop.maven.plugin.versioninfo.VersionInfoMojo.byteArrayToString(byte[])}} creates malformed hex strings:
{code}
private String byteArrayToString(byte[] array) {
    StringBuilder sb = new StringBuilder();
    for (byte b : array) {
        sb.append(Integer.toHexString(0xff & b));
    }
    return sb.toString();
}
{code}

The issue here is that {{toHexString}} returns only a single hex char if the value is < 16. Therefore {{1, 0}} and {{16}} would both have the result {{""10""}}.

The correct implementation would be:
{code}
private String byteArrayToString(byte[] array) {
    StringBuilder sb = new StringBuilder();
    for (byte b : array) {
        int unsignedB = b & 0xff;
        if (unsignedB < 16) {
            sb.append('0');
        }
        sb.append(Integer.toHexString(unsignedB));
    }
    return sb.toString();
}
{code}"
DistCp: Reduce memory usage using a fixed size ThreadPoolExecutor,13361776,Open,Major,,02/Mar/21 07:51,,,"For S3 and other object stores, where listing is slow, use a fixed size TPE for building listing"
netgroup-user is not added to Groups.cache,13352114,In Progress,Major,,13/Jan/21 22:38,,3.4.0,"After the optimization in HADOOP-17079, both {{JniBasedUnixGroupsNetgroupMapping}} and {{ShellBasedUnixGroupsNetgroupMapping}} do not implement {{getGroupSet}}.
As a result, {{Groups.load()}} load the cache calling {{fetchGroupSet}} which yield to the superclass {{JniBasedUnixGroupsMapping}} / {{ShellBasedUnixGroupsMapping}}.
In other words, the groups mapping will never fetch from {{NetgroupCache}}.

This alters the behavior of the implementation. Is there a reason to bypass loading. CC: [~xyao]

This jira is to add missing implementation {{getGroupSet}} to {{JniBasedUnixGroupsNetgroupMapping}} and {{ShellBasedUnixGroupsNetgroupMapping}} ."
Add ApacheCon Event Banner to website,13366287,Open,Major,,19/Mar/21 09:06,,,"We should add an ApacheCon Event Banner to our website.
- https://www.apachecon.com/event-images/"
Correct abfs test assertion reversed in HADOOP-13327,13360036,Resolved,Major,Fixed,22/Feb/21 08:55,15/Mar/21 15:02,3.3.1,"HADOOP-13327 introduces, among other changes, functions to simplify assert for checking stream capabilities. This PR fixes (originally) assertFalse statements whose logic has been reversed to assertTrue when the above change was checked-in."
ABFS: Fix boundary conditions in InputStream seek and skip,13358321,Resolved,Major,Won't Fix,12/Feb/21 09:51,15/Mar/21 14:59,3.3.0,"Modify AbfsInputStream seek method to throw EOF exception on seek to contentLength for a non-empty file. With this change, it will no longer be possible for the inputstream position (as obtained by getPos() API) to be moved to contentlength manually, except post reading the last byte."
Yarn Job execution get failed when LZ4 Compression Codec is used,13359069,Resolved,Major,Fixed,17/Feb/21 13:35,15/Mar/21 04:16,,"When we try to compress a file using the LZ4 codec compression type then the yarn job gets failed with the error message :
{code:java}
net.jpountz.lz4.LZ4Compressorcompres(Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;)V
{code}"
NullPointerException from register in MetricsSystemImpl,13352232,Open,Major,,14/Jan/21 10:58,,,"This is an error from Ozone's unit test case [HDDS-4688|https://github.com/apache/ozone/pull/1795#issuecomment-760052788]. The error is as follows:

 
{code:java}
java.lang.NullPointerException: configjava.lang.NullPointerException: config
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:897) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277) at org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:298)
...{code}
The reason should be happened here ""https://github.com/apache/hadoop/commit/2f500e4635ea4347a55693b1a10a4a4465fe5fac#"", if the _name_ is contained in _allSinks_ but not in _sink_, it will invoke the method of _registerSink_. But if the variable of config is null, it will throw the exception of NullPointerException.

 

A suggestion would be checking if the variable of config is null before calling _registerSink._"
Yetus does not run qbt-trunk,13360159,Resolved,Major,Done,22/Feb/21 18:16,11/Mar/21 16:27,,"On Feb20th, qbt-reports started to generate empty reports

{code:bash}
https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/424/
ERROR: File 'out/email-report.txt' does not exist
{code}

On Jenkins, the job fails with the following error:
https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/425/console
{code:bash}
ERROR: /home/jenkins/jenkins-home/workspace/hadoop-qbt-trunk-java8-linux-x86_64//dev-support/bin/hadoop.sh does not exist.
Build step 'Execute shell' marked build as failure
Archiving artifacts
[Fast Archiver] No prior successful build to compare, so performing full copy of artifacts
Recording test results
ERROR: Step ?Publish JUnit test result report? failed: No test report files were found. Configuration error?
{code}

[~aajisaka], I think this would be caused by HADOOP-16748 . I noticed that the PR of that HADOOP-16748 ceased from showing any reports, but for some reason I forgot about that while reviewing."
Misconfigure buffer size to 0 causing TestDFSIO to hang in infinite loops,13363763,Open,Major,,11/Mar/21 07:18,,3.3.0,"TestDFSIO lacks sanity checks on input buffer size for function doIO()

If misconfigured to 0, doIO() hangs in infinite loops. 

 
{code:java}
public Long doIO(... ) throws IOException {
    …..
 while (actualSize < totalSize) {
       int curSize = in.read(buffer, 0, bufferSize);
       if(curSize < 0) break;
       actualSize += curSize;
     ….
     }
   …
}

{code}
{code:java}
public Long doIO(... ) throws IOException {
    …..
        for (...; nrRemaining > 0; nrRemaining -= bufferSize) {
           If  (bufferSize < nrRemaining){ curSize = bufferSize }else{
                curSize = (int)nrRemaining
           out.write(buffer, 0, curSize);
        }
   …
}

{code}
Similar parameter _Io.file.buffer.size_ is handled by Java IO library BufferedInputStream. We suggest adding sanity checks for the benchmark's buffer size as well. "
Build failure on trunk,13363514,Resolved,Major,Duplicate,10/Mar/21 10:51,10/Mar/21 16:27,3.4.0,"Build is broken on trunk:

hadoop-huaweicloud: Compilation failure

[ERROR] hadoop/hadoop-cloud-storage-project/hadoop-huaweicloud/src/main/java/org/apache/hadoop/fs/obs/OBSFileSystem.java:[396,58] incompatible types: org.apache.hadoop.util.BlockingThreadPoolExecutorService cannot be converted to org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListeningExecutorService"
Job will hang when disk for logging is full,13362257,Open,Major,,04/Mar/21 09:24,,3.3.0,"When we put the log directory of Hadoop on a devoted disk space and the disk is closed to full. The job we run will suspend and never abort. 

The job we run is 'share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar'

 

From jps, seems everything is running normally

hadoop@lily-OptiPlex-3070:~/hadoop-3.3.0$ jps
 23409 NameNode
 23889 SecondaryNameNode
 24338 NodeManager
 23622 DataNode
 14408 RunJar
 29098 HRegionServer
 14526 Jps

 "
hadoop-client-runtime-3.3.0 not properly shading javax.xml.bind,13360149,Open,Major,,22/Feb/21 16:46,,3.3.0,"In the file hadoop-client-modules/hadoop-client-runtime/pom.xml, it excludes javax.xml.bind because it says it is part of the SE JDK.  But, this is no longer true in Java 11. 

 
|<exclusions>|
| |<!-- these APIs are a part of the SE JDK -->|
| |<exclusion>|
| |<groupId>javax.xml.bind</groupId>|
| |<artifactId>jaxb-api</artifactId>|
| |</exclusion>|

 

So, when I try to use Guice to inject objects at start up,  I am getting the error

Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/javax/xmlbind/ModuleUtil (wrong name: javax/xml/bind/ModuleUtil)

 

I was able to get past this by using zip to change the position of the ModuleUtil.class inside the META-INF directory.

Before I made any changes:
 jar tvf hadoop-client-runtime-3.3.0.jar | grep ModuleUtil*

META-INF/versions/9/javax/xml/bind/ModuleUtil.class
 org/apache/hadoop/shaded/javax/xml/bind/ModuleUtil.class

After I modified the jar:

jar tvf hadoop-client-runtime-3.3.0-MODIFIED.jar | grep ModuleUtil*

META-INF/versions/9/org/apache/hadoop/shaded/javax/xml/bind/ModuleUtil.class
 org/apache/hadoop/shaded/javax/xml/bind/ModuleUtil.class

With the revised jar file, my code started.  I don't know if that is the correct way to fix but it works."
magic committer to be enabled for all S3 buckets,13353722,Resolved,Major,Fixed,21/Jan/21 10:36,27/Jan/21 19:07,3.3.0,"* core-default.xml updated so that fs.s3a.committer.magic.enabled = true
* CommitConstants updated to match
* All tests which previously enabled the magic committer now rely on
  default settings. This helps make sure it is enabled.
* Docs cover the switch, mention its enabled and explain why you may
  want to disable it.
Note: this doesn't switch to using the committer -it just enables the path
rewriting magic which it depends on.

"
Server IPC version 9 cannot communicate with client version 4,13359124,Resolved,Major,Invalid,17/Feb/21 18:33,17/Feb/21 19:04,,"`I want to connect to hdfs using java jast like this
 _String url = ""hdfs://c7301.ambari.apache.org:8020/file.txt"";_

_FileSystem fs = null;_
 _InputStream in = null;_
 _try {_
 _Configuration conf = new Configuration();_
 _fs = FileSystem.get(URI.create(url), conf, ""admin"");_

_in = fs.open(new Path(url));_

_IOUtils.copyBytes(in, System.out, 4096, false);_

_} catch (Exception e) {_
 _e.printStackTrace();_
 _} finally {_
 _IOUtils.closeStream(fs);_
 _}_ 

*Error that i got*
 [2021-02-17 20:02:06,115] ERROR PriviledgedActionException as:admin cause:org.apache.hadoop.ipc.RemoteException: *Server IPC version 9 cannot communicate with* *client version 4 (*org.apache.hadoop.security.UserGroupInformation:1124)
 org.apache.hadoop.ipc.RemoteException: *Server IPC version 9 cannot communicate with client version 4*
 at org.apache.hadoop.ipc.Client.call(Client.java:1070)
 at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
 at com.sun.proxy.$Proxy4.getProtocolVersion(Unknown Source)
 at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
 at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
 at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
 at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
 at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
 at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
 at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:117)
 at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:115)
 at java.base/java.security.AccessController.doPrivileged(Native Method)
 at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:115)
 at Main.main(Main.java:38) 
 \{{ I tried different solutions to the problem, but nothing helped. }}

*Its my pom.xml file*

 
  <?xml version=""1.0"" encoding=""UTF-8""?>
 <project xmlns=""http://maven.apache.org/POM/4.0.0""
 xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
 xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 [http://maven.apache.org/xsd/maven-4.0.0.xsd]"">
 <modelVersion>4.0.0</modelVersion>

<groupId>org.example</groupId>
 <artifactId>producer</artifactId>
 <version>1.0-SNAPSHOT</version>

<properties>
 <maven.compiler.source>11</maven.compiler.source>
 <maven.compiler.target>11</maven.compiler.target>
 </properties>

<dependencies>
 <dependency>
 <groupId>org.apache.kafka</groupId>
 <artifactId>kafka-clients</artifactId>
 <version>0.10.0.0</version>
 </dependency>

<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-common</artifactId>
 <version>3.2.0</version>
 </dependency>

<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-hdfs</artifactId>
 <version>3.2.0</version>
 </dependency>

<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-yarn-common</artifactId>
 <version>3.2.0</version>
 </dependency>

<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-mapreduce-client-common</artifactId>
 <version>3.2.0</version>
 </dependency>

<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-mapreduce-client-core</artifactId>
 <version>3.2.0</version>
 </dependency>

</dependencies>
 <build>
 <plugins>
 <plugin>
 <groupId>org.apache.maven.plugins</groupId>
 <artifactId>maven-shade-plugin</artifactId>
 <version>3.2.4</version>
 <executions>
 <execution>
 <phase>package</phase>
 <goals>
 <goal>shade</goal>
 </goals>
 <configuration>
 <transformers>
 <transformer
 implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"">
 <mainClass>Main</mainClass>
 </transformer>
 </transformers>
 </configuration>
 </execution>
 </executions>
 </plugin>
 </plugins>
 </build>

</project>
 \{{}}

*And its version of hdfs*

Hadoop 3.1.1.3.1.4.0-315 Source code repository git@github.com:hortonworks/hadoop.git -r 58d0fd3d8ce58b10149da3c717c45e5e57a60d14 Compiled by jenkins on 2019-08-23T05:15Z Compiled with protoc 2.5.0 From source with checksum fcbd146ffa6d48fef0ed81332f9d6f0 This command was run using /usr/ddp/3.1.4.0-315/hadoop/hadoop-common-3.1.1.3.1.4.0-315.jar

 

if someone knew a similar problem, please help"
Looking to build Hadoop but start-build-env.sh errors,13358149,Resolved,Major,Duplicate,11/Feb/21 11:37,11/Feb/21 15:35,,"Hey everyone, we're trying to build Hadoop from source and in building.txt we see 

 

The easiest way to get an environment with all the appropriate tools is by means
of the provided Docker config.
This requires a recent version of docker (1.4.1 and higher are known to work).

On Linux / Mac:
 Install Docker and run this command:

$ ./start-build-env.sh

 

However, and admittedly this is on 3.1.4, which I want to build - 

 

 

{{Step 26/36 : RUN pip2 install     configparser==4.0.2     pylint==1.9.2}}{{ ---> Running in acf678bc8314}}{{Collecting configparser==4.0.2}}{{  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl}}{{Collecting pylint==1.9.2}}{{  Downloading https://files.pythonhosted.org/packages/f2/95/0ca03c818ba3cd14f2dd4e95df5b7fa232424b7fc6ea1748d27f293bc007/pylint-1.9.2-py2.py3-none-any.whl (690kB)}}{{Collecting singledispatch; python_version < ""3.4"" (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl}}{{Collecting isort>=4.2.5 (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/a2/f7/f50fc9555dc0fe2dc1e7f69d93f71961d052857c296cad0fb6d275b20008/isort-5.7.0.tar.gz (169kB)}}{{Collecting astroid<2.0,>=1.6 (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/8b/29/0f7ec6fbf28a158886b7de49aee3a77a8a47a7e24c60e9fd6ec98ee2ec02/astroid-1.6.6-py2.py3-none-any.whl (305kB)}}{{Collecting backports.functools-lru-cache; python_version == ""2.7"" (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl}}{{Collecting mccabe (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl}}{{Collecting six (from pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl}}{{Collecting enum34>=1.1.3; python_version < ""3.4"" (from astroid<2.0,>=1.6->pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl}}{{Collecting wrapt (from astroid<2.0,>=1.6->pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz}}{{Collecting lazy-object-proxy (from astroid<2.0,>=1.6->pylint==1.9.2)}}{{  Downloading https://files.pythonhosted.org/packages/69/fc/79080e582c17b7f45ac0bb4a13b0260992dcc2519b5a29ac6cde3e81b6fa/lazy_object_proxy-1.5.2-cp27-cp27mu-manylinux1_x86_64.whl (52kB)}}{{Building wheels for collected packages: isort, wrapt}}{{  Running setup.py bdist_wheel for isort: started}}{{  Running setup.py bdist_wheel for isort: finished with status 'error'}}{{  Complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-u9WX19/isort/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" bdist_wheel -d /tmp/tmp60t8Japip-wheel- --python-tag cp27:}}{{  /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'}}{{    warnings.warn(msg)}}{{  running bdist_wheel}}{{  running build}}{{  running build_py}}{{  creating build}}{{  creating build/lib.linux-x86_64-2.7}}{{  creating build/lib.linux-x86_64-2.7/isort}}{{  copying isort/_version.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/settings.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/logo.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/hooks.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/wrap_modes.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/sections.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/parse.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/utils.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/setuptools_commands.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/files.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/format.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/wrap.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/place.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/profiles.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/__main__.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/output.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/core.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/exceptions.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/pylama_isort.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/literal.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/comments.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/io.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/identify.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/main.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/api.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/__init__.py -> build/lib.linux-x86_64-2.7/isort}}{{  copying isort/sorting.py -> build/lib.linux-x86_64-2.7/isort}}{{  creating build/lib.linux-x86_64-2.7/isort/_future}}{{  copying isort/_future/_dataclasses.py -> build/lib.linux-x86_64-2.7/isort/_future}}{{  copying isort/_future/__init__.py -> build/lib.linux-x86_64-2.7/isort/_future}}{{  creating build/lib.linux-x86_64-2.7/isort/_vendored}}{{  creating build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  copying isort/_vendored/toml/ordered.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  copying isort/_vendored/toml/tz.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  copying isort/_vendored/toml/decoder.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  copying isort/_vendored/toml/encoder.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  copying isort/_vendored/toml/__init__.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{  creating build/lib.linux-x86_64-2.7/isort/deprecated}}{{  copying isort/deprecated/finders.py -> build/lib.linux-x86_64-2.7/isort/deprecated}}{{  copying isort/deprecated/__init__.py -> build/lib.linux-x86_64-2.7/isort/deprecated}}{{  creating build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py38.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py39.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py35.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py37.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/all.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py3.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py27.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py2.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/py36.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  copying isort/stdlibs/__init__.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{  error: can't copy 'isort/stdlibs': doesn't exist or not a regular file}}{{  }}{{  ----------------------------------------}}{{  Failed building wheel for isort}}{{  Running setup.py clean for isort}}{{  Running setup.py bdist_wheel for wrapt: started}}{{  Running setup.py bdist_wheel for wrapt: finished with status 'done'}}{{  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a}}{{Successfully built wrapt}}{{Failed to build isort}}{{Installing collected packages: configparser, six, singledispatch, isort, enum34, wrapt, backports.functools-lru-cache, lazy-object-proxy, astroid, mccabe, pylint}}{{  Running setup.py install for isort: started}}{{    Running setup.py install for isort: finished with status 'error'}}{{    Complete output from command /usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-u9WX19/isort/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-Pa5mnZ-record/install-record.txt --single-version-externally-managed --compile:}}{{    /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'}}{{      warnings.warn(msg)}}{{    running install}}{{    running build}}{{    running build_py}}{{    creating build}}{{    creating build/lib.linux-x86_64-2.7}}{{    creating build/lib.linux-x86_64-2.7/isort}}{{    copying isort/_version.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/settings.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/logo.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/hooks.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/wrap_modes.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/sections.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/parse.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/utils.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/setuptools_commands.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/files.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/format.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/wrap.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/place.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/profiles.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/__main__.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/output.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/core.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/exceptions.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/pylama_isort.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/literal.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/comments.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/io.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/identify.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/main.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/api.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/__init__.py -> build/lib.linux-x86_64-2.7/isort}}{{    copying isort/sorting.py -> build/lib.linux-x86_64-2.7/isort}}{{    creating build/lib.linux-x86_64-2.7/isort/_future}}{{    copying isort/_future/_dataclasses.py -> build/lib.linux-x86_64-2.7/isort/_future}}{{    copying isort/_future/__init__.py -> build/lib.linux-x86_64-2.7/isort/_future}}{{    creating build/lib.linux-x86_64-2.7/isort/_vendored}}{{    creating build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    copying isort/_vendored/toml/ordered.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    copying isort/_vendored/toml/tz.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    copying isort/_vendored/toml/decoder.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    copying isort/_vendored/toml/encoder.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    copying isort/_vendored/toml/__init__.py -> build/lib.linux-x86_64-2.7/isort/_vendored/toml}}{{    creating build/lib.linux-x86_64-2.7/isort/deprecated}}{{    copying isort/deprecated/finders.py -> build/lib.linux-x86_64-2.7/isort/deprecated}}{{    copying isort/deprecated/__init__.py -> build/lib.linux-x86_64-2.7/isort/deprecated}}{{    creating build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py38.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py39.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py35.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py37.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/all.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py3.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py27.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py2.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/py36.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    copying isort/stdlibs/__init__.py -> build/lib.linux-x86_64-2.7/isort/stdlibs}}{{    error: can't copy 'isort/stdlibs': doesn't exist or not a regular file}}{{    }}{{    ----------------------------------------}}{{Command ""/usr/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-u9WX19/isort/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-Pa5mnZ-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-u9WX19/isort/}}

You are using pip version 8.1.1, however version 21.0.1 is available.

You should consider upgrading via the 'pip install --upgrade pip' command.

 

If I add that into the Dockerfile, I get

{{}}{{ ---> Running in 5d4c46c61d29}}{{Traceback (most recent call last):}}{{  File ""/usr/local/bin/pip2"", line 7, in <module>}}{{    from pip._internal.cli.main import main}}{{  File ""/usr/local/lib/python2.7/dist-packages/pip/_internal/cli/main.py"", line 60}}{{    sys.stderr.write(f""ERROR: \{exc}"")}}{{                                   ^}}{{SyntaxError: invalid syntax}}"
Non-randomized password used,13357162,Resolved,Major,Invalid,05/Feb/21 19:09,08/Feb/21 21:25,,"In file [https://github.com/apache/hadoop/blob/a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java] (at Line 322) non-randomized password is used.

*Security Impact*:

Hackers can get access to the non-randomized passwords and compromise the system.

*Solution we suggest*:

Password should be generated randomly

*Please share with us your opinions/comments if there is any*:

Is the bug report helpful?"
BZip2Codec incorrectly throws IndexOutOfBoundsException: offs(X) + len(X+1) > dest.length(Y).,13348699,Open,Major,,02/Jan/21 04:18,,3.1.2,"In org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream
 around line 496 seems to mistakenly add the offset to the length.
{noformat}
      if (this.posSM == POS_ADVERTISEMENT_STATE_MACHINE.ADVERTISE) {
        result = this.input.read(b, off, off + 1 << HERE);
{noformat}

Here's a reference [BZip2Codec.java:L496|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java#L496]"
"Backport HADOOP-16947 ""Stale record should be remove when MutableRollingAverages generating aggregate data."" to branch 2.10",13354560,Resolved,Major,Fixed,26/Jan/21 02:08,30/Jan/21 04:14,,
New connection requires a retry to refresh NameNode IP changes,13355272,Patch Available,Major,,28/Jan/21 20:13,,2.8.0,"Hadoop-17068 is to handle the case of NameNode IP address changes in which HDFS client will update the IP address after the connection failure.  

DataNodes also use the same logic to refresh IP address for the connection. Such connection is reused with the default idle time 10 seconds. (set by ipc.client.connection.maxidletime). If the connection is closed and the DataNode will use the old NameNode IP address to connect and refresh to the new IP address after the first failure.  

The problem with the refresh logic in org.apache.hadoop.ipc.Client is: the server value getting refreshed will not reflect in remoteId.address, while the next connection creation will use remoteId.address.

{{if (!server.equals(currentAddr)) {}}
{{  LOG.warn(""Address change detected. Old: "" + server.toString() +}}
{{          "" New: "" + currentAddr.toString()); }}
{{   server = currentAddr;}}

 

Such kind of retry in a big cluster will cause random ""BLOCK* blk_16987635027_18010098516 is COMMITTED but not COMPLETE(numNodes= 0 < minimum = 1) in fie"" error if all three replicas take one retry to read/write the block. 

 "
renaming S3A Statistic DELEGATION_TOKENS_ISSUED to DELEGATION_TOKEN_ISSUED broke tests downstream,13354527,Resolved,Major,Fixed,25/Jan/21 20:49,27/Jan/21 16:49,3.3.1,"HADOOP-16830/HADOOP-17271 renamed DELEGATION_TOKENS_ISSUED to DELEGATION_TOKENS_ISSUED while trying to unify naming. This breaks downstream code.

Fix: revert back the name change."
"S3A docs to state s3 is consistent, deprecate S3Guard",13353550,Resolved,Major,Fixed,20/Jan/21 14:47,25/Jan/21 13:43,3.3.0,"Review S3 docs, remove all statements of inconsistency, s3guard docs to say unneeded"
"Implement listFiles(path, recursive=true)",13354365,Open,Major,,25/Jan/21 10:33,,3.3.0,"Azure storage and the abfs client offer recursive listings, but currently there's no listFiles() method to connect to it instead a recursive treewalk is used.

Implementing a listFiles(path, recursive) could offer significant speedups to applications which use it, for example HIVE-24669, and again, async prefetching could provide even more benefits

Would need changes in {{org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore}} to join everything up"
Race condition refreshing NetgroupCache ,13353995,Open,Major,,22/Jan/21 15:57,,2.10.1,"There is potential concurrency bug in the {{NetgroupCache}} implementation.

{{NetgroupCache}} is static. When ACL is built, its groups will be added to the {{NetgroupCache}}.

A {{-refreshUserToGroupsMappings}} forces the cache to reload the users for each group.
 This is done by first getting the keys, clearing the cache, then finally reloading the users for each group.
 The problem that the three steps are not atomic.
 Adding ACLs concurrently may take place between L80-L81 ([JniBasedUnixGroupsNetgroupMapping#L79|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java#L79]). This results in the loss of the most recently added group.
 Since group names are used in the JNI level, the users of that group won't be retrieved.
{code:java}
78 @Override
79  public void cacheGroupsRefresh() throws IOException {
80    List<String> groups = NetgroupCache.getNetgroupNames();
81     NetgroupCache.clear();
82    cacheGroupsAdd(groups);
83  }
{code}
+Solution:+

Refreshing {{NetgroupCache}} should not clear the cache keys."
Groups user mapping based on Netgroup do not auto-refresh ,13353593,Open,Major,,20/Jan/21 20:12,,,"It was noticed that a manual refresh ({{yarn rmadmin -refreshUserToGroupsMappings}}) is required for services to pick a user-group membership.
The auto-refresh of the {{Groups.caching}} only works for the upper layer and won't fetch the groups from the system.

There is a need to automatically reload all the groups (currently in {{NetgroupCache}}). The interval can be a new configuration, with a default value of 6 hours.
"
Replace deprecated getGroups with getGroupsSet,13353981,Open,Major,,22/Jan/21 15:09,,2.10.1,"This ticket is a follow up to HADOOP-17079 which added an optimization to use sets instead of lists retrieving user groups.
However, the deprecated getGroups is still being called throughout the code.
This is an attempt to clean up the code.

CC: [~xyao], [~Jim_Brennan]"
Provide fallbacks for callqueue ipc namespace properties,13353837,Open,Major,,21/Jan/21 20:16,,3.1.4,"Filing this proposal on behalf of [~daryn], based on comments he made in one of our internal Jiras.

The following settings are currently specified per port:
{noformat}
  /**
   * CallQueue related settings. These are not used directly, but rather
   * combined with a namespace and port. For instance:
   * IPC_NAMESPACE + "".8020."" + IPC_CALLQUEUE_IMPL_KEY
   */
  public static final String IPC_NAMESPACE = ""ipc"";
  public static final String IPC_CALLQUEUE_IMPL_KEY = ""callqueue.impl"";
  public static final String IPC_SCHEDULER_IMPL_KEY = ""scheduler.impl"";
  public static final String IPC_IDENTITY_PROVIDER_KEY = ""identity-provider.impl"";
  public static final String IPC_COST_PROVIDER_KEY = ""cost-provider.impl"";
  public static final String IPC_BACKOFF_ENABLE = ""backoff.enable"";
  public static final boolean IPC_BACKOFF_ENABLE_DEFAULT = false;
 {noformat}
If one of these properties is not specified for the port, the defaults are hard-coded.
It would be nice to provide a way to specify a fallback default property that would be used for all ports.  If the property for a specific port is not defined, the fallback would be used, and if the fallback is not defined it would use the hard-coded defaults.

We would likely need to make the same change for properties specified by these classes.  For example, properties used in WeightedTimeCostProvider.

The fallback properties could be specified by dropping the port from the property name.  For example, the fallback for {{ipc.8020.cost-provider.impl}} would be {{ipc.cost-provider.impl}}.
Another option would be to use something more explicit like {{ipc.default.cost-provider.impl}}.
"
[SBN read] Implement msync() for ViewFS,13353410,Open,Major,,20/Jan/21 01:54,,,"We should implement {{FileSystem.msync()}} for {{ViewFS}} and {{ViewFileSystem}}.
It should call msync() on all volumes for which observer reads are enabled and {{dfs.namenode.state.context.enabled = true}}"
s3guard tool dumpStorageStatistics to move to IOStatistics,13351074,Open,Major,,08/Jan/21 11:54,,3.4.0,S3GuardTool cli's -verbose option prints storage statistics of the FS. If it moves to IOStatistics it will print latencies as well as op counts
[s3a] Intermittent failure of ITestS3ADeleteCost.testDeleteSingleFileInDir,13349217,Resolved,Major,Fixed,05/Jan/21 14:40,14/Jan/21 13:55,3.3.0,"Test failed against ireland intermittently with the following config:

{{mvn clean verify -Dparallel-tests -DtestsThreadCount=8}}
xml based config in auth-keys.xml:
{code:xml}
    <property>
        <name>fs.s3a.metadatastore.impl</name>
        <value>org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore</value>
    </property>
{code}"
Hadoop Client getRpcResponse May Return Wrong Result,13351148,Resolved,Major,Not A Problem,08/Jan/21 18:47,09/Jan/21 03:56,,"{code:java|Title=Client.java}
  /** @return the rpc response or, in case of timeout, null. */
  private Writable getRpcResponse(final Call call, final Connection connection,
      final long timeout, final TimeUnit unit) throws IOException {
    synchronized (call) {
      while (!call.done) {
        try {
          AsyncGet.Util.wait(call, timeout, unit);
          if (timeout >= 0 && !call.done) {
            return null;
          }
        } catch (InterruptedException ie) {
          Thread.currentThread().interrupt();
          throw new InterruptedIOException(""Call interrupted"");
        }
      }

 */
  static class Call {
    final int id;               // call id
    final int retry;           // retry count
...
    boolean done;               // true when call is done
...
}
{code}

The {{done}} variable is not marked as {{volatile}} so the thread which is checking its status is free to cache the value and never reload it even though it is expected to change by a different thread.  The while loop may be stuck waiting for the change, but is always looking at a cached value.  If that happens, timeout will occur and then return 'null'.

In previous versions of Hadoop, there was no time-out at this level, so it would cause endless loop.  Really tough error to track down if it happens."
Optimise abfs incremental listings,13352964,Open,Minor,,18/Jan/21 04:31,,3.4.0,
Usage of incorrect regex range A-z,13357181,Resolved,Minor,Fixed,05/Feb/21 21:44,20/May/23 00:51,3.4.0,"There are two cases where the regex {{A-z}} is used. I assume that is a typo (and should be {{A-Z}}) because {{A-z}} matches:
- {{A-Z}}
- {{\[}}, {{&#92;}}, {{\]}}, {{^}}, {{_}}, {{`}}
- {{a-z}}

Affected:
- https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/util/Check.java#L109
(and https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/util/Check.java#L115)
- https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/resourcetypes/ResourceTypesTestHelper.java#L38"
Fix DelegationTokenRenewer#updateRenewalTime java doc error.,13368895,Resolved,Minor,Fixed,31/Mar/21 14:35,04/Apr/21 09:23,3.4.0,"The param of updateRenewalTime should be the renew cycle, not the new time."
Fix java doc issue introduced by HADOOP-17578,13366844,Resolved,Minor,Fixed,22/Mar/21 23:57,23/Mar/21 04:54,3.4.0,Remove the unused throw declaration. 
Update Description of hadoop-http-auth-signature-secret in HttpAuthentication.md,13360887,Resolved,Minor,Fixed,25/Feb/21 06:49,04/Mar/21 05:58,2.10.2,"The HttpAuthentication.md document says ""The same secret should be used for all nodes in the cluster, ResourceManager, NameNode, DataNode and NodeManager""  but the secret should be different for each service. This description is updated in [core-default.xml|https://github.com/apache/hadoop/commit/d82009599a2e9f48050e0c41440b36c759ec068f#diff-268b9968a4db21ac6eeb7bcaef10e4db744d00ba53989fc7251bb3e8d9eac7df] but has to be updated in HttpAuthentication.md as well."
AbstractContractStreamIOStatisticsTest fails if read buffer not full,13354966,Open,Minor,,27/Jan/21 15:15,,3.3.1,"{code}
[ERROR] testInputStreamStatisticRead(org.apache.hadoop.fs.s3a.statistics.ITestS3AContractStreamIOStatistics)  Time elapsed: 6.252 s  <<< FAILURE!
org.junit.ComparisonFailure: [Counter named stream_read_bytes with expected value 129] expected:<[129]L> but was:<[3]L>
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
{code}

Test should handle all cases where bytes read > 0"
ADL Gen1: Fix the NoClassDefFound error for assertJ.,13351926,Open,Minor,,13/Jan/21 05:48,,3.4.0,
IOStatistics collection for listStatusIterator(),13355096,Open,Minor,,28/Jan/21 06:50,,3.4.0,Add IOStatistics collection for listStatusIterator
Remove the enable/disable flag for ABFSRemoteListIterator,13356237,Open,Minor,,02/Feb/21 16:14,,3.3.1,
HDFS Put was failed with IPV6 cluster ,13360229,Open,Minor,,23/Feb/21 04:28,,3.1.1,
"S3A to treat ""SdkClientException: Data read has a different length than the expected"" as EOFException",13350679,Resolved,Minor,Fixed,06/Jan/21 14:11,27/Jul/21 12:41,3.4.0,"A test run with network problems caught exceptions ""com.amazonaws.SdkClientException: Data read has a different length than the expected:"", which then escalated to failure.

these should be recoverable if they are recognised as such. 

translateException could do this. Yes, it would have to look @ the text, but as {{signifiesConnectionBroken()}} already does that for ""Failed to sanitize XML document destined for handler class"", it'd just be adding a new text string to look for.

"
Add option to downgrade S3A rejection of Syncable to warning,13366730,Resolved,Minor,Fixed,22/Mar/21 13:07,26/Apr/21 10:43,3.3.1,"The Hadoop Filesystem Syncable API is intended to meet the requirements laid out in [StoneBraker81] _Operating System Support for Database Management_

bq.  The service required from an OS buffer manager is a selectedforce out which would push the intentions list and the commit flag to disk in the proper order. Such a service is not present in any buffer manager known to us.

It's an expensive operation -so expensive that {{Syncable.hsync()}} isn't even called on {{DFSOutputStream.close()}}. I

Even though S3A does not manifest any data until close() is called, applications coming from HDFS may call Syncable methods and expect to them to persist data with the durability guarantees offered by HDFS.

Since the output stream hardening of HADOOP-13327, S3A throws UnsupportedOperationException to indicate that the synchronization semantics of Syncable absolutely cannot be met. 

As a result, applications which have been calling the Syncable APIs are finding the call failing. In the absence of exception handling to recognise that the durability semantics are being met, they fail.

If the user and the application actually expects data to be persisted, this is the correct behaviour. The data cannot be persisted this way.

If, however, they were calling this on HDFS more as a {{flush()}} than the full and expensive DBMS-class persistence call, then this failure is unwelcome. The applications really needs to catch the UnsupportedOperationException raised by S3A _or any other FS strictly reporting failures_, report the problem and perform some other means of safe data storage

Even better, they can use hasPathCapability on the FS or hasCapability() on the stream to probe before even opening a file or trying to sync it. the hasCapability() on a stream was actually implemented in Hadooop-2.x precisely to allow applications to identify when a stream could not meet the guarantees (e.g some of the encrypted streams, file:// before HADOOP-13...)

Until they can correct their code, I propose adding the option for s3a to downgrade

fs.s3a.downgrade.syncable.exceptions 

This will

* Log once per process at WARN
* downgrade the calls to noop() 
* increment counters in S3A stats and IO stats of invocations of the Syncable methods. This will allow for stats gathering to let us identify which applications need fixing in cloud deployments

Testing: copy the hsync tests but expect exceptions to be swallowed and stats to be collected

Also: UnsupportedException text will link to this JIRA"
Fix the wrong CIDR range example in Proxy User documentation,13365757,Resolved,Minor,Fixed,17/Mar/21 08:56,22/Mar/21 02:47,3.2.2,"The CIDR range example on the Proxy user description page is wrong.

 

In the Configurations section of Proxy user page, CIDR format 10.222.0.0/16 means 10.222.0.0-15.

 

But It's not true. the CIDR format 10.222.0.0/16 means 10.222.0.0-10.222.255.255.

 

as-is : hosts in the range `10.222.0.0-15` and

to-be : hosts in the range `10.222.0.0-10.222.255.255` and"
Fix the examples of hadoop config prefix,13353452,Resolved,Minor,Fixed,20/Jan/21 06:18,07/May/22 23:11,,"In https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/DownstreamDev.html#XML_Configuration_Files
{quote}e.g. hadoop, io, ipc, fs, net, file, ftp, kfs, ha, file, dfs, mapred, mapreduce, and yarn.
{quote}

* There are two ""file""
* kfs has been removed since Hadoop 2.x"
Provide mechanism for explicitly specifying the compression codec for input files,13362163,Open,Minor,,03/Mar/21 17:57,,,"I come to you via SPARK-29280.

I am looking for the file _input_ equivalents of the following settings:
{code:java}
mapreduce.output.fileoutputformat.compress
mapreduce.map.output.compress{code}
Right now, I understand that Hadoop infers the codec to use when reading a file from the file's extension.

However, in some cases the files may have the incorrect extension or no extension. There are links to some examples from SPARK-29280.

Ideally, you should be able to explicitly specify the codec to use to read those files. I don't believe that's possible today. Instead, the current workaround appears to be to [create a custom codec class|https://stackoverflow.com/a/17152167/877069] and override the getDefaultExtension method to specify the extension to expect.

Does it make sense to offer an explicit way to select the compression codec for file input, mirroring how things work for file output?"
Parallelize building of dependencies,13355977,Resolved,Minor,Fixed,01/Feb/21 15:41,02/Feb/21 15:12,3.3.0,Need to use make -j$(nproc) to parallelize building of Protocol buffers and Intel ISA - L dependency.
Remove Commons Logger from FileSystem Class,13353595,Resolved,Minor,Fixed,20/Jan/21 20:13,12/Mar/21 19:28,,"Remove reference to Commons Logger in FileSystem, it already has SLF4J, so it's a bit weird to be mixing and matching and interweaving loggers in this way.  Also, my hope is to eventually migrate everything to SLF4J to simplify things for downstream consumers of the common library."
IPV6 support in Netutils#createSocketAddress ,13360228,Resolved,Minor,Fixed,23/Feb/21 04:23,10/Aug/21 10:51,3.1.1,"Currently NetUtils#createSocketAddress not supporting if target is IPV6 ip. If target is IPV6 ip then it throw ""Does not contain a valid host:port authority: "".

This need be support.

public static InetSocketAddress createSocketAddr(String target,
 int defaultPort,
 String configName,
 boolean useCacheIfPresent) {
 String helpText = """";
 if (configName != null)

{ helpText = "" (configuration property '"" + configName + ""')""; }

if (target == null)

{ throw new IllegalArgumentException(""Target address cannot be null."" + helpText); }

target = target.trim();
 boolean hasScheme = target.contains(""://"");
 URI uri = createURI(target, hasScheme, helpText, useCacheIfPresent);

String host = uri.getHost();
 int port = uri.getPort();
 if (port == -1)

{ port = defaultPort; }

String path = uri.getPath();

if ((host == null) || (port < 0) ||
 (!hasScheme && path != null && !path.isEmpty()))

{ throw new IllegalArgumentException( *""Does not contain a valid host:port authority: "" + target + helpText* ); }

return createSocketAddrForHost(host, port);
 }"
Improve the description of hadoop.http.authentication.signature.secret.file,13353445,Resolved,Minor,Fixed,20/Jan/21 05:55,24/Jan/21 14:01,,"The document says the same secret should be used for RM/NM/NN/DN configurations, however, the secret should be different for each service."
refresh freatures description of HuaweiCloud OBS Adapter for Hadoop Support,13363985,Open,Minor,,12/Mar/21 01:45,,,"Refresh freatures description of HuaweiCloud OBS Adapter for Hadoop Support, such as :
rename, Append , hflush&hsync support.

link : hadoop/hadoop-cloud-storage-project/hadoop-huaweicloud/src/site/markdown/index.md

Features
Present a hierarchical HDFS file system by implementing the standard Hadoop FileSystem interface.
In hadoop, Read and write data stored in a HuaweiCloud OBS bucket.
Can act as a source of data in a MapReduce job, or a sink.
Support multipart upload for a large file.
Reference file system paths using URLs using the obs scheme.
Uses HuaweiCloud OBS’s Java SDK with support for latest OBS features and authentication schemes.
For OBS 'Parallel file system bucket'(Posix), support rename with atomic semantics, Append, hflush&hsync.
For OBS 'Parallel file system bucket'(Posix), Provide trash mechanism and quickly delete, which using rename and lifecircle of OBS bucket.
Tested for scale."
refresh freatures description of HuaweiCloud OBS Adapter for Hadoop Support,13363984,Resolved,Minor,Duplicate,12/Mar/21 01:44,13/May/21 12:06,3.3.0,"Refresh freatures description of HuaweiCloud OBS Adapter for Hadoop Support, such as :
rename, Append , hflush&hsync support.

link : hadoop/hadoop-cloud-storage-project/hadoop-huaweicloud/src/site/markdown/index.md

Features
Present a hierarchical HDFS file system by implementing the standard Hadoop FileSystem interface.
In hadoop, Read and write data stored in a HuaweiCloud OBS bucket.
Can act as a source of data in a MapReduce job, or a sink.
Support multipart upload for a large file.
Reference file system paths using URLs using the obs scheme.
Uses HuaweiCloud OBS’s Java SDK with support for latest OBS features and authentication schemes.
For OBS 'Parallel file system bucket'(Posix), support rename with atomic semantics, Append, hflush&hsync.
For OBS 'Parallel file system bucket'(Posix), Provide trash mechanism and quickly delete, which using rename and lifecircle of OBS bucket.
Tested for scale."
ADLFS: Update SDK version from 2.3.6 to 2.3.9,13368774,Resolved,Minor,Fixed,31/Mar/21 05:06,12/May/21 17:20,3.3.0,
ABFS: Suport for customer provided encryption key,13359503,Resolved,Minor,Fixed,19/Feb/21 04:34,27/Apr/21 12:17,3.4.0,"The data for a particular customer needs to be encrypted on account level. At server side the APIs will start accepting the encryption key as part of request headers. The data will be encrypted/decrypted with the given key at the server. 

Since the ABFS FileSystem APIs are implementations for Hadoop FileSystem APIs there is no direct way with which customer can pass the key to ABFS driver. In this case driver should have the following capabilities so that it can accept and pass the encryption key as one of the request headers. 
 # There should be a way to configure the encryption key for different accounts.
 # If there is a key specified for a particular account, the same needs to be sent along with the request headers. 

*Config changes* 

They key for an account can be specified in the core-site as follows. 

fs.azure.account.client-provided-encryption-key.{account name}.dfs.core.windows.net "
Separate string metric from tag in hadoop metrics2,13368025,Patch Available,Minor,,26/Mar/21 21:52,,,"Right now in hadoop metrics2, String metrics from method are categorized as tag (v.s. metrics as other number types), this caused later when reporting beans, it will add a prefix ""tag."" before the metric name.

It will be cleaner if we have another child inherit MutableMetric for string (maybe MutableText?) thus the String metrics from method can get rid of the tag."
"Time.monotonicNow() should be used to compute time interval, but not Time.now()",13367132,Open,Minor,,24/Mar/21 03:32,,3.4.0,"According to the specification of method Time.now() and Time.monotonicNow(), the latter should be used for measuring durations."
Remove NULL checks before instanceof,13366906,Resolved,Minor,Fixed,23/Mar/21 07:16,23/Mar/21 15:46,,The NULL checks before instanceof check should be removed.
ABFS: Fix auth failures getting counted as throttling errors in ABFS throttling interceptor,13355209,Resolved,Minor,Duplicate,28/Jan/21 14:35,16/Mar/21 05:48,3.4.0,Currently the throttling intercepter counts the auth failures against throttling errors. This needs to be fixed.
Remove trace subcommand from hadoop CLI,13356642,Resolved,Minor,Fixed,04/Feb/21 04:47,12/Mar/21 01:42,,TraceAdmin protocol and utility class were removed by HADOOP-17424.
Image scan shows something in Hadoop using jackson-databind 2.4.0...what?,13361620,Open,Minor,,01/Mar/21 13:32,,,"Hi everyone, I've done a Twistlock container-level scan of a Flink/Hadoop image (so, it's the Hadoop shaded uber jar specifically, for Hadoop 3.3.1 snapshot and Flink 1.11.3).

The most interesting result is as follows I think it is used in Hadoop and not Flink because my container scan without the Hadoop jar does not show this result. 

_{{ ""version"": ""2.4.0"",_
 _""name"": ""com.fasterxml.jackson.core_jackson-databind"",_
 _""path"": ""/opt/flink/lib/flink-shaded-hadoop-3-uber-3.3.1-SNAPSHOT-10.0.jar""}}_

 

That's a very old version and likely very susceptible to CVEs I would imagine, does anybody know what might be using it and if we can upgrade the version?

 

[https://github.com/apache/hadoop/search?l=Maven+POM&q=2.4.0] shows 113 results and searching with [https://github.com/apache/hadoop/search?q=com.fasterxml.jackson.core_jackson-databind] isn't helpful either unfortunately (in fact less so).

 

So I am wondering what could be using it..any input would be awesome, thank you! I will do my own digging as well to keep looking but if anyone knows off-hand that would be fantastic"
Provide snapshot builds on Maven central,13360651,Resolved,Minor,Done,24/Feb/21 15:14,26/Feb/21 17:41,,"Hey everyone, I'm looking to build the shaded Hadoop/Flink jar using the very latest Hadoop code that isn't yet in a Maven repository AFAIK (I'm looking at [https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/).] Entirely possible that's not the right place...

 

Are there plans or is anyone working on, or have I just missed, binaries being available (say, Hadoop 3.3-snapshot)?

 

I remember working on Spark and that was a thing, IIRC you could set a flag to true in any of the pom.xmls to accept snapshots and not published things.

 

It's entirely possible I've just forgot how to do it and can't see it well documented anywhere, and I don't believe I have to go through the steps of setting up a Maven repository somewhere (I want to do the build in Docker, and in the pom.xml I would love to just say: use Hadoop version 3.3-snapshot).

 

To give some context, I would like to build the Hadoop/Flink shaded jar using the yet to be released Hadoop 3.3 branch's code, so I can then go ahead and security scan that and test it out.

 

Thanks in advance!"
Allow to set s3 object metadata,13361089,Open,Minor,,26/Feb/21 03:38,,,"It's currently impossible to set custom S3 Object Metadata such as `ContentType`, `ContentEncoding`, `ContentDisposition`, and a few others.

Being able to do so would greatly increase the usefulness of S3 storage."
Add kms-default.xml and httpfs-default.xml to site index,13360113,Resolved,Minor,Fixed,22/Feb/21 13:23,23/Feb/21 22:58,,"While there are links to *-default.xml in the site index, kms-default.xml and httpfs-default.xml are not included. Adding them could be useful as a quick reference."
"Update os-maven-plugin to version 1.7.0, to support RISC-V architecture (JDK11)",13358564,Open,Minor,,14/Feb/21 22:27,,3.4.0,
Two tests perform faulty assertEquals calls comparing variable with itself,13358575,Open,Minor,,15/Feb/21 01:27,,,"The following test classes perform faulty {{assertEquals}} calls which erroneously compare a variable with itself.

- {{org.apache.hadoop.hdfs.server.namenode.TestNameNodeReconfigure}}
https://github.com/apache/hadoop/blob/c3134ab3a99d4109d9ae3fcf216018351eb1d36f/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeReconfigure.java#L342
{code}
assertEquals(property + "" has wrong value"", isSPSRunning, isSPSRunning);
{code}
- {{org.apache.hadoop.yarn.server.router.clientrm.TestFederationClientInterceptor}}
https://github.com/apache/hadoop/blob/c3134ab3a99d4109d9ae3fcf216018351eb1d36f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestFederationClientInterceptor.java#L222
{code}
Assert.assertEquals(scIdResult, scIdResult);
{code}

It would be good if you could create a pull request for this because I am not very familiar with the project."
Remove outdated contents from tracing documentation,13356679,Open,Minor,,04/Feb/21 07:10,,,Tracer using HTrace and TraceAdmin were removed by HADOOP-17424.
Minor improvement use isEmpty,13354031,Resolved,Minor,Fixed,22/Jan/21 21:15,22/Jan/21 21:16,,"Use isEmpty instead size() > o.

 

{{size()}} can be *O(1)* or *O(N)*, depending on the {{data structure}}; {{.isEmpty()}} is never *O(N)*."
Optimise abfs incremental listings,13352963,Resolved,Minor,Duplicate,18/Jan/21 04:28,20/Jan/21 13:37,3.4.0,[Uber JIRA|https://issues.apache.org/jira/browse/HADOOP-17474]
S3A ITestPartialRenamesDeletes.testPartialDirDelete[bulk-delete=true] failure,13349223,Resolved,Minor,Fixed,05/Jan/21 15:10,14/Jan/21 13:55,3.4.0,"Failure in {{ITestPartialRenamesDeletes.testPartialDirDelete}}; wrong #of delete requests. 

build options: -Dparallel-tests -DtestsThreadCount=6 -Dscale -Dmarkers=delete -Ds3guard -Ddynamo

The assert fails on a line changes in HADOOP-17271; assumption being, there are some test run states where things happen differently. 
"
ADL Gen1: Fix the test case failures which are failing after the contract test update in hadoop-common,13351055,Resolved,Minor,Fixed,08/Jan/21 10:01,12/Jan/21 14:05,3.4.0,"Fix the following test case failures which are failing after the contract test update in hadoop-common

[ERROR] Failures:
[ERROR] TestAdlContractRenameLive>AbstractContractRenameTest.testRenameFileOverExistingFile:131->Assert.fail:88 expected rename(/test/source-256.txt, /test/dest-512.txt) to be rejected with exception, but got false
[ERROR] TestAdlContractRenameLive.testRenameFileUnderFile:46 Expecting org.apache.hadoop.security.AccessControlException with text Parent path is not a folder. but got : ""void"""
 Fix some spelling errors,13361617,Resolved,Trivial,Fixed,01/Mar/21 12:37,03/Mar/21 02:41,3.4.0,Fix some spelling errors (invaild -> invalid)
Fix the wrong CIDR range example in Proxy User documentation,13365756,Resolved,Trivial,Duplicate,17/Mar/21 08:52,25/May/22 06:23,3.2.2,"The CIDR range example on the Proxy user description page is wrong.

 

In the Configurations section of the Proxy user page, CIDR 10.222.0.0/16 means range 10.222.0.0-15.

 

But It's not true. CIDR format 10.222.0.0/16 means 10.222.0.0-10.222.255.255

 

as-is : 10.222.0.0-15

to-be : 10.222.0.0-10.222.255.255

 "
Fix typo in UnixShellGuide.html,13362205,Resolved,Trivial,Fixed,04/Mar/21 02:48,22/Apr/22 17:01,,"The file name of hadoop-user-functions.sh.examples should be hadoop-user-functions.sh.example in UnixShellGuide.html.

This is reported by [~aref.kh] in HADOOP-17561."
Add build instructions for installing GCC 9 and CMake 3.19,13355723,Resolved,Trivial,Fixed,31/Jan/21 11:41,31/Jan/21 14:41,3.3.0,
typo in MagicCommitTracker,13362770,Resolved,Trivial,Fixed,06/Mar/21 12:07,10/Mar/21 15:59,3.3.0,""" commit metadata for {} parts in {}. sixe: {} byte(s)"" should be  "" commit metadata for {} parts in {}. size: {} byte(s)"""
Simplify dependency installation instructions,13355744,Resolved,Trivial,Fixed,31/Jan/21 15:46,02/Feb/21 05:33,3.3.0,The instructions for installing Protocol buffers and Boost seem to just be copied from Dockerfile and isn't friendly to read.
Fix typo in BUILDING.txt,13355720,Resolved,Trivial,Fixed,31/Jan/21 10:46,31/Jan/21 14:18,3.3.0,
Typo in hadop-aws index.md,13353795,Resolved,Trivial,Fixed,21/Jan/21 16:15,21/Jan/21 17:32,3.4.0,- https://github.com/apache/hadoop/pull/2634/files
prune dependency exports of hadoop-* modules,13569402,Resolved,Blocker,Fixed,21/Feb/24 20:21,03/Mar/24 10:13,3.4.0,"this is probably caused by HADOOP-18613:

ZK is pulling in some extra transitive stuff which surfaces in applications which import hadoop-common into their poms. It doesn't seem to show up in our distro, but downstream you get warnings about duplicate logging stuff

{code}
|  +- org.apache.zookeeper:zookeeper:jar:3.8.3:compile
|  |  +- org.apache.zookeeper:zookeeper-jute:jar:3.8.3:compile
|  |  |  \- (org.apache.yetus:audience-annotations:jar:0.12.0:compile - omitted for duplicate)
|  |  +- org.apache.yetus:audience-annotations:jar:0.12.0:compile
|  |  +- (io.netty:netty-handler:jar:4.1.94.Final:compile - omitted for conflict with 4.1.100.Final)
|  |  +- (io.netty:netty-transport-native-epoll:jar:4.1.94.Final:compile - omitted for conflict with 4.1.100.Final)
|  |  +- (org.slf4j:slf4j-api:jar:1.7.30:compile - omitted for duplicate)
|  |  +- ch.qos.logback:logback-core:jar:1.2.10:compile
|  |  +- ch.qos.logback:logback-classic:jar:1.2.10:compile
|  |  |  +- (ch.qos.logback:logback-core:jar:1.2.10:compile - omitted for duplicate)
|  |  |  \- (org.slf4j:slf4j-api:jar:1.7.32:compile - omitted for conflict with 1.7.30)
|  |  \- (commons-io:commons-io:jar:2.11.0:compile - omitted for conflict with 2.14.0)

{code}

proposed: exclude the zk dependencies we either override outselves or don't need. "
Vectored Read into off-heap buffer broken in fallback implementation,13570728,Resolved,Blocker,Fixed,04/Mar/24 20:45,10/Apr/24 12:43,3.3.6,"{{VectoredReadUtils.readInDirectBuffer()}} always starts off reading at position zero even when the range is at a different offset. As a result: you can get incorrect information.

Thanks for this is straightforward: we pass in a FileRange and use its offset as the starting position.

However, this does mean that all shipping releases 3.3.5-3.4.0 cannot safely read vectorIO into direct buffers through HDFS, ABFS or GCS. Note that we have never seen this in production because the parquet and ORC libraries both read into on-heap storage.

Those libraries needs to be audited to make sure that they never attempt to read into off-heap DirectBuffers. This is a bit trickier than you would think because an allocator is passed in. For PARQUET-2171 we will 
* only invoke the API on streams which explicitly declare their support for the API (so fallback in parquet itself)
* not invoke when direct buffer allocation is in use."
S3 public test bucket landsat-pds unreadable -needs replacement,13566779,Resolved,Critical,Fixed,30/Jan/24 19:59,14/Feb/24 19:35,3.2.4,"The s3 test bucket used in hadoop-aws tests of S3 select and large file reads is no longer publicly accessible

{code}
java.nio.file.AccessDeniedException: landsat-pds: getBucketMetadata() on landsat-pds: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 403, Request ID: 06QNYQ9GND5STQ2S, Extended Request ID: O+u2Y1MrCQuuSYGKRAWHj/5LcDLuaFS8owNuXXWSJ0zFXYfuCaTVLEP351S/umti558eKlUqV6U=):null

{code}

* Because HADOOP-18830 has cut s3 select, all we need in 3.4.1+ is a large file for some reading tests
* changing the default value disables s3 select tests on older releases
* if fs.s3a.scale.test.csvfile is set to "" "" then other tests which need it will be skipped

Proposed
* we locate a new large file under the (requester pays) s3a://usgs-landsat/ bucket . All releases with HADOOP-18168 can use this
* update 3.4.1 source to use this; document it
* do something similar for 3.3.9 + maybe even cut s3 select there too.
* document how to use it on older releases with requester-pays support
* document how to completely disable it on older releases.

h2. How to fix (most) landsat test failures on older releases

add this to your auth-keys.xml file. Expect some failures in a few tests with-hardcoded references to the bucket (assumed role delegation tokens)

{code}

  <property>
    <name>fs.s3a.scale.test.csvfile</name>
    <value>s3a://noaa-cors-pds/raw/2023/017/ohfh/OHFH017d.23_.gz</value>
    <description>file used in scale tests</description>
  </property>

  <property>
    <name>fs.s3a.bucket.noaa-cors-pds.endpoint.region</name>
    <value>us-east-1</value>
  </property>

  <property>
    <name>fs.s3a.bucket.noaa-isd-pds.multipart.purge</name>
    <value>false</value>
    <description>Don't try to purge uploads in the read-only bucket, as
    it will only create log noise.</description>
  </property>

  <property>
    <name>fs.s3a.bucket.noaa-isd-pds.probe</name>
    <value>0</value>
    <description>Let's postpone existence checks to the first IO operation </description>
  </property>

  <property>
    <name>fs.s3a.bucket.noaa-isd-pds.audit.add.referrer.header</name>
    <value>false</value>
    <description>Do not add the referrer header</description>
  </property>

  <property>
    <name>fs.s3a.bucket.noaa-isd-pds.prefetch.block.size</name>
    <value>128k</value>
    <description>Use a small prefetch size so tests fetch multiple blocks</description>
  </property>


  <property>
    <name>fs.s3a.select.enabled</name>
    <value>false</value>
  </property>

{code}

Some delegation token tests will still fail; these have hard-coded references to the old bucket. *Do not worry about these*

{code}

[ERROR]   ITestDelegatedMRJob.testJobSubmissionCollectsTokens[0] » AccessDenied s3a://la...
[ERROR]   ITestDelegatedMRJob.testJobSubmissionCollectsTokens[1] » AccessDenied s3a://la...
[ERROR]   ITestDelegatedMRJob.testJobSubmissionCollectsTokens[2] » AccessDenied s3a://la...
[ERROR]   ITestRoleDelegationInFilesystem>ITestSessionDelegationInFilesystem.testDelegatedFileSystem:347->ITestSessionDelegationInFilesystem.readLandsatMetadata:614 » AccessDenied
[ERROR]   ITestSessionDelegationInFilesystem.testDelegatedFileSystem:347->readLandsatMetadata:614 » AccessDenied

{code}
"
[ABFS] move to jdk11 HttpClient for http2 and connection keep alive,13563299,Open,Critical,,01/Jan/24 23:11,,3.4.0,"As described in Jira Title: ""in hadoop-azure, use jdk11 HttpClient instead of legacy java.net.HttpURLConnection, for supporting http2 and connection keep alive""

Few remarks:

1/ The official Azure SDK supports either OkHttp or Netty for the Http transport.

2/ the actual hadoop-azure use the class java.net.HttpURLConnection, which is slow.
  It does not use Http2, does not optimize SSL Hand-shake very well, and does not keep TCP connection alive for re-use.

3/ JDK since version >=11 have a new class HttpClient which should be a better replacement 

4/ it might be possible to introduce a configuration property (with defaut to use legacy class) , and an abstract factory to create connection via either HttpURLConnection or any other pluggeable implementation (jdk 11 HttpClient, OkHttp, Netty, ...)

5/ the official Azure SDK is maintained by Microsoft, so should better follow bug fixes and improvements than custom hadoop implementation?
[https://learn.microsoft.com/en-us/java/api/overview/azure/storage-file-datalake-readme?view=azure-java-stable
|https://learn.microsoft.com/en-us/java/api/overview/azure/storage-file-datalake-readme?view=azure-java-stable]

6/ when we use code with the official Azure SDK and Hadoop(in Spark), it is chocking to have 2 different implementations within the same JVM... 

7/ The official Azure SDK has more features that what allows the legacy hadoop class FileSystem to do... In particular, we can append (=upload) file by multiple threads (upload by fragments at different offsets), then flush when every fragments are sent.

"
Add support for Tez to MagicS3GuardCommitter,13569885,Open,Major,,26/Feb/24 19:20,,3.3.6,"The MagicS3GuardCommitter assumes that the JobID of the task is the same as that of the job's application master when writing/reading the .pendingset file. This assumption is not valid when running with Tez, which creates slightly different JobIDs for tasks and the application master.

 

While the MagicS3GuardCommitter is intended only for MRv2, it mostly works fine with an MRv1 wrapper with Hive/Pig (with some minor changes to Hive) run in MR mode. This issue only crops up when running queries with the Tez execution engine. I can upload a patch to Hive 3.1 to reproduce this error on EMR if needed.

 

Fixing this will probably require work from both Tez and Hadoop, wanted to start a discussion here so we can figure out how exactly we go about this."
CVE-2024-23454: Apache Hadoop: Temporary File Local Information Disclosure,13564026,Resolved,Major,Fixed,09/Jan/24 10:26,17/Jan/24 07:10,3.4.0,"Apache Hadoop’s RunJar.run() does not set permissions for temporary directory by default. If sensitive data will be present in this file, all the other local users may be able to view the content.

This is because, on unix-like systems, the system temporary directory is shared between all local users. As such, files written in this directory, without setting the correct posix permissions explicitly, may be viewable by all other local users.

Credit: Andrea Cosentino (finder)

See: https://www.cve.org/CVERecord?id=CVE-2024-23454"
Release Hadoop 3.4.1,13569665,Resolved,Major,Fixed,23/Feb/24 16:01,16/Jan/25 22:18,3.4.0,"Release a minor update to hadoop 3.4.0 with

* packaging enhancements
* updated dependencies (where viable)
* fixes for critical issues found after 3.4.0 released
* low-risk feature enhancements (those which don't impact schedule...)"
S3A: Recover from Vector IO read failures,13571318,Open,Major,,08/Mar/24 13:48,,3.3.6,"s3a vector IO doesn't try to recover from read failures the way read() does.

Need to
* abort HTTP stream if considered needed
* retry active read which failed
* but not those which had succeeded

On a full failure we need to do something about any allocated buffer, which means we really need the buffer pool {{ByteBufferPool}} to return or also provide a ""release"" (Bytebuffer -> void) call which does the return.  we would need to
* add this as a new api with the implementations in s3a, local, rawlocal
* classic single allocator method remaps to the new one with (() -> null) as the response"
S3A: detect and recover from SSL ConnectionReset exceptions,13565067,Open,Major,,17/Jan/24 10:35,,3.3.6,"s3a input stream doesn't recover from SSL exceptions, specifically ConnectionReset

This is a variant of HADOOP-19027, except it's surfaced on an older release...
# need to make sure the specific exception is handled by aborting stream and retrying -so map to the new HttpChannelEOFException
# all of thisd needs to be backported


"
"[JDK-17] Fix UT Failures in hadoop common, hdfs, yarn",13566838,Open,Major,,31/Jan/24 05:58,,,"Most of the UT's failed with below exception:
Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.reflect.InaccessibleObjectException: Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int) throws java.lang.ClassFormatError accessible: module java.base does not ""opens java.lang"" to unnamed module @d13f7c [in thread ""Time-limited test""]"
Migrate abstract contract tests to AssertJ,13563623,Open,Major,,04/Jan/24 18:35,,3.4.0,"Replace JUnit4 assertions with equivalent functionality from AssertJ, to make contract tests more independent of JUnit version."
S3A: S3A: ITestS3AConfiguration failing with region problems,13564516,Open,Major,,12/Jan/24 15:26,,3.4.0,"After commented out the default region in my ~/.aws/config [default} profile, test ITestS3AConfiguration. testS3SpecificSignerOverride() fails


{code}
[ERROR] testS3SpecificSignerOverride(org.apache.hadoop.fs.s3a.ITestS3AConfiguration)  Time elapsed: 0.054 s  <<< ERROR!
software.amazon.awssdk.core.exception.SdkClientException: Unable to load region from any of the providers in the chain software.amazon.awssdk.regions.providers.DefaultAwsRegionProviderChain@12c626f8: [software.amazon.awssdk.regions.providers.SystemSettingsRegionProvider@ae63559: Unable to load region from system settings. Region must be specified either via environment variable (AWS_REGION) or  system property (aws.region)., software.amazon.awssdk.regions.providers.AwsProfileRegionProvider@6e6cfd4c: No region provided in profile: default, software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider@139147de: EC2 Metadata is disabled. Unable to retrieve region information from EC2 Metadata service.]

{code}

I'm worried the sdk update has rolled back to the 3.3.x region problems where well-configured developer setups / ec2 deployments hid problems. certainly we can see the code is checking these paths"
S3A : ITestS3AConcurrentOps#testParallelRename intermittent timeout failure,13563526,Open,Major,,04/Jan/24 05:10,,3.4.0,"Need to configure higher timeout for the test.

 
{code:java}
[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 256.281 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.scale.ITestS3AConcurrentOps
[ERROR] testParallelRename(org.apache.hadoop.fs.s3a.scale.ITestS3AConcurrentOps)  Time elapsed: 72.565 s  <<< ERROR!
org.apache.hadoop.fs.s3a.AWSApiCallTimeoutException: Writing Object on fork-0005/test/testParallelRename-source0: software.amazon.awssdk.core.exception.ApiCallTimeoutException: Client execution did not complete before the specified timeout configuration: 15000 millis
	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:215)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)
	at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)
	at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)
	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)
	at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.retry(WriteOperationHelper.java:214)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.putObject(WriteOperationHelper.java:532)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.lambda$putObject$0(S3ABlockOutputStream.java:620)
	at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)
	at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease.run(SemaphoredDelegatingExecutor.java:225)
	at org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease.run(SemaphoredDelegatingExecutor.java:225)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: software.amazon.awssdk.core.exception.ApiCallTimeoutException: Client execution did not complete before the specified timeout configuration: 15000 millis
	at software.amazon.awssdk.core.exception.ApiCallTimeoutException$BuilderImpl.build(ApiCallTimeoutException.java:97)
	at software.amazon.awssdk.core.exception.ApiCallTimeoutException.create(ApiCallTimeoutException.java:38)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.generateApiCallTimeoutException(ApiCallTimeoutTrackingStage.java:151)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.handleInterruptedException(ApiCallTimeoutTrackingStage.java:139)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.translatePipelineException(ApiCallTimeoutTrackingStage.java:107)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)
	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)
	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)
	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)
	at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)
	at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)
	at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)
	at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)
	at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)
	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)
	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)
	at software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:10187)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$putObjectDirect$19(S3AFileSystem.java:3261)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier(IOStatisticsBinding.java:651)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:3258)
	at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$putObject$7(WriteOperationHelper.java:533)
	at org.apache.hadoop.fs.store.audit.AuditingFunctions.lambda$withinAuditSpan$0(AuditingFunctions.java:62)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)
	... 15 more {code}"
S3A HeaderProcessing to process all metadata entries of HEAD response,13571155,Open,Major,,07/Mar/24 14:32,,3.4.0,"S3A HeaderProcessing builds up an incomplete list of headers as its mapping of md to header. entries omits headers including
x-amz-server-side-encryption-aws-kms-key-id

proposed
* review all headers which are stripped from ""raw"" responses and mapped into headers
* make sure result of headers matches v1; looks like etags are different
* make sure x-amz-server-side-encryption-aws-kms-key-id gets back
* plus new checksum values


v1 sdk

{code}

# file: s3a://noaa-cors-pds/raw/2024/001/akse/AKSE001x.24_.gz
header.Content-Length=""524671""
header.Content-Type=""binary/octet-stream""
header.ETag=""3e39531220fbd3747d32cf93a79a7a0c""
header.Last-Modified=""Tue Jan 02 00:15:13 GMT 2024""
header.x-amz-server-side-encryption=""AES256""

{code}

v2 SDK. note how etag is now double quoted.

{code}

# file: s3a://noaa-cors-pds/raw/2024/001/akse/AKSE001x.24_.gz
header.Content-Length=""524671""
header.Content-Type=""binary/octet-stream""
header.ETag=""""3e39531220fbd3747d32cf93a79a7a0c""""
header.Last-Modified=""Tue Jan 02 00:15:13 GMT 2024""
header.x-amz-server-side-encryption=""AES256""

{code}
"
S3A: ITestCustomSigner failing against S3Express Buckets,13565449,Open,Major,,19/Jan/24 14:18,,3.5.0,"getting test failures against S3 Express buckets with {{ItestCustomSigner}}; not seen with classic s3 stores.


{code}
[ERROR] testCustomSignerAndInitializer[simple-delete](org.apache.hadoop.fs.s3a.auth.ITestCustomSigner)  Time elapsed: 6.12 s  <<< ERROR!
org.apache.hadoop.fs.s3a.AWSBadRequestException: PUT 0-byte object  on fork-0006/test/testCustomSignerAndInitializer[simple-delete]/customsignerpath1: software.amazon.awssdk.services.s3.model.S3Exception: x-amz-sdk-checksum-algorithm specified, but no corresponding x-amz-checksum-* or x-amz-trailer headers were found. (Service: S3, Status Code: 400, Request ID: 0033eada6b00018d21962f1b05094a80435cca52, Extended Request ID: kZJZG05LGCBu7lsNKNf):InvalidRequest: x-amz-sdk-checksum-algorithm specified, but no corresponding x-amz-checksum-* or x-amz-trailer headers were found. (Service: S3, Status Code: 400, Request ID: 0033eada6b00018d21962f1b05094a80435cca52, Extended Request ID: kZJZG05LGCBu7lsNKNf)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:259)
...
Caused by: software.amazon.awssdk.services.s3.model.S3Exception: x-amz-sdk-checksum-algorithm specified, but no corresponding x-amz-checksum-* or x-amz-trailer headers were found. (Service: S3, Status Code: 400, Request ID: 0033eada6b00018d21962f1b05094a80435cca52, Extended Request ID: kZJZG05LGCBu7lsNKNf)
        at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)
        at software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)


{code}
"
S3 Express: document use,13571648,Open,Major,,12/Mar/24 18:00,,3.4.0,"The 3.4.0 release doesn't explicitly cover S3 Express.

It's support is automatic
* library handles it
* hadoop shell commands know that there may be ""missing"" dirs in treewalks due to in-flight uploads
* s3afs automatically switches to deleting pending uploads in delete(dir) call.

we just need to provide a summary of features, how to probe etc.

"
S3A: ITestS3AFileContextURI: MultiObjectDeleteException bulk delete of odd filenames,13564206,Open,Major,,10/Jan/24 14:40,,3.4.0,"Possibly transient. note bucket is versioned.

{code}
org.apache.hadoop.fs.s3a.AWSS3IOException: 
Remove S3 Dir Markers on s3a://stevel-london/Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-aws/target/test-dir/7/testContextURI/createTest: org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteException: [S3Error(Key=Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-aws/target/test-dir/7/testContextURI/createTest/()&^%$#@!~_+}{><?/, Code=InternalError, Message=We encountered an internal error. Please try again.)] (Service: Amazon S3, Status Code: 200, Request ID: null):MultiObjectDeleteException: 

{code}
"
[ABFS] Reverting Back Support of setXAttr() and getXAttr() on root path,13569826,Resolved,Major,Fixed,26/Feb/24 10:40,25/Mar/24 14:22,3.4.0,"A while back changes were made to support HDFS.setXAttr() and HDFS.getXAttr() on root path for ABFS Driver.
For these, filesystem level APIs were introduced and used to set/get metadata of container.
Refer to Jira: [HADOOP-18869] ABFS: Fixing Behavior of a File System APIs on root path - ASF JIRA (apache.org)

Ideally, same set of APIs should be used, and root should be treated as a path like any other path.
This change is to avoid calling container APIs for these HDFS calls.

As a result of this these APIs will fail on root path (as earlier) because service does not support get/set of user properties on root path.
This change will also update the documentation to reflect that these operations are not supported on root path."
[ABFS] Enhancing Client-Side Throttling Metrics Updation Logic,13570301,Resolved,Major,Fixed,29/Feb/24 10:05,11/Apr/24 13:12,3.4.1,"ABFS has a client-side throttling mechanism which works on the metrics collected from past requests made. I requests are getting failed due to throttling at server, we update our metrics and client side backoff is calculated based on those metrics.

This PR enhances the logic to decide which requests should be considered to compute client side backoff interval as follows:

For each request made by ABFS driver, we will determine if they should contribute to Client-Side Throttling based on the status code and result:
 # Status code in 2xx range: Successful Operations should contribute.
 # Status code in 3xx range: Redirection Operations should not contribute.
 # Status code in 4xx range: User Errors should not contribute.
 # Status code is 503: Throttling Error should contribute only if they are due to client limits breach as follows:
 ## 503, Ingress Over Account Limit: Should Contribute
 ## 503, Egress Over Account Limit: Should Contribute
 ## 503, TPS Over Account Limit: Should Contribute
 ## 503, Other Server Throttling: Should not Contribute.
 # Status code in 5xx range other than 503: Should not Contribute.
 # IOException and UnknownHostExceptions: Should not Contribute."
ITestExponentialRetryPolicy failing in branch-3.4,13571787,Resolved,Major,Fixed,13/Mar/24 18:15,13/Apr/24 12:22,3.4.0,"{code:java}
[ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 2, Time elapsed: 91.416 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.services.ITestExponentialRetryPolicy
[ERROR] testThrottlingIntercept(org.apache.hadoop.fs.azurebfs.services.ITestExponentialRetryPolicy)  Time elapsed: 0.622 s  <<< ERROR!
Failure to initialize configuration for dummy.dfs.core.windows.net key =""null"": Invalid configuration value detected for fs.azure.account.key
	at org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:53)
	at org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:646)
	at org.apache.hadoop.fs.azurebfs.services.ITestAbfsClient.createTestClientFromCurrentContext(ITestAbfsClient.java:339)
	at org.apache.hadoop.fs.azurebfs.services.ITestExponentialRetryPolicy.testThrottlingIntercept(ITestExponentialRetryPolicy.java:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) {code}"
ABFS: Fixing Test Script Bug and Some Known test Failures in ABFS Test Suite,13573314,Resolved,Major,Fixed,26/Mar/24 09:27,13/Apr/24 12:24,3.4.0,"Test Script used by ABFS to validate changes has following two issues:
 # When there are a lot of test failures or when error message of any failing test becomes very large, the regex used today to filter test results does not work as expected and fails to report all the failing tests.
To resolve this, we have come up with new regex that will only target one line test names for reporting them into aggregated test results.
 # While running the test suite for different combinations of Auth type and account type, we add the combination specific configs first and then include the account specific configs in core-site.xml file. This will override the combination specific configs like auth type if the same config is present in account specific config file. To avoid this, we will first include the account specific configs and then add the combination specific configs.

Due to above bug in test script, some test failures in ABFS were not getting our attention. This PR also targets to resolve them. Following are the tests fixed:
 # ITestAzureBlobFileSystemAppend.testCloseOfDataBlockOnAppendComplete(): It was failing only when append blobs were enabled. In case of append blobs we were not closing the active block on outputstrea,close() due to which block.close() was not getting called and assertions around it were failing. Fixed by updating the production code to close the active block on flush.
 # ITestAzureBlobFileSystemAuthorization: Tests in this class works with an existing remote filesystem instead of creating a new file system instance. For this they require file system configured in account settings using following config: ""fs.contract.test.fs.abfs"". Tests weref ailing with NPE when this config was not present. Updated code to skip thsi test if required config is not present.
 # ITestAbfsClient.testListPathWithValueGreaterThanServerMaximum(): Test was failing Intermittently only for HNS enabled accounts. Test wants to assert that client.listPath() does not return more objects than what is configured in maxListResults. Assertions should be that number of objects returned could be less than expected as server might end up returning even lesser due to partition splits along with a continuation token.
 # ITestGetNameSpaceEnabled.testGetIsNamespaceEnabledWhenConfigIsTrue(): Fail when ""fs.azure.test.namespace.enabled"" config is missing. Ignore the test if config is missing.
 # ITestGetNameSpaceEnabled.testGetIsNamespaceEnabledWhenConfigIsFalse(): Fail when ""fs.azure.test.namespace.enabled"" config is missing. Ignore the test if config is missing.
 # ITestGetNameSpaceEnabled.testNonXNSAccount(): Fail when ""fs.azure.test.namespace.enabled"" config is missing. Ignore the test if config is missing.
 # ITestAbfsStreamStatistics.testAbfsStreamOps: Fails when ""fs.azure.test.appendblob.enabled"" is set to true. Test wanted to assert that number of read operations can be more in case of append blobs as compared to normal blob because of automatic flush. It could be same as that of normal blob as well.
 # ITestAzureBlobFileSystemCheckAccess.testCheckAccessForAccountWithoutNS: Fails for FNS Account only when following config is present:  fs.azure.account.hns.enabled"". Failure is because test wants to assert that when driver does not know if the account is HNS enabled or not it makes a server call and fails. But above config is letting driver know the account type and skipping the head call. Remove these configs from the test specific configurations and not from the account settings file.
 # ITestAbfsTerasort.test_120_terasort: Fails with OAuth on HNS account. Failure is because of identity mismatch. OAuth uses service principle OID as owner of the resources whereas Shared Key uses local system identities. Fix is to set configs that will allow overwrite of OID to localidentity. This will require a new config to be set by user that specify which OID has to be substituted. OAuth by default uses Superuser Identity, so same needs to be configured to be overwritten as well.
*New test config: ""fs.azure.account.oauth2.client.service.principal.object.id""*
 # ITestExponentialRetryPolicy.testThrottlingIntercept: Fails with SharedKey only. Test was using a dummy account to create a new instance of AbfsConfiguration and for that dummy account, SharedKey was not configured. Fix is to Add non-account specific SharedKey in accountconfigs.
 # ITestAzureBlobFileSystemLease:testTwoCreate(): Fail when ""fs.azure.test.namespace.enabled"" config is missing. Fix is to Ignore the test if config is missing.
 # ITestAzureBlobFileSystemChecksum.testAppendWithChecksumAtDifferentOffsets: Test fails with append blob enabled because position parameter in append call was passed wrong. Fixed the test to work with append blobs."
[ABFS] All tests of. ITestAzureBlobFileSystemAuthorization fails with NPE,13571527,Resolved,Major,Fixed,11/Mar/24 19:08,12/Nov/24 11:23,3.4.0,"When below config set to true all of the tests fails else it skips.

<property>

    <name>fs.azure.test.namespace.enabled</name>

    <value>true</value>

</property>

 

[*ERROR*] testOpenFileAuthorized(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAuthorization)  Time elapsed: 0.064 s  <<< ERROR!

java.lang.NullPointerException

 at org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAuthorization.runTest(ITestAzureBlobFileSystemAuthorization.java:273)

 at org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAuthorization.testOpenFileAuthorized(ITestAzureBlobFileSystemAuthorization.java:132)

 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

 at java.lang.reflect.Method.invoke(Method.java:498)

 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)

 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)

 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)"
provide hadoop binary tarball without aws v2 sdk,13569380,Resolved,Major,Fixed,21/Feb/24 17:25,08/Oct/24 17:45,3.4.0,"Have the default hadoop binary .tar.gz exclude the aws v2 sdk by default. 

This SDK brings the total size of the distribution to about 1 GB.

Proposed
* add a profile to include the aws sdk in the dist module
* document it for local building
* for release builds, we modify our release ant builds to generate modified x86 and arm64 releases without the file.



"
Drop support for HBase v1 timeline service & upgrade HBase v2,13571582,Resolved,Major,Fixed,12/Mar/24 09:13,24/Apr/24 10:20,3.4.0,"Drop support for Hbase V1 as the back end of the YARN Application Timeline service, which becomes HBase 2 only.

This does not have any effect on HBase deployments themselves.

Dev List:

[https://lists.apache.org/thread/vb2gh5ljwncbrmqnk0oflb8ftdz64hhs]

https://lists.apache.org/thread/o88hnm7q8n3b4bng81q14vsj3fbhfx5w"
Upgrade hadoop3 docker scripts to 3.4.0,13573800,Resolved,Major,Done,29/Mar/24 06:42,04/Oct/24 17:25,,
[ABFS]: ApacheHttpClient adaptation as network library,13572809,Resolved,Major,Fixed,21/Mar/24 10:49,22/Jul/24 18:06,3.5.0,"Apache HttpClient is more feature-rich and flexible and gives application more granular control over networking parameter.

ABFS currently relies on the JDK-net library. This library is managed by OpenJDK and has no performance problem. However, it limits the application's control over networking, and there are very few APIs and hooks exposed that the application can use to get metrics, choose which and when a connection should be reused. ApacheHttpClient will give important hooks to fetch important metrics and control networking parameters.

A custom implementation of connection-pool is used. The implementation is adapted from the JDK8 connection pooling. Reasons for doing it:
1. PoolingHttpClientConnectionManager heuristic caches all the reusable connections it has created. JDK's implementation only caches limited number of connections. The limit is given by JVM system property ""http.maxConnections"". If there is no system-property, it defaults to 5. Connection-establishment latency increased with all the connections were cached. Hence, adapting the pooling heuristic of JDK netlib,
2. In PoolingHttpClientConnectionManager, it expects the application to provide `setMaxPerRoute` and `setMaxTotal`, which the implementation uses as the total number of connections it can create. For application using ABFS, it is not feasible to provide a value in the initialisation of the connectionManager. JDK's implementation has no cap on the number of connections it can have opened on a moment. Hence, adapting the pooling heuristic of JDK netlib,"
WrappedIO to export modern filesystem/statistics APIs in a reflection friendly form,13573733,Resolved,Major,Fixed,28/Mar/24 16:50,21/Aug/24 13:22,3.3.6,"parquet, avro etc are still stuck building with older hadoop releases. 

This makes using new APIs hard (PARQUET-2171) and means that APIs which are 5 years old such as HADOOP-15229 just aren't picked up.

This lack of openFIle() adoption hurts working with files in cloud storage as
* extra HEAD requests are made
* read policies can't be explicitly set
* split start/end can't be passed down

HADOOP-18679 added a new WrappedIO class.

This jira proposes extending this with
* more of the filesystem/input stream methods
* iOStatistics
* Pull in parquet DynMethods to dynamially wrap and invoke through tests. This class, DynamicWrappedIO is intended to be copied into libraries (parquet, iceberg) for their own use. 
* existing tests to use the dynamic binding for end-to-end testing.

+then get into the downstream libraries and use where appropriate"
update to zookeeper client 3.8.4 due to  CVE-2024-23944,13572300,Resolved,Major,Fixed,18/Mar/24 23:01,25/Mar/24 15:22,3.3.6,https://github.com/advisories/GHSA-r978-9m6m-6gm6
Transitive dependencies with CVEs in Hadoop distro,13567964,Open,Major,,09/Feb/24 05:52,,3.4.0,"Our ongoing security scans are turning up several long-standing CVEs, even in the most recent version of Hadoop, which is making it difficult for us to use Hadoop in our echo system. A comprehensive list of all the long-standing CVEs and the JARs holding them is attached. I'm asking for community assistance to address these high-risk vulnerabilities as soon as possible.

 
|Vulnerability ID|Severity|Package name|Package version|Package type|Package path|Package suggested fix|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2022-25647|High|com.google.code.gson:gson|2.8.5|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/hbase-shaded-gson-3.0.0.jar|v2.8.9|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar|v1.11.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v1.11.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar|v1.11.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2018-10237|Medium|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v32.0.0-android|
|CVE-2023-2976|High|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2022-25647|High|com.google.code.gson:gson|2.8.5|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/hbase-shaded-gson-3.0.0.jar|v2.8.9|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3171|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3509|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2022-3510|High|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar|v1.11.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v1.11.3|
|CVE-2023-39410|High|org.apache.avro:avro|1.9.2|java|/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar|v1.11.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2021-22570|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|2.5.0|java|/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2021-22569|Medium|com.google.protobuf:protobuf-java|3.7.1|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar|v3.16.3|
|CVE-2018-10237|Medium|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|30.1.1-jre|java|/hadoop-3.4.0/share/hadoop/client/hadoop-client-runtime-3.4.0-SNAPSHOT.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|12.0.1|java|/hadoop-3.4.0/share/hadoop/yarn/timelineservice/lib/guava-12.0.1.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar|v32.0.0-android|
|CVE-2020-8908|Low|com.google.guava:guava|27.0-jre|java|/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar|v32.0.0-android|"
"S3A: expand optimisations on stores with ""fs.s3a.performance.flags"" for mkdir",13567906,Resolved,Major,Fixed,08/Feb/24 15:00,24/Aug/24 05:07,3.4.0,"on an s3a store with fs.s3a.create.performance set, speed up other operations

*  mkdir to skip parent directory check: just do a HEAD to see if there's a file at the target location
"
ABFS phase 4: post Hadoop 3.4.0 features,13569990,Open,Major,,27/Feb/24 11:08,,3.4.0,"Uber-JIRA for ABFS work so we can close HADOOP-18072 as done for 3.4.0

Assuming 3.4.1 is a rapid roll of packing, dependencies and critical fixes, this should target 3.4.2 and beyond"
Use StringBuilder instead of StringBuffer,13573976,Resolved,Major,Fixed,30/Mar/24 15:57,18/Aug/24 15:59,,"StringBuilder is basically the same as StringBuffer but doesn't use synchronized. String appending rarely needs locking like this.

There are some public and package private APIs that use StringBuffers as input or return types - I have left these alone for compatibility reasons."
Vector IO: consistent specified rejection of overlapping ranges,13570494,Resolved,Major,Fixed,01/Mar/24 14:54,10/Apr/24 12:43,3.3.6,"Related to PARQUET-2171 q: ""how do you deal with overlapping ranges?""

I believe s3a rejects this, but the other impls may not.

Proposed

FS spec to say 
* ""overlap triggers IllegalArgumentException"". 
* special case: 0 byte ranges may be short circuited to return empty buffer even without checking file length etc.

Contract tests to validate this

(+ common helper code to do this).

I'll copy the validation stuff into the parquet PR for consistency with older releases"
add shaded jersey jar hadoop-shaded-jersey,13568259,In Progress,Major,,13/Feb/24 06:09,,thirdparty-1.3.0,"We need to address the issue of upgrading Jersey, replacing Jersey 1.x with Jersey 2.x. Jersey 2 and Jersey 1 are completely different, with many differences, and the same project cannot simultaneously introduce Jersey 2 and Jersey 1.
In order to allow the project to reference both Jersey 2 and Jersey 1 simultaneously, facilitating the step-by-step module upgrade process, I plan to introduce Jersey 2 into hadoop-thirdparty."
Update maven-surefire-plugin from 3.0.0 to 3.2.5	,13567825,In Progress,Major,,08/Feb/24 02:45,,3.4.0,
Use bouncycastle jdk18 1.77,13563606,Resolved,Major,Fixed,04/Jan/24 16:04,30/Mar/24 14:35,,"They have stopped patching the JDK 1.5 jars that Hadoop uses (see https://issues.apache.org/jira/browse/HADOOP-18540).

The new artifacts have similar names - but the names are like bcprov-jdk18on as opposed to bcprov-jdk15on.

CVE-2023-33201 is an example of a security issue that seems only to be fixed in the JDK 1.8 artifacts (ie no JDK 1.5 jar has the fix).

https://www.bouncycastle.org/releasenotes.html#r1rv77 latest current release but the CVE was fixed in 1.74."
upgrade to commons-compress 1.26.1 due to cves,13572297,Resolved,Major,Fixed,18/Mar/24 22:48,07/Jun/24 13:17,3.4.0,"2 recent CVEs fixed - https://mvnrepository.com/artifact/org.apache.commons/commons-compress


Important: Denial of Service CVE-2024-25710
Moderate: Denial of Service CVE-2024-26308

"
Improve rate limiting through ABFS in Manifest Committer,13570005,Open,Major,,27/Feb/24 12:49,,3.4.0,"I need a load test to verify that the rename resilience of the manifest committer actually works as intended

* test suite with name ILoadTest* prefix (as with s3)
* parallel test running with many threads doing many renames
* verify that rename recovery should be detected
* and that all renames MUST NOT fail.

maybe also: metrics for this in fs and doc update. 
Possibly; LogExactlyOnce to warn of load issues"
WASB: Fix connection leak in FolderRenamePending,13567719,Resolved,Major,Fixed,07/Feb/24 10:05,15/May/24 13:39,3.3.6,Fix connection leak in FolderRenamePending in getting bytes  
Migrate abstract permission tests to AssertJ,13563758,Open,Major,,06/Jan/24 02:56,,,
Standardize Maven Initialization Across Operating Systems,13564488,In Progress,Major,,12/Jan/24 11:18,,3.4.0,"The differences in initializing Maven for various operating systems in the build scripts are as follows:

- For Ubuntu and Debian, Maven is installed using the yum repository.
- For CentOS 7 and CentOS 8, Maven is downloaded remotely.
- The Maven version used on Windows is inconsistent with other operating systems."
Compatibility Benchmark over HCFS Implementations,13563311,Resolved,Major,Fixed,02/Jan/24 04:18,17/Mar/24 10:43,3.4.0,"{*}Background：{*}Hadoop-Compatible File System (HCFS) is a core conception in big data storage ecosystem, providing unified interfaces and generally clear semantics, and has become the de-factor standard for industry storage systems to follow and conform with. There have been a series of HCFS implementations in Hadoop, such as S3AFileSystem for Amazon's S3 Object Store, WASB for Microsoft's Azure Blob Storage and OSS connector for Alibaba Cloud Object Storage, and more from storage service's providers on their own.

{*}Problems：{*}However, as indicated by introduction.md, there is no formal suite to do compatibility assessment of a file system for all such HCFS implementations. Thus, whether the functionality is well accomplished and meets the core compatible expectations mainly relies on service provider's own report. Meanwhile, Hadoop is also developing and new features are continuously contributing to HCFS interfaces for existing implementations to follow and update, in which case, Hadoop also needs a tool to quickly assess if these features are supported or not for a specific HCFS implementation. Besides, the known hadoop command line tool or hdfs shell is used to directly interact with a HCFS storage system, where most commands correspond to specific HCFS interfaces and work well. Still, there are cases that are complicated and may not work, like expunge command. To check such commands for an HCFS, we also need an approach to figure them out.

{*}Proposal：{*}Accordingly, we propose to define a formal HCFS compatibility benchmark and provide corresponding tool to do the compatibility assessment for an HCFS storage system. The benchmark and tool should consider both HCFS interfaces and hdfs shell commands. Different scenarios require different kinds of compatibilities. For such consideration, we could define different suites in the benchmark.

*Benefits:* We intend the benchmark and tool to be useful for both storage providers and storage users. For end users, it can be used to evalute the compatibility level and determine if the storage system in question is suitable for the required scenarios. For storage providers, it helps to quickly generate an objective and reliable report about core functioins of the storage service. As an instance, if the HCFS got a 100% on a suite named 'tpcds', it is demonstrated that all functions needed by a tpcds program have been well achieved. It is also a guide indicating how storage service abilities can map to HCFS interfaces, such as storage class on S3.

Any thoughts? Comments and feedback are mostly welcomed. Thanks in advance."
[ABFS]: FooterReadBufferSize should not be greater than readBufferSize,13570965,Resolved,Major,Fixed,06/Mar/24 10:37,23/Apr/24 11:50,3.4.0,"The method `optimisedRead` creates a buffer array of size `readBufferSize`. If footerReadBufferSize is greater than readBufferSize, abfs will attempt to read more data than the buffer array can hold, which causes an exception.

Change: To avoid this, we will keep footerBufferSize = min(readBufferSizeConfig, footerBufferSizeConfig)

 

 "
FTPFileSystem rename with full qualified path broken,13573445,Resolved,Major,Fixed,27/Mar/24 02:00,17/Apr/24 17:43,0.20.2,"   When use fs shell to put/rename file in ftp server with full qualified path , it always get ""Input/output error""(eg. [ftp://user:password@localhost/pathxxx]), the reason is that changeWorkingDirectory command underneath is being passed a string with [file://|file:///] uri prefix which will not be understand by ftp server

!image-2024-03-27-09-59-12-381.png|width=948,height=156!

 

in our case, after client.changeWorkingDirectory(""ftp://mytest:mytest@10.5.xx.xx/files"")

executed, the workingDirectory of ftp server is still ""/"", which is incorrect(not understand by ftp server)

!image-2024-03-28-09-58-19-721.png|width=745,height=431!

the solution should be pass absoluteSrc.getParent().toUri().getPath().toString to avoid
[file://|file:///] uri prefix, like this: 
{code:java}
--- a/FTPFileSystem.java
+++ b/FTPFileSystem.java
@@ -549,15 +549,15 @@ public class FTPFileSystem extends FileSystem {
       throw new IOException(""Destination path "" + dst
           + "" already exist, cannot rename!"");
     }
-    String parentSrc = absoluteSrc.getParent().toUri().toString();
-    String parentDst = absoluteDst.getParent().toUri().toString();
+    URI parentSrc = absoluteSrc.getParent().toUri();
+    URI parentDst = absoluteDst.getParent().toUri();
     String from = src.getName();
     String to = dst.getName();
-    if (!parentSrc.equals(parentDst)) {
+    if (!parentSrc.toString().equals(parentDst.toString())) {
       throw new IOException(""Cannot rename parent(source): "" + parentSrc
           + "", parent(destination):  "" + parentDst);
     }
-    client.changeWorkingDirectory(parentSrc);
+    client.changeWorkingDirectory(parentSrc.getPath().toString());
     boolean renamed = client.rename(from, to);
     return renamed;
   }{code}
already related issue  as follows 
https://issues.apache.org/jira/browse/HADOOP-8653

I create this issue and add related unit test.

 

 "
RBF: The latest STANDBY and UNAVAILABLE nn should be the lowest priority.,13572890,Resolved,Major,Abandoned,22/Mar/24 03:39,22/Mar/24 03:39,,
Update org.ehcache from 3.3.1 to 3.8.2.,13573062,Resolved,Major,Fixed,23/Mar/24 14:34,29/Mar/24 01:56,3.4.1,We need to enhance the caching functionality in Yarn Federation by adding a limit on the number of cached entries. I noticed that the version of org.ehcache is relatively old and requires an upgrade.
S3A: Update AWS SDK V2 to 2.24.6,13569224,Resolved,Major,Fixed,20/Feb/24 15:42,05/Mar/24 10:17,3.4.0,Update the AWS SDK to 2.24.6 from 2.23.5 for latest updates in packaging w.r.t. imds module.
move ssh/sftp code out of hadoop-common into a dedicated jar,13568790,Open,Major,,16/Feb/24 15:49,,3.3.6,"We could call it hadoop-ssh-common. This code is only used in 1 or 2 other places and it means that hadoop-common (which is used in a lot of places) leaks dependencies on ssh-core and jsch jars to many places.

See [~steve_l] comments in HADOOP-19076

"
HttpExceptionUtils to check that loaded class is really an exception before instantiation,13568670,Resolved,Major,Fixed,16/Feb/24 02:11,11/Apr/24 18:47,,"It can be dangerous taking class names as inputs from HTTP messages even if we control the source. Issue is in HttpExceptionUtils in hadoop-common (validateResponse method).

I can provide a PR that will highlight the issue."
reduce use of javax.ws.rs.core.MediaType,13568636,Open,Major,,15/Feb/24 18:48,,,makes it easier to support jersey 1 and 2
Update commons-configuration2 to 2.10.1 due to CVE,13573054,Resolved,Major,Fixed,23/Mar/24 12:53,02/Apr/24 19:56,,https://github.com/advisories/GHSA-9w38-p64v-xpmv
upgrade to nimbus-jose-jwt 9.37.2 due to CVE,13572298,Resolved,Major,Fixed,18/Mar/24 22:56,02/Apr/24 17:38,3.4.0,https://github.com/advisories/GHSA-gvpg-vgmx-xg6w
Remove use of javax.ws.rs.core.HttpHeaders,13568598,Resolved,Major,Fixed,15/Feb/24 14:28,01/Apr/24 07:15,,"One step towards removing Hadoop's dependence on Jersey1 and jsr311-api.

We have other classes where we can get HTTP header names."
further use of StandardCharsets,13564641,Resolved,Major,Fixed,14/Jan/24 22:39,29/Mar/24 03:20,,builds on HADOOP-18957
Exclude some files from Apache RAT check,13573170,Resolved,Major,Fixed,25/Mar/24 10:15,25/Mar/24 16:15,3.5.0,"The following files cause the Apache RAT check to fail on Windows and must be excluded from the RAT check -
 # src/main/winutils/winutils.sln - Visual Studio solution file

 # src/test/resources/lz4/sequencefile - Binary file"
Update Protocol Buffers installation to 3.23.4,13569866,Resolved,Major,Fixed,26/Feb/24 16:21,07/Mar/24 09:39,3.3.9,"We are seeing issues with Java 8 usage of protobuf-java

See https://issues.apache.org/jira/browse/HADOOP-18197 and comments about

java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer;"
Support InMemory Tracking Of S3A Magic Commits,13565408,Resolved,Major,Fixed,19/Jan/24 10:18,26/Mar/24 17:31,,"The following are the operations which happens within a Task when it uses S3A Magic Committer. 

*During closing of stream*

1. A 0-byte file with a same name of the original file is uploaded to S3 using PUT operation. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicCommitTracker.java#L152] for more information. This is done so that the downstream application like Spark could get the size of the file which is being written.

2. MultiPartUpload(MPU) metadata is uploaded to S3. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicCommitTracker.java#L176] for more information.

*During TaskCommit*

1. All the MPU metadata which the task wrote to S3 (There will be 'x' number of metadata file in S3 if a single task writes to 'x' files) are read and rewritten to S3 as a single metadata file. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicS3GuardCommitter.java#L201] for more information


Since these operations happens with the Task JVM, We could optimize as well as save cost by storing these information in memory when Task memory usage is not a constraint. Hence the proposal here is to introduce a new MagicCommit Tracker called ""InMemoryMagicCommitTracker"" which will store the 

1. Metadata of MPU in memory till the Task is committed
2. Store the size of the file which can be used by the downstream application to get the file size before it is committed/visible to the output path.

This optimization will save 2 PUT S3 calls, 1 LIST S3 call, and 1 GET S3 call given a Task writes only 1 file.

"
upgrade to jersey-json 1.22.0,13569744,Resolved,Major,Fixed,24/Feb/24 20:09,26/Mar/24 05:21,3.3.6,Tidies up support for Jettison and Jackson versions used by Hadoop
Do not run unit tests on Windows pre-commit CI,13573173,Resolved,Major,Fixed,25/Mar/24 10:31,25/Mar/24 16:17,3.5.0,"We currently have a good number of unit tests failing on Windows. The author of a patch won't be able to determine if it really was his/her PR that caused the failure or if it was failing historically.

Thus, we need to skip running the unit tests on Windows until we have fixed the ones that are failing currently.

This helps us to proceed with enabling the pre-commit that watches out for any regression against building Hadoop on Windows.

Please refer to this thread for more context - https://github.com/apache/hadoop/pull/5820#issuecomment-1871957975."
Setup pre-commit CI for Windows,13573172,Resolved,Major,Duplicate,25/Mar/24 10:25,25/Mar/24 10:27,3.5.0,"We need to setup a multi-branch pre-commit CI on Jenkins for validation of the PRs against Windows, prior to merging."
Hadoop use Shell command to get the count of the hard link which takes a lot of time,13566270,Resolved,Major,Fixed,26/Jan/24 09:27,24/Mar/24 02:15,3.4.0,"Using Hadoop 3.3.4

 

When the QPS of `append` executions is very high, at a rate of above 10000/s. 

 

We found that the write speed in hadoop is very slow. We traced some datanodes' log and find that there is a warning :
{code:java}
2024-01-26 11:09:44,292 WARN impl.FsDatasetImpl (InstrumentedLock.java:logwaitWarning(165)) Waited above threshold(300 ms) to acquire lock: lock identifier: FsDatasetRwlock waitTimeMs=336 ms.Suppressed 0 lock wait warnings.Longest supressed waitTimeMs=0.The stack trace is
java.lang.Thread,getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1060)
org.apache.hadoop.util.Instrumentedlock.logWaitWarning(InstrumentedLock.java:171)
org.apache.hadoop.util.InstrumentedLock.check(InstrumentedLock.java:222)
org.apache.hadoop.util.InstrumentedLock.lock(InstrumentedLock, iaya:105)
org.apache.hadoop.util.AutocloseableLock.acquire(AutocloseableLock.java:67)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:1239)
org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:230)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver (DataXceiver.java:1313)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock (DataXceiver.java:764)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:176)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:293)
java.lang.Thread.run(Thread.java:748)
{code}
 

Then we traced the method _org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl. java:1239),_ and print how long each command take to finish the execution, and find that it takes us 700ms to get the linkCount of the file which is really slow.

!debuglog.png!

 

We traced the code and  find that java1.8 use a Shell Command to get the linkCount, in which execution it will start a new Process and wait for the Process to fork, when the QPS is very high, it will sometimes take a long time to fork the process.

Here is the shell command.
{code:java}
stat -c%h /path/to/file
{code}
 

Solution:

For the FileStore that supports the file attributes ""unix"", we can use the method _Files.getAttribute(f.toPath(), ""unix:nlink"")_ to get the linkCount, this method doesn't need to start a new process, and will return the result in a very short time.

 

When we use this method to get the file linkCount, we rarely get the WARN log above when the QPS of append execution is high.

.

 "
KeyShell fails with NPE when KMS throws Exception with null as message,13572389,Open,Major,,19/Mar/24 10:27,,3.3.6,"There is an issue in specific Ranger versions (where RANGER-3989 is not fixed) which throws Exception in case of concurrent access to a HashMap with Message {*}null{*}.
{noformat}
java.util.ConcurrentModificationException: null
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1469)
        at java.util.HashMap$EntryIterator.next(HashMap.java:1503)
        at java.util.HashMap$EntryIterator.next(HashMap.java:1501) {noformat}
This manifests in Hadoop's KeyShell as an Exception with message {*}null{*}.

So when
{code:java}
  private String prettifyException(Exception e) {
    return e.getClass().getSimpleName() + "": "" +
        e.getLocalizedMessage().split(""\n"")[0];
  } {code}
tries to print out the Exception the user experiences NPE
{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.KeyShell.prettifyException(KeyShell.java:541)
	at org.apache.hadoop.crypto.key.KeyShell.printException(KeyShell.java:536)
	at org.apache.hadoop.tools.CommandShell.run(CommandShell.java:79)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)
	at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:553) {noformat}
This is an unwanted behaviour because the user does not have any feedback what and where went wrong.

 

My suggestion is to add *null checking* into the affected *prettifyException* method.

I'll create the Github PR soon."
Hadoop 3.4.0 release wrap-up,13572138,Resolved,Major,Fixed,17/Mar/24 07:45,19/Mar/24 12:51,3.4.0,
3.4.0 release documents,13572325,Resolved,Major,Fixed,19/Mar/24 05:25,19/Mar/24 12:51,3.4.0,
Fix Spotbugs warnings in the build,13570627,Open,Major,,04/Mar/24 07:22,,,"We are getting spotbugs warnings in every PR.

[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-common-project-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-httpfs-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-hdfs-project-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-services_hadoop-yarn-services-core-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1517/artifact/out/branch-spotbugs-hadoop-yarn-project-warnings.html]


[https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/1517/artifact/out/branch-spotbugs-root-warnings.html]

 

Source: https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-linux-x86_64/1532/console"
Use hadoop-thirdparty 1.2.0,13567682,Resolved,Major,Fixed,07/Feb/24 05:11,08/Feb/24 11:23,3.4.0,
AWS SDK V2 - Enabling FIPS should be allowed with central endpoint,13567357,Resolved,Major,Fixed,05/Feb/24 06:08,13/Mar/24 13:31,3.4.1,"FIPS support can be enabled by setting ""fs.s3a.endpoint.fips"". Since the SDK considers overriding endpoint and enabling fips as mutually exclusive, we fail fast if fs.s3a.endpoint is set with fips support (details on HADOOP-18975).

Now, we no longer override SDK endpoint for central endpoint since we enable cross region access (details on HADOOP-19044) but we would still fail fast if endpoint is central and fips is enabled.

Changes proposed:
 * S3A to fail fast only if FIPS is enabled and non-central endpoint is configured.
 * Tests to ensure S3 bucket is accessible with default region us-east-2 with cross region access (expected with central endpoint).
 * Document FIPS support with central endpoint on connecting.html.

h3. Note: there are two patches here on trunk; they've been coalesced into one on branch-3.4. "
Bind abstract contract tests into JUnit5 lifecycle,13563733,Open,Major,,05/Jan/24 18:31,,,"I plan to add JUnit5 lifecycle annotations while keeping the existing JUnit4 ones, too.  This would allow downstream contract tests to be implemented in / migrated to JUnit5 gradually, without breaking other implementations."
S3A: disable checksums when fs.s3a.checksum.validation = false,13564224,Resolved,Major,Fixed,10/Jan/24 17:18,19/Jan/24 14:09,3.4.0,"AWS v2 sdk turns on client-side checksum validation; this kills performance

Given we are using TLS to download from AWS s3, there's implicit channel checksumming going on on, that's along with the IPv4 TCP checksumming.

We don't need it, all it does is slow us down.

proposed: disable in DefaultS3ClientFactory

I don't want to add an option to enable it as it only complicates life (yet another config option), but I am open to persuasion
"
Add Protobuf Compatibility Notes,13570574,Resolved,Major,Fixed,03/Mar/24 10:52,04/Mar/24 05:21,3.4.0,"In HADOOP-18197, we upgraded the Protobuf in hadoop-thirdparty to version 3.21.12. This version may have compatibility issues with certain versions of JDK8. We will document this situation in the index.md file of hadoop-3.4.0 and inform users that we will discontinue support for JDK8 in the future."
Update Protocol Buffers installation to 3.21.12 ,13567266,Resolved,Major,Fixed,03/Feb/24 08:55,22/Feb/24 17:10,3.4.0,"
Update docs and docker script to cover downloading the 3.21.12 protobuf compiler"
Release Hadoop-Thirdparty 1.2.0,13566424,Resolved,Major,Fixed,28/Jan/24 09:39,07/Feb/24 07:36,thirdparty-1.2.0,"Release hadoop-thirdparty 1.2.0 JAR

This contains
* "
move jersey code in hadoop-common jar to a new hadoop-jersey1-common jar,13568584,Open,Major,,15/Feb/24 12:50,,,"Hadoop's Jersey dependencies are causing us real trouble.

I'm wondering if it would be a good idea to take the jersey and javax.ws code out of hadoop-common and move it into a dedicated hadoop-jersey1-common jar. We could later create a hadoop-jersey2-common. 

hadoop-jersey1-common and hadoop-jersey2-common would have equivalent classes - just depend on different versions of Jersey.

Example code:
* https://github.com/apache/hadoop/blob/12498b35bbb754225b0b2ca90d5ad4f5cf628d56/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java#L1030
* https://github.com/apache/hadoop/blob/12498b35bbb754225b0b2ca90d5ad4f5cf628d56/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HttpExceptionUtils.java#L89

Hadoop modules that need access to the common jersey code could start with depending on hadoop-jersey1-common but later be refactored to use hadoop-jersey2-common. We could do this on a module by module basis (one a time).

hadoop-common jar would have its jersey and jsr311-api dependencies removed.

Wdyt [~slfan1989], [~steve_l], [~ayushsaxena] ?"
Java 17 compile support,13567824,Open,Major,,08/Feb/24 02:44,,,"We plan to make Hadoop compatible with Java 17. There is a lot of work to be done, but we still aim to complete this task by 2024."
S3A: S3AInputStream doesn't recover from HTTP/channel exceptions,13563732,Resolved,Major,Fixed,05/Jan/24 17:57,16/Jan/24 14:14,3.4.0,"

S3AInputStream doesn't seem to recover from Http exceptions raised through HttpClient or through OpenSSL.

* review the recovery code to make sure it is retrying enough, it looks suspiciously like it doesn't
* detect the relevant openssl, shaded httpclient and unshaded httpclient exceptions, map to a standard one and treat as comms error in our retry policy

This is not the same as the load balancer/proxy returning 443/444 which we map to AWSNoResponseException. We can't reuse that as it expects to be created from an {{software.amazon.awssdk.awscore.exception.AwsServiceException}} exception with the relevant fields...changing it could potentially be incompatible.
"
S3A: update AWS sdk versions to 2.23.5 and 1.12.599,13565300,Resolved,Major,Fixed,18/Jan/24 18:37,24/Jan/24 16:45,3.4.0,"Move up to the most recent versions of the v2 sdk, with a v1 update just to keep some CVE checking happy.


{code}
    <aws-java-sdk.version>1.12.599</aws-java-sdk.version>
    <aws-java-sdk-v2.version>2.23.5</aws-java-sdk-v2.version>

{code}

The v1 SDK is only used for testing...it is not bundled or declared as a dependency of the hadoop-aws module
"
HADOOP-19045. S3A: CreateSession Timeout after 10 seconds,13565297,Resolved,Major,Fixed,18/Jan/24 18:30,07/Feb/24 15:23,3.4.0,"s3a client timeout settings are getting down to http client, but not sdk timeouts, so you can't have a longer timeout than the default. This surfaces in the inability to tune the timeouts for CreateSession calls even now the latest SDK does pick it up

The default value of {{fs.s3a.connection.request.timeout}} is now 60s; if an S3 store takes longer than this to return then the operation will be reported by the SDK as a timeout.

When cherrypicking there are two patches
* change constants.java and add test for passdown
* remove core-default and test/core-site values of 0 and update docs. Without this (or a config override) the 10s timeout is maintained"
AWS SDK V2 - Update S3A region logic ,13565236,Resolved,Major,Fixed,18/Jan/24 11:10,02/Feb/24 17:07,3.4.0,"If both fs.s3a.endpoint & fs.s3a.endpoint.region are empty, Spark will set fs.s3a.endpoint to 

s3.amazonaws.com here:

[https://github.com/apache/spark/blob/9a2f39318e3af8b3817dc5e4baf52e548d82063c/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala#L540] 

 

HADOOP-18908, updated the region logic such that if fs.s3a.endpoint.region is set, or if a region can be parsed from fs.s3a.endpoint (which will happen in this case, region will be US_EAST_1), cross region access is not enabled. This will cause 400 errors if the bucket is not in US_EAST_1. 

 

Proposed: Updated the logic so that if the endpoint is the global s3.amazonaws.com , cross region access is enabled.  

 

 "
Class loader leak caused by StatisticsDataReferenceCleaner thread,13565803,Resolved,Major,Fixed,23/Jan/24 11:58,03/Feb/24 14:56,3.3.6,"The ""org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner"" daemon thread was created by FileSystem. 

This is fine if the thread's context class loader is the system class loader, but it's bad if the context class loader is a custom class loader. The reference held by this daemon thread means that the class loader can never become eligible for GC."
Hadoop 3.4.0 Highlight big features and improvements.,13564583,Resolved,Major,Fixed,13/Jan/24 14:26,25/Jan/24 07:42,3.4.0,"While preparing for the release of Hadoop-3.4.0, I've noticed the inclusion of numerous commits in this version.  Therefore, highlighting significant features and improvements becomes crucial.  I've completed the initial version and now seek the review of more experienced partner to ensure the finalization of the version's highlights."
[thirdparty] Fix the docker image setuptools-scm && lazy-object-proxy,13567125,Resolved,Major,Fixed,02/Feb/24 05:09,04/Feb/24 00:43,thirdparty-1.3.0,setuptools-scm and lazy-object-proxy have been upgraded many times. Directly relying on pip to install dependencies will report errors. We need to specify the installation version.
Improve create-release RUN script,13567122,Resolved,Major,Fixed,02/Feb/24 04:51,04/Feb/24 00:43,thirdparty-1.3.0,"Using create-release will create a docker image locally, but three of the RUN scripts may fail to execute.

1. RUN groupadd --non-unique -g 0 root
{code:java}
=> ERROR [16/20] RUN groupadd --non-unique -g 0 root                                                            0.2s
------
 > [16/20] RUN groupadd --non-unique -g 0 root:
0.154 groupadd: group 'root' already exists
------
Dockerfile:100
--------------------
  98 |
  99 |     LABEL org.apache.hadoop.create-release=""cr-19697""
 100 | >>> RUN groupadd --non-unique -g 0 root
 101 |     RUN useradd -g 0 -u 0 -m root
 102 |     RUN chown -R root /home/root
{code}
2. RUN useradd -g 0 -u 0 -m root
{code:java}
 > [17/20] RUN useradd -g 0 -u 0 -m root:
0.165 useradd: user 'root' already exists
------
Dockerfile:101
--------------------
  99 |     LABEL org.apache.hadoop.create-release=""cr-12068""
 100 |     RUN groupadd --non-unique -g 0 root; exit 0;
 101 | >>> RUN useradd -g 0 -u 0 -m root
 102 |     RUN chown -R root /home/root
 103 |     ENV HOME /home/root
{code}
3. RUN chown -R root /home/root
{code:java}
 > [18/20] RUN chown -R root /home/root:
0.168 chown: cannot access '/home/root': No such file or directory
------
Dockerfile:102
--------------------
 100 |     RUN groupadd --non-unique -g 0 root; exit 0;
 101 |     RUN useradd -g 0 -u 0 -m root; exit 0;
 102 | >>> RUN chown -R root /home/root
 103 |     ENV HOME /home/root
 104 |     RUN mkdir -p /maven
--------------------
{code}
Even if these three scripts fail, subsequent steps can continue to be executed, so I added exit 0 after the script."
[thirdparty]  add -mvnargs option to create-release command line,13567126,Resolved,Major,Fixed,02/Feb/24 05:19,04/Feb/24 00:42,thirdparty-1.3.0,"We need mvn to support extended parameters. This JIRA is similar to HADOOP-18198.

I use this parameter to refer to the user.home."
Capture exception in rpcRequestSender.start() in IPC.Connection.run(),13567087,Resolved,Major,Fixed,01/Feb/24 20:54,03/Feb/24 00:53,3.5.0,"rpcRequestThread.start() can fail due to OOM. This will immediately crash the Connection thread, without removing itself from the connections pool. Then for all following getConnection(remoteid), we will get this bad connection object and all rpc requests will be hanging, because this is a bad connection object, without threads being properly running (Neither Connection or Connection.rpcRequestSender thread is running due to OOM.).

In this PR, we moved the rpcRequestThread.start() to be within the try{}-catch{} block, to capture OOM from rpcRequestThread.start() and proper cleaning is followed if we hit OOM.

{code:java}
IPC.Connection.run()

  @Override
    public void run() {
      // Don't start the ipc parameter sending thread until we start this
      // thread, because the shutdown logic only gets triggered if this
      // thread is started.
      rpcRequestThread.start();
      if (LOG.isDebugEnabled())
        LOG.debug(getName() + "": starting, having connections "" 
            + connections.size());      

      try {
        while (waitForWork()) {//wait here for work - read or close connection
          receiveRpcResponse();
        }
      } catch (Throwable t) {
        // This truly is unexpected, since we catch IOException in receiveResponse
        // -- this is only to be really sure that we don't leave a client hanging
        // forever.
        LOG.warn(""Unexpected error reading responses on connection "" + this, t);
        markClosed(new IOException(""Error reading responses"", t));
      }{code}

Because there is no rpcRequestSender thread consuming the rpcRequestQueue, all rpc request enqueue operations for this connection will be blocked and will be hanging at this while loop forever during sendRpcRequest().
{code:java}
while (!shouldCloseConnection.get()) {
  if (rpcRequestQueue.offer(Pair.of(call, buf), 1, TimeUnit.SECONDS)) {
    break;
  }
}{code}

OOM exception in starting the rpcRequestSender thread.
{code:java}
Exception in thread ""IPC Client (1664093259) connection to nn01.grid.linkedin.com/IP-Address:portNum from kafkaetl"" java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1034)
{code}

Multiple threads blocked by queue.offer(). and we don't found any ""IPC Client"" or ""IPC Parameter Sending Thread"" in thread dump. 
{code:java}
Thread 2156123: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.SynchronousQueue$TransferQueue.awaitFulfill(java.util.concurrent.SynchronousQueue$TransferQueue$QNode, java.lang.Object, boolean, long) @bci=156, line=764 (Compiled frame)
 - java.util.concurrent.SynchronousQueue$TransferQueue.transfer(java.lang.Object, boolean, long) @bci=148, line=695 (Compiled frame)
 - java.util.concurrent.SynchronousQueue.offer(java.lang.Object, long, java.util.concurrent.TimeUnit) @bci=24, line=895 (Compiled frame)
 - org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(org.apache.hadoop.ipc.Client$Call) @bci=88, line=1134 (Compiled frame)
 - org.apache.hadoop.ipc.Client.call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean, org.apache.hadoop.ipc.AlignmentContext) @bci=36, line=1402 (Interpreted frame)
 - org.apache.hadoop.ipc.Client.call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean, org.apache.hadoop.ipc.AlignmentContext) @bci=9, line=1349 (Compiled frame)
 - org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) @bci=248, line=230 (Compiled frame)
 - org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) @bci=4, line=118 (Compiled frame)
 - com.sun.proxy.$Proxy11.getBlockLocations({code}"
Improve create-release RUN script,13564581,Resolved,Major,Fixed,13/Jan/24 13:04,18/Jan/24 11:13,3.4.0,"Using create-release will create a docker image locally, but three of the RUN scripts may fail to execute.

1. RUN groupadd --non-unique -g 0 root


{code:java}
=> ERROR [16/20] RUN groupadd --non-unique -g 0 root                                                            0.2s
------
 > [16/20] RUN groupadd --non-unique -g 0 root:
0.154 groupadd: group 'root' already exists
------
Dockerfile:100
--------------------
  98 |
  99 |     LABEL org.apache.hadoop.create-release=""cr-19697""
 100 | >>> RUN groupadd --non-unique -g 0 root
 101 |     RUN useradd -g 0 -u 0 -m root
 102 |     RUN chown -R root /home/root
{code}


2. RUN useradd -g 0 -u 0 -m root

{code:java}
 > [17/20] RUN useradd -g 0 -u 0 -m root:
0.165 useradd: user 'root' already exists
------
Dockerfile:101
--------------------
  99 |     LABEL org.apache.hadoop.create-release=""cr-12068""
 100 |     RUN groupadd --non-unique -g 0 root; exit 0;
 101 | >>> RUN useradd -g 0 -u 0 -m root
 102 |     RUN chown -R root /home/root
 103 |     ENV HOME /home/root
{code}


3. RUN chown -R root /home/root


{code:java}
 > [18/20] RUN chown -R root /home/root:
0.168 chown: cannot access '/home/root': No such file or directory
------
Dockerfile:102
--------------------
 100 |     RUN groupadd --non-unique -g 0 root; exit 0;
 101 |     RUN useradd -g 0 -u 0 -m root; exit 0;
 102 | >>> RUN chown -R root /home/root
 103 |     ENV HOME /home/root
 104 |     RUN mkdir -p /maven
--------------------
{code}

Even if these three scripts fail, subsequent steps can continue to be executed, so I added exit 0 after the script.
"
Highlight RBF features and improvements targeting version 3.4,13566462,Resolved,Major,Fixed,29/Jan/24 05:49,31/Jan/24 05:31,3.4.0,"I want to hight recent RBF features and improvements.

- Support observer node from Router-Based Federation
- Enhanced IPC throughput between Router and NameNode
- Improved isolation for downstream name nodes."
Preparing for 1.3.0 development,13566436,Resolved,Major,Fixed,28/Jan/24 15:21,31/Jan/24 05:33,thirdparty-1.3.0,
Update hadoop-thirdparty index.md.vm,13566435,Resolved,Major,Fixed,28/Jan/24 15:14,31/Jan/24 05:33,thirdparty-1.2.0,
Hadoop 3.4.0 Big feature/improvement highlight addendum,13566089,Resolved,Major,Fixed,25/Jan/24 08:09,26/Jan/24 05:36,3.4.0,"Capacity Scheduler was redesigned to add new capacity modes, it should be mentioned as part of 3.4.0 YARN improvements. Reference: YARN-10496/YARN-10888/YARN-10889"
CrcUtil/CrcComposer should not throw IOException for non-IO,13564423,Resolved,Major,Fixed,11/Jan/24 21:26,25/Jan/24 18:35,,"CrcUtil and CrcComposer should throw specific exceptions for non-IO cases
- IllegalArgumentException: invalid arguments
- ArrayIndexOutOfBoundsException: index exceeds array size
- IllegalStateException: unexpected computation state"
Fix Download Maven Url Not Found,13564338,Resolved,Major,Fixed,11/Jan/24 10:59,14/Jan/24 10:40,3.4.0,"In the process of preparing Hadoop 3.4.0 Release, I found that when opening the link to download maven, it will prompt not found. We need to fix this issue."
mvn site commands fails due to MetricsSystem And MetricsSystemImpl changes.,13564593,Resolved,Major,Fixed,14/Jan/24 01:52,16/Jan/24 14:11,3.4.0,
core-default fs.s3a.connection.establish.timeout value too low -warning always printed,13570305,Resolved,Minor,Fixed,29/Feb/24 10:43,05/Mar/24 10:20,3.4.0,"caused by HADOOP-18915.

in core-default we set the value of fs.s3a.connection.establish.timeout to 5s

{code}
<property>
  <name>fs.s3a.connection.establish.timeout</name>
  <value>5s</value>
</property>
{code}

but there is a minimum of 15s, so this prints a warning

{code}
2024-02-29 10:39:27,369 WARN impl.ConfigurationHelper: Option fs.s3a.connection.establish.timeout is too low (5,000 ms). Setting to 15,000 ms instead
{code}
"
S3A: TestIAMInstanceCredentialsProvider.testIAMInstanceCredentialsInstantiate failure,13563699,Open,Minor,,05/Jan/24 11:42,,3.4.0,"test failure in TestIAMInstanceCredentialsProvider; looks like the test is running in an EC2 VM whose IAM service isn't providing credentials -and the test isn't set up to ignore that.


{code}
Caused by: software.amazon.awssdk.core.exception.SdkClientException: The requested metadata is not found
at http://169.254.169.254/latest/meta-data/iam/security-credentials/
	at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)
	at software.amazon.awssdk.regions.util.HttpResourcesUtils.readResource(HttpResourcesUtils.java:125)
	at software.amazon.awssdk.regions.util.HttpResourcesUtils.readResource(HttpResourcesUtils.java:91)
	at software.amazon.awssdk.auth.credentials.InstanceProfileCredentialsProvider.lambda$getSecurityCredentials$3(InstanceProfileCredentialsProvider.java:256)
	at software.amazon.awssdk.utils.FunctionalUtils.lambda$safeSupplier$4(FunctionalUtils.java:108)
	at software.amazon.awssdk.utils.FunctionalUtils.invokeSafely(FunctionalUtils.java:136)
	at software.amazon.awssdk.auth.credentials.InstanceProfileCredentialsProvider.getSecurityCredentials(InstanceProfileCredentialsProvider.java:256)
	at software.amazon.awssdk.auth.credentials.InstanceProfileCredentialsProvider.createEndpointProvider(InstanceProfileCredentialsProvider.java:204)
	at software.amazon.awssdk.auth.credentials.InstanceProfileCredentialsProvider.refreshCredentials(InstanceProfileCredentialsProvider.java:150)

{code}
"
Add S3 Access Grants Support in S3A,13565720,Resolved,Minor,Fixed,22/Jan/24 21:38,19/Mar/24 17:51,3.4.0,Add support for S3 Access Grants (https://aws.amazon.com/s3/features/access-grants/) in S3A.
[ABFS] testListPathWithValueGreaterThanServerMaximum assert failure on heavily loaded store,13572971,Resolved,Minor,Fixed,22/Mar/24 14:35,12/Nov/24 11:31,3.4.0,on an azure store which may be experiencing throttling. the listPath call returns less than the 5K limit. the assertion needs to be changed for this.
Add logic for verifying that the STS URL is in the correct format,13571095,Open,Minor,,07/Mar/24 05:17,,,"* At present an invalid URL can be supplied as an STS endpoint. It will attempt to create an STSClient with it and then fail with,

{quote}java.net.UnknownHostException: request session credentials: software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.:    software.amazon.awssdk.core.exception.SdkClientException: Received an UnknownHostException when attempting to interact with a service. See cause for the exact endpoint that is failing to resolve. If this is happening on an endpoint that previously worked, there may be a network connectivity issue or your DNS cache could be storing endpoints for too long.: https
{quote} * This is inefficient. An invalid URL can be parsed much earlier and can be failed based on the URL format itself.
 * The error message is not very clear and does not indicate a problem in the URL format.
 * In this Jira issue, we attempt to parse the STS URL and fail fast with a more relevant error message."
Unified use of placeholder for log calling,13573279,Open,Minor,,26/Mar/24 05:09,,,"There're some classes logging with pure string like 'AMLauncher':
{code:java}
LOG.info(""Setting up container "" + masterContainer
    + "" for AM "" + application.getAppAttemptId()); {code}
It would be better use as follows?:
{code:java}
LOG.info(""Setting up container {} for AM {}"", masterContainer, application.getAppAttemptId());  {code}
 

If the community decides to do so, I am happy to contribute to it~"
Fix metrics description,13571745,Resolved,Minor,Not A Problem,13/Mar/24 12:24,10/Apr/24 09:03,3.3.0,"This description of the RpcLockWaitTimeNumOps  metrics seems to be incorrect：
{code:java}
| `RpcQueueTimeNumOps` | Total number of RPC calls |
| `RpcQueueTimeAvgTime` | Average queue time in milliseconds |
| `RpcLockWaitTimeNumOps` | Total number of RPC calls (same as RpcQueueTimeNumOps) |{code}
I think the description of this metrics should be more clear：
{code:java}
| `RpcLockWaitTimeNumOps` | Total number of waiting for lock acquisition |{code}"
spotbugs complaining about possible NPE in org.apache.hadoop.crypto.key.kms.ValueQueue.getSize(),13572400,Resolved,Minor,Fixed,19/Mar/24 11:16,19/Mar/24 17:22,3.3.9,"PRs against hadoop-common are reporting spotbugs problems

{code}
Dodgy code Warnings
Code	Warning
NP	Possible null pointer dereference in org.apache.hadoop.crypto.key.kms.ValueQueue.getSize(String) due to return value of called method
Bug type NP_NULL_ON_SOME_PATH_FROM_RETURN_VALUE (click for details)
In class org.apache.hadoop.crypto.key.kms.ValueQueue
In method org.apache.hadoop.crypto.key.kms.ValueQueue.getSize(String)
Local variable stored in JVM register ?
Dereferenced at ValueQueue.java:[line 332]
Known null at ValueQueue.java:[line 332]

{code}
"
"""No test bucket"" error in ITestS3AContractVectoredRead if provided via CLI property",13573897,Open,Minor,,29/Mar/24 19:04,,3.3.6,"ITestS3AContractVectoredRead fails with {{NullPointerException: No test bucket}} if test bucket is defined as {{-Dtest.fs.s3a.name=...}} via CLI , not in {{auth-keys.xml}}.  The same setup works for other S3A contract tests.  Tested on 3.3.6.

{code:title=src/test/resources/auth-keys.xml}
<configuration>
  <property>
    <name>fs.s3a.endpoint</name>
    <value>${test.fs.s3a.endpoint}</value>
  </property>
  <property>
    <name>fs.contract.test.fs.s3a</name>
    <value>${test.fs.s3a.name}</value>
  </property>
</configuration>
{code}

{code}
export AWS_ACCESS_KEY_ID='<redacted>'
export AWS_SECRET_KEY='<redacted>'
mvn -Dtest=ITestS3AContractVectoredRead -Dtest.fs.s3a.name=""s3a://mybucket"" -Dtest.fs.s3a.endpoint=""http://localhost:9878/"" clean test
{code}

{code:title=test results}
Tests run: 46, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 7.879 s <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead
testMinSeekAndMaxSizeDefaultValues[Buffer type : direct](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 1.95 s  <<< ERROR!
java.lang.NullPointerException: No test bucket
  at org.apache.hadoop.util.Preconditions.checkNotNull(Preconditions.java:88)
  at org.apache.hadoop.fs.s3a.S3ATestUtils.getTestBucketName(S3ATestUtils.java:714)
  at org.apache.hadoop.fs.s3a.S3ATestUtils.removeBaseAndBucketOverrides(S3ATestUtils.java:775)
  at org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.testMinSeekAndMaxSizeDefaultValues(ITestS3AContractVectoredRead.java:104)
  ...

testMinSeekAndMaxSizeConfigsPropagation[Buffer type : direct](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.176 s  <<< ERROR!
testMultiVectoredReadStatsCollection[Buffer type : direct](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.179 s  <<< ERROR!
testNormalReadVsVectoredReadStatsCollection[Buffer type : direct](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.155 s  <<< ERROR!
testMinSeekAndMaxSizeDefaultValues[Buffer type : array](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.116 s  <<< ERROR!
testMinSeekAndMaxSizeConfigsPropagation[Buffer type : array](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.102 s  <<< ERROR!
testMultiVectoredReadStatsCollection[Buffer type : array](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.105 s  <<< ERROR!
testNormalReadVsVectoredReadStatsCollection[Buffer type : array](org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead)  Time elapsed: 0.107 s  <<< ERROR!
{code}"
Support hadoop client authentication through keytab configuration.,13566972,Resolved,Minor,Won't Fix,01/Feb/24 02:46,26/Mar/24 09:20,,"*Shield references to {{UserGroupInformation}} Class.*

The current HDFS client keytab authentication code is as follows:
{code:java}
Configuration conf = new Configuration();
conf.addResource(new Path(""/usr/local/service/hadoop/etc/hadoop/hdfs-site.xml""));
conf.addResource(new Path(""/usr/local/service/hadoop/etc/hadoop/core-site.xml""));
UserGroupInformation.setConfiguration(conf);
UserGroupInformation.loginUserFromKeytab(""foo"", ""/var/krb5kdc/foo.keytab"");
FileSystem fileSystem = FileSystem.get(conf);
FileStatus[] fileStatus = fileSystem.listStatus(new Path(""/""));
for (FileStatus status : fileStatus) {
    System.out.println(status.getPath());
} {code}
This feature supports configuring keytab information in core-site.xml or hdfs site.xml. The authentication code is as follows:
{code:java}
Configuration conf = new Configuration();
conf.addResource(new Path(""/usr/local/service/hadoop/etc/hadoop/hdfs-site.xml""));
conf.addResource(new Path(""/usr/local/service/hadoop/etc/hadoop/core-site.xml""));
FileSystem fileSystem = FileSystem.get(conf);
FileStatus[] fileStatus = fileSystem.listStatus(new Path(""/""));
for (FileStatus status : fileStatus) {
    System.out.println(status.getPath());
} {code}
The config of core-site.xml related to authentication is as follows:
{code:java}
<configuration>
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.client.keytab.principal</name>
        <value>foo</value>
    </property>
    <property>
        <name>hadoop.client.keytab.file.path</name>
        <value>/var/krb5kdc/foo.keytab</value>
    </property>
</configuration> {code}"
Compatibility Benchmark over HCFS Implementations: Improvement,13572177,Open,Minor,,18/Mar/24 02:27,,,"Compatibility benchmark is a tool to quickly assess availabilities of Hadoop-Compatible File System APIs. The basic benchmark has been established, while there are still some improvements that has not been completed.

This issue tracks these further improvements of the benchmark tool."
S3A: Regression: ITestS3AOpenCost fails on prefetch test runs,13565108,Resolved,Minor,Fixed,17/Jan/24 15:28,08/Mar/24 12:55,3.4.0,"Getting test failures in the new ITestS3AOpenCost tests when run with {{-Dprefetch}}

Thought I'd tested this, but clearly not
* class cast failures on asserts (fix: skip)
* bytes read different in one test: (fix: identify and address)"
hadoop-aws: downgrade openssl export to test,13570187,Open,Minor,,28/Feb/24 16:14,,3.3.0,As seen in dependency scans and mentioned in HADOOP-16346; wildfly/openssl jar is exported as runtime; it is only needed at test. proposed: downgrade
move commons-logging to 1.2,13569529,Open,Minor,,22/Feb/24 16:25,,3.4.0,"although hadoop doesn't use the APIs itself, it bundles commons-logging as things it depends on (http components) do.

the version hadoop declares (1.1.3) is out of date compared to its dependencies

* update pom and LICENSE-binary"
S3A to support writing to object lock buckets,13568726,Open,Minor,,16/Feb/24 12:43,,3.3.6,"s3 bucket with object lock enabled fails in createFakeDirectory (reported on S.O)

error implies that we need to calculate and include the md5 checksum on the PUT, which gets complex once you include CSE into the mix: the checksum of the encrypted data is what'd be required."
S3A: update AWS SDK to 2.23.19 to support S3 Access Grants,13565719,Resolved,Minor,Fixed,22/Jan/24 21:34,08/Feb/24 20:55,3.4.0,"In order to support S3 Access Grants(https://aws.amazon.com/s3/features/access-grants/) in S3A, we need to update AWS SDK in hadooop package."
ITestS3AClosedFS.testClosedInstrumentation fails on trunk on Mac OS Silicon,13567560,Open,Minor,,06/Feb/24 10:46,,3.5.0,"Running {{mvn clean verify -pl :hadoop-aws}} on my laptop against my s3 bucket on eu-west-1 the test \{{ITestS3AClosedFS.testClosedInstrumentation}} fails every time, while when running it in isolation passes correctly.
It does not fail on branch-3.4, the attached stack trace is tested on commit: 9a7eeadaac818258b319cdb0dc19e9bb1e4fa11a.
h4. aws endpoint:

s3.eu-west-1.amazonaws.com
h4. os:

[INFO] os.detected.name: osx
[INFO] os.detected.arch: aarch_64
[INFO] os.detected.bitness: 64
[INFO] os.detected.version: 14.3
[INFO] os.detected.version.major: 14
[INFO] os.detected.version.minor: 3
[INFO] os.detected.classifier: osx-aarch_64
h4. java -version:

openjdk version ""1.8.0_292""
OpenJDK Runtime Environment (Zulu 8.54.0.21-CA-macos-aarch64) (build 1.8.0_292-b10)
OpenJDK 64-Bit Server VM (Zulu 8.54.0.21-CA-macos-aarch64) (build 25.292-b10, mixed mode)
h4. stack trace:

{{[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.706 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3AClosedFS}}
{{[ERROR] testClosedInstrumentation(org.apache.hadoop.fs.s3a.ITestS3AClosedFS) Time elapsed: 0.447 s <<< FAILURE!}}
{{org.junit.ComparisonFailure: [S3AInstrumentation.hasMetricSystem()] expected:<[fals]e> but was:<[tru]e>}}
{{at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)}}
{{at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)}}
{{at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)}}
{{at org.apache.hadoop.fs.s3a.ITestS3AClosedFS.testClosedInstrumentation(ITestS3AClosedFS.java:111)}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)}}
{{at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)}}
{{at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)}}
{{at java.lang.reflect.Method.invoke(Method.java:498)}}
{{at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)}}
{{at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)}}
{{at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)}}
{{at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)}}
{{at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)}}
{{at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)}}
{{at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)}}
{{at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)}}
{{at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)}}
{{at java.util.concurrent.FutureTask.run(FutureTask.java:266)}}
{{at java.lang.Thread.run(Thread.java:748)}}"
Allow tag passing to AWS Assume Role Credential Provider,13567481,Open,Minor,,05/Feb/24 20:29,,3.4.0,"[https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AssumedRoleCredentialProvider.java#L131-L133] passes a session name and role arn to AssumeRoleRequest. The AWS AssumeRole API also supports passing a list of tags: [https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/sts/model/AssumeRoleRequest.html#tags()]

These tags could be used by platforms to enhance the data encoded into CloudTrail entries to provide better information about the client. For example, a 'notebook' based platform could encode the notebook / jobname / invoker-id in these tags, enabling more granular access controls and leaving a richer breadcrumb-trail as to what operations are being performed.

This is particularly useful in larger environments where jobs do not get individual roles to assume, and there is a desire to track what jobs/notebooks are reading a given set of files in S3."
S3A : ITestS3AConfiguration#testRequestTimeout failure,13563525,Resolved,Minor,Duplicate,04/Jan/24 05:00,30/Jan/24 19:46,3.4.0,"""fs.s3a.connection.request.timeout"" should be specified in milliseconds as per
{code:java}
Duration apiCallTimeout = getDuration(conf, REQUEST_TIMEOUT,
    DEFAULT_REQUEST_TIMEOUT_DURATION, TimeUnit.MILLISECONDS, Duration.ZERO); {code}
The test fails consistently because it sets 120 ms timeout which is less than 15s (min network operation duration), and hence gets reset to 15000 ms based on the enforcement.

 
{code:java}
[ERROR] testRequestTimeout(org.apache.hadoop.fs.s3a.ITestS3AConfiguration)  Time elapsed: 0.016 s  <<< FAILURE!
java.lang.AssertionError: Configured fs.s3a.connection.request.timeout is different than what AWS sdk configuration uses internally expected:<120000> but was:<15000>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.apache.hadoop.fs.s3a.ITestS3AConfiguration.testRequestTimeout(ITestS3AConfiguration.java:444) {code}"
Fix Blocks are always -1 and DataNode`s version are always UNKNOWN in federationhealth.html,13563891,Resolved,Minor,Invalid,08/Jan/24 12:16,09/Jan/24 13:52,,Blocks are always -1 and DataNode`s version are always UNKNOWN in federationhealth.html
ipc client print client info message duplicate,13572115,Resolved,Trivial,Fixed,16/Mar/24 05:30,25/Mar/24 03:45,,"When sending rpc request,the debug message print client name twice, this is confusing.
like: 
{quote}2024-03-16 11:58:49,564 DEBUG ipc.Client: :1113 IPC Client (382750013) connection to kdcserver/172.20.10.12:8888 from nn/bigdata@WUZK.COM IPC Client (382750013) connection to kdcserver/172.20.10.12:8888 from nn/bigdata@WUZK.COM sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
{quote}
the debug message above should delete duplicate information."
Fix shadedclient test failure,13442715,Resolved,Blocker,Fixed,01/May/22 17:39,02/May/22 11:25,3.4.0,"Two of the shaded client tests are failing on Debian 10 ever since this commit - https://github.com/apache/hadoop/commit/63187083cc3b9bb1c1e90e692e271958561f9cc8. The failures are as follows -

1st test failure -
{code}
[INFO] Running org.apache.hadoop.example.ITUseMiniCluster
[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 18.315 s <<< FAILURE! - in org.apache.hadoop.example.ITUseMiniCluster
[ERROR] useWebHDFS(org.apache.hadoop.example.ITUseMiniCluster)  Time elapsed: 12.048 s  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:384)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$300(MiniYARNCluster.java:129)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:500)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:123)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceStart(MiniYARNCluster.java:333)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.example.ITUseMiniCluster.clusterUp(ITUseMiniCluster.java:84)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:479)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1443)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1552)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:376)
	... 37 more
Caused by: java.io.IOException: Unable to initialize WebAppContext
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1380)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:475)
	... 41 more
Caused by: org.apache.hadoop.shaded.com.google.inject.ProvisionException: Unable to provision, see the following errors:

1) Error injecting constructor, javax.xml.bind.JAXBException
 - with linked exception:
[java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:54)
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:57)
  while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver

1 error
	at org.apache.hadoop.shaded.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:226)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1097)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1131)
	at org.apache.hadoop.shaded.com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:345)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.<init>(IoCProviderFactory.java:202)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:123)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:116)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:278)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:151)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1332)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:795)
	at org.apache.hadoop.shaded.com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:339)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:605)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:207)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:394)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:744)
	at org.apache.hadoop.shaded.com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:110)
	at org.apache.hadoop.shaded.com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:98)
	at org.apache.hadoop.shaded.com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:232)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:140)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:731)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:755)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:910)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:253)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.Server.start(Server.java:423)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.Server.doStart(Server.java:387)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1343)
	... 42 more
Caused by: javax.xml.bind.JAXBException
 - with linked exception:
[java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:241)
	at javax.xml.bind.ContextFinder.find(ContextFinder.java:477)
	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:656)
	at org.apache.hadoop.shaded.com.sun.jersey.api.json.JSONJAXBContext.<init>(JSONJAXBContext.java:255)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:122)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver$$FastClassByGuice$$6a7be7f6.newInstance(<generated>)
	at org.apache.hadoop.shaded.com.google.inject.internal.DefaultConstructionProxyFactory$FastClassProxy.newInstance(DefaultConstructionProxyFactory.java:89)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:114)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:91)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:306)
	at org.apache.hadoop.shaded.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
	at org.apache.hadoop.shaded.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168)
	at org.apache.hadoop.shaded.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1094)
	... 95 more
Caused by: java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at javax.xml.bind.ContextFinder.safeLoadClass(ContextFinder.java:594)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:239)
	... 108 more
{code}

2nd test failure -
{code}
[ERROR] useHdfsFileSystem(org.apache.hadoop.example.ITUseMiniCluster)  Time elapsed: 6.202 s  <<< ERROR!
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:384)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$300(MiniYARNCluster.java:129)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:500)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:123)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceStart(MiniYARNCluster.java:333)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.example.ITUseMiniCluster.clusterUp(ITUseMiniCluster.java:84)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:479)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1443)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1552)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:376)
	... 37 more
Caused by: java.io.IOException: Unable to initialize WebAppContext
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1380)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:475)
	... 41 more
Caused by: org.apache.hadoop.shaded.com.google.inject.ProvisionException: Unable to provision, see the following errors:

1) Error injecting constructor, javax.xml.bind.JAXBException
 - with linked exception:
[java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:54)
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:57)
  while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver

1 error
	at org.apache.hadoop.shaded.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:226)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1097)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1131)
	at org.apache.hadoop.shaded.com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:345)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.<init>(IoCProviderFactory.java:202)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:123)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:116)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:278)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:151)
	at org.apache.hadoop.shaded.com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1332)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)
	at org.apache.hadoop.shaded.com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:795)
	at org.apache.hadoop.shaded.com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:339)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:605)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:207)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:394)
	at org.apache.hadoop.shaded.com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:744)
	at org.apache.hadoop.shaded.com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:110)
	at org.apache.hadoop.shaded.com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:98)
	at org.apache.hadoop.shaded.com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:232)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:140)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:731)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:755)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:910)
	at org.apache.hadoop.shaded.org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:253)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.Server.start(Server.java:423)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)
	at org.apache.hadoop.shaded.org.eclipse.jetty.server.Server.doStart(Server.java:387)
	at org.apache.hadoop.shaded.org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1343)
	... 42 more
Caused by: javax.xml.bind.JAXBException
 - with linked exception:
[java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory]
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:241)
	at javax.xml.bind.ContextFinder.find(ContextFinder.java:477)
	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:656)
	at org.apache.hadoop.shaded.com.sun.jersey.api.json.JSONJAXBContext.<init>(JSONJAXBContext.java:255)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:122)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver$$FastClassByGuice$$6a7be7f6.newInstance(<generated>)
	at org.apache.hadoop.shaded.com.google.inject.internal.DefaultConstructionProxyFactory$FastClassProxy.newInstance(DefaultConstructionProxyFactory.java:89)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:114)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:91)
	at org.apache.hadoop.shaded.com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:306)
	at org.apache.hadoop.shaded.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
	at org.apache.hadoop.shaded.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:168)
	at org.apache.hadoop.shaded.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:39)
	at org.apache.hadoop.shaded.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1094)
	... 95 more
Caused by: java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at javax.xml.bind.ContextFinder.safeLoadClass(ContextFinder.java:594)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:239)
	... 108 more
{code}"
Install Maven from Apache archives,13447377,Resolved,Blocker,Fixed,29/May/22 11:54,02/Jun/22 11:36,3.4.0,"The Jenkins CI for Hadoop is failing to build since it's unable to download and install maven -
{code}
22:38:13  #11 [ 7/16] RUN pkg-resolver/install-maven.sh centos:7
22:38:13  #11 sha256:8b1823a6197611693af5daa2888f195db76ae5e9d0765f799becc7e7d5f7b019
22:40:25  #11 131.5 curl: (7) Failed to connect to 2403:8940:3:1::f: Cannot assign requested address
22:40:25  #11 ERROR: executor failed running [/bin/bash --login -c pkg-resolver/install-maven.sh centos:7]: exit code: 7
{code}

Jenkins run - https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4370/4/console
We need to switch to using Maven from Apache archives to prevent such issues."
Use CMake 3.19.0 in Debian 10,13448105,Resolved,Blocker,Fixed,02/Jun/22 11:31,02/Jun/22 18:16,3.4.0,"HDFS Native Client fails to build on Debian 10 due to the following error -
{code}
[WARNING] CMake Error at main/native/libhdfspp/CMakeLists.txt:68 (FetchContent_MakeAvailable):
[WARNING]   Unknown CMake command ""FetchContent_MakeAvailable"".
[WARNING] 
[WARNING] 
[WARNING] -- Configuring incomplete, errors occurred!
{code}
Jenkins run - https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4371/2/artifact/out/branch-compile-hadoop-hdfs-project_hadoop-hdfs-native-client.txt

This cause for this issue is that the version of CMake on Debian 10 (which is installed through apt) is 3.13 and *FetchContent_MakeAvailable* was [introduced in CMake 3.14|https://cmake.org/cmake/help/v3.14/module/FetchContent.html] 

Thus, we upgrade CMake by installing through the [install-cmake.sh|https://github.com/apache/hadoop/blob/34a973a90ef89b633c9b5c13a79aa1ac11c92eb5/dev-support/docker/pkg-resolver/install-cmake.sh] script from pkg-resolver which installs CMake 3.19.0, instead of installing CMake through apt on Debian 10."
vulnerability:  we may leak sensitive information in LocalKeyStoreProvider,13444938,In Progress,Critical,,14/May/22 05:50,,,"Currently, we implement flush like:
{code:java}
//  public void flush() throws IOException {
    super.flush();
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Resetting permissions to '"" + permissions + ""'"");
    }
    if (!Shell.WINDOWS) {
      Files.setPosixFilePermissions(Paths.get(file.getCanonicalPath()),
          permissions);
    } else {
      // FsPermission expects a 10-character string because of the leading
      // directory indicator, i.e. ""drwx------"". The JDK toString method returns
      // a 9-character string, so prepend a leading character.
      FsPermission fsPermission = FsPermission.valueOf(
          ""-"" + PosixFilePermissions.toString(permissions));
      FileUtil.setPermission(file, fsPermission);
    }
  } {code}
we wirite the Credential first, then set permission.

The correct order is setPermission first, then write Credential .

Otherswise, we may leak Credential . For example, the origin perms of file is 755(default on linux),  when the Credential  is flushed, Credential can be leaked when 

 

1)between flush and setPermission,  others have a chance to access the file.

2)  CredentialShell(or the machine node )  crash between flush and setPermission,   the file permission is 755 for ever before we run the CredentialShell again.

 "
Fix when to read an additional record from a BZip2 text file split,13469492,Resolved,Critical,Fixed,30/Jun/22 15:41,06/Jul/22 04:30,3.3.3,"Fix data correctness issue with TextInputFormat that can occur when reading BZip2 compressed text files. When triggered this bug would cause a split to return the first record of the succeeding split that reads the next BZip2 block, thereby duplicating that record.

*When the bug is triggered?*

The condition for the bug to occur requires that the flag ""needAdditionalRecord"" in CompressedSplitLineReader to be set to true by #fillBuffer at an inappropriate time: when we haven't read the remaining bytes of split. This can happen when the inDelimiter parameter is true while #fillBuffer is invoked while reading the next line. The inDelimiter parameter is true when either 1) the last byte of the buffer is a CR character ('\r') if using the default delimiters, or 2) the last bytes of the buffer are a common prefix of the delimiter if using a custom delimiter.

This can occur in various edge cases, illustrated by five unit tests added in this change -- specifically the five that would fail without the fix are as listed below:
 # BaseTestLineRecordReaderBZip2.customDelimiter_lastRecordDelimiterStartsAtNextBlockStart
 # BaseTestLineRecordReaderBZip2.firstBlockEndsWithLF_secondBlockStartsWithCR
 # BaseTestLineRecordReaderBZip2.delimitedByCRSpanningThreeBlocks
 # BaseTestLineRecordReaderBZip2.usingCRDelimiterWithSmallestBufferSize
 # BaseTestLineRecordReaderBZip2.customDelimiter_lastThreeBytesInBlockAreDelimiter

For background, the purpose of ""needAdditionalRecord"" field in CompressedSplitLineReader is to indicate to LineRecordReader via the #needAdditionalRecordAfterSplit method that an extra record lying beyond the split range should be included in the split. This complication arises due to a problem when splitting text files. When a split starts at a position greater than zero, we do not know whether the first line belongs to the last record in the prior split or is a new record. The solution done in Hadoop is to make splits that start at position greater than zero to always discard the first line and then have the prior split decide whether it should include the first line of the next split or not (as part of the last record or as a new record). This works well even in the case of a single line spanning multiple splits.

*What is the fix?*

The fix is to prevent ever setting ""needAdditionalRecord"" if the bytes filled to the buffer are not the bytes immediately outside the range of the split.

When reading compressed data, CompressedSplitLineReader requires/assumes that the stream's #read method never returns bytes from more than one compression block at a time. This ensures that #fillBuffer gets invoked to read the first byte of the next block. This next block may or may not be part of the split we are reading. If we detect that the last bytes of the prior block maybe part of a delimiter, then we may decide that we should read an additional record, but we should only do that when this next block is not part of our split *and* we aren't filling the buffer again beyond our split range. This is because we are only concerned whether the we need to read the very first line of the next split as a separate record. If it going to be part of the last record, then we don't need to read an extra record, or in the special case of CR + LF (i.e. ""\r\n""), if the LF is the first byte of the next split, it will be treated as an empty line, thus we don't need to include an extra record into the mix.

Thus, to emphasize. It is when we read the first bytes outside our split range that matters. But the current logic doesn't take that into account in CompressedSplitLineReader. This is in contrast to UncompressedSplitLineReader which does."
Interrupting RPC Client calls can lead to thread exhaustion,13469534,Resolved,Critical,Fixed,30/Jun/22 22:56,18/Nov/22 16:51,2.10.2,Currently the IPC client creates a boundless number of threads to write the rpc request to the socket. The NameNode uses timeouts on its RPC calls to the Journal Node and a stuck JN will cause the NN to create an infinite set of threads.
Remove shading exclusion of javax.ws.rs-api from hadoop-client-runtime,13450848,Resolved,Critical,Won't Fix,19/Jun/22 22:32,24/Jul/22 06:11,,"As part of HADOOP-18033, we have excluded shading of javax.ws.rs-api from both hadoop-client-runtime and hadoop-client-minicluster. This has caused issues for downstreamers e.g. [https://github.com/apache/incubator-kyuubi/issues/2904], more discussions.

We should put the shading back in hadoop-client-runtime to fix CNFE issues for downstreamers.

cc [~ayushsaxena] [~pan3793] "
Bump javax.ws.rs-api To Version 3.1.0,13469527,Resolved,Critical,Not A Problem,30/Jun/22 21:37,05/Aug/22 15:34,3.3.3,"Current version of  javax.ws.rs-api is 2.1.1 - which has a vulnerable dependency to [CVE-2020-15250|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250]

Lets Upgrade to 3.1.0"
Yetus build failure in branch-3.3.,13469508,Resolved,Critical,Fixed,30/Jun/22 17:24,14/Jul/22 00:47,,"[ERROR] The build could not read 1 project -> [Help 1] [ERROR] [ERROR] The project org.apache.hadoop:hadoop-benchmark:3.4.0-SNAPSHOT (/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4517/src/hadoop-tools/hadoop-benchmark/pom.xml) has 1 error [ERROR] Non-resolvable parent POM for org.apache.hadoop:hadoop-benchmark:3.4.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-project:pom:3.4.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 22, column 11 -> 

 

 

https://github.com/apache/hadoop/pull/4517#issuecomment-1170777719"
Introduce hadoop-logging module,13439922,Reopened,Major,,16/Apr/22 04:23,,,"There are several goals here:
 # Provide the ability to change log level, get log level, etc.
 # Place all the appender implementation(?)
 # Hide the real logging implementation.
 # Later we could remove all the log4j references in other hadoop module.
 # Move as much log4j usage to the module as possible.

 "
Understand status of S3 access point alias support in S3A,13447994,Open,Major,,01/Jun/22 18:28,,3.3.5,"Adding support for using access point alias to access s3 bucket.

 

When you create an access point, Amazon S3 automatically generates an alias that you can use instead of an Amazon S3 bucket name for data access. You can use this access point alias instead of an Amazon Resource Name (ARN) for any access point data plane operation.

https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html"
Analyzing S3A Audit Logs ,13446762,In Progress,Major,,25/May/22 10:17,,,"The main aim is to analyze S3A Audit logs to give better insights in Hive and Spark jobs.
Steps involved are:
 * Merging audit log files containing huge number of audit logs collected from a job containing various S3 requests.
 * Parsing audit logs using regular expressions i.e., dividing them into key value pairs.
 * Converting the key value pairs into CSV file and AVRO file formats.
 * Querying on data which would give better insights for different jobs.
 * Visualizing the audit logs on Zeppelin or Jupyter notebook with graphs."
Update to Apache LDAP API 2.0.x,13453349,Resolved,Major,Fixed,21/Jun/22 06:20,07/Oct/24 13:57,3.3.3,Update from Apache LDAP API 1.x to 2.0.x.
Clarify in which branch CVE-2022-26612 is fixed,13469103,Resolved,Major,Done,29/Jun/22 12:47,30/Sep/24 14:32,,"According to HADOOP-18198, CVE-2022-26612 has been fixed in version 3.3.3.

The underlying ticket where the fix occured is HADOOP-18155. This ticket has fix version including 2.10.2.

On top of that, it's clear to me that CVE-2022-26612 is fixed in hadoop-common:2.10.2.
Howerver, it is still reported as an issue in different places:
 * [https://github.com/advisories/GHSA-gx2c-fvhc-ph4j]
 * [https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/2.10.2]

It may just be a false positive in a CVE database, still I prefer to double-check with the hadoop community.

So, could you please state here whether CVE-2022-26612 is really fixed in below version of hadoop-common ?
 * >= 2.10.2
 * >= 3.2.3
 * >= 3.3.3
 * >= 3.4.0"
Fix Hadoop Common Java Doc Errors,13443955,Resolved,Major,Fixed,09/May/22 10:38,18/May/22 11:14,3.4.0,"I found that when hadoop-multibranch compiled PR-4266, some errors would pop up, I tried to solve it

The wrong compilation information is as follows, I try to fix the Error information


{code:java}
[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-4266/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java:432: error: exception not thrown: java.io.IOException
[ERROR]    * @throws IOException
[ERROR]              ^
[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-4266/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java:885: error: unknown tag: username
[ERROR]    *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
[ERROR]                           ^
[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-4266/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java:885: error: bad use of '>'
[ERROR]    *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
[ERROR]                                            ^
[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-4266/ubuntu-focal/src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java:910: error: unknown tag: username
[ERROR]    *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(?<username>\w+)
{code}
"
Update protobuf 3.7.1 to a version without CVE-2021-22569,13438913,Resolved,Major,Fixed,11/Apr/22 10:12,28/Jan/24 09:47,thirdparty-1.2.0,"The artifact `org.apache.hadoop:hadoop-common` brings in a dependency `com.google.protobuf:protobuf-java:2.5.0`, which is an outdated version released in 2013 and it contains a vulnerability [CVE-2021-22569|https://nvd.nist.gov/vuln/detail/CVE-2021-22569].

Therefore, requesting you to clarify if this library version is going to be updated in the following releases"
ABFS: Add support for cache handling when filesystem instance has clientCorrelationID enabled,13469017,Open,Major,,29/Jun/22 05:33,,3.3.3,"This is to mainly handle the scenarios for cache enablement or disablement when the filesystem instance has clientCorrelationId as a part of it's configuration.

If a filesystem instance has clientCorrelationId as a part of it's configuration and if we enable cache for the same, there may be another job using a filesystem instance with same configuration and uri as the one cached, but the issue here is that since these 2 filesystem instances are part of 2 different running jobs, they can't share the same clientCorrelationId."
Release Hadoop 3.3.3: hadoop-3.3.2 with some fixes,13438968,Resolved,Major,Fixed,11/Apr/22 13:42,18/May/22 13:19,3.3.2,"Hadoop 3.3.3 is a minor followup release to Hadoop 3.3.2 with all the incremental changes which went in to the 3.2.4 release

* minor CVE fixes in Hadoop source
* CVE fixes in dependencies we know of (protobuf unmarshalling leading to DoS, jackson stack overflow,...)
* replacement of log4j 1.2.17 to reload4j
* node.js update

This is not a release off branch-3.3, it is a fork of 3.3.2 with the changes.

The next release of branch-3.3 will be numbered hadoop-3.3.4; updating maven versions and JIRA fix versions is part of this release process.

The changes here are already in branch 3.2.4; this completes the set

CVEs fixed

* CVE-2022-26612: Apache Hadoop: Arbitrary file write in FileUtil#unpackEntries on Windows (HADOOP-18155)
* CVE-2022-25168 Verify FileUtils.unTar() handling of missing .tar files. (HADOOP-18136)
"
Add .asf.yaml to hadoop-thirdparty,13449067,Resolved,Major,Fixed,08/Jun/22 20:02,09/Jun/22 09:37,thirdparty-1.2.0,"no yaml file in thirdparty, it is dropping mails to common-dev for everything."
Support nested mount points in INodeTree,13437946,Resolved,Major,Fixed,05/Apr/22 16:40,12/May/22 00:03,2.10.0,"Defining following client mount table config is not supported in  INodeTree and will throw FileAlreadyExistsException

 
{code:java}
fs.viewfs.mounttable.link./foo/bar=hdfs://nn1/foo/bar
fs.viewfs.mounttable.link./foo=hdfs://nn02/foo
{code}
INodeTree has 2 methods that need change to support nested mount points.
{code:java}
createLink(): build INodeTree during fs init.
resolve(): resolve path in INodeTree with viewfs apis.
{code}
ViewFileSystem and ViewFs maintains an INodeTree instance(fsState) in both classes and call fsState.resolve(..) to resolve path to specific mount point. INodeTree.resolve encapsulates the logic of nested mount point resolving. So no changes are expected in both classes. 

AC:
 # INodeTree.createlink should support creating nested mount points.(INodeTree is constructed during fs init)
 # INodeTree.resolve should support resolve path based on nested mount points. (INodeTree.resolve is used in viewfs apis)
 # No regression in existing ViewFileSystem and ViewFs apis.
 # Ensure some important apis are not broken with nested mount points. (Rename, getContentSummary, listStatus...)

 

Spec:

Please review attached pdf for spec about this feature."
Remove replace-guava from replacer plugin,13438600,Resolved,Major,Fixed,08/Apr/22 13:05,15/Apr/22 14:25,3.4.0,"While running the build, realized that all replacer plugin executions run only after ""banned-illegal-imports"" enforcer plugin.

For instance,
{code:java}
[INFO] --- maven-enforcer-plugin:3.0.0:enforce (banned-illegal-imports) @ hadoop-cloud-storage ---
[INFO] 
[INFO] --- replacer:1.5.3:replace (replace-generated-sources) @ hadoop-cloud-storage ---
[INFO] Skipping
[INFO] 
[INFO] --- replacer:1.5.3:replace (replace-sources) @ hadoop-cloud-storage ---
[INFO] Skipping
[INFO] 
[INFO] --- replacer:1.5.3:replace (replace-guava) @ hadoop-cloud-storage ---
[INFO] Replacement run on 0 file.
[INFO]  {code}
Hence, if our source code uses com.google.common, banned-illegal-imports will cause the build failure and replacer plugin would not even get executed.

We should remove it as it is only redundant execution step."
create-release fails fatal: unsafe repository ('/build/source' is owned by someone else),13439694,Resolved,Major,Fixed,14/Apr/22 16:55,18/Apr/22 18:34,3.3.3,"The first git command in the create-release docker container is failing as git doesn't consider /build/source to belong to the current user.

since CVE-2022-24765 and the april 2022 git releases to refuse to work in such directories

https://github.blog/2022-04-12-git-security-vulnerability-announced/

{code}

$ /usr/bin/git clean -xdf -e /patchprocess
fatal: unsafe repository ('/build/source' is owned by someone else)
To add an exception for this directory, call:

	git config --global --add safe.directory /build/source

{code}
"
Cleanup the commons-logging references in the code base,13439921,Resolved,Major,Fixed,16/Apr/22 04:17,13/Feb/23 19:24,3.4.0,Should always use slf4j
Prevent DelegationTokenSecretManagerMetrics from registering multiple times ,13443421,Resolved,Major,Fixed,05/May/22 16:17,10/May/22 21:56,3.3.5,"After committing HADOOP-18167, we received reports of the following error when ResourceManager is initialized:
{noformat}
Caused by: java.io.IOException: Problem starting http server
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1389)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:475)
        ... 4 more
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source DelegationTokenSecretManagerMetrics already exists!
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
        at org.apache.hadoop.metrics2.MetricsSystem.register(MetricsSystem.java:71)
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics.create(AbstractDelegationTokenSecretManager.java:878)
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.<init>(AbstractDelegationTokenSecretManager.java:152)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager.<init>(DelegationTokenManager.java:72)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:122)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.initTokenManager(DelegationTokenAuthenticationHandler.java:161)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.init(DelegationTokenAuthenticationHandler.java:130)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.initializeAuthHandler(DelegationTokenAuthenticationFilter.java:214)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:180)
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53){noformat}
This can happen if MetricsSystemImpl#init is called and multiple metrics are registered with the same name. A proposed solution is to declare the metrics in AbstractDelegationTokenSecretManager as singleton, which would prevent multiple instances DelegationTokenSecretManagerMetrics from being registered."
Upgrade maven compiler plugin to 3.10.1,13443482,Resolved,Major,Fixed,05/May/22 23:05,20/May/22 18:23,3.4.0,"Currently we are using maven-compiler-plugin 3.1 version, which is quite old (2013) and it's also pulling in vulnerable log4j dependency:
{code:java}
[INFO]    org.apache.maven.plugins:maven-compiler-plugin:maven-plugin:3.1:runtime
[INFO]       org.apache.maven.plugins:maven-compiler-plugin:jar:3.1
[INFO]       org.apache.maven:maven-plugin-api:jar:2.0.9
[INFO]       org.apache.maven:maven-artifact:jar:2.0.9
[INFO]       org.codehaus.plexus:plexus-utils:jar:1.5.1
[INFO]       org.apache.maven:maven-core:jar:2.0.9
[INFO]       org.apache.maven:maven-settings:jar:2.0.9
[INFO]       org.apache.maven:maven-plugin-parameter-documenter:jar:2.0.9
...
...
...
[INFO]       log4j:log4j:jar:1.2.12
[INFO]       commons-logging:commons-logging-api:jar:1.1
[INFO]       com.google.collections:google-collections:jar:1.0
[INFO]       junit:junit:jar:3.8.2
 {code}
 

We should upgrade to 3.10.1 (latest Mar, 2022) version of maven-compiler-plugin."
Initialization race condition with TemporaryAWSCredentialsProvider,13444693,Resolved,Major,Fixed,12/May/22 19:40,31/Oct/22 18:23,3.3.1,"I'm in the process of upgrading spark+hadoop versions for my workflows and observing a weird behavior regression.  I'm setting
{code:java}
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.sql.catalogImplementation=hive
spark.hadoop.aws.region=us-west-2
...many other things, I think these might be the relevant ones though...{code}
in Spark config and I'm observing some non-fatal warnings/exceptions (see below for some examples).  The warnings/exceptions randomly appear for some tasks, which causes them to fail, but then when Spark retries the task it will succeed.  The initial tasks don't always fail either, just sometimes.

I also found that if I switch to a SimpleAWSCredentials and use static keys, then I don't see any issues.

My old setup was spark v3.0.2 with hadoop-aws v3.2.1 which also does not have these warnings/exceptions.

From reading some other tickets I thought perhaps adding
{code:java}
spark.sql.hive.metastore.sharedPrefixes=com.amazonaws {code}
would help, but it did not.

Appreciate any suggestions for how to proceed or debug further :)

 

Example stack traces:

First one for an s3 read
{code:java}
 WARN TaskSetManager: Lost task 27.0 in stage 4.0 (TID 29) (<ip> executor 13): java.nio.file.AccessDeniedException: s3a://bucket/path/to/part.snappy.parquet: org.apache.hadoop.fs.s3a.CredentialInitializationException: Provider TemporaryAWSCredentialsProvider has no credentials
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:206)
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3289)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)
    at org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:268)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:267)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:270)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:164)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
    at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.columnartorow_nextBatch_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.fs.s3a.CredentialInitializationException: Provider TemporaryAWSCredentialsProvider has no credentials
    at org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider.getCredentials(AbstractSessionCredentialsProvider.java:130)
    at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1266)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:842)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:792)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)
    at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6408)
    at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6381)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5422)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)
    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)
    ... 27 more{code}
And here is one for an s3 write which is similar but slightly different
{code:java}
WARN TaskSetManager: Lost task 21.0 in stage 78.0 (TID 1358) (<ip> executor 11): org.apache.spark.SparkException: Task failed while writing rows.
    at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.AccessDeniedException: s3a://bucket/path/to/_temporary/0/_temporary/attempt_0123456789: org.apache.hadoop.fs.s3a.CredentialInitializationException: Provider TemporaryAWSCredentialsProvider has no credentials
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:206)
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3289)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)
    at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)
    at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.needsTaskCommit(FileOutputCommitter.java:674)
    at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.needsTaskCommit(FileOutputCommitter.java:663)
    at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:61)
    at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:269)
    at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)
    at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)
    at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:305)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)
    ... 9 more
Caused by: org.apache.hadoop.fs.s3a.CredentialInitializationException: Provider TemporaryAWSCredentialsProvider has no credentials
    at org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider.getCredentials(AbstractSessionCredentialsProvider.java:130)
    at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1266)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:842)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:792)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)
    at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6408)
    at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6381)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5422)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)
    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)
    at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)
    ... 23 more {code}
 "
Upgrade Apache Xerces Java to 2.12.2,13445310,Resolved,Major,Fixed,17/May/22 03:32,22/Jun/22 17:55,3.3.3,"Description

https://github.com/advisories/GHSA-h65f-jvqw-m9fj


There's a vulnerability within the Apache Xerces Java (XercesJ) XML parser when handling specially crafted XML document payloads. This causes, the XercesJ XML parser to wait in an infinite loop, which may sometimes consume system resources for prolonged duration. This vulnerability is present within XercesJ version 2.12.1 and the previous versions.

References
[https://nvd.nist.gov/vuln/detail/CVE-2022-23437]
https://lists.apache.org/thread/6pjwm10bb69kq955fzr1n0nflnjd27dl
http://www.openwall.com/lists/oss-security/2022/01/24/3
https://www.oracle.com/security-alerts/cpuapr2022.html"
Upgrade Yetus to 0.14.0,13445467,Resolved,Major,Fixed,17/May/22 16:57,25/May/22 08:36,3.3.5,Yetus 0.14.0 is released. Let's upgrade.
Extend KMS related exceptions that get mapped to ConnectException ,13445723,Resolved,Major,Fixed,18/May/22 21:41,19/May/22 20:25,3.4.0,"Based on production workload, we found that it is not enough to map just SSLHandshakeException to ConnectException in Loadbalancing KMS Client but that needs to be extended to SSLExceptions and SocketExceptions.

Sample JDK code that can raise these exceptions: https://github.com/openjdk/jdk/blob/jdk-18%2B32/src/java.base/share/classes/sun/security/ssl/SSLSocketImpl.java#L1409-L1428

Sample Exception backtrace: 
22/04/13 16:25:53 WARN kms.LoadBalancingKMSClientProvider: KMS provider at [https://bdgtr041x10h5.nam.nsroot.net:16001/kms/v1/] threw an IOException:
javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake
        at sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1470)
        at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1298)
        at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1199)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:373)
        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:587)
        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDe
Caused by: java.io.EOFException: SSL peer shut down incorrectly
        at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:480)
        at sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:469)
        ... 59 more"
Fix Junit Test Deprecated assertThat,13445935,Resolved,Major,Duplicate,20/May/22 02:27,23/May/22 14:26,3.4.0,"javac will give a warning for compilation, as follows:

org.junit.Assert.assertThat Deprecated. use org.hamcrest.MatcherAssert.assertThat()
{code:java}
TestIncrementalBrVariations.java:141:4:[deprecation] <T>assertThat(T,Matcher<? super T>) in Assert has been deprecated {code}
a related issue will be resolved in HDFS-16590."
Fix getUri() in HttpRequest has been deprecated,13445951,Resolved,Major,Fixed,20/May/22 03:34,24/May/22 17:41,3.4.0,"When reading the code, I found that the method used has been deprecated due to the upgrade of the netty component. The main methods are as follows:

io.netty.handler.codec.http#HttpRequest

@Deprecated
HttpMethod getMethod();
Deprecated. Use method() instead.

@Deprecated
String getUri()
Deprecated. Use uri() instead.
io.netty.handler.codec.http#DefaultHttpResponse

@Deprecated
public HttpResponseStatus getStatus()

{         return this.status(); }

Deprecated. Use status()  instead.

 

WebHdfsHandler.java:125:35:[deprecation] getUri() in HttpRequest has been deprecated

HostRestrictingAuthorizationFilterHandler.java:200:27:[deprecation] getUri() in HttpRequest has been deprecated"
Cancel fileMonitoringTimer even if trustManager isn't defined,13448887,Resolved,Major,Fixed,08/Jun/22 00:45,01/Feb/23 20:05,3.3.5,"The key stores factory starts a timer for monitoring file changes that should be reloaded.  The call to destroy() doesn't cancel the timer when a trust manager isn't defined.  This leaves the timer running, during shutdown.  This can be seen in unit tests that do not stop when the test completes."
Total requests and total requests per sec served by RPC servers,13449643,Resolved,Major,Fixed,12/Jun/22 01:15,23/Jun/22 09:31,3.3.5,"RPC Servers provide bunch of useful information like num of open connections, slow requests, num of in-progress handlers, RPC processing time, queue time etc, however so far it doesn't provide accumulation of all requests as well as current snapshot of requests per second served by the server. Exposing them would benefit from operational viewpoint in identifying how busy the servers have been and how much load they are currently serving in the presence of cluster wide high load."
Add some description for PowerShellFencer,13468575,Resolved,Major,Fixed,27/Jun/22 11:58,29/Jun/22 02:07,3.3.3,"Some descriptions related to PowerShellFencer are missing here, such as: core-default.xml, HDFSHighAvailabilityWithQJM.md, HDFSHighAvailabilityWithNFS.md, NodeFencer.java.
We should have as much documentation as possible."
Update ZooKeeper to 3.6.4 when released,13439385,Open,Major,,13/Apr/22 09:03,,,"Regarding ZOOKEEPER-4455, ZooKeeper 3.6.4 uses reload4j instead of log4j 1.x. Let's upgrade when released."
Prometheus metrics tag changed unexpectedly.,13446997,Open,Major,,26/May/22 12:24,,3.2.1,"Prom web print metrics like below
{code:java}
fs_namesystem_blocks_total{context=""dfs"",enabledecpolicies=""RS-6-3-1024k"",hastate=""active"",totalsynctimes=""10 43 "",hostname=""xxx""} 3674006 {code}
getTotalSyncTimes return String, so totalsynctimes will change to tag.

It is difficult to show in grafana. We may get wrong panel, though only one metric is set.

!截屏2022-05-26 下午8.15.50.png|width=562,height=236!"
remove hadoop-cos as a dependency of hadoop-cloud-storage,13451387,Resolved,Major,Fixed,20/Jun/22 17:43,24/Jun/22 13:13,3.3.3,"to deal without HADOOP-18159 without qualifying an updated cos library, remove it as an explicit dependency of hadoop cloud storage

it will still be built and published, "
Remove all the log4j reference in modules other than hadoop-logging,13439923,Open,Major,,16/Apr/22 04:24,,,
S3A prefetch - Implement LRU cache for SingleFilePerBlockCache,13450247,Resolved,Major,Fixed,15/Jun/22 14:33,14/Jul/23 17:40,3.4.0,"Currently there is no limit on the size of disk cache. This means we could have a large number of files on files, especially for access patterns that are very random and do not always read the block fully. 

 

eg:

in.seek(5);

in.read(); 

in.seek(blockSize + 10) // block 0 gets saved to disk as it's not fully read

in.read();

in.seek(2 * blockSize + 10) // block 1 gets saved to disk

.. and so on

 

The in memory cache is bounded, and by default has a limit of 72MB (9 blocks). When a block is fully read, and a seek is issued it's released [here|https://github.com/apache/hadoop/blob/feature-HADOOP-18028-s3a-prefetch/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/read/S3CachingInputStream.java#L109]. We can also delete the on disk file for the block here if it exists. 

 

Also maybe add an upper limit on disk space, and delete the file which stores data of the block furthest from the current block (similar to the in memory cache) when this limit is reached. "
Ensure S3A prefetching stream memory consumption scales,13442342,Open,Major,,28/Apr/22 15:10,,3.4.0,"A recurrent problem in cloud store IO is running out of memory because blocks are buffered in reads or writes.

We need to make sure that data/memory is managed in the prefetch code such that it works in processes with many worker threads (hive, spark....) and does not exhaust resources."
Review s3a prefetching input stream retry code; synchronization,13449220,Open,Major,,09/Jun/22 12:00,,3.4.0,"Need to review S3A prefetching stream retry logic

* no attempt to retry on unrecoverable errors
* do try on recoverable ones
* no wrap of retry by retry.
* annotate classes with Retries annotations to aid the review.

a key concern has to be that transient failure of  prefetch is recovered from; things like deleted/shortened file fails properly on the next read call

the other issue is: review all thread safety. the outer stream appears thread safe, so review to make sure that is sufficient"
Upgrade dependencies to address several CVEs,13458586,Open,Major,,22/Jun/22 14:08,,3.3.3,"The following CVEs can be addressed by upgrading dependencies within the build.  This includes a replacement of HTrace with a noop implementation.
 * CVE-2018-7489
 * CVE-2020-10663
 * CVE-2020-28491
 * CVE-2020-35490
 * CVE-2020-35491
 * CVE-2020-36518
 * PRISMA-2021-0182

This addresses all of the CVEs from 3.3.3 except for ones that would require upgrading Netty to 4.x.  I'll be submitting a pull request for 3.3.4."
Merging of S3A Audit Logs,13446765,Open,Major,,25/May/22 10:19,,,"Merging audit log files containing huge number of audit logs collected from a job like Hive or Spark job containing various S3 requests like list, head, get and put requests."
Add input stream IOstats for vectored IO api in S3A.,13443702,Resolved,Major,Fixed,06/May/22 18:22,11/Aug/22 21:14,,
ABFS Rename Failure when tracking metadata is in incomplete state,13445547,Resolved,Major,Fixed,18/May/22 04:02,21/Sep/22 10:16,,"If a node in the datacenter crashes while processing an operation, occasionally it can leave the Storage-internal blob tracking metadata in an incomplete state.  We expect this to happen occasionally, and so all API’s are designed in such a way that if this incomplete state is observed on a blob, the situation is resolved before the current operation proceeds.  However, this incident has exposed a bug specifically with the Rename API, where the incomplete state fails to resolve, leading to this incorrect failure.  As a temporary mitigation, if any other operation is performed on this blob – GetBlobProperties, GetBlob, GetFileProperties, SetFileProperties, etc – it should resolve the incomplete state, and rename will no longer hit this issue.

StackTrace:
{code:java}
2022-03-22 17:52:19,789 DEBUG [regionserver/euwukwlss-hg50:16020.logRoller] services.AbfsClient: HttpRequest: 404,RenameDestinationParentPathNotFound,cid=ef5cbf0f-5d4a-4630-8a59-3d559077fc24,rid=35fef164-101f-000b-1b15-3ed818000000,sent=0,recv=212,PUT,https://euwqdaotdfdls03.dfs.core.windows.net/eykbssc/apps/hbase/data/oldWALs/euwukwlss-hg50.tdf.qa%252C16020%252C1647949929877.1647967939315?timeout=90   {code}"
[IPv6] Yarn WebApps to support IPv6 address.,13468369,Open,Major,,24/Jun/22 22:54,,,"org.apache.hadoop.yarn.webapp.WebApps.Builder#at(java.lang.String) 

This fails to parse an IPV6 address. Only IPv4 supported by the builder. "
replace with HashSet/TreeSet constructor directly in hadoop-yarn-project ,13447876,Open,Major,,01/Jun/22 08:28,,,
Release Hadoop 3.3.4: minor update of hadoop-3.3.3,13451378,Resolved,Major,Fixed,20/Jun/22 17:01,10/Aug/22 11:03,3.3.3,"Create a Hadoop 3.3.4 release with

* critical fixes
* ARM artifacts as well as the intel ones"
Fix 3.3 build problems caused by backport of HADOOP-11867.,13468993,Open,Major,,29/Jun/22 00:41,,3.3.5,
AliyunOSS: AliyunOSSBlockOutputStream should not mark the temporary file for deletion,13468395,Resolved,Major,Fixed,25/Jun/22 10:09,04/Aug/22 04:04,2.10.2,"AliyunOSSBlockOutputStream buffers data in local directory before uploading to OSS. It uses LocalDirAllocator.createTmpFileForWrite which will create a temp file on this directory and invoke DeleteOnExitHook.add to add the path to a set and will be deleted on VM exit through a shutdown hook. However, the size of set will be growing if the VM does not exit."
[Umbrella] Remove unused Imports in hadoop project,13443826,Resolved,Major,Fixed,08/May/22 03:29,28/Jun/22 03:32,,"h3. Optimize Imports to keep code clean
 # Remove any unused imports
 # -Sort the import statements.-
 # -Remove .* imports-"
Fix links to DataNodes in NameNode UI to support IPv6,13439986,Patch Available,Major,,17/Apr/22 03:09,,,"IPv6 second group getting displayed as port number and which lead to error in opening the DN webpage from NN Web UI.

 

!image-2022-04-17-08-39-20-926.png!

 

After fix: IPv6 Address & Port parsed correctly & link open the DN webpage normally.

!image-2022-04-18-21-52-46-661.png!"
Fix some compatibility issues with 3.3.3 release notes,13449904,Open,Major,,14/Jun/22 02:30,,3.3.3,"3.3.3 Release Notes:
https://hadoop.apache.org/docs/r3.3.3/hadoop-project-dist/hadoop-common/release/3.3.3/RELEASENOTES.3.3.3.html

There are some compatibility issues here. E.g:
 !image-2022-06-14-10-27-23-027.png! 

I think this is happening due to a syntax issue.
It would be more appropriate to change it to this:
 !image-2022-06-14-10-28-53-822.png! 
"
Add option and make 400 bad request retryable,13454639,Open,Major,,21/Jun/22 18:40,,3.3.4,"When one is using a customized credential provider via fs.s3a.aws.credentials.provider, e.g. org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider, when the provided credential by this pluggable provider is expired and return an error code of 400 as bad request exception.

Here, the current S3ARetryPolicy will fail immediately and does not retry on the S3A level. 

Our recent use case in HBase found this use case could lead to a Region Server got immediate abandoned from this Exception without retry, when the file system is trying open or S3AInputStream is trying to reopen the file. especially the S3AInputStream use cases, we cannot find a good way to retry outside of the file system semantic (because if a ongoing stream is failing currently it's considered as irreparable state), and thus we come up with this optional flag for retrying in S3A.


{code}
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: XYZ; S3 Extended Request ID: ABC; Proxy: null), S3 Extended Request ID: 123
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5453)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5400)
	at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1524)
	at org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl.getObject(S3AFileSystem.java:1506)
	at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$reopen$0(S3AInputStream.java:217)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:117)
	... 35 more
{code}"
Upgrade bundled Tomcat to 8.5.76 or higher,13454538,Resolved,Major,Fixed,21/Jun/22 10:36,23/Jun/22 08:48,2.10.1,"Currently we are using 8.5.75 which is affected by {color:#222233}CVE-2022-25762{color}

More Details - [https://lists.apache.org/thread/qzkqh2819x6zsmj7vwdf14ng2fdgckw7]

Lets upgrade  8.5.76 or higher

 

 "
Release Hadoop 3.3.4 critical fix update,13450390,Resolved,Major,Duplicate,16/Jun/22 09:45,22/Jun/22 13:35,,"Create a new release off the branch-3.3.3 line with a few more changes

* wrap up of security changes
* cut hadoop-cos out of hadoop-cloud-storage as its dependencies break s3a client...reinstate once the updated jar is tested
* try to get an arm build out tool"
replace with HashSet/TreeSet constructor directly in hadoop-hdfs-project ,13447262,Resolved,Major,Duplicate,27/May/22 17:07,22/Jun/22 04:51,,Associated PR : [https://github.com/apache/hadoop/pull/4400] Please review 
Fix unexpected contents in hadoop-client-check-test-invariants on aarch64,13450670,Resolved,Major,Cannot Reproduce,17/Jun/22 18:40,18/Jun/22 06:34,3.3.3,Build on aarch64 failed due to banned classes. {{com.github.stephenc.findbugs:findbugs-annotations}} seems to be pulled somewhere.
Hadoop AWS | Staging committer Multipartupload not completing on minio,13450533,Resolved,Major,Invalid,17/Jun/22 05:35,17/Jun/22 09:25,3.3.1,"In Hadoop aws staging committer(org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter), Committer uploads files from local to s3(method- commitTaskInternal) which calls uploadFileToPendingCommit of CommitOperation to upload file using multipart upload.

 

Multipart upload consists of three steps-

1)Initialise multipartupload.

2) Breaks the file to part and upload Parts.

3) Merge all the parts of files and finalize multipart.

 

In the implementation of uploadFileToPendingCommit, first 2 steps are implemented. However, 3rd part is missing which leads to uploading the parts file but because it is not merged at the end of job no files are there in destination directory.

 

S3 logs before implement 3rd steps-

 
{code:java}
2022-05-30T13:49:31:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/processed/output-parquet-staging-7/part-00000-ce0a965f-622a-4950-bb4b-550470883134-c000-b552fb34-6156-4aa8-9085-679ad14fab6e.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               8.677ms      ↑ 137 B ↓ 724 B
2022-05-30T13:49:31:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/processed/output-parquet-staging-7/part-00000-ce0a965f-622a-4950-bb4b-550470883134-c000-b552fb34-6156-4aa8-9085-679ad14fab6e.snappy.parquet?uploadId=f3beae8e-3001-48be-9bc4-306b71940e50&partNumber=1  240b:c1d1:123:664f:c5d2:2::                443.156ms    ↑ 51 KiB ↓ 325 B
2022-05-30T13:49:32:000 [200 OK] s3.ListObjectsV2 localhost:9000/minio-feature-testing/?list-type=2&delimiter=%2F&max-keys=2&prefix=spark-job%2Fprocessed%2Foutput-parquet-staging-7%2F_SUCCESS%2F&fetch-owner=false  240b:c1d1:123:664f:c5d2:2::                3.414ms      ↑ 137 B ↓ 646 B
2022-05-30T13:49:32:000 [200 OK] s3.PutObject localhost:9000/minio-feature-testing/spark-job/processed/output-parquet-staging-7/_SUCCESS 240b:c1d1:123:664f:c5d2:2::                52.734ms     ↑ 8.7 KiB ↓ 380 B
2022-05-30T13:49:32:000 [200 OK] s3.DeleteMultipleObjects localhost:9000/minio-feature-testing/?delete  240b:c1d1:123:664f:c5d2:2::                73.954ms     ↑ 350 B ↓ 432 B
2022-05-30T13:49:32:000 [404 Not Found] s3.HeadObject localhost:9000/minio-feature-testing/spark-job/processed/output-parquet-staging-7/_temporary 240b:c1d1:123:664f:c5d2:2::                2.658ms      ↑ 137 B ↓ 291 B
2022-05-30T13:49:32:000 [200 OK] s3.ListObjectsV2 localhost:9000/minio-feature-testing/?list-type=2&delimiter=%2F&max-keys=2&prefix=spark-job%2Fprocessed%2Foutput-parquet-staging-7%2F_temporary%2F&fetch-owner=false  240b:c1d1:123:664f:c5d2:2::                 4.807ms      ↑ 137 B ↓ 648 B
2022-05-30T13:49:32:000 [200 OK] s3.ListMultipartUploads localhost:9000/minio-feature-testing/?uploads&prefix=spark-job%2Fprocessed%2Foutput-parquet-staging-7%2F  240b:c0e0:102:553e:b4c2:2::               1.081ms      ↑ 137 B ↓ 776 B
2022-05-30T13:49:32:000 [404 Not Found] s3.HeadObject localhost:9000/minio-feature-testing/spark-job/processed/output-parquet-staging-7/.spark-staging-ce0a965f-622a-4950-bb4b-550470883134 240b:c1d1:123:664f:c5d2:2::                 5.68ms       ↑ 137 B ↓ 291 B
2022-05-30T13:49:32:000 [200 OK] s3.ListObjectsV2 localhost:9000/minio-feature-testing/?list-type=2&delimiter=%2F&max-keys=2&prefix=spark-job%2Fprocessed%2Foutput-parquet-staging-7%2F.spark-staging-ce0a965f-622a-4950-bb4b-550470883134%2F&fetch-owner=false  240b:c1d1:123:664f:c5d2:2::              2.452ms      ↑ 137 B ↓ 689 B
  {code}
Here , After s3.PutObjectPart there is no completeMultipartupload call for 3rd step.

 

S3 logs after implement 3rd steps-

 
{code:java}
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.116ms      ↑ 137 B ↓ 750 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D45/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.416ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D45/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               8.506ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.815ms      ↑ 137 B ↓ 750 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D30/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               10.09ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.851ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D17/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.006ms      ↑ 137 B ↓ 750 B
2022-06-17T10:56:12:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               9.217ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D45/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=7da87f0a-f8ff-4f9c-b877-b2fdd18d3c5f&partNumber=1  240b:c1d1:123:664f:c5d2:2::               817.474ms    ↑ 52 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=782769d0-43f1-43b8-aae0-54ac4c8c6603&partNumber=1  240b:c1d1:123:664f:c5d2:2::               818.363ms    ↑ 85 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D17/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=2c509073-e2b6-4d0a-a65a-bb4f154a432c&partNumber=1  240b:c1d1:123:664f:c5d2:2::               819.765ms    ↑ 54 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=c7e09609-6193-4d41-bc05-4020291725e4&partNumber=1  240b:c1d1:123:664f:c5d2:2::               818.782ms    ↑ 55 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=3bb4278e-455a-4dc4-af01-ed3227430590&partNumber=1  240b:c1d1:123:664f:c5d2:2::               817.97ms     ↑ 51 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=8fe799e3-c712-43b7-a074-a2359232de07&partNumber=1  240b:c1d1:123:664f:c5d2:2::               819.183ms    ↑ 80 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D45/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=c2e1477b-5457-4cbe-8fdb-4e80eaca63fe&partNumber=1  240b:c1d1:123:664f:c5d2:2::               818.126ms    ↑ 53 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.PutObjectPart localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D30/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=992167c8-fbde-4a0d-bd4d-5ce7ddd51a87&partNumber=1  240b:c1d1:123:664f:c5d2:2::               818.176ms    ↑ 56 KiB ↓ 325 B
2022-06-17T10:56:12:000 [200 OK] s3.CompleteMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D15/quarter%3D45/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=7da87f0a-f8ff-4f9c-b877-b2fdd18d3c5f  240b:c1d1:123:664f:c5d2:2::               632.761ms    ↑ 272 B ↓ 1.1 KiB
2022-06-17T10:56:13:000 [200 OK] s3.NewMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D17/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploads  240b:c1d1:123:664f:c5d2:2::               6.231ms      ↑ 137 B ↓ 751 B
2022-06-17T10:56:12:000 [200 OK] s3.CompleteMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D16/quarter%3D15/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=3bb4278e-455a-4dc4-af01-ed3227430590  240b:c1d1:123:664f:c5d2:2::               697.946ms    ↑ 272 B ↓ 1.1 KiB
2022-06-17T10:56:12:000 [200 OK] s3.CompleteMultipartUpload localhost:9000/minio-feature-testing/spark-job/pm-processed/output-parquet-staging-39/day%3D23/hour%3D17/quarter%3D0/part-00004-d0b529ca-112f-43f2-a7dd-44de4db6aa7f-dffa7213-d492-48f9-9e6a-fb08bc81ceeb.c000.snappy.parquet?uploadId=2c509073-e2b6-4d0a-a65a-bb4f154a432c  240b:c1d1:123:664f:c5d2:2::               714.377ms    ↑ 272 B ↓ 1.1 KiB
 {code}
 

 

Needs to be implement -

 

After uploadPart call and all upload id's are added to commitData, innerCommit should be called."
Provide a shim library for modern FS APIs,13449544,Open,Major,,10/Jun/22 16:48,,3.3.0,"Add a shim library to give libraries and applications built against hadoop 3.2 access to APIs and features in later versions, especially those delivering higher performance in cloud deployments. This will give them the ability to invoke those APIs when available, so gain from the work everyone has done. Key APIs are:

* openFile
* ByteBufferPositionedReadable
* Vectored IO

The library will either downgrade gracefully to existing code (openFile) or simply thrown UnsupportedException when invoked -but offer probes for every operation before invocation.

This module will compile against hadoop 3.2.0; it will be tested against that and later releases.

We can and should release this on a different schedule; though ideally we should issue releases in sync with new hadoop releases adding new supported API calls.

For that reason I think we could consider having separate git repository for it. Verifying that the shim works against hadoop PRs could actually become one of our regression tests -indeed, it should become one."
S3a should retry when being throttled by STS (assumed roles),13449454,Open,Major,,10/Jun/22 13:10,,3.3.3,"We ran into an issue where we were being throttled by AWS when reading from a bucket using the sts assume-role mechanism.

 

The stacktrace looks like this:

 
{code:java}
Caused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: Rate exceeded (Service: AWSSecurityTokenService; Status Code: 400; Error Code: Throttling; Request ID: 02f32511-418c-4b2a-96ef-2d7ba8dafab1; Proxy: null)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)    1654700598727
        at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)    1654700598727
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)    1654700598727
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)    1654700598727
        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1682)    1654700598727
        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1649)    1654700598727
        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1638)    1654700598727
        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:498)    1654700598727
        at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:467)    1654700598727
        at com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.newSession(STSAssumeRoleSessionCredentialsProvider.java:348)    1654700598727
        at com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.access$000(STSAssumeRoleSessionCredentialsProvider.java:44)    1654700598727
        at com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:93)    1654700598727
        at com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$1.call(STSAssumeRoleSessionCredentialsProvider.java:90)    1654700598727
        at com.amazonaws.auth.RefreshableTask.refreshValue(RefreshableTask.java:295)    1654700598727
        at com.amazonaws.auth.RefreshableTask.blockingRefresh(RefreshableTask.java:251)    1654700598727
        at com.amazonaws.auth.RefreshableTask.getValue(RefreshableTask.java:192)    1654700598727
        at com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider.getCredentials(STSAssumeRoleSessionCredentialsProvider.java:320)    1654700598727{code}

I read the code and from what I can see the Exception is being handled by S3AUtils here [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java#L240]

It does not further inspect the message and assumes that the 400 is indeed a bad request. Because of this it gets handled as a {color:#24292f}AWSBadRequestException{color} which then will lead to the request to fail instead of retry in the S3ARetryPolicy.

[https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java#L215-L217]

 

A better approach seems to be to look at the sub-type and message of the original exception and handle it as a back-off and retry by throwing a different exception than {color:#24292f}AWSBadRequestException{color}

 "
Move to Java 11,13445484,Open,Major,,17/May/22 17:57,,3.4.0,https://lists.apache.org/thread/h5lmpqo2tz7tc02j44qxpwcnjzpxo0k2
Fix failure of build of hadoop-hdfs-bkjournal in branch-2.10 without ptotoc in the PATH,13447372,Patch Available,Major,,29/May/22 09:28,,2.10.1,We still need protoc in branch-2.10 even after HADOOP-16598.
S3A audit log referrer header path contains '&',13446758,Open,Major,,25/May/22 09:46,,,"{code:java}
""https://audit.example.org/hadoop/1/op_mkdirs/028e8604-c244-44be-9c5f-dd070c3000d6-00000180/?op=op_mkdirs&p1=Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-aws/target/test-dir/6/testContextURI/test/hadoop/&*%23$%23$@234&pr=stevel&ps=b73b8f7d-9a44-4259-94c2-da1dd28a7706&id=028e8604-c244-44be-9c5f-dd070c3000d6-00000180&t0=1&fs=028e8604-c244-44be-9c5f-dd070c3000d6&t1=1&ts=1645621486477"" {code}
here 'p1' shouldn't contain '&', but in some cases it doesn't escape in the referrer header"
Visualizing the audit logs,13446769,Open,Major,,25/May/22 10:21,,,"Visualizing the audit logs on Zeppelin or Jupyter notebook with graphs, which help us in better analyzation."
Integrating the code into cloudstore,13446768,Open,Major,,25/May/22 10:21,,,"Converting the key-value pairs into different file formats like CSV, Avro, Parquet etc., so that we can make the logs readable and can perform required operations easily."
Converting key-value pairs into different file formats,13446767,Open,Major,,25/May/22 10:21,,,"Converting the key-value pairs into different file formats like CSV, Avro, Parquet etc., so that we can make the logs readable and can perform required operations easily."
Parsing the S3A audit logs into key-value pairs,13446766,Open,Major,,25/May/22 10:20,,,"Parsing the audit logs using regular expressions i.e, dividing them into key value pairs."
Rebase s3a prefetching feature branch on top of trunk,13445611,Resolved,Major,Fixed,18/May/22 10:29,30/May/22 16:54,,"This task tracks rebasing the prefetching feature branch on top of trunk.

Any PRs that should be in before rebase should be added as a linked ticket."
Fix reentrancy check in SFTPFileSystem.close(),13445317,Resolved,Major,Fixed,17/May/22 04:10,30/May/22 16:35,3.3.1,"{code:java}
@Override
public void close() throws IOException {
  if (closed.getAndSet(true)) {
    return;
  }
  try {
    super.close();
  } finally {
    if (connectionPool != null) {
      connectionPool.shutdown();
    }
  }
}
{code}
if you execute this method, the fs can not execute the deleteOnExit method because the fs is closed.

如果手动调用，sftp fs执行close方法关闭连接池，让jvm能正常退出，deleteOnExsist 将因为fs已关闭无法执行成功。如果不关闭，则连接池不会释放，jvm不能退出。

https://issues.apache.org/jira/browse/HADOOP-17528，这是3.2.0 sftpfilesystem的问题

 

Translated:
{quote}If it is called manually, sftp fs executes the close method to close the connection pool, so that the jvm can exit normally, deleteOnExist will fail to execute successfully because fs is closed. If it is not closed, the connection pool will not be released and the jvm cannot exit.
{quote}
 "
Fix Hadoop-Common JavaDoc Error on branch-3.3,13445628,Resolved,Major,Fixed,18/May/22 12:17,29/May/22 06:01,3.3.0,"Fix Hadoop-Common JavaDoc Error on branch-3.3.

In the PR([#4267|https://github.com/apache/hadoop/pull/4267]) of HADOOP-18224. Upgrade maven compiler plugin to 3.10.1, I found that hadoop-common has a lot of javadoc compilation errors. I fixed it on the trunk, hoping to backport these changes to branch 3.3. These changes will ensure that hadoop common moudle javadoc will pass in JDK11 compilation."
Hadoop 3.3.3 Spark write Mode.Overwrite breaks partitioned tables,13446582,Resolved,Major,Invalid,24/May/22 13:32,24/May/22 13:58,,"During testing Hadoop 3.3.3 with S3A with Versioning enabled ran into an issue where spark/hadoop tries to load the partitions that don't exist anymore

 
{noformat}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-19-7f082b1a75b6> in <module>
----> 1 test_load = spark.read.parquet(test_loc)

/usr/local/spark/python/pyspark/sql/readwriter.py in parquet(self, *paths, **options)
    456                        modifiedAfter=modifiedAfter)
    457 
--> 458         return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
    459 
    460     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,

/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-> 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    109     def deco(*a, **kw):
    110         try:
--> 111             return f(*a, **kw)
    112         except py4j.protocol.Py4JJavaError as e:
    113             converted = convert_exception(e.java_exception)

/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--> 326                 raise Py4JJavaError(
    327                     ""An error occurred while calling {0}{1}{2}.\n"".
    328                     format(target_id, ""."", name), value)

Py4JJavaError: An error occurred while calling o183.parquet.
: java.io.FileNotFoundException: No such file or directory: s3a://test/s32/singleday_parts_simple2/Part=TESTING_1
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2269)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1903)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)
	at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)
	at org.apache.spark.util.HadoopFSUtils$.$anonfun$listLeafFiles$7(HadoopFSUtils.scala:281)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)
	at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:271)
	at org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)
	at org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:833)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748){noformat}
There used to be data in the partition but after an write overwrite

 
{noformat}
% mc ls -r databcp/test/s32/singleday_parts_simple2/Part=TESTING_1/
% mc ls -r --versions databcp/test/s32/singleday_parts_simple2/Part=TESTING_1/
[2022-05-24 08:38:13 EDT]     0B STANDARD 89ca1b4d-492c-43fd-9f20-ac6ba5a7beaa v2 DEL part-00000-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-24 08:37:27 EDT]  81MiB STANDARD f9994b02-ee92-4518-aa73-db0c2c6b6d7f v1 PUT part-00000-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-20 11:12:35 EDT]     0B STANDARD 0bce060a-4293-4936-957f-2b139063e9d7 v2 DEL part-00000-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:11:50 EDT]  81MiB STANDARD 8f6b1310-4709-4312-be5d-0805f215b868 v1 PUT part-00000-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:07:03 EDT]     0B STANDARD b0418804-fcff-4edb-8bd0-afa04522d066 v2 DEL part-00000-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:05:38 EDT]  81MiB STANDARD 9acaa83f-1842-4e9f-b56a-a65a46d7eac4 v1 PUT part-00000-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:21:04 EDT]     0B STANDARD 54fb3801-8a59-4efd-9818-fc1403f4b5fd v2 DEL part-00000-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-20 11:15:27 EDT]  81MiB STANDARD c4bae059-6e32-47ed-b461-7f1cd9d78a71 v1 PUT part-00000-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-24 08:38:13 EDT]     0B STANDARD 5bf22dba-9e37-4036-a94e-6346ca65c86c v2 DEL part-00001-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-24 08:37:27 EDT]  81MiB STANDARD 82ce73b4-0716-4d7b-9301-5b248aa67063 v1 PUT part-00001-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-20 11:12:35 EDT]     0B STANDARD a357d608-edfb-4f2c-b753-6dc85556b427 v2 DEL part-00001-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:11:50 EDT]  81MiB STANDARD 4817edc0-d35b-4a30-9ec2-ad70c8bcd352 v1 PUT part-00001-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:07:03 EDT]     0B STANDARD daabd5ae-630f-45b7-bdf9-0ec1cd876157 v2 DEL part-00001-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:05:38 EDT]  81MiB STANDARD 63a33b13-3001-4b42-bc54-c648756e9543 v1 PUT part-00001-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:21:04 EDT]     0B STANDARD 55a8213d-b0b5-4724-b53f-4ccf71d15bb7 v2 DEL part-00001-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-20 11:15:27 EDT]  81MiB STANDARD 1cada789-4e16-4241-bbf0-22db5507657f v1 PUT part-00001-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-24 08:37:26 EDT]     0B STANDARD 16568a5a-544c-4622-83a3-d46dc871defd v2 DEL part-00002-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:21:13 EDT]  81MiB STANDARD a6e90fa8-e5dc-431e-a89b-30c45bea5884 v1 PUT part-00002-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:11:49 EDT]     0B STANDARD ea751b23-1b98-4ff5-8e43-b71417f8d8a8 v2 DEL part-00002-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:07:13 EDT]  81MiB STANDARD 8338839d-00c8-4dc6-826f-1131f44e5ff7 v1 PUT part-00002-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:15:25 EDT]     0B STANDARD cd38cdcb-177a-4f8a-9f91-f0ad3aea2815 v2 DEL part-00002-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-20 11:12:36 EDT]  81MiB STANDARD 7623ef59-79e1-4df5-a2c2-20f32deb9f28 v1 PUT part-00002-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-24 09:19:23 EDT]     0B STANDARD de2f8f8d-f74a-4bd0-814f-fe74e53f08bc v2 DEL part-00002-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:38:13 EDT]  81MiB STANDARD 66815d49-da22-44f6-b4dd-b8040fdfc120 v1 PUT part-00002-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:37:26 EDT]     0B STANDARD 48993698-d03f-4f60-af6e-a4923d8c09c6 v2 DEL part-00003-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:21:29 EDT]  81MiB STANDARD f2bfbe57-1241-470d-9e87-f34003fe9acb v1 PUT part-00003-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:11:49 EDT]     0B STANDARD 1d695c38-fc42-4cfa-97f0-5ad2d64256da v2 DEL part-00003-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:07:13 EDT]  81MiB STANDARD ab5f2f1b-78e7-409c-ab17-1f6bc2259c18 v1 PUT part-00003-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:15:25 EDT]     0B STANDARD 23c0b362-fe97-431f-a4c5-fdfe2c86826b v2 DEL part-00003-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-20 11:12:37 EDT]  81MiB STANDARD 2773bdb9-ae7e-49df-8d49-73ee0c9a4c3c v1 PUT part-00003-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-24 09:19:23 EDT]     0B STANDARD fd7d5309-4be8-4d5d-a4d6-daa6ed957ccd v2 DEL part-00003-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:38:14 EDT]  81MiB STANDARD 2646ddc1-a32c-41e1-89fa-16805cbdb0f7 v1 PUT part-00003-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet{noformat}"
Hadoop 3.3.3 Spark write Mode.Overwrite breaks partitioned tables,13446581,Resolved,Major,Invalid,24/May/22 13:32,24/May/22 13:58,,"During testing Hadoop 3.3.3 with S3A with Versioning enabled ran into an issue where spark/hadoop tries to load the partitions that don't exist anymore

 
{noformat}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-19-7f082b1a75b6> in <module>
----> 1 test_load = spark.read.parquet(test_loc)

/usr/local/spark/python/pyspark/sql/readwriter.py in parquet(self, *paths, **options)
    456                        modifiedAfter=modifiedAfter)
    457 
--> 458         return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
    459 
    460     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,

/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-> 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    109     def deco(*a, **kw):
    110         try:
--> 111             return f(*a, **kw)
    112         except py4j.protocol.Py4JJavaError as e:
    113             converted = convert_exception(e.java_exception)

/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--> 326                 raise Py4JJavaError(
    327                     ""An error occurred while calling {0}{1}{2}.\n"".
    328                     format(target_id, ""."", name), value)

Py4JJavaError: An error occurred while calling o183.parquet.
: java.io.FileNotFoundException: No such file or directory: s3a://test/s32/singleday_parts_simple2/Part=TESTING_1
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2269)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1903)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)
	at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)
	at org.apache.spark.util.HadoopFSUtils$.$anonfun$listLeafFiles$7(HadoopFSUtils.scala:281)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)
	at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:271)
	at org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)
	at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)
	at org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:833)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748){noformat}
There used to be data in the partition but after an write overwrite

 
{noformat}
% mc ls -r databcp/test/s32/singleday_parts_simple2/Part=TESTING_1/
% mc ls -r --versions databcp/test/s32/singleday_parts_simple2/Part=TESTING_1/
[2022-05-24 08:38:13 EDT]     0B STANDARD 89ca1b4d-492c-43fd-9f20-ac6ba5a7beaa v2 DEL part-00000-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-24 08:37:27 EDT]  81MiB STANDARD f9994b02-ee92-4518-aa73-db0c2c6b6d7f v1 PUT part-00000-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-20 11:12:35 EDT]     0B STANDARD 0bce060a-4293-4936-957f-2b139063e9d7 v2 DEL part-00000-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:11:50 EDT]  81MiB STANDARD 8f6b1310-4709-4312-be5d-0805f215b868 v1 PUT part-00000-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:07:03 EDT]     0B STANDARD b0418804-fcff-4edb-8bd0-afa04522d066 v2 DEL part-00000-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:05:38 EDT]  81MiB STANDARD 9acaa83f-1842-4e9f-b56a-a65a46d7eac4 v1 PUT part-00000-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:21:04 EDT]     0B STANDARD 54fb3801-8a59-4efd-9818-fc1403f4b5fd v2 DEL part-00000-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-20 11:15:27 EDT]  81MiB STANDARD c4bae059-6e32-47ed-b461-7f1cd9d78a71 v1 PUT part-00000-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-24 08:38:13 EDT]     0B STANDARD 5bf22dba-9e37-4036-a94e-6346ca65c86c v2 DEL part-00001-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-24 08:37:27 EDT]  81MiB STANDARD 82ce73b4-0716-4d7b-9301-5b248aa67063 v1 PUT part-00001-039c3462-a522-4f6c-8614-7fe2cdedc3af.c000.snappy.parquet
[2022-05-20 11:12:35 EDT]     0B STANDARD a357d608-edfb-4f2c-b753-6dc85556b427 v2 DEL part-00001-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:11:50 EDT]  81MiB STANDARD 4817edc0-d35b-4a30-9ec2-ad70c8bcd352 v1 PUT part-00001-355353a2-638a-4004-a6b2-e81b8e9f8960.c000.snappy.parquet
[2022-05-20 11:07:03 EDT]     0B STANDARD daabd5ae-630f-45b7-bdf9-0ec1cd876157 v2 DEL part-00001-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:05:38 EDT]  81MiB STANDARD 63a33b13-3001-4b42-bc54-c648756e9543 v1 PUT part-00001-7f486fe0-ec9e-451e-9b26-68d9dd253c2f.c000.snappy.parquet
[2022-05-20 11:21:04 EDT]     0B STANDARD 55a8213d-b0b5-4724-b53f-4ccf71d15bb7 v2 DEL part-00001-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-20 11:15:27 EDT]  81MiB STANDARD 1cada789-4e16-4241-bbf0-22db5507657f v1 PUT part-00001-89b521eb-447d-4354-90a5-2750157763ea.c000.snappy.parquet
[2022-05-24 08:37:26 EDT]     0B STANDARD 16568a5a-544c-4622-83a3-d46dc871defd v2 DEL part-00002-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:21:13 EDT]  81MiB STANDARD a6e90fa8-e5dc-431e-a89b-30c45bea5884 v1 PUT part-00002-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:11:49 EDT]     0B STANDARD ea751b23-1b98-4ff5-8e43-b71417f8d8a8 v2 DEL part-00002-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:07:13 EDT]  81MiB STANDARD 8338839d-00c8-4dc6-826f-1131f44e5ff7 v1 PUT part-00002-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:15:25 EDT]     0B STANDARD cd38cdcb-177a-4f8a-9f91-f0ad3aea2815 v2 DEL part-00002-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-20 11:12:36 EDT]  81MiB STANDARD 7623ef59-79e1-4df5-a2c2-20f32deb9f28 v1 PUT part-00002-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-24 09:19:23 EDT]     0B STANDARD de2f8f8d-f74a-4bd0-814f-fe74e53f08bc v2 DEL part-00002-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:38:13 EDT]  81MiB STANDARD 66815d49-da22-44f6-b4dd-b8040fdfc120 v1 PUT part-00002-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:37:26 EDT]     0B STANDARD 48993698-d03f-4f60-af6e-a4923d8c09c6 v2 DEL part-00003-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:21:29 EDT]  81MiB STANDARD f2bfbe57-1241-470d-9e87-f34003fe9acb v1 PUT part-00003-3afd5aa6-dd6c-4077-8e41-19354e26bd59.c000.snappy.parquet
[2022-05-20 11:11:49 EDT]     0B STANDARD 1d695c38-fc42-4cfa-97f0-5ad2d64256da v2 DEL part-00003-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:07:13 EDT]  81MiB STANDARD ab5f2f1b-78e7-409c-ab17-1f6bc2259c18 v1 PUT part-00003-44214757-070f-43de-9853-cebdfd9b6543.c000.snappy.parquet
[2022-05-20 11:15:25 EDT]     0B STANDARD 23c0b362-fe97-431f-a4c5-fdfe2c86826b v2 DEL part-00003-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-20 11:12:37 EDT]  81MiB STANDARD 2773bdb9-ae7e-49df-8d49-73ee0c9a4c3c v1 PUT part-00003-b8456920-9109-414f-880b-dbddc510f521.c000.snappy.parquet
[2022-05-24 09:19:23 EDT]     0B STANDARD fd7d5309-4be8-4d5d-a4d6-daa6ed957ccd v2 DEL part-00003-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet
[2022-05-24 08:38:14 EDT]  81MiB STANDARD 2646ddc1-a32c-41e1-89fa-16805cbdb0f7 v1 PUT part-00003-c01bad8a-f8f1-44fa-999a-d17d60d20c1a.c000.snappy.parquet{noformat}"
Fix newInstance() in Class has been deprecated,13446006,Open,Major,,20/May/22 08:40,,,
hadoop-client-runtime latest version 3.3.2 has security issues,13440545,Resolved,Major,Duplicate,19/Apr/22 23:19,20/Apr/22 11:36,3.3.2,"Currently in highest version of hadoop-client-runtime 3.3.2, there are following security vulnerabilities comes from dependencies:

com.fasterxml.jackson.core_jackson-databind in version 2.13.0, per [CVE-2020-36518|[https://nvd.nist.gov/vuln/detail/CVE-2020-36518],] need to be upgraded to 2.13.2.2.



commons-codec_commons-codec in version 1.11 per CODEC-134, need to be upgraded to 1.13 or higher

Thanks."
Update guava to 30.1.1-jre,13445444,Resolved,Major,Duplicate,17/May/22 14:14,17/May/22 14:15,,Update guava to 30.1.1-jre
Upgrade cglib to 3.3.0,13440470,Open,Major,,19/Apr/22 15:55,,3.3.2,Upgrade cglib to 3.3.0
make jackson v1 a runtime scope dependency,13438409,Resolved,Major,Duplicate,07/Apr/22 14:12,28/Apr/22 05:26,,"In trunk, jackson v1 is only needed as a transitive dependency of jersey-json (1.19).

The jackson v1 jars still appear in hadoop pom so it would be useful to make them 'runtime' scope so that no hadoop code can compile with links to this old code.

 

[https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-project/3.4.0-SNAPSHOT/hadoop-project-3.4.0-20220407.091529-1274.pom]

 

the linked PR is not needed if [https://github.com/apache/hadoop/pull/3988] is merged"
Bump aliyun-sdk-oss to 3.14.0,13440476,Open,Major,,19/Apr/22 16:19,,,Bump aliyun-sdk-oss to 3.14.0
create release docker builds failing with network issues retrieving artifacts,13440879,Open,Major,,21/Apr/22 09:19,,3.3.3,"the docker runs fail regularly when trying to retrieve artifacts off maven central due to (transient?) network errors


Two examples. 

{code}
[ERROR] Failed to execute goal on project hadoop-hdfs-httpfs: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdfs-httpfs:jar:3.3.3-SNAPSHOT: Failed to collect dependencies at com.googlecode.json-simple:json-simple:jar:1.1.1: Failed to read artifact descriptor for com.googlecode.json-simple:json-simple:jar:1.1.1: Could not transfer artifact com.googlecode.json-simple:json-simple:pom:1.1.1 from/to maven-central (https://repo1.maven.org/maven2): Connection reset -> [Help 1]
{code}

{code}
[ERROR] Failed to execute goal on project hadoop-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-common:jar:3.3.3-SNAPSHOT: The following artifacts could not be resolved: org.assertj:assertj-core:jar:3.12.2, org.apache.ant:ant:jar:1.10.11, com.google.protobuf:protobuf-java:jar:2.5.0, com.jcraft:jsch:jar:0.1.55, org.apache.curator:curator-recipes:jar:4.2.0, org.apache.sshd:sshd-core:jar:1.6.0, org.apache.ftpserver:ftpserver-core:jar:1.0.0, org.apache.ftpserver:ftplet-api:jar:1.0.0, org.apache.zookeeper:zookeeper:jar:tests:3.5.6, org.apache.commons:commons-compress:jar:1.21, org.codehaus.woodstox:stax2-api:jar:4.2.1, com.fasterxml.woodstox:woodstox-core:jar:5.3.0, com.squareup.okhttp3:mockwebserver:jar:3.7.0, com.squareup.okhttp3:okhttp:jar:3.7.0, com.squareup.okio:okio:jar:1.12.0, dnsjava:dnsjava:jar:2.1.7, org.xerial.snappy:snappy-java:jar:1.1.8.2, org.lz4:lz4-java:jar:1.7.1: Could not transfer artifact org.assertj:assertj-core:jar:3.12.2 from/to maven-central (https://repo1.maven.org/maven2): GET request of: org/assertj/assertj-core/3.12.2/assertj-core-3.12.2.jar from maven-central failed: Premature end of Content-Length delimited message body (expected: 4382102; received:
{code}"
Remove base and bucket overrides for endpoint in ITestS3ARequesterPays.java,13439451,Resolved,Major,Fixed,13/Apr/22 13:42,14/Apr/22 16:02,3.3.3,"If a user has set the endpoint for their bucket in a test environment, we should ignore that in a test that won't use the bucket that the endpoint is set for. In this case, we are using ""s3a://usgs-landsat/"" which is in the region us-west-2, and would fail if the user has explicitly set the endpoint to something else.

Example (I have set the endpoint to ap-south-1):
{code:java}
[ERROR] testRequesterPaysDisabledFails(org.apache.hadoop.fs.s3a.ITestS3ARequesterPays)  Time elapsed: 9.323 s  <<< ERROR!
org.apache.hadoop.fs.s3a.AWSRedirectException: getFileStatus on s3a://usgs-landsat/collection02/catalog.json: com.amazonaws.services.s3.model.AmazonS3Exception: The bucket is in this region: us-west-2. Please use this region to retry the request (Service: Amazon S3; Status Code: 301; Error Code: 301 Moved Permanently; Request ID: Z09V8PMEEN5PHDRZ; S3 Extended Request ID: B7KDQntCuVmLJAyXvuY4UNXjdUrgn3xd26n8u7ThueNNxvKas6g3RsXo7oxBcvHrpcous2L+Lbk=; Proxy: null), S3 Extended Request ID: B7KDQntCuVmLJAyXvuY4UNXjdUrgn3xd26n8u7ThueNNxvKas6g3RsXo7oxBcvHrpcous2L+Lbk=:301 Moved Permanently: The bucket is in this region: us-west-2. Please use this region to retry the request (Service: Amazon S3; Status Code: 301; Error Code: 301 Moved Permanently; Request ID: Z09V8PMEEN5PHDRZ; S3 Extended Request ID: B7KDQntCuVmLJAyXvuY4UNXjdUrgn3xd26n8u7ThueNNxvKas6g3RsXo7oxBcvHrpcous2L+Lbk=; Proxy: null)
 at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:233)
 at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:171)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3440)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3346)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4890)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$executeOpen$6(S3AFileSystem.java:1437)
 at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:543)
 at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:524)
 at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:445)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.executeOpen(S3AFileSystem.java:1435)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1409)
  
<......>{code}
CC: [~stevel@apache.org]  [~mthakur]  [~dannycjones]"
Fix ZStandardCompressor bytesRead meta when using BlockCompressorStream,13439125,Resolved,Major,Not A Bug,12/Apr/22 08:11,12/Apr/22 08:52,3.3.1,"{{There is a bug when we use ZStandardCompressor which wrapped by `BlockCompressorStream`. }}

The reason is the  `bytesRead` in ZStandardCompressor implement is always zero, which should be set to bytes length when call setInput.

!image-2022-04-12-16-14-37-105.png!

Thus I makes the `rawWriteInt` always write zero

!image-2022-04-12-16-11-10-443.png!"
Fix multiple_bindings warning about slf4j-reload4j,13437623,Resolved,Major,Fixed,04/Apr/22 12:54,06/Apr/22 05:17,,"Jar of slf4j-relad4j exists in libdirs of both common and hdfs.
{noformat}
$ ~/dist/hadoop-3.2.4-SNAPSHOT/bin/hdfs dfs -ls /
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/ext/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/ext/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-reload4j-1.7.35.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]


$ find ~/dist/hadoop-3.2.4-SNAPSHOT/ -name 'slf4j-reload4j-*.jar'
/home/centos/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar
/home/centos/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-reload4j-1.7.35.jar
{noformat}"
Public dataset class for S3A integration tests,13438141,Open,Minor,,06/Apr/22 10:16,,,"Introduction of PublicDatasetTestUtils as proposed previously in some of the ideas for refactoring S3A incrementally. Some of its responsibilities:

- Source of truth for getting URI based on public data set.
- Maybe keep the methods specific to their purpose where possible? We might need {{s3a://landsat-pds/scene_list.gz}} specifically for some tests, but other tests may just need a bucket with a bunch of keys.
- Introduce test assumptions about the S3 endpoint or AWS partition. If we’re not looking at 'aws' partition, skip test.

How might we make this generic for non-{{aws}} partition S3 or S3API-compatible object stores?

- Ideally allow for future extension to provide some easy ways to override the bucket if tester has an alternative source? I see ""fs.s3a.scale.test.csvfile"" already has a little bit of this.
- We could have something which takes a path to a hadoop XML config file; we'd have a default resource but the maven build could be pointed at another via a command line property. this file could contain all the settings for a test against a partition or internal s3-compatible store
"
Memory fragmentation in ChecksumFileSystem Vectored IO implementation.,13450477,Resolved,Minor,Won't Fix,16/Jun/22 21:07,14/Jul/22 20:35,3.4.0,"As we have implemented merging of ranges in the ChecksumFSInputChecker implementation of vectored IO api, it can lead to memory fragmentation. Let me explain by example.

 

Suppose client requests for 3 ranges. 

0-500, 700-1000 and 1200-1500.

Now because of merging, all the above ranges will get merged into one and we will allocate a big byte buffer of 0-1500 size but return sliced byte buffers for the desired ranges.

Now once the client is done reading all the ranges, it will only be able to free the memory for requested ranges and memory of the gaps will never be released for eg here (500-700 and 1000-1200).

 

Note this only happens for direct byte buffers."
ABFS: Fix FileSystemAlreadyExists error for AzureAD errors,13469352,Open,Minor,,30/Jun/22 04:46,,3.3.3,"FileSystemAlreadyExists error being thrown for internal 429 error code, AzureAD error"
Upgrade commons-io to 2.11.0,13450696,Resolved,Minor,Fixed,17/Jun/22 23:21,03/Aug/22 01:45,3.2.3,"Current version 2.8.0 is almost ~2 years old

Upgrading to new release to keep up for new features and bug fixes."
Log retry count while handling exceptions in RetryInvocationHandler,13437602,Resolved,Minor,Fixed,04/Apr/22 11:19,11/Apr/22 06:26,2.10.2,"As part of failure handling in RetryInvocationHandler, we log details of the Exception details with which API was invoked, failover attempts, delay.

For the purpose of better debugging as well as fine-tuning of retry params, it would be good to also log retry count that we already maintain in the Counter object."
Update BUILDING.txt,13440978,Resolved,Minor,Fixed,21/Apr/22 17:36,21/Apr/22 17:46,3.3.2,"update building.txt to match the docker build settings.

this patch has already gone in, just without a jira in its name
https://github.com/apache/hadoop/pull/3811"
Enhance WritableName to be able to return aliases for classes that use serializers,13441004,Resolved,Minor,Fixed,21/Apr/22 20:50,17/Feb/23 06:07,3.3.6,"WritableName allows users shim in aliases for writables, in the case where a SequenceFile was written with a Writable class that has since been renamed or moved to another package. However, this requires that the aliased class extend Writable. 

Separately it's possible to configure jobs with keys and values which don't actually extend Writable. Instead they are meant to be serialized/deserialized using the serialization classes defined in {{io.serializations}} config.

Unfortunately, the current implementation does not support these key/value classes. All we need to do to support this is remove the {{.asSubclass(Writable.class)}} as is already the case for the default."
"Document ""io.file.buffer.size"" must be greater than zero",13441115,Resolved,Minor,Fixed,22/Apr/22 10:39,26/Apr/22 18:54,3.4.0,"when the configuration file in the ""io.file.buffer.size"" field is set to a value less than or equal to zero, hdfs can start normally, but read and write data will have problems.


When the value is less than zero, the shell will throw the following exception:
{code:java}
hadoop@ljq1:~/hadoop-3.1.3-work/bin$ ./hdfs dfs -cat mapred
-cat: Fatal internal error
java.lang.NegativeArraySizeException: -4096
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)
        at org.apache.hadoop.fs.shell.Display$Cat.printToStdout(Display.java:101)
        at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:96)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:303)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:285)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:269)
        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:176)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:391){code}
When the value is equal to zero, the shell command will always block
{code:java}
hadoop@ljq1:~/hadoop-3.1.3-work/bin$ ./hdfs dfs -cat mapred
^Z
[2]+  Stopped                 ./hdfs dfs -cat mapred{code}
The description of the configuration file is not clear enough, it may make people think that set to 0 to enter the non-blocking mode.
 
{code:java}
<property>   
    <name>io.file.buffer.size</name>   
    <value>4096</value>   
    <description>The size of buffer for use in sequence files.   
    The size of this buffer should probably be a multiple of hardware   
    page size (4096 on Intel x86), and it determines how much data is   
    buffered during read and write operations.</description> 
</property>{code}
 
Considering that this value is uesd by hdfs and mapreduce frequently, we should make this value must be a number greater than zero.

 "
shutdownhookmanager should not be multithreaded (deadlock possible),13441186,Resolved,Minor,Fixed,22/Apr/22 15:34,13/Jul/22 11:44,2.10.1,"the ShutdownHookManager class uses an executor to run hooks to have a ""timeout"" notion around them. It does this using a single threaded executor. It can leads to deadlock leaving a never-shutting-down JVM with this execution flow:
 * JVM need to exit (only daemon threads remaining or someone called System.exit)
 * ShutdowHookManager kicks in
 * SHMngr executor start running some hooks
 * SHMngr executor thread kicks in and, as a side effect, run some code from one of the hook that calls System.exit (as a side effect from an external lib for example)
 * the executor thread is waiting for a lock because another thread already entered System.exit and has its internal lock, so the executor never returns.
 * SHMngr never returns
 * 1st call to System.exit never returns
 * JVM stuck

 

using an executor with a single thread does ""fake"" timeouts (the task keeps running, you can interrupt it but until it stumble upon some piece of code that is interruptible (like an IO) it will keep running) especially since the executor is a single threaded one. So it has this bug for example :
 * caller submit 1st hook (bad one that would need 1 hour of runtime and that cannot be interrupted)
 * executor start 1st hook
 * caller of the future 1st hook result timeout
 * caller submit 2nd hook
 * bug : 1 hook still running, 2nd hook triggers a timeout but never got the chance to run anyway, so 1st faulty hook makes it impossible for any other hook to have a chance to run, so running hooks in a single separate thread does not allow to run other hooks in parallel to long ones.

 

If we really really want to timeout the JVM shutdown, even accepting maybe dirty shutdown, it should rather handle the hooks inside the initial thread (not spawning new one(s) so not triggering the deadlock described on the 1st place) and if a timeout was configured, only spawn a single parallel daemon thread that sleeps the timeout delay, and then use Runtime.halt (which bypass the hook system so should not trigger the deadlock). If the normal System.exit ends before the timeout delay everything is fine. If the System.exit took to much time, the JVM is killed and so the reason why this multithreaded shutdown hook implementation was created is satisfied (avoding having hanging JVMs)

 

Had the bug with both oracle and open jdk builds, all in 1.8 major version. hadoop 2.6 and 2.7 did not have the issue because they do not run hooks in another thread

 

Another solution is of course to configure the timeout AND to have as many threads as needed to run the hooks so to have at least some gain to offset the pain of the dealock scenario

 

EDIT: added some logs and reproduced the problem. in fact it is located after triggering all the hook entries and before shutting down the executor. Current code, after running the hooks, creates a new Configuration object and reads the configured timeout from it, applies this timeout to shutdown the executor. I sometimes run with a classloader doing remote classloading, Configuration loads its content using this classloader, so when shutting down the JVM and some network error occurs the classloader fails to load the ressources needed by Configuration. So the code crash before shutting down the executor and ends up inside the thread's default uncaught throwable handler, which was calling System.exit, so got stuck, so shutting down the executor never returned, so does the JVM.

So, forget about the halt stuff (even if it is a last ressort very robust safety net). Still I'll do a small adjustement to the final executor shutdown code to be slightly more robust to even the strangest exceptions/errors it encounters."
tests in ITestS3AInputStreamPerformance are failing ,13444162,Resolved,Minor,Fixed,10/May/22 12:04,15/Jul/22 12:52,3.4.0,"The following tests are failing when prefetching is enabled:

testRandomIORandomPolicy - expects stream to be opened 4 times (once for every random read), but prefetching will only open twice. 

testDecompressionSequential128K - expects stream to be opened once, but prefetching will open once for each block the file has. landsat file used in the test has size 42MB, prefetching block size = 8MB, expected open count is 6.

 testReadWithNormalPolicy - same as above. 

testRandomIONormalPolicy - executes random IO, but with a normal policy. S3AInputStream will abort the stream and change the policy, prefetching handles random IO by caching blocks so doesn't do any of that. 

testRandomReadOverBuffer - multiple assertions failing here, also depends a lot on readAhead values, not very relevant for prefetching"
Remove lower limit on s3a prefetching/caching block size,13445863,Resolved,Minor,Fixed,19/May/22 15:46,02/Feb/23 18:45,3.4.0,"The minimum allowed block size currently is {{PREFETCH_BLOCK_DEFAULT_SIZE}} (8MB).
{code:java}
this.prefetchBlockSize = intOption(
                    conf, PREFETCH_BLOCK_SIZE_KEY, PREFETCH_BLOCK_DEFAULT_SIZE, PREFETCH_BLOCK_DEFAULT_SIZE);{code}
[https://github.com/apache/hadoop/blob/3aa03e0eb95bbcb066144706e06509f0e0549196/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L487-L488]

Why is this the case and should we lower or remove it?"
Fix failure of extracting JIRA id from commit message in git_jira_fix_version_check.py,13446330,Resolved,Minor,Fixed,23/May/22 09:00,26/May/22 03:32,2.10.3,"git_jira_fix_version_check.py is confused by commit message like {{""YARN-1151. Ability to configure auxiliary services from HDFS-based JAR files.""}} which contains both {{YARN-}} and {{{}HDFS-{}}}. The latter {{HDFS-}} is unexpectedly picked as JIRA issue id then 404 is thrown on accessing invalid URL like ""https://issues.apache.org/jira/rest/api/2/issue/HDFS-""."
Add in configuration option to enable prefetching,13446603,Resolved,Minor,Fixed,24/May/22 15:39,15/Jul/22 12:55,3.4.0,"Currently prefetching is enabled by default, we should instead add in a config option to enable it."
Remove unused Imports in Hadoop Common project,13447740,Resolved,Minor,Fixed,31/May/22 16:53,23/Jun/22 07:00,3.4.0,"h3. Optimize Imports to keep code clean
 # Remove any unused imports"
Remove Unnecessary semicolon ';' ,13449322,Resolved,Minor,Fixed,09/Jun/22 23:42,29/Jun/22 09:51,3.4.0,"while reading the code, I found a very tiny optimization point, part of the code contains 2 semicolons at the end, I will fix it. Because this change is simple, I fixed it in One JIRA.

{code:java}
private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();;
{code}
"
Remove WhiteBox in hadoop-kms module.,13449666,Resolved,Minor,Fixed,12/Jun/22 10:37,17/Jun/22 01:13,3.4.0,"WhiteBox is deprecated, try to remove this method in hadoop-kms."
Ensure build folder exists before writing checksum file.ProtocRunner#writeChecksums,13450422,Resolved,Minor,Fixed,16/Jun/22 13:23,12/Jul/22 11:16,3.3.3,Ensure build folder exists before writing checksum file.ProtocRunner#writeChecksums
Upgrade dependency-check-maven to 7.1.1,13450514,Resolved,Minor,Fixed,17/Jun/22 01:30,05/Jul/22 08:09,3.3.3,"The OWASP dependency-check-maven Plugin version has corrected various false positives in 7.1.1. We can upgrade to it.

https://github.com/jeremylong/DependencyCheck/milestone/45?closed=1"
Remove WhiteBox in hadoop-common module.,13450702,Resolved,Minor,Fixed,18/Jun/22 03:51,12/Sep/22 14:28,3.3.5,"WhiteBox is deprecated, try to remove this method in hadoop-common."
Improve S3A delegations token documentation,13469462,Resolved,Minor,Fixed,30/Jun/22 12:48,09/Jan/23 16:49,3.4.0,"The current [delegations token documentation|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/delegation_tokens.md] has some typos, this task tracks fixing those. "
Update google-gson to 2.9.0,13450694,Resolved,Minor,Fixed,17/Jun/22 23:07,23/Jun/22 00:23,3.3.5,"Update to the Gson 2.9.0 that has many [fixes|https://github.com/google/gson/releases/tag/gson-parent-2.9.0], and backward-compatible as long as Java 7+ is used.

Fixes CVE-2022-25647; Dos when unmarshalling data"
Update hadoop-vote to use HADOOP_RC_VERSION dir,13443707,Resolved,Minor,Fixed,06/May/22 18:48,16/May/22 14:36,3.4.0,The recent changes in release script requires a minor change in hadoop-vote to use Hadoop RC version dir before verifying signature and checksum of .tar.gz files.
Remove org.apache.hadoop.test#Whitebox Class,13448691,In Progress,Minor,,07/Jun/22 03:17,,,"org.apache.hadoop.test#Whitebox is marked as deprecated, I personally feel that this is unnecessary, which leads to a large number of junit test code appearing deprecated, and a large number of Warnings appear in the compilation, I checked the code and think the deprecated mark is unreasonable."
Do not perform a LIST call when creating a file,13448842,Resolved,Minor,Duplicate,07/Jun/22 16:35,08/Jun/22 10:57,3.3.3,"Hello,

We've noticed that when creating a file, which does not exist in S3, we see an extra LIST call gets issued to see if it's a directory (i.e. if key = ""bar"", it will issue an object list request for ""bar/""). 

Is this really necessary, shouldn't a HEAD request be sufficient to determine if it actually exists or not? As we're creating 1000s of files, this is quite expensive, as we're effectively doubling our costs for file creation. Curious if others have experienced similar or identical issues, or if there are any workarounds. 

[https://github.com/apache/hadoop/blob/516a2a8e440378c868ddb02cb3ad14d0d879037f/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L3359-L3369]

 

Thanks,

Sam"
Tune S3A storage class support,13449061,Open,Minor,,08/Jun/22 19:03,,3.3.5,"Followup to HADOOP-12020, with work/review from rebasing HADOOP-17833 atop it.

* Can we merge ITestS3AHugeFilesStorageClass into one of the existing test cases? just because it is slow...ideally we want as few of those as possible, even if by testing multiple things at the same we break the rules of testing.
* move setting the storage class into
setOptionalMultipartUploadRequestParameters and setOptionalPutRequestParameters
* both newPutObjectRequest() calls to set storage class
* docs to list the valid option strings. I had to delve into the AWS SDK to work them out

Once HADOOP-17833 is in, make this a new option something which can be explicitly used in createFile().
I've updated PutObjectOptions to pass a value around, and made sure it gets down to to the request factory. that leaves
* setting the storage class from the options {{CreateFileBuilder}}
* testing!
* doc update
"
s3a storage class reduced redundancy breaks s3 select tests,13450250,Open,Minor,,15/Jun/22 14:45,,3.3.5,"when you set your fs client to work with reduced redundancy, the s3 select tests fail

probably need to clear the storage class option on the bucket before running those suites

"
Update class names to be clear they belong to S3A prefetching,13469137,Resolved,Minor,Fixed,29/Jun/22 15:53,17/Aug/22 08:52,,"tune classnames, e.g S3InputStream -> S3ABufferedStream, S3Reader -> StoreBlockReader, S3File -> OpenS3File. I think we just want to get the S3 prefixes off as all too often that means an AWS SDK class, not something in our own code"
stream warns Not all bytes were read from the S3ObjectInputStream when closed,13443291,Resolved,Minor,Fixed,05/May/22 08:13,21/Jul/22 14:57,,Issue: [https://github.com/aws/aws-sdk-java/issues/1211] has resurfaced in the prefetching stream when it is closed before reading for blocks is complete. This can be fixed by draining the stream before closing 
Use async drain threshold to decide b/w async and sync draining,13444012,Resolved,Minor,Fixed,09/May/22 15:42,15/Jun/22 15:37,,"[https://github.com/apache/hadoop/pull/4294] introduces changes to drain the stream asynchronously. [https://github.com/apache/hadoop/pull/2584] introduced ASYNC_DRAIN_THRESHOLD, use this value to decide when to drain asynchronously. This can be done once the prefetching branch has been rebased."
Tests in ITestS3AOpenCost are failing,13445865,Resolved,Minor,Fixed,19/May/22 15:57,21/Jul/22 14:55,,"After rebasing, when prefetching is enabled testOpenFileLongerLength & testOpenFileShorterLength fail"
Warnings should not be shown on cli console when linux user not present on client,13451385,Resolved,Minor,Fixed,20/Jun/22 17:41,28/Jun/22 18:22,,"{noformat}
22/04/26 10:51:18 WARN security.ShellBasedUnixGroupsMapping: unable to return groups for user om
PartialGroupNameException The user name 'om' is not found. id: ‘om’: no such user
id: ‘om’: no such user

	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:291)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:215)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroupsSet(ShellBasedUnixGroupsMapping.java:123)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupSet(Groups.java:413)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:351)
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:300)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)
	at org.apache.hadoop.security.Groups.getGroupInternal(Groups.java:258)
	at org.apache.hadoop.security.Groups.getGroupsSet(Groups.java:230)
	at org.apache.hadoop.security.UserGroupInformation.getGroupsSet(UserGroupInformation.java:1760)
	at org.apache.hadoop.security.UserGroupInformation.getPrimaryGroupName(UserGroupInformation.java:1603)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getGroupName(BasicRootedOzoneClientAdapterImpl.java:992)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatusAdapterForVolume(BasicRootedOzoneClientAdapterImpl.java:1016)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneClientAdapterImpl.getFileStatus(BasicRootedOzoneClientAdapterImpl.java:574)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.getFileStatus(BasicRootedOzoneFileSystem.java:858)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:202)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2106)
	at org.apache.hadoop.fs.ozone.BasicRootedOzoneFileSystem.globStatus(BasicRootedOzoneFileSystem.java:915)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:353)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:250)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:104)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:328)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.ozone.OzoneFsShell.main(OzoneFsShell.java:83)
{noformat}"
"fsdatainputstreambuilder.md refers to hadoop 3.3.3, when it shouldn't",13446639,Resolved,Minor,Fixed,24/May/22 21:16,20/Jun/22 05:24,3.3.5,"fsdatainputstreambuilder.md refers to hadoop 3.3.3, when it means whatever ships off hadoop branch-3.3"
Add S3A configuration property for `no_proxy` hosts,13450426,Open,Minor,,16/Jun/22 13:40,,,"Seeing as there are configuration options for proxy host, port, username, and password, there should also be an option to be able to provide to the S3 client a list of hosts to not use the proxy for (i.e. `no_proxy`)

 

I'm happy to contribute the code, but figured I'd file a ticket first to see if this the hadoop community would be open to this idea or have any desire for this feature. "
S3a: allow custom retry policies,13449459,Open,Minor,,10/Jun/22 13:15,,3.3.3,"This is related to HADOOP-18285

It would be great if the retry policies were pluggable so that one could inject their own implementation to overcome exceptional cases not correctly covered by the current policy. Currently {color:#24292f}S3ARetryPolicy{color} is hard-wired and can not be replaced w/o heavy sub-classing gymnastics."
update os-maven-plugin to 1.7.0,13448193,Resolved,Minor,Fixed,02/Jun/22 18:13,06/Jun/22 13:18,3.3.3,"the os-maven-plugin we build with is 1.15; the release is up to 1.7.0 -update it.

when backporting this patch, YARN-11173 must be applied to keep yarn-csi in sync"
Rename CHANGES to CHANGELOG in branch-2.10,13448433,Resolved,Minor,Fixed,05/Jun/22 01:16,07/Jun/22 09:44,2.10.2,"The file names of changelog generated by releasedocmaker of Yetus was changed from CHANGES.md to CHANGELOG.md. Hyperlinks in the generated index.html of ""Changelog and Release Notes"" page were also affected. The CHANGES*.md generated in previous releases must be renamed to avoid 404."
impove import * In Hadoop Project,13447721,Resolved,Minor,Not A Problem,31/May/22 15:41,01/Jun/22 16:30,3.4.0,
Misleading method name in DistCpOptions,13441030,Resolved,Minor,Fixed,22/Apr/22 01:25,30/May/22 13:07,3.3.2,"Currently method in DistCpOptions withCRC was used as the following
```
withCRC(true) means check without crc
withCRC(false) means check with crc
```

which mislead the developer when we pass the paramter, we can rename the method to clear that.after that it should be:
```
withSkipCRC(true) means check without crc
withSkipCRC(false) means check with crc
```

so it will be more understandable.

 

 "
Remove duplicate locks in NetworkTopology,13445063,Open,Minor,,16/May/22 03:34,,,"During reading the hadoop NetworkTopology.java, I suspect there is a duplicate lock.

chooseRandom(line 532), and code is:
{code:java}
final int availableNodes;
if (excludedScope == null) {
  availableNodes = countNumOfAvailableNodes(scope, excludedNodes);
} else {
  netlock.readLock().lock();
  try {
    availableNodes = countNumOfAvailableNodes(scope, excludedNodes) -
        countNumOfAvailableNodes(excludedScope, excludedNodes);
  } finally {
    netlock.readLock().unlock();
  }
} {code}
All the places where called `chooseRandom` have the global read lock, so the internal read lock is duplicated."
s3a access point xml examples are wrong,13444813,Resolved,Minor,Fixed,13/May/22 10:24,27/May/22 23:54,3.3.2,"the examples of s3a access point bindings are wrong, as the .bucket prefix is missing


{code}
<property>
    <name>fs.s3a.sample-bucket.accesspoint.arn</name>
    <value> {ACCESSPOINT_ARN_HERE} </value>
    <description>Configure S3a traffic to use this AccessPoint</description>
</property>
{code}

the property should be fs.s3a.bucket.sample-bucket.accesspoint.arn
"
ZKDelegationTokenSecretManager should handle duplicate Token sequenceNums,13447257,Open,Minor,,27/May/22 16:08,,,"The ZKDelegationTokenSecretManager relies on the TokenIdentifier sequenceNumber to identify each Token in the ZK Store. It's possible for multiple TokenIdentifiers to share the same sequenceNumber, as this is an int that can overflow. 

The AbstractDelegationTokenSecretManager uses a Map<TokenIdent, DelegationTokenInformation> so all properties in the TokenIdentifier must match. ZKDelegationTokenSecretManager should follow the same logic."
Backport HADOOP-15033 to branch-2.10,13439729,Open,Minor,,14/Apr/22 22:38,,,"We need to backport this to cleanly backport HADOOP-15033

which is needed to make this backport HADOOP-12760 work properly"
ABFS: Two BlobCreated get triggered for writing one ABFS file,13443688,Open,Minor,,06/May/22 17:03,,3.1.1,"Using the new ABFS driver to write a file on ADLS gen storage account triggers 2 BlobCreated events in the Azure backend while we are expecting one.

Here is an example code snippet for creating a file (in scala):
{code:java}
import java.io._
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, RemoteIterator}
val conf = new Configuration()
val path = new Path(""abfss://container@some-ADLS-account.dfs.core.windows.net/test.txt"")
val fs = path.getFileSystem(conf)
val bs = new BufferedOutputStream(fs.create(path, true))
bs.write(""test"".getBytes(""UTF-8""))
bs.close() {code}"
Publishing maven package in com.hadoop namespace,13443435,Open,Minor,,05/May/22 17:33,,,"*I'm looking to get opinions on Twitter publishing the hadoop-lzo project on maven central in the com.hadoop namespace.*

Hey, I work in the open source office at Twitter, and we publish the [hadoop-lzo project|https://github.com/twitter/hadoop-lzo], which is used by a handful of folks. Some of you may know the history better than I, but this was an old project from Google Code that got migrated to Twitter's GitHub org years ago, and is currently published on our maven host at maven.twttr.com (and only there).

I don't know the full history of why it is only published there, but we've had a few incidents of that service being unavailable, [breaking builds for downstream users|https://github.com/twitter/hadoop-lzo/issues/148]. I'd like to remove this reliance on the maven.twttr.com host by publishing this on maven central.

Ideally, I'd like to do it in a way that requires the least amount of work for downstream users, which would mean using the existing group and project ID: com.hadoop.gplcompression.hadoop-lzo. The com.hadoop namespace is mostly unused on maven ([https://repo1.maven.org/maven2/com/hadoop/])... interestingly, only used by what I believe is another fork of the same compression library. But since hadoop.com is presumably owned by the Hadoop project (whois records are hidden, but it [appears to have always redirected|https://web.archive.org/web/*/hadoop.com] to the project website), I wanted to run this by someone associated with the project before we did this.

Is there any objection to us publishing this maven library into that namespace?  The alternative of course is to switch to a com.twitter namespace, but that would require more changes for downstream users."
Improve build performance of modules,13443001,Open,Minor,,03/May/22 17:12,,,"Hi, I'm now using {*}hadoop{*}. I found that the build time of the project is not very fast when I used  *mvn -T 1C install -DskipTests* command to build the project during Github Actions. So I try to speed up the build performance of the project. The goal here is to clean up dependencies between maven modules in the project in order to improve the build performance.

Thread usage before cleaning up module dependencies was this:

!image-2022-05-04-01-03-59-169.png|width=559,height=116!

Using *mvn dependency:analyze* command I managed to have an overview of the unused dependencies of every module. I focused only on the dependencies between modules. The following dependencies can be cleaned up:
{noformat}
org.apache.hadoop:hadoop-client-minicluster(hadoop-client-modules/hadoop-client-minicluster) -> org.apache.hadoop:hadoop-client-runtime(hadoop-client-modules/hadoop-client-runtime)
org.apache.hadoop:hadoop-client-runtime(hadoop-client-modules/hadoop-client-runtime) -> org.apache.hadoop:hadoop-client-api(hadoop-client-modules/hadoop-client-api)
org.apache.hadoop:hadoop-client-runtime(hadoop-client-modules/hadoop-client-runtime) -> org.apache.hadoop:hadoop-client(hadoop-client-modules/hadoop-client)
org.apache.hadoop:hadoop-dist(hadoop-dist) -> org.apache.hadoop:hadoop-client-check-test-invariants(hadoop-client-modules/hadoop-client-check-test-invariants){noformat}
After clean up, the build time has been reduced by *3min* when I use parallel build during Github Actions.
Thread usage after cleaning up module dependencies was this:

!image-2022-05-04-01-02-55-509.png|width=553,height=102!

Could you help me review this issue? I can submit a PR to improve build performance of the project.

Thank you very much for your attention.
Best regards."
Start Developing the UI,13439872,Open,Minor,,15/Apr/22 16:44,,,"Hi,

 

Please start developing the UI part."
Replace with HashSet/TreeSet constructor in Hadoop-common-project,13447277,Resolved,Trivial,Fixed,27/May/22 20:13,20/Jun/22 06:41,3.3.5,
Improve S3A committers documentation clarity,13451362,Resolved,Trivial,Fixed,20/Jun/22 15:13,19/Oct/22 12:10,3.3.5,"I recently was learning more about the S3A committers. I'm hoping to provide some improvements as someone who has recently read [this documentation|https://github.com/apache/hadoop/blob/1f157f802d2d6142d21482eaa86baf1bef458ed4/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/committers.md#L495] without fully understanding prior.

For instance, referencing different components more explicitly and adding pre-requisite info."
Fix typos in the definition of zstd,13448989,Resolved,Trivial,Duplicate,08/Jun/22 11:34,08/Jun/22 11:47,,"There are some typos in file *hadoop-common/pom.xml* and *ZStandardCompressor.c*
hadoop-common/pom.xml:
{code:xml|title=pom.xml|borderStyle=solid}
<require.ztsd>false</require.ztsd> 
<argument>/p:RequireZstd=${require.ztsd}</argument>
{code}
The correct print as follows:
{code:xml}
<require.zstd>false</require.zstd>
<argument>/p:RequireZstd=${require.zstd}</argument>
{code}
ZStandardCompressor.c:
{code:c|title=ZStandardCompressor.c|borderStyle=solid}
// Load the libztsd.so from disk 
{code}
The correct print as follows:
{code:c}
// Load the libzstd.so from disk
{code}"
list AWS SDK v2 libraries in LICENSE-binary,13544159,Resolved,Blocker,Duplicate,19/Jul/23 17:20,23/Aug/23 10:34,3.4.0,"LICENSE.binary needs to be updated to list all new jars, and remove all that are gone"
ZKDelegationTokenSecretManager causes ArithmeticException due to improper numRetries value checking,13544679,Resolved,Critical,Fixed,24/Jul/23 16:38,14/Sep/23 22:53,3.4.0,"h2. What happened

There is no value checking for parameter {{{}zk-dt-secret-manager.zkNumRetries{}}}. This may cause improper calculations and crashes the system like division by 0.
h2. Buggy code

In {{{}ZKDelegationTokenSecretManager.java{}}}, there is no value checking for {{numRetries}} which is passed directly in {{RetryNTimes}} constructor. When {{numRetries}} is mistakenly set to 0, the code would cause division by 0 and throw ArithmeticException to crash the system.
{noformat}
public ZKDelegationTokenSecretManager(Configuration conf) {
        ...
        int numRetries =
            conf.getInt(ZK_DTSM_ZK_NUM_RETRIES, ZK_DTSM_ZK_NUM_RETRIES_DEFAULT);
        builder =
            ...
                .retryPolicy(
                    new RetryNTimes(numRetries, sessionT / numRetries));
        ...{noformat}
h2. How to reproduce
 # set zk-dt-secret-manager.zkNumRetries=0
 # run org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations
 # You will see the following stack trace.

{noformat}
java.lang.RuntimeException: Could not Load ZK acls or auth: java.lang.ArithmeticException: / by zero
    at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.<init>(ZKDelegationTokenSecretManager.java:227)
    at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager.<init>(DelegationTokenManager.java:99)
    at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:120)
    at org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeOperations(TestZKDelegationTokenSecretManager.java:113)
        ...{noformat}
For an easy reproduction, run the reproduce.sh in the attachment. We are happy to provide a patch if this issue is confirmed."
S3A: V2 SDK client does not work with third-party store,13550382,Resolved,Critical,Fixed,12/Sep/23 14:58,12/Oct/23 16:48,3.4.0,"testing against an external store without specifying region now blows up because the region is queried off eu-west-1.

What are we do to here? require the region setting *which wasn't needed before? what even region do we provide for third party stores?
"
Direct class cast causes ClassCastException in TestRPC#testReaderExceptions,13544238,Open,Critical,,20/Jul/23 07:19,,,"h2. What happened:

Test {{TestRPC#testReaderExceptions}} assumes there is a RemoteException has been thrown and directly cast the root cause of {{ServiceException}} to {{{}RemoteException{}}}. But this cast may lead to ClassCastException when the root cause exception is not {{{}RemoteException{}}}.
h2. Buggy code:
{code:java}
@Test (timeout=30000)
public void testReaderExceptions() throws Exception {
      ...
      try {
        FakeRequestClass.exception = doDisconnect ? rseFatal : rseError;
        proxy.ping(null, newEmptyRequest());
        fail(reqName + "" didn't fail"");
      } catch (ServiceException e) {
        RemoteException re = (RemoteException)e.getCause();       // <--- Here the test assumes the root cause is RemoteException
        assertEquals(reqName, expectedIOE, re.unwrapRemoteException());
      } {code}
h2. How to reproduce:

(1) Set {{hadoop.security.authentication}} to {{kerberos}}
(2) Run test {{TestRPC#testReaderExceptions}}
h2. Stack trace:
{code:java}
java.lang.ClassCastException: class java.io.IOException cannot be cast to class org.apache.hadoop.ipc.RemoteException (java.io.IOException is in module java.base of loader 'bootstrap'; org.apache.had
oop.ipc.RemoteException is in unnamed module of loader 'app')                  
        at org.apache.hadoop.ipc.TestRPC.testReaderExceptions(TestRPC.java:1742)                                                                                                                       
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)                              
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)     
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)                                                                                          
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)                      
        at java.base/java.lang.Thread.run(Thread.java:829)   {code}
h2. Fix:

A better way to do is to firstly check whether the root cause is RemoteException, if not, fail the test rather than causing the CastClassException:


{code:java}
  try {
    FakeRequestClass.exception = doDisconnect ? rseFatal : rseError;
    proxy.ping(null, newEmptyRequest());
    fail(reqName + "" didn't fail"");
  } catch (ServiceException e) {
    if (e.getCause() instanceof RemoteException) {
      RemoteException re = (RemoteException)e.getCause();
      assertEquals(reqName, expectedIOE, re.unwrapRemoteException());
    } else {
      fail(reqName + "" didn't fail with RemoteException"");
    }
  } {code}"
ClassCastException in test TestRPC#testWrappedStopProxy,13543559,Open,Critical,,14/Jul/23 13:33,,,"h2. What happened:

In HCommon, test TestRPC#testWrappedStopProxy tries to cast org.apache.hadoop.ipc.WritableRpcEngine$Invoker to org.apache.hadoop.ipc.TestRPC$StoppedInvocationHandler and causes ClassCastException.
h2. Buggy code:

In TestRPC.java:
{code:java}
@Test
public void testWrappedStopProxy() throws IOException {
  StoppedProtocol wrappedProxy = RPC.getProxy(StoppedProtocol.class,
      StoppedProtocol.versionID, null, conf);
  StoppedInvocationHandler invocationHandler = (StoppedInvocationHandler)    // <--- Here causes ClassCastException
      Proxy.getInvocationHandler(wrappedProxy);
  ...
} {code}
h2. How to reproduce:

(1) Just directly run test TestRPC#testWrappedStopProxy.

You can use the reproduce.sh in the attachment to easily reproduce the bug.

We are happy to provide a patch if this issue is confirmed. "
NegativeArraySizeException thrown in FSOutputSummer.java given large file.bytes-per-checksum,13550601,Open,Critical,,14/Sep/23 00:38,,3.3.6,"Buffer size of FSOutputSummer equals to `file.bytes-per-checksum` times `BUFFER_NUM_CHUNKS`. A large `file.bytes-per-checksum` causes buffer size to overflow and crash with NegativeArraySizeException.

To reproduce:
1. set `file.bytes-per-checksum` to 238609295
2. `mvn surefire:test -Dtest=org.apache.hadoop.hdfs.TestDecommissionWithStriped#testFileSmallerThanOneStripe`

We created a PR that provides a fix which checks the buffer size is positive after multiplying `file.bytes-per-checksum` with `BUFFER_NUM_CHUNKS`"
LdapGroupsMapping crashes with NullPointerException while going up the group hierarchy ,13544320,Open,Critical,,20/Jul/23 16:36,,3.3.6,"h2. What happened:

When set {{hadoop.security.group.mapping.ldap.search.group.hierarchy.levels}} to a value larger than 0, {{goUpGroupHierarchy}} in {{org/apache/hadoop/security/LdapGroupsMapping.java}} may return a null {{{}groupResults{}}}and use it without checking null.
h2. Buggy code:
{noformat}
  void goUpGroupHierarchy(Set<String> groupDNs, int goUpHierarchy, Set<String> groups) throws NamingException {
    if (goUpHierarchy <= 0 || groups.isEmpty()) {
      return;
    }
    ...
    NamingEnumeration<SearchResult> groupResults = context.search(groupbaseDN, filter.toString(), SEARCH_CONTROLS);
    while (groupResults.hasMoreElements()) {          // <--- Here groupResults may be null
    ...
    }
    ...
  }{noformat}
h2. How to reproduce:

(1) Set {{hadoop.security.group.mapping.ldap.search.group.hierarchy.levels}} to 1
(2) Run test {{org.apache.hadoop.security.TestLdapGroupsMapping#testGetGroupsWithConnectionClosed}}
h2. Stack trace:
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.security.LdapGroupsMapping.goUpGroupHierarchy(LdapGroupsMapping.java:612)
        at org.apache.hadoop.security.LdapGroupsMapping.lookupGroup(LdapGroupsMapping.java:489)
        at org.apache.hadoop.security.LdapGroupsMapping.doGetGroups(LdapGroupsMapping.java:552)
        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:365){noformat}
For an easy reproduction, run the reproduce.sh in the attachment.


We also create a PR provides a fix by checking the groupResults is not Null before it is accessed, similar to what's done in `org.apache.hadoop.security.LdapGroupsMapping#lookupGroup`"
Missing null check when running doRun method,13545149,Open,Critical,,27/Jul/23 14:44,,3.3.3,"h2. What happened?

Got NullPointerException when running {{doRun}} method in {{{}ZKFailoverController.java{}}}.
h2. Where's the bug?

In line 258 of {{{}ZKFailoverController.java{}}},the code lacks a check to verify whether {{rpcServer}} is null or not.
{noformat}
private int doRun(String[] args)
    throws Exception {
      ...
    } catch (Exception e) {
          LOG.error(""The failover controller encounters runtime error: "", e);
          throw e;
    } finally {
      rpcServer.stopAndJoin();
      ...
    }{noformat}
As a result, when the configuration provides a null rpcServer, the {{rpcServer.stopAndJoin()}} operation will throw a NullPointerException.
It is essential to add a null check for the rpcServer parameter before using it.
h2. How to reproduce?

(1) set {{ipc.server.handler.queue.size}} to {{0}}
(2) run {{org.apache.hadoop.ha.TestZKFailoverController#testAutoFailoverOnLostZKSession}}
h2. Stacktrace
{noformat}
Caused by: java.lang.NullPointerException
    at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:258)
    at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63)
    at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)
    at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:177)
    at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:503)
    at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:177)
    at org.apache.hadoop.ha.MiniZKFCCluster$DummyZKFCThread.doWork(MiniZKFCCluster.java:301)
    at org.apache.hadoop.test.MultithreadedTestUtil$TestingThread.run(MultithreadedTestUtil.java:189){noformat}
For an easy reproduction, run the reproduce.sh in the attachment.

We are happy to provide a patch if this issue is confirmed."
Collision of config key name fs.viewfs.mounttable.default.name.key to other keys that specify the entry point to mount tables,13543557,Open,Critical,,14/Jul/23 13:06,,,"h2. What happened:

When manually set fs.viewfs.mounttable.default.name.key to default (the same as default value) in HCommon, test org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusWithMultipleWildCardMatches would fail.
But the test can pass if this parameter is not manually set in the configuration file.
h2. Where's the bug:

In the constructor of InodeTree, the tree attempts to get all the mount table entry points set by user in the configuration and process them one by one:
{code:java}
for (Entry<String, String> si : config) {       
    final String key = si.getKey();       
    if (!key.startsWith(mountTablePrefix)) {         
        continue;       
    }  
    
    gotMountTableEntry = true;       
    LinkType linkType;       
    String src = key.substring(mountTablePrefix.length());       
    ...
{code}
Here mountTablePrefix=""fs.viewfs.mounttable.default."". However, it just so happens that the name of the configuration users use to specify the default mount table is fs.viewfs.mounttable.default.name.key. Thus, if a user specifies the default mount table and uses InodeTree the name.key would be falsely parsed as the entry point to one of the mount tables, which would cause InodeTree to throw an exception since name.key is not a valid entry.
h2. Stack trace:
{code:java}
java.lang.RuntimeException: java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key    
        at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:470)
        at org.apache.hadoop.fs.viewfs.ViewFsTestSetup.setupForViewFsLocalFs(ViewFsTestSetup.java:88)
        at org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs.setUp(TestFcMainOperationsLocalFs.java:38){code}
h2. How to reproduce:

(1) Set fs.viewfs.mounttable.default.name.key to default
(2) Run test org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusWithMultipleWildCardMatches

You can use the reproduce.sh in the attachment to easily reproduce the bug.

We are happy to provide a patch if this issue is confirmed. "
Bad ipc.client.connection.idle-scan-interval.ms cause resource leaks,13543206,Open,Critical,,12/Jul/23 02:55,,,"When setting ipc.client.connection.idle-scan-interval.ms to a bad value (e.g. a negative value), Hadoop Server fails to schedule the idle connection scan task and causes resource leaks.

h2. Buggy code:
{code:java}
private void scheduleIdleScanTask() {
  ...
  TimerTask idleScanTask = new TimerTask(){
    @Override
    public void run() {
      ...
      try {
        closeIdle(false);
      } finally {
        // explicitly reschedule so next execution occurs relative
        // to the end of this scan, not the beginning
        scheduleIdleScanTask();
      }
    }
  };
  idleScanTimer.schedule(idleScanTask, idleScanInterval);   // <--- idleScanInterval is a negative value
}
{code}

In schedule, the task will not be scheduled if the delay is negative, which causes resource leaks due to unscheduled idleScanTask.
{code:java}
public void schedule(TimerTask task, long delay) {
    if (delay < 0)
        throw new IllegalArgumentException(""Negative delay."");
    sched(task, System.currentTimeMillis()+delay, 0);        // <-- the task will not be scheduled when delay is negative
}
{code}

h2. How to reproduce:
We can use the test org.apache.hadoop.ipc.TestIPC#testSocketLeak to check the resource leaks.
(1) Set ipc.client.connection.idle-scan-interval.ms to -1;
(2) Run test org.apache.hadoop.ipc.TestIPC#testSocketLeak
(3) You will see the following message (note that the number of leaked descriptors can vary from run to run):
{code}
java.lang.AssertionError: Leaked 142 file descriptors
        at org.junit.Assert.fail(Assert.java:89)
        at org.junit.Assert.assertTrue(Assert.java:42)
        at org.apache.hadoop.ipc.TestIPC.testSocketLeak(TestIPC.java:1155)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}

You can use the reproduce.sh in the attachment to easily reproduce the bug:

We are happy to provide a patch if this issue is confirmed. "
unnecessary NullPointerException encountered when starting HttpServer2 with prometheus enabled ,13544250,Open,Critical,,20/Jul/23 08:13,,3.3.3,"h2. What happened?

Attempt to start an {{HttpServer2}} failed due to an NPE thrown in {{{}MetricsSystemImpl{}}}.
h2. Where's the bug?

In line 1278 of {{{}HttpServer2{}}}, if the support for prometheus is enabled the server registers a prometheus sink:
{noformat}
        if (prometheusSupport) {
          DefaultMetricsSystem.instance()
              .register(""prometheus"", ""Hadoop metrics prometheus exporter"",
                  prometheusMetricsSink);
        }{noformat}
However, a problem is that if the MetricsSystemImpl returned by the DefaultMetricsSystem.instance has not been start nor init, the config of the metric system would be set to null, thus failing the nullity check at the start of MetricsSystemImpl.registerSink. A better way of handling this would be to check in advance if the metric system has been initialized and initialize it if it has not been initialized.
h2. How to reproduce?

(1) set hadoop.prometheus.endpoint.enabled to true

(2) run org.apache.hadoop.http.TestHttpServer#testHttpResonseContainsDeny
h2. Stacktrace
{noformat}
java.io.IOException: Problem starting http server
        ...
Caused by: java.lang.NullPointerException: config
    at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899)
    at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298)
    at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277)
    at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1279)
    ... 34 more{noformat}
For an easy reproduction, run the reproduce.sh in the attachment.

We are happy to provide a patch if this issue is confirmed."
ZKDTSM could be stuck when meet znode version overflow,13549494,Open,Critical,,03/Sep/23 16:21,,,"ZKDTSM could be stuck when meet znode (/zkdtsm/ZKDTSMRoot/ZKDTSMSeqNumRoot) version int overflow (2147483647). It can not recovery even restart Application which may include YARN Router, DFS Router, KMS and other modules who use zookeeper to manage Token. One solution (not very smooth) is delete this znode first and then restart Service.

The root cause is following code snippet and curator could not compatible with version overflow. I try to give a draft improvement at CURATOR-688. Welcome to any discussion if we could resolve it at Hadoop side smooth.

org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager#incrSharedCount

{code:java}
  private int incrSharedCount(SharedCount sharedCount, int batchSize)
      throws Exception {
    while (true) {
      // Loop until we successfully increment the counter
      VersionedValue<Integer> versionedValue = sharedCount.getVersionedValue();
      if (sharedCount.trySetCount(
          versionedValue, versionedValue.getValue() + batchSize)) {
        return versionedValue.getValue();
      }
    }
  }
{code}
"
Out of Memory when mistakenly set decay-scheduler.metrics.top.user.count to a large number,13544321,Open,Critical,,20/Jul/23 16:43,,,"h2. What happened:

When setting {{decay-scheduler.metrics.top.user.count}} to a large number, {{DecayRpcScheduler}} in Hcommon throws an out-of-memory exception due to inappropriate checking and handling.
Hcommon only checks the value should be larger than 0.
h2. Buggy code:

In DecayRpcScheduler.java
{noformat}
public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {
  ...
  topUsersCount =                                                                                      
    conf.getInt(DECAYSCHEDULER_METRICS_TOP_USER_COUNT,                                               
      DECAYSCHEDULER_METRICS_TOP_USER_COUNT_DEFAULT);    <<---- topUsersCount gets the config value                                          
  Preconditions.checkArgument(topUsersCount > 0,     <<--- Only checks for positivity                      
    ""the number of top users for scheduler metrics must be at least 1"");
  ...
}
private void addTopNCallerSummary(MetricsRecordBuilder rb) {                                           
  TopN topNCallers = getTopCallers(topUsersCount);    <<--- calls getTopCallers with n equals topUsersCount
  ...
}
private TopN getTopCallers(int n) {                                                                    
  TopN topNCallers = new TopN(n); <<--- starts an priorityQ with initial capacity n, causing out of memory
  ...
}{noformat}
h2. StackTrace:
{noformat}
java.lang.OutOfMemoryError: Java heap space                                                                    
        at java.base/java.util.PriorityQueue.<init>(PriorityQueue.java:172)                                    
        at java.base/java.util.PriorityQueue.<init>(PriorityQueue.java:139)                                    
        at org.apache.hadoop.metrics2.util.Metrics2Util$TopN.<init>(Metrics2Util.java:80)                      
        at org.apache.hadoop.ipc.DecayRpcScheduler.getTopCallers(DecayRpcScheduler.java:1002)                  
        at org.apache.hadoop.ipc.DecayRpcScheduler.addTopNCallerSummary(DecayRpcScheduler.java:982)            
        at org.apache.hadoop.ipc.DecayRpcScheduler.getMetrics(DecayRpcScheduler.java:935)                      
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:893)         
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:200)      
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:183)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:156)
        at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMB
eanServerInterceptor.java:329)
        at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServ
erInterceptor.java:315)
        at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:100)
        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:73)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:101)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:8
19)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:792)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:800)
        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:260){noformat}
h2. Reproduce:

(1) Set {{decay-scheduler.metrics.top.user.count}} to a large value, e.g., 1419140791
(2) Run a simple test that exercises this parameter, e.g. {{org.apache.hadoop.ipc.TestDecayRpcScheduler#testNPEatInitialization}}

 

For an easy reproduction, run the reproduce.sh in the attachment.

We are happy to provide a patch if this issue is confirmed."
Buggy ZKFCRpcServer constructor creates null object and crashes the rpcServer,13544156,Open,Critical,,19/Jul/23 16:44,,,"h2. What happened:

In ZKFailoverController.java, initRPC() function gets ZKFC RpcServer binding address and create a new ZKFCRpcServer object rpcServer. However rpcServer may be null when the ZKFCRpcServer constructor accepts a null policy provider and cause any later rpcServer usage a null pointer exception.
h2. Buggy code:

In ZKFailoverController.java
{code:java}
protected void initRPC() throws IOException {
  InetSocketAddress bindAddr = getRpcAddressToBindTo();
  LOG.info(""ZKFC RpcServer binding to {}"", bindAddr);
  rpcServer = new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());  // <-- Here getpolicyProvider might be null
}
{code}
ZKFCRpcServer() eventually calls refreshWithLoadedConfiguration() function below. This function directly use provider without check null and this turns out making rpcServer above to be a null object.

In ServiceAuthorizationManager.java
{code:java}
  @Private
  public void refreshWithLoadedConfiguration(Configuration conf, PolicyProvider provider) {
    ...
    // Parse the config file
    Service[] services = provider.getServices();   // <--- provider might be null here
    ... {code}
h2. How to trigger this bug:

(1) Set hadoop.security.authorization to true

(2) Run test org.apache.hadoop.ha.TestZKFailoverControllerStress#testRandomExpirations

(3) You will see the following stack trace:
{code:java}
java.lang.NullPointerException                                                          
        at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:258)                                                                                                              
        at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63)      
        at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)                                                                                                              
        at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:177)                                                                                                              
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:503)                                                                                                         
        at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:177)            
        at org.apache.hadoop.ha.MiniZKFCCluster$DummyZKFCThread.doWork(MiniZKFCCluster.java:301)   
        at org.apache.hadoop.test.MultithreadedTestUtil$TestingThread.run(MultithreadedTestUtil.java:189){code}
(4) The null pointer exception here is due to the null {{rpcServer}} object caused by the bug described above.

You can use the reproduce.sh in the attachment to easily reproduce the bug:

We are happy to provide a patch if this issue is confirmed. "
Out of Memory when mistakenly set io.file.buffer.size to a large number,13543561,Open,Critical,,14/Jul/23 14:26,,,"h2. What happened:

When setting io.file.buffer.size to a large number, BufferedIOStatisticsOutputStream in Hcommon throws an out-of-memory exception due to inappropriate checking and handling.
The config is used to initialize a file system by passing it as one of the parameters bufferSize.
h2. Buggy code:

In RawLocalFileSystem.java
{code:java}
private FSDataOutputStream create(Path f, boolean overwrite,
      boolean createParent, int bufferSize, short replication, long blockSize,
      Progressable progress, FsPermission permission) throws IOException {
  ...
  return new FSDataOutputStream(new BufferedIOStatisticsOutputStream(
    createOutputStreamWithMode(f, false, permission), bufferSize, true), <<--- creates a BufferedIOStatisticsOutputStream with bufferSize, often set to config io.file.buffer.size
    statistics);
} {code}
In BufferedIOStatisticsOutputStream.java:
{code:java}
public class BufferedIOStatisticsOutputStream extends BufferedOutputStream
  implements IOStatisticsSource, Syncable, StreamCapabilities {
  ...
  public BufferedIOStatisticsOutputStream(
      final OutputStream out,
      final int size,
      final boolean downgradeSyncable) {
    super(out, size); <<--- init the BufferedOutputStream with a huge buffer size
    ...
  }{code}
h2. StackTrace:
{code:java}
java.lang.OutOfMemoryError: Java heap space
        at java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75)
        at org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputSt
ream.java:78)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
        at org.apache.hadoop.fs.contract.ContractTestUtils.writeDataset(ContractTestUtils.java:183)
        at org.apache.hadoop.fs.contract.ContractTestUtils.writeDataset(ContractTestUtils.java:152)
        at org.apache.hadoop.fs.contract.AbstractContractRenameTest.expectRenameUnderFileFails(AbstractContract
RenameTest.java:335)
...{code}
h2. Reproduce:

(1) Set io.file.buffer.size to a large value, e.g., 2112001717
(2) Run a simple test that exercises this parameter, e.g. org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFile"
S3A: AWS SDK V2 Migration: stabilization and S3Express,13550223,Resolved,Major,Fixed,11/Sep/23 13:35,06/Dec/24 15:25,3.4.0,"The final stabilisation changes to the V2 SDK MIgration; those moved off the HADOOP-18073 JIRA so we can close that.

also adds support to Amazon S3 Express One Zone storage"
[JDK17] MiniYarnClusters don't launch in hadoop-aws integration tests,13548370,Resolved,Major,Not A Problem,23/Aug/23 10:46,14/Jan/25 16:54,3.4.0,"I've tried running hadoop-aws tests under java17; everything which tries to launch a MiniYarnCluster fails because google guice is trying to stuff in java.land module that is now forbidden

{code}
Caused by: java.lang.ExceptionInInitializerError: Exception com.google.inject.internal.cglib.core.$CodeGenerationException: java.lang.reflect.InaccessibleObjectException-->Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain) throws java.lang.ClassFormatError accessible: module java.base does not ""opens java.lang"" to unnamed module @7ee7980d [in thread ""Thread-109""]

{code}

short term fix is to add the params to the surefire and failsafe jvm launcher to allow access

{code}
--add-opens java.base/java.lang=ALL-UNNAMED

{code}

I don't know if updating guice will make it go away completely. if it doesn't then the history server itself needs to be launched with this

rather than just add an option for hadoop-aws, we ought to consider a general cross-module variable for junit.jvm.options which is set everywhere; the base impl is """" and a java profile could add the new stuff"
add options to disable range merging of vectored io,13547878,Resolved,Major,Done,18/Aug/23 14:15,09/Jan/25 11:22,3.3.5,"I'm seeing test failures in my PARQUET-2171 pr because assertions about the #of bytes read isn't holding -small files are being read and the vector range merging is pulling in the whole file.

```
[ERROR]   TestInputOutputFormat.testReadWriteWithCounter:338 bytestotal != bytesread expected:<5510> but was:<11020>
```

I think for parquet i will add an option to disable vector io, but really the filesystems which support it should allow for merging to be disabled"
Add ability to configure ConnectionTTL of http connections while creating S3 Client.,13546706,Resolved,Major,Fixed,09/Aug/23 17:54,25/Aug/23 17:54,3.3.6,"The option fs.s3a.connection.ttl sets the maximum time an idle connection may be retained in the http connection pool. 

A lower value: fewer connections kept open, networking problems related to long-lived connections less likely
A higher value: less time spent negotiating TLS connections when new connections are needed

"
AWS SDK V2 - Move to S3 Java async client,13549384,Open,Major,,01/Sep/23 08:49,,3.4.0,"With the upgrade, S3A now has two S3 clients the Java async client and the Java sync client.

Java async is required for the transfer manager.

Java sync is used for everything else. 

 

* Move all operations to use the Java async client and remove the sync client. 

* Provide option to configure java async client with the CRT HTTP client. 

* Create a new interface for S3Client operations, move them out of S3AFS. interface will take request and span, and return response.  

 "
Add a way to get the IOStatistics of active filesystems in long-lived processes,13546566,Open,Major,,08/Aug/23 17:27,,3.3.9,"we can configure abfs and s3a to print out the filesystem IOStats in close(), but this doesn't let us see the state of long-lived processes, including latencies, error rates, and other important information.

Proposed: we add a means by which all cached filesystems which provide IOStatistics can log them to a file/console *while the process continues to run*


* a configurable scheduled thread which enumerates all open filesystem instances, gets their iostats and if non empty dumps to a log...log configuration can set that to the console or elsewhere. Good: can run without intervention; bad: one more thread to manage.
* option to add a signal handler which will do this whenever a SIGUSR2 is raised. JVM signal handling is a bit of a brittle feature, so it should be off by default.
{{org.apache.hadoop.service.launcher.IrqHandler}} supports signal wire-up. good: gives us an equivalent of kill -quit to get a view of iostats. bad: you need to be on the machine"
remove/deprecate fs.s3a.multipart.purge,13549432,Open,Major,,01/Sep/23 16:36,,3.3.6,"the fs.s3a.multipart.purge option has been in for a long time, to help avoid running up costs from incomplete uploads, especially during testing.

but
* adds overhead on startup (list + delete)
* dangerous if one client has a very short lifespan; all active s3a committer jobs will be broken
* obsoleted by s3 lifecycle rules
* and ""hadoop s3guard uploads"" cli.

proposed: deprecate for the next release, delete after."
Support Overwrite Directory On Commit For S3A Committers,13546359,Open,Major,,07/Aug/23 12:28,,3.4.0,"The goal is to add a new kind of commit mechanism in which the destination directory is cleared off before committing the file.

*Use Case*

In case of dynamicPartition insert overwrite queries, The destination directory which needs to be overwritten are not known before the execution and hence it becomes a challenge to clear off the destination directory.

 

One approach to handle this is, The underlying engines/client will clear off all the destination directories before calling the commitJob operation but the issue with this approach is that, In case of failures while committing the files, We might end up with the whole of previous data being deleted making the recovery process difficult or time consuming.

 

*Solution*

Based on mode of commit operation either *INSERT* or *OVERWRITE* , During commitJob operations, The committer will map each destination directory with the commits which needs to be added in the directory and if the mode is *OVERWRITE* , The committer will delete the directory recursively and then commit each of the files in the directory. So in case of failures (worst case) The number of destination directory which will be deleted will be equal to the number of threads if we do it in multi-threaded way as compared to the whole data if it was done in the engine side."
[ABFS] Support VectorIO in ABFS Input Stream,13549948,Open,Major,,07/Sep/23 16:39,,3.3.9,"the hadoop vector IO APIs are supported in file;// and s3a://; there's a hive ORC patch for this and PARQUET-2171 adds it for parquet -after which all apps using the library with a matching hadoop version and the feature enabled will get a significant speedup.

abfs needs to support too, which needs support for parallel GET requests for different ranges"
"abfs getFileStatus(/) fails with ""Value for one of the query parameters specified in the request URI is invalid."", 400",13544857,Resolved,Major,Fixed,25/Jul/23 20:35,08/Aug/23 18:07,3.3.1,"I am using hadoop-azure-3.3.0.jar and have written code:
{code:java}
static final String ROOT_DIR = ""abfs://ssh-test-fs@sshadlsgen2.dfs.core.windows.net"",
Configuration config = new Configuration();        config.set(""fs.defaultFS"",ROOT_DIR);        config.set(""fs.adl.oauth2.access.token.provider.type"",""ClientCredential"");        config.set(""fs.adl.oauth2.client.id"","""");        config.set(""fs.adl.oauth2.credential"","""");        config.set(""fs.adl.oauth2.refresh.url"","""");        config.set(""fs.azure.account.key.sshadlsgen2.dfs.core.windows.net"",ACCESS_TOKEN);        config.set(""fs.azure.skipUserGroupMetadataDuringInitialization"",""true"");
	FileSystem fs = FileSystem.get(config);
	System.out.println( ""\nfs:'""+fs.toString()+""'"");
	FileStatus status = fs.getFileStatus(new Path(ROOT_DIR)); // !!! Exception in 3.3.1-*
	System.out.println( ""\nstatus:'""+status.toString()+""'"");
 {code}
It did work properly till 3.3.1. 

But in 3.3.1 it fails with exception:
{code:java}
Caused by: Operation failed: ""Value for one of the query parameters specified in the request URI is invalid."", 400, HEAD, https://sshadlsgen2.dfs.core.windows.net/ssh-test-fs?upn=false&action=getAccessControl&timeout=90  at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:218) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:181) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494) at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465) at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:179) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:942) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:924) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:846) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:507) {code}
I performed some research and found:

In hadoop-azure-3.3.0.jar we see:
{code:java}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore{
	...
	public FileStatus getFileStatus(final Path path) throws IOException {
	...
Line 604:		op = client.getAclStatus(AbfsHttpConstants.FORWARD_SLASH + AbfsHttpConstants.ROOT_PATH);
	...
	}
	...
} {code}
and this code produces REST request:
{code:java}
https://sshadlsgen2.dfs.core.windows.net/ssh-test-fs//?upn=false&action=getAccessControl&timeout=90
  {code}
There is finalizes slash in path part ""...ssh-test-fs{*}{color:#de350b}//{color}{*}?upn=false..."" This request does work properly.

But since hadoop-azure-3.3.1.jar till latest hadoop-azure-3.3.6.jar we see:
{code:java}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore {
	...
	public FileStatus getFileStatus(final Path path) throws IOException {
		...
				perfInfo.registerCallee(""getAclStatus"");
Line 846:       op = client.getAclStatus(getRelativePath(path));
		...
	}
	...
}
Line 1492:
private String getRelativePath(final Path path) {
	...
	return path.toUri().getPath();
} {code}
and this code prduces REST request:
{code:java}
https://sshadlsgen2.dfs.core.windows.net/ssh-test-fs?upn=false&action=getAccessControl&timeout=90 {code}
There is not finalizes slash in path part ""...ssh-test-fs?upn=false..."" It happens because the new code ""path.toUri().getPath();"" produces empty string.

This request fails with message:
{code:java}
Caused by: Operation failed: ""Value for one of the query parameters specified in the request URI is invalid."", 400, HEAD, https://sshadlsgen2.dfs.core.windows.net/ssh-test-fs?upn=false&action=getAccessControl&timeout=90
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:218)
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:181)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)
	at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:179)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:942)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:924)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:846)
	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:507) {code}
Such us it is for all hadoop-azure-3.3.*.jar versions which does use log4j 2.* not 1.2.17 we can't update using version

 

I attach a sample of Maven project to try: test_hadoop-azure-3_3_1-FileSystem_getFileStatus - Copy.zip"
ABFS: Misreporting Retry Count for Sub-sequential and Parallel Operations,13549108,Resolved,Major,Fixed,30/Aug/23 07:26,13/Nov/23 19:38,3.3.6,"There was a bug identified where retry count in the client correlation id was wrongly reported for sub-sequential and parallel operations triggered by a single file system call. This was due to reusing same tracing context for all such calls.
We create a new tracing context as soon as HDFS call comes. We keep on passing that same TC for all the client calls.

For instance, when we get a createFile call, we first call metadata operations. If those metadata operations somehow succeeded after a few retries, the tracing context will have that many retry count in it. Now when actual call for create is made, same retry count will be used to construct the headers(clientCorrelationId). Alhough the create operation never failed, we will still see retry count from the previous request.

Fix is to use a new tracing context object for all the network calls made. All the sub-sequential and parallel operations will have same primary request Id to correlate them, yet they will have their own tracing of retry count."
ABFS: Adding Server returned request id in Exception method thrown to caller.,13549154,Resolved,Major,Resolved,30/Aug/23 12:34,26/Feb/24 10:39,,"Each request made to Azure server has its unique ActivityId (rid) which is returned in response of the request whether is succeed or fails.
When a HDFS call fails due to an error from Azure service, An ABFSRestOperationException is throws to the caller. This task is to add a server returned activity id (rid) in the exception message which can be used to investigate the failure on service side."
ABFS: Adding Support for MD5 Hash based integrity verification of the request content during transport ,13551485,Resolved,Major,Done,21/Sep/23 05:12,26/Feb/24 10:39,,"Azure Storage Supports Content-MD5 Request Headers in Both Read and Append APIs.
Read: [Path - Read - REST API (Azure Storage Services) | Microsoft Learn|https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/read]
Append: [Path - Update - REST API (Azure Storage Services) | Microsoft Learn|https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update]

This change is to make client-side changes to support them. In Read request, we will send the appropriate header in response to which server will return the MD5 Hash of the data it sends back. On Client we will tally this with the MD5 hash computed from the data received.

In Append request, we will compute the MD5 Hash of the data that we are sending to the server and specify that in appropriate header. Server on finding that header will tally this with the MD5 hash it will compute on the data received. 

This whole Checksum Validation Support is guarded behind a config, Config is by default disabled because with the use of ""https"" integrity of data is preserved anyways. This is introduced as an additional data integrity check which will have a performance impact as well.

Users can decide if they want to enable this or not by setting the following config to *""true""* or *""false""* respectively. *Config: ""fs.azure.enable.checksum.validation""*"
ABFS: Fixing Behavior of a File System APIs on root path,13548973,Resolved,Major,Resolved,29/Aug/23 11:45,08/Nov/23 06:31,3.3.6,"Following HDFS Apis are failing when called on a root path.

{*}{*}{*}{*}{*}{*}
|FS Call|Status|Error thrown to caller|
|create()|Failing|Operation failed: ""The request URI is invalid."", 400, PUT, https://anujtesthns.dfs.core.windows.net/abfs-testcontainer-02076119-21ed-4ada-bcd0-14afaae54013/?resource=file&timeout=90, InvalidUri, ""The request URI is invalid. RequestId:1d23f8c2-d01f-0059-61b6-c60c24000000 Time:2023-08-04T09:29:55.4813818Z""|
|createNonRecursive()|Failing|Runtime Exception: java.lang.IllegalArgumentException: null path (This is occuring because getParentPath is null and getFileStatus is called on null)|
|setXAttr()|Failing|Operation failed: ""The request URI is invalid."", 400, HEAD, https://anujtesthns.dfs.core.windows.net/abfs-testcontainer-491399b3-c3d0-4568-9d4a-a26e0aa8f000/?upn=false&timeout=90|
|getXAttr()|Failing|Operation failed: ""The request URI is invalid."", 400, HEAD, https://anujtesthns.dfs.core.windows.net/abfs-testcontainer-491399b3-c3d0-4568-9d4a-a26e0aa8f000/?upn=false&timeout=91|

h2. important: xattr support was removed in HADOOP-19089; include that change when cherrypicking this"
[ABFS][Retry Policy] Using hadoop-common code to refractor Abfs Retry Policy Implementation,13546300,Open,Major,,07/Aug/23 07:15,,3.3.3,"AbfsRetryPolicy is an independent module in hadoop-azure code. There is a lot of reimplementation of a more generic and advanced retry policy already present in hadoop-common. AbfsRetryPolicy should either inherit or directly use io.retry.Retrypolicy instead of reimplementing the same functionalities again.

AbfsRetryPolicy has only two versions: exponential and static. These both along with many others are already present in io.retry.Retrypolicy.

Issue identified in this PR: [Hadoop-18759: [ABFS][Backoff-Optimization] Have a Static retry policy for connection timeout. by anujmodi2021 · Pull Request #5881 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/5881]"
ABFS: AbfsOutputStream doesnt close DataBlocks object.,13549109,Open,Major,,30/Aug/23 07:27,,3.3.4,"AbfsOutputStream doesnt close the dataBlock object created for the upload.

What is the implication of not doing that:
DataBlocks has three implementations:
 # ByteArrayBlock
 ## This creates an object of DataBlockByteArrayOutputStream (child of ByteArrayOutputStream: wrapper arround byte-arrray for populating, reading the array.
 ## This gets GCed.
 # ByteBufferBlock:
 ## There is a defined *DirectBufferPool* from which it tries to request the directBuffer.
 ## If nothing in the pool, a new directBuffer is created.
 ## the `close` method on the this object has the responsiblity of returning back the buffer to pool so it can be reused.
 ## Since we are not calling the `close`:
 ### The pool is rendered of less use, since each request creates a new directBuffer from memory.
 ### All the object can be GCed and the direct-memory allocated may be returned on the GC. What if the process crashes, the memory never goes back and cause memory issue on the machine.
 # DiskBlock:
 ## This creates a file on disk on which the data-to-upload is written. This file gets deleted in startUpload().close().

 

startUpload() gives an object of BlockUploadData which gives method of `toByteArray()` which is used in abfsOutputStream to get the byteArray in the dataBlock.

 

Method which uses the DataBlock object: https://github.com/apache/hadoop/blob/fac7d26c5d7f791565cc3ab45d079e2cca725f95/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java#L298"
Support Concurrent Writes With S3A Magic Committer,13543015,Resolved,Major,Fixed,10/Jul/23 13:34,21/Sep/23 04:51,3.4.0,"h2. warning: no guarantee of safe/consistent results due to non-atomic job commit

Important: There is no guarantee that concurrent jobs writing to the same table are safe. This is as true for the classic FileOutputCommitter as it is for the S3A Magic Committer.

Why not? Because neither of these committers (or any other which works on a file-by-file basis) has an atomic job commit operation. If two jobs commit at the same time, the results are *completely undefined*. And this may not be detected by the applications.

If you want safe, concurrent parallel writes, use a manifest-file format with applications able to handle commit conflicts.

h2. problem

There is a failure in the commit process when multiple jobs are writing to a s3 directory *concurrently* using {*}magic committers{*}.

This issue is closely related HADOOP-17318.

When multiple Spark jobs write to the same S3A directory, they upload files simultaneously using ""__magic"" as the base directory for staging. Inside this directory, there are multiple ""/job-some-uuid"" directories, each representing a concurrently running job.

To fix some preoblems related to concunrrency a property was introduced in the previous fix: ""spark.hadoop.fs.s3a.committer.abort.pending.uploads"". When set to false, it ensures that during the cleanup stage, finalizing jobs do not abort pending uploads from other jobs. So we see in logs this line: 
{code:java}
DEBUG [main] o.a.h.fs.s3a.commit.AbstractS3ACommitter (819): Not cleanup up pending uploads to s3a ...{code}
(from [AbstractS3ACommitter.java#L952|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java#L952])

However, in the next step, the {*}""__magic"" directory is recursively deleted{*}:
{code:java}
INFO  [main] o.a.h.fs.s3a.commit.magic.MagicS3GuardCommitter (98): Deleting magic directory s3a://my-bucket/my-table/__magic: duration 0:00.560s {code}
(from [AbstractS3ACommitter.java#L1112 |https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java#L1112]and [MagicS3GuardCommitter.java#L137)|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicS3GuardCommitter.java#L137)]

This deletion operation *affects the second job* that is still running because it loses pending uploads (i.e., "".pendingset"" and "".pending"" files).

The consequences can range from an exception in the best case to a silent loss of data in the worst case. The latter occurs when Job_1 deletes files just before Job_2 executes ""listPendingUploadsToCommit"" to list "".pendingset"" files in the job attempt directory previous to complete the uploads with POST requests.

To resolve this issue, it's important {*}to ensure that only the prefix associated with the job currently finalizing is cleaned{*}.

Here's a possible solution:
{code:java}
/**
 * Delete the magic directory.
 */
public void cleanupStagingDirs() {
  final Path out = getOutputPath();
 //Path path = magicSubdir(getOutputPath());
  Path path = new Path(magicSubdir(out), formatJobDir(getUUID()));

  try(DurationInfo ignored = new DurationInfo(LOG, true,
      ""Deleting magic directory %s"", path)) {
    Invoker.ignoreIOExceptions(LOG, ""cleanup magic directory"", path.toString(),
        () -> deleteWithWarning(getDestFS(), path, true));
  }
} {code}
 

The side effect of this issue is that the ""__magic"" directory is never cleaned up. However, I believe this is a minor concern, even considering that other folders such as ""_SUCCESS"" also persist after jobs end."
VectorIO API tuning/stabilization,13547881,In Progress,Major,,18/Aug/23 14:47,,3.3.9,"Changes needed to get the Vector IO code stable.

Specifically
* consistent behaviour across implementations
* broader testing
* resilience

+Ideally, abfs support. (s3a prefetching needs this too; see HADOOP-19144)

This work will be shaped by the experience of merging support into libraries and identifying issues/improvement opportunities

h2. the patches which MUST be cherrypicked from this to any branch with vector IO support are (currently)

* HADOOP-19098
* HADOOP-19204"
Performance improvement for DelegationTokenSecretManager,13547550,Resolved,Major,Fixed,16/Aug/23 12:30,16/May/24 04:32,3.4.0,"*Context:*

KMS depends on hadoop-common for DT management. Recently we were analysing one performance issue and following is out findings:
 # Around 96% (196 out of 200) KMS container threads were in BLOCKED state at following:
 ## *AbstractDelegationTokenSecretManager.verifyToken()*
 ## *AbstractDelegationTokenSecretManager.createPassword()* 
 # And then process crashed.

 
{code:java}
http-nio-9292-exec-200PRIORITY : 5THREAD ID : 0X00007F075C157800NATIVE ID : 0X2C87FNATIVE ID (DECIMAL) : 182399STATE : BLOCKED
stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.verifyToken(AbstractDelegationTokenSecretManager.java:474)
- waiting to lock <0x00000005f2f545e8> (a org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager)
at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.verifyToken(DelegationTokenManager.java:213)
at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:396)
at  {code}
All the 199 out of 200 were blocked at above point.

And the lock they are waiting for is acquired by a thread that was trying to createPassword and publishing the same on ZK.

 
{code:java}
stackTrace:
java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
at java.lang.Object.wait(Object.java:502)
at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1598)
- locked <0x0000000749263ec0> (a org.apache.zookeeper.ClientCnxn$Packet)
at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1570)
at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:2235)
at org.apache.curator.framework.imps.SetDataBuilderImpl$7.call(SetDataBuilderImpl.java:398)
at org.apache.curator.framework.imps.SetDataBuilderImpl$7.call(SetDataBuilderImpl.java:385)
at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93)
at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:382)
at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:358)
at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:36)
at org.apache.curator.framework.recipes.shared.SharedValue.trySetValue(SharedValue.java:201)
at org.apache.curator.framework.recipes.shared.SharedCount.trySetCount(SharedCount.java:116)
at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.incrSharedCount(ZKDelegationTokenSecretManager.java:586)
at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.incrementDelegationTokenSeqNum(ZKDelegationTokenSecretManager.java:601)
at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.createPassword(AbstractDelegationTokenSecretManager.java:402)
- locked <0x00000005f2f545e8> (a org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager)
at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.createPassword(AbstractDelegationTokenSecretManager.java:48)
at org.apache.hadoop.security.token.Token.<init>(Token.java:67)
at org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.createToken(DelegationTokenManager.java:183) {code}
We can say that this thread is slow and has blocked remaining all. But following is my observation:

 
 # verifyToken() and createPaswword() has been synchronized because one is reading the tokenMap and another is updating the map. If it's only to protect the map, then can't we simply use ConcurrentHashMap and remove the ""synchronized"" keyword. Because due to this, all reader threads ( to verifyToken()) are also blocking each other.
 # IN HA env, It is recommended to use ZK to store DTs. We know that CuratorFramework is thread safe.  ZKDelegationTokenSecretManager.incrementDelegationTokenSeqNum() only requires to be protected from concurrent execution and it should be protected using some other locks instead of ""this"". 
 # With these changes, verifyToken() and createPaswword() will not block each other. It will be blocked only at the time of updating the map.
 # Similarly other methods can also be considered but these two are critical.

I made these changes on my local and got the significant performance improvement. 

I request community to provide their input and if we agree, I can provide the patch. Please let me know if any other details are required.

Thanks. 

 "
S3A: Cut S3 Select,13545132,Resolved,Major,Fixed,27/Jul/23 12:39,30/Jan/24 19:44,3.4.0,"getting s3 select to work with the v2 sdk is tricky, we need to add extra libraries to the classpath beyond just bundle.jar. we can do this but

* AFAIK nobody has ever done CSV predicate pushdown, as it breaks split logic completely
* CSV is a bad format
* one-line JSON more structured but also way less efficient
ORC/Parquet benefit from vectored IO and work spanning the cluster.

accordingly, I'm wondering what to do about s3 select
# cut?
# downgrade to optional and document the extra classes on the classpath

Option #2 is straightforward and effectively the default. we can also declare the feature deprecated.

{code}

[ERROR] testReadLandsatRecordsNoMatch(org.apache.hadoop.fs.s3a.select.ITestS3SelectLandsat)  Time elapsed: 147.958 s  <<< ERROR!
java.io.IOException: java.lang.NoClassDefFoundError: software/amazon/eventstream/MessageDecoder
        at org.apache.hadoop.fs.s3a.select.SelectObjectContentHelper.select(SelectObjectContentHelper.java:75)
        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$select$10(WriteOperationHelper.java:660)
        at org.apache.hadoop.fs.store.audit.AuditingFunctions.lambda$withinAuditSpan$0(AuditingFunctions.java:62)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)

{code}

"
remove okhttp usage,13550398,Resolved,Major,Fixed,12/Sep/23 16:48,13/Oct/23 12:58,3.4.0,"* relates to HADOOP-18496
* simplifies the dependencies if hadoop doesn't use multiple 3rd party libs to make http calls
* okhttp brings in other dependencies like the kotlin runtime
* hadoop already uses apache httpclient in some places"
Tune/extend S3A http connection and thread pool settings,13552412,Resolved,Major,Fixed,29/Sep/23 10:32,29/Nov/23 15:24,3.4.0,"Increases existing pool sizes, as with server scale and vector
IO, larger pools are needed

  fs.s3a.connection.maximum 200
  fs.s3a.threads.max 96

Adds new configuration options for v2 sdk internal timeouts,
both with default of 60s:

  fs.s3a.connection.acquisition.timeout
  fs.s3a.connection.idle.time

All the pool/timoeut options are covered in performance.md

Moves all timeout/duration options in the s3a FS to taking
temporal units (h, m, s, ms,...); retaining the previous default
unit (normally millisecond)

Adds a minimum duration for most of these, in order to recover from
deployments where a timeout has been set on the assumption the unit
was seconds, not millis.

Uses java.time.Duration throughout the codebase;
retaining the older numeric constants in
org.apache.hadoop.fs.s3a.Constants for backwards compatibility;
these are now deprecated.

Adds new class AWSApiCallTimeoutException to be raised on
sdk-related methods and also gateway timeouts. This is a subclass
of org.apache.hadoop.net.ConnectTimeoutException to support
existing retry logic.

+ reverted default value of fs.s3a.create.performance to false; 
inadvertently set to true during testing.
"
ABFS: Add sendMs and recvMs for all AbfsHttpOperation calls,13549270,Open,Major,,31/Aug/23 10:33,,3.3.6,ABFS: Add sendMs and recvMs for all AbfsHttpOperation calls
Expect-100 JDK bug resolution: prevent multiple server calls,13549922,Resolved,Major,Fixed,07/Sep/23 11:28,21/Jan/24 19:15,,"This is inline to JDK bug: [https://bugs.openjdk.org/browse/JDK-8314978].

 
With the current implementation of HttpURLConnection if server rejects the “Expect 100-continue” then there will be ‘java.net.ProtocolException’ will be thrown from 'expect100Continue()' method.

After the exception thrown, If we call any other method on the same instance (ex getHeaderField(), or getHeaderFields()). They will internally call getOuputStream() which invokes writeRequests(), which make the actual server call. 




In the AbfsHttpOperation, after sendRequest() we call processResponse() method from AbfsRestOperation. Even if the conn.getOutputStream() fails due to expect-100 error, we consume the exception and let the code go ahead. So, we can have getHeaderField() / getHeaderFields() / getHeaderFieldLong() which will be triggered after getOutputStream is failed. These invocation will lead to server calls."
Guava version 32.0.1 bump to fix CVE-2023-2976 (hadoop-thirdparty PR#23),13546563,Resolved,Major,Fixed,08/Aug/23 16:59,04/Jan/24 09:43,thirdparty-1.2.0,Create the corresponding jira for hadoop-thirdparty PR#23.
Merge aws v2 upgrade feature branch into trunk,13544314,Resolved,Major,Fixed,20/Jul/23 16:22,11/Sep/23 13:32,3.4.0,"do the merge, with everything we need as a  blocker for that marked as a blocker of this task."
AWS SDK v2: make the v1 bridging support optional,13544319,Resolved,Major,Fixed,20/Jul/23 16:31,23/Aug/23 10:33,3.4.0,"The AWS SDK v2 code includes the v1 sdk core for plugin support of

* existing credential providers
* delegation token binding

I propose we break #2 and rely on those who have implemented to to upgrade. apart from all the needless changes the v2 SDK did to the api (why?) this is fairly straighforward

for #1: fix through reflection, retaining a v1 sdk dependency at test time so we can verify that the binder works. "
Add Labeler Github Action.,13544518,Resolved,Major,Fixed,22/Jul/23 12:18,24/Jul/23 22:11,3.4.0,"Add auto github lablel for ease of reviewing:

Pull Request Labeler Github Action Configuration: https://github.com/marketplace/actions/labeler"
Upgrade aws-java-sdk to 1.12.499+,13545408,Resolved,Major,Fixed,30/Jul/23 07:10,16/Aug/23 13:53,3.4.0,aws sdk versions < 1.12.499 uses a vulnerable version of netty and hence showing up in security CVE scans (CVE-2023-34462). The safe version for netty is 4.1.94.Final and this is used by aws-java-sdk:1.12.499+
Upgrade Okio to 3.4.0 due to CVE-2023-3635,13545844,Resolved,Major,Fixed,02/Aug/23 11:57,07/Aug/23 12:05,3.4.0,"Upgrade Okio to 3.4.0 due to CVE-2023-3635

GzipSource does not handle an exception that might be raised when parsing a malformed gzip buffer. This may lead to denial of service of the Okio client when handling a crafted GZIP archive, by using the GzipSource class.

CVSSv3 Score:- 7.5(High)

[https://nvd.nist.gov/vuln/detail/CVE-2023-3635] "
Enable dual-layer server-side encryption with AWS KMS keys (DSSE-KMS),13547529,Resolved,Major,Fixed,16/Aug/23 10:04,01/Nov/23 13:31,3.4.0,"Add support for DSSE-KMS

https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-dsse-encryption.html"
AWS SDK V2 - Upgrade SDK to 2.20.28 and restores multipart copy,13547706,Resolved,Major,Fixed,17/Aug/23 10:31,23/Aug/23 14:41,3.4.0,"With 2.20.121, the TM has MPU functionality. Upgrading to the latest version (2.20.28) will also solve the issue with needing to include the CRT dependency. "
AWS v2 SDK: fail meaningfully if legacy v2 signing requested,13548081,Resolved,Major,Fixed,21/Aug/23 13:39,12/Oct/23 16:49,3.4.0,"if the v2 signer is requested on an AWS SDKv2 build, you get an NPE. 

proposed: special recognition of  fs.s3a.signing-algorithm=S3SignerType  and fail with a message that it isn't supported



{code}
> bin/hadoop jar $CLOUDSTORE storediag -D fs.s3a.signing-algorithm=S3SignerType -w  s3a://stevel-london/

2023-08-21 14:36:41,229 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(137)) - Duration of Creating filesystem for s3a://stevel-london/: 0:00:357
java.lang.NullPointerException
        at org.apache.hadoop.fs.s3a.auth.SignerFactory.createSigner(SignerFactory.java:104)
        at org.apache.hadoop.fs.s3a.impl.AWSClientConfig.createClientConfigBuilder(AWSClientConfig.java:90)
        at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createClientOverrideConfiguration(DefaultS3ClientFactory.java:163)
        at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.configureClientBuilder(DefaultS3ClientFactory.java:147)
        at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:89)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:989)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:610)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3601)

{code}

"
AWS SDK V2 - AuditFailureExceptions aren't being translated properly,13548404,Resolved,Major,Fixed,23/Aug/23 14:38,11/Sep/23 13:31,3.4.0,"{{ITestS3AHugeFilesNoMultipart}} is failing because the {{AuditFailureException}} variant raised in the sdk handler is being wrapped as it makes its way back to the s3a code -but S3AUtiis.translateException() isn't looking at the inner cause.

looks like aws v2 sdk class {{.GenericMultipartHelper.handleException}} is wrapping an SdkException with a SdkClientException even though it is not needed.

we probably have to start looking at the inner cause of any exception during translation to see if that is also a AuditFailureException.

Filed https://github.com/aws/aws-sdk-java-v2/issues/4356"
Optimize the configuration and use of callqueue overflow trigger failover,13548735,Resolved,Major,Fixed,27/Aug/23 03:42,23/Oct/23 21:07,3.4.0,"In HADOOP-16268 implemented enable callqueue overflow trigger failover when RPC call queue is filled for hdfs router servers .

In order to configure and use parameters more conveniently, we are going to make the following optimizations:

* provide default properties for callqueue.overflow.trigger.failover such that if properties with port is not configured, we can fallback to default property (port-less)
* support refreshCallQueue to update callqueue.overflow.trigger.failover
* update callqueue.overflow.trigger.failover description information in core-site.xml
"
CURATOR-599 change broke functionality introduced in HADOOP-18139 and HADOOP-18709,13549002,Resolved,Major,Fixed,29/Aug/23 15:04,07/Sep/23 01:37,3.3.5,"[Curator PR#391 |https://github.com/apache/curator/pull/391/files#diff-687a4ed1252bfb4f56b3aeeb28bee4413b7df9bec4b969b72215587158ac875dR59] introduced a default method in the ZooKeeperFactory interface, hence the override of the 4-parameter NewZookeeper method in the HadoopZookeeperFactory class is not taking effect due to this. 

Proposing routing the 4-parameter method to a 5-parameter method, which instantiates the ZKClientConfig as the 5th parameter. This is a non-breaking change, as the ZKClientConfig is currently instantiated within the method."
ABFS: Change default from disk to bytebuffer for fs.azure.data.blocks.buffer,13549275,Resolved,Major,Fixed,31/Aug/23 11:35,09/Oct/23 15:52,3.3.6,"Change default from disk to bytebuffer for fs.azure.data.blocks.buffer.

Gathered from multiple workload runs, the presented data underscores a noteworthy enhancement in performance. The adoption of ByteBuffer for *reading operations* exhibited a remarkable improvement of approximately *64.83%* when compared to traditional disk-based reading. Similarly, the implementation of ByteBuffer for *write operations* yielded a substantial efficiency gain of about {*}60.75%{*}. These findings underscore the consistent and substantial advantages of integrating ByteBuffer across a range of workload scenarios."
Add some rpc related metrics to Metrics.md,13549468,Resolved,Major,Fixed,02/Sep/23 13:54,05/Sep/23 09:35,3.4.0,"Add some rpc related metrics to Metrics.md
such as 
# DeferredRpcProcessingTime
# RpcResponseTime
# RpcRequeueCalls"
S3A. createS3AsyncClient() always enables multipart,13550373,Resolved,Major,Fixed,12/Sep/23 14:08,15/Sep/23 14:45,3.4.0,DefaultS3ClientFactory.createS3AsyncClient() always creates clients with multipart enabled; if it is disabled in s3a config it should be disabled here and in the transfer manager
Negative timeout in ZKFailovercontroller due to overflow,13550933,Resolved,Major,Fixed,16/Sep/23 16:20,29/Oct/23 08:00,3.3.6,"Graceful fence timeout of FailoverController in ZKFailovercontroller equals to `ha.failover-controller.graceful-fence.rpc-timeout.ms` * 2. Since users are unaware of this calculation, it thus has risks of overflowing to a negative number if users set `ha.failover-controller.graceful-fence.rpc-timeout.ms` to a large value.
 
To reproduce:
1. set `ha.failover-controller.graceful-fence.rpc-timeout.ms` to 1092752431
2. run `mvn surefire:test -Dtest=org.apache.hadoop.ha.TestZKFailoverController#testGracefulFailoverFailBecomingStandby`
 
We create a PR that provides a fix by checking the timeout after multiplication is at least 0.
 "
"Improve s3a region handling, including determining from endpoint",13551076,Resolved,Major,Fixed,18/Sep/23 12:40,17/Oct/23 14:38,3.4.0,"S3A region logic improved for better inference and
to be compatible with previous releases

1. If you are using an AWS S3 AccessPoint, its region is determined
   from the ARN itself.
2. If fs.s3a.endpoint.region is set and non-empty, it is used.
3. If fs.s3a.endpoint is an s3.*.amazonaws.com url, 
   the region is determined by by parsing the URL 
   Note: vpce endpoints are not handled by this.
4. If fs.s3a.endpoint.region==null, and none could be determined
   from the endpoint, use us-east-2 as default.
5. If fs.s3a.endpoint.region=="""" then it is handed off to
   The default AWS SDK resolution process.

Consult the AWS SDK documentation for the details on its resolution
process, knowing that it is complicated and may use environment variables,
entries in ~/.aws/config, IAM instance information within
EC2 deployments and possibly even JSON resources on the classpath.
Put differently: it is somewhat brittle across deployments.
"
upgrade sshd-core due to CVEs,13550520,Resolved,Major,Fixed,13/Sep/23 11:54,21/Jan/24 00:23,3.4.0,"https://mvnrepository.com/artifact/org.apache.sshd/sshd-core

hadoop currently uses v1.6.0"
ipc.server.handler.queue.size missing from core-default.xml,13542870,Resolved,Major,Fixed,08/Jul/23 11:03,11/Jul/23 11:10,3.4.0,This config should be documented. It is necessary to help users gain a clearer understanding of the usage of this property.
Document missing a lot of properties in core-default.xml,13544095,Resolved,Major,Fixed,19/Jul/23 10:07,07/Aug/23 23:38,3.4.0,"!image-2023-07-19-18-02-23-700.png|width=600,height=333!

As the picture shows, there are many configs in core-site.xml, which should be documented."
Some properties are missing from hadoop-policy.xml.,13545687,Resolved,Major,Fixed,01/Aug/23 13:25,07/Aug/23 12:03,3.4.0," 
{noformat}
security.datanode.lifeline.protocol.acl
security.get.user.mappings.protocol.acl
security.reconfiguration.protocol.acl
security.refresh.callqueue.protocol.acl
security.refresh.generic.protocol.acl{noformat}
The properties above are missing from hadoop-policy.xml."
Close child file systems in ViewFileSystem when cache is disabled.,13543688,Resolved,Major,Fixed,16/Jul/23 15:04,21/Jul/23 11:30,3.3.6,"When the cache is configured to disabled (namely, `fs.viewfs.enable.inner.cache=false` and `fs.*.impl.disable.cache=true`), even if `FileSystem.close()` is called, the client cannot truly close the child file systems in a ViewFileSystem. This caused our long-running clients to constantly produce resource leaks."
s3a prefetch LRU cache eviction metric,13545034,Resolved,Major,Fixed,27/Jul/23 05:25,19/Oct/23 21:22,3.4.0,"Follow-up from HADOOP-18291:

Add new IO statistics metric to capture s3a prefetch LRU cache eviction."
upgrade to commons-compress 1.24.0 due to CVE,13550550,Resolved,Major,Fixed,13/Sep/23 15:02,16/Jan/24 09:28,3.4.0,Includes some important bug fixes including https://lists.apache.org/thread/g9lrsz8j9nrgltcoc7v6cpkopg07czc9 - CVE-2023-42503
Increase default batch size of ZKDTSM token seqnum to reduce overflow speed of zonde dataVersion.,13550997,Resolved,Major,Fixed,18/Sep/23 05:02,19/Sep/23 02:59,3.4.0,"By default, increase seq number by 100 each time to reduce overflow speed of znode dataVersion which is 32-integer now."
upgrade snappy-java to 1.1.10.4 due to CVE,13551924,Resolved,Major,Fixed,25/Sep/23 16:53,28/Sep/23 05:53,3.3.6,"follow up to HADOOP-18782

https://github.com/xerial/snappy-java/security/advisories/GHSA-55g7-9cwv-5qfv"
module-info classes from external dependencies appearing in uber jars,13552529,Resolved,Major,Fixed,30/Sep/23 12:05,14/Oct/23 09:00,3.3.6,"hadoop-client-minicluster and hadoop-client-runtime try unsuccessfully to exclude module-info classes from dependencies. Over time, more and more jars have these classes.

The module-info classes are causing issue with CI builds. Builds can fail if there are more than module-inf class that is not excluded.

It seems better to exclude them all, especially since they will be affected by shading anyway."
upgrade to commons-io 2.14.0,13552554,Resolved,Major,Fixed,30/Sep/23 23:40,05/Oct/23 20:28,3.3.6,The release contains some hardening of support in some areas
Install bats for building Hadoop on Windows,13545409,Open,Major,,30/Jul/23 07:19,,3.4.0,"We get the following error while building Hadoop on Windows (logs attached -  [^archive.zip] ) -

{code}
[INFO] --- maven-antrun-plugin:1.8:run (common-test-bats-driver) @ hadoop-common ---
[INFO] Executing tasks

main:
     [exec] 
     [exec] 
     [exec] ERROR: bats not installed. Skipping bash tests.
     [exec] ERROR: Please install bats as soon as possible.
     [exec] 
{code}

We need to install bats to fix this."
Install strings utility for git bash on Windows,13545413,Open,Major,,30/Jul/23 07:57,,3.4.0,"We get the following error while building Hadoop on Windows 10 -

{code}
[2023-07-28T07:16:22.389Z] ============================================================================
[2023-07-28T07:16:22.389Z] ============================================================================
[2023-07-28T07:16:22.389Z]                          Determining needed tests
[2023-07-28T07:16:22.389Z] ============================================================================
[2023-07-28T07:16:22.389Z] ============================================================================
[2023-07-28T07:16:22.389Z] 
[2023-07-28T07:16:22.389Z] 
[2023-07-28T07:16:22.389Z] (Depending upon input size and number of plug-ins, this may take a while)
[2023-07-28T07:20:59.610Z] /c/out/precommit/plugins.d/maven.sh: line 275: strings: command not found
{code}

We need to install the strings utility for git bash on Windows to fix this."
Add rpcRejectedByObserverCalls metric  to quantify the number of rejected RPCs by Observer NameNode,13550030,Open,Major,,08/Sep/23 11:23,,,"When observer Node's serverStateId  is too far behind  clientStateId will rejected client RPC request,

mabe we need add rpcRejectedByObserverCalls metric to RpcMetrics to quantify the number of rejected RPCs by Observer NameNode to help determine how well the server."
Java 17 Runtime Support,13550276,Open,Major,,11/Sep/23 21:26,,,"This JIRA feature aims to extend the Java runtime support for Hadoop to include Java 17 in addition to Java 8 and Java 11. Currently Hadoop version 3.3.6 supports Java 8 compile and Java 8/11 runtime. The goal is to make Hadoop compatible with Java 17 runtime as well.

The plan for this release is to allow Hadoop to default to Java 11/Java 17, while still providing the flexibility for customers to configure Hadoop to use Java 8, Java 11, or Java 17 based on their specific needs. This project's objectives include:
 # Certifying that Hadoop works seamlessly on Java 8/11/17 for common use cases.
 # Ensuring that running Hadoop on Java 11/17 does not disrupt other applications and libraries such as Spark, Hive, Flink, Presto/Trino, and HBase.

The decision to support Java 17 runtime is motivated by customer requests and significant performance improvements observed in downstream applications like Apache Hive and Apache Spark. The testing process encompasses unit tests, integration tests, and performance tests, as well as verifying the proper functioning of all Hadoop daemons with Java 17.


The project will address compile time issues across various Hadoop components, ensuring that Hadoop remains compatible with Java 17 throughout the entire codebase.

This ticket serves as a vital step in enhancing Hadoop's capabilities, providing customers with more choices and improved performance for their big data processing needs."
Pass additional arguments to YARN / Child JVM containers,13550715,Resolved,Major,Duplicate,14/Sep/23 15:57,27/Oct/23 17:58,,
Make Trash Policy pluggable for different FileSystems,13550504,Open,Major,,13/Sep/23 10:34,,,Add capability for Trash Policy to be pluggable using a property leveraging the schema of different filesystems.
DfsClientShmManager allocSlot hang on awaitUninterruptibly itself,13551898,Open,Major,,25/Sep/23 13:19,,3.3.1,"Maybe the same issue of  HADOOP-14451

Spark task Driver Thread hang on awaitUninterruptibly

thread stack like below:
{code:java}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitUninterruptibly(AbstractQueuedSynchronizer.java:1976)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.allocSlot(DfsClientShmManager.java:244)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.allocSlot(DfsClientShmManager.java:417)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.allocShmSlot(ShortCircuitCache.java:1006)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:535)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.create(ShortCircuitCache.java:786)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.fetchOrCreate(ShortCircuitCache.java:723)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getBlockReaderLocal(BlockReaderFactory.java:483)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:360)
org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
java.io.DataInputStream.read(DataInputStream.java:100)
java.nio.file.Files.copy(Files.java:2908)
java.nio.file.Files.copy(Files.java:3027)
sun.net.www.protocol.jar.URLJarFile$1.run(URLJarFile.java:220)
sun.net.www.protocol.jar.URLJarFile$1.run(URLJarFile.java:216)
java.security.AccessController.doPrivileged(Native Method)
sun.net.www.protocol.jar.URLJarFile.retrieve(URLJarFile.java:215)
sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:71)
sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)
sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)
sun.net.www.protocol.jar.JarURLConnection.getJarFile(JarURLConnection.java:89)
sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:944)
sun.misc.URLClassPath$JarLoader.access$800(URLClassPath.java:801)
sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:886)
sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:879)
java.security.AccessController.doPrivileged(Native Method)
sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:878)
sun.misc.URLClassPath$JarLoader.(URLClassPath.java:829)
sun.misc.URLClassPath$3.run(URLClassPath.java:575)
sun.misc.URLClassPath$3.run(URLClassPath.java:565)
java.security.AccessController.doPrivileged(Native Method)
sun.misc.URLClassPath.getLoader(URLClassPath.java:564)
sun.misc.URLClassPath.getLoader(URLClassPath.java:529)
sun.misc.URLClassPath.getNextLoader(URLClassPath.java:494) => holding Monitor(sun.misc.URLClassPath@929546251})
sun.misc.URLClassPath.findResource(URLClassPath.java:224)
java.net.URLClassLoader$2.run(URLClassLoader.java:572)
java.net.URLClassLoader$2.run(URLClassLoader.java:570)
java.security.AccessController.doPrivileged(Native Method)
java.net.URLClassLoader.findResource(URLClassLoader.java:569)
java.lang.ClassLoader.getResource(ClassLoader.java:1096)
java.lang.ClassLoader.getResource(ClassLoader.java:1091)
org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2809)
org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3081)
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3040)
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3013)
org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2893) => holding Monitor(org.apache.hadoop.conf.Configuration@1190238736})
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2875) => holding Monitor(org.apache.hadoop.conf.Configuration@1190238736})
org.apache.hadoop.conf.Configuration.get(Configuration.java:1225)
org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1279)
org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1686)
org.apache.hadoop.io.nativeio.NativeIO$POSIX.(NativeIO.java:334)
org.apache.hadoop.io.nativeio.NativeIO.initNative(Native Method)
org.apache.hadoop.io.nativeio.NativeIO.(NativeIO.java:831)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm.(ShortCircuitShm.java:469)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShm.(DfsClientShm.java:70)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.requestNewShm(DfsClientShmManager.java:181)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.allocSlot(DfsClientShmManager.java:251)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.allocSlot(DfsClientShmManager.java:417)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.allocShmSlot(ShortCircuitCache.java:1006)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:535)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.create(ShortCircuitCache.java:786)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.fetchOrCreate(ShortCircuitCache.java:723)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getBlockReaderLocal(BlockReaderFactory.java:483)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:360)
org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957) => holding Monitor(org.apache.hadoop.hdfs.DFSInputStream@59925309})
java.io.DataInputStream.read(DataInputStream.java:100)
java.nio.file.Files.copy(Files.java:2908)
java.nio.file.Files.copy(Files.java:3027)
sun.net.www.protocol.jar.URLJarFile$1.run(URLJarFile.java:220)
sun.net.www.protocol.jar.URLJarFile$1.run(URLJarFile.java:216)
java.security.AccessController.doPrivileged(Native Method)
sun.net.www.protocol.jar.URLJarFile.retrieve(URLJarFile.java:215)
sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:71)
sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)
sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)
sun.net.www.protocol.jar.JarURLConnection.getJarFile(JarURLConnection.java:89)
sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:944)
sun.misc.URLClassPath$JarLoader.access$800(URLClassPath.java:801)
sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:886)
sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:879)
java.security.AccessController.doPrivileged(Native Method)
sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:878)
sun.misc.URLClassPath$JarLoader.(URLClassPath.java:829)
sun.misc.URLClassPath$3.run(URLClassPath.java:575)
sun.misc.URLClassPath$3.run(URLClassPath.java:565)
java.security.AccessController.doPrivileged(Native Method)
sun.misc.URLClassPath.getLoader(URLClassPath.java:564)
sun.misc.URLClassPath.getLoader(URLClassPath.java:529)
sun.misc.URLClassPath.getNextLoader(URLClassPath.java:494) => holding Monitor(sun.misc.URLClassPath@929546251})
sun.misc.URLClassPath.access$100(URLClassPath.java:66)
sun.misc.URLClassPath$1.next(URLClassPath.java:276)
sun.misc.URLClassPath$1.hasMoreElements(URLClassPath.java:287)
java.net.URLClassLoader$3$1.run(URLClassLoader.java:604)
java.net.URLClassLoader$3$1.run(URLClassLoader.java:602)
java.security.AccessController.doPrivileged(Native Method)
java.net.URLClassLoader$3.next(URLClassLoader.java:601)
java.net.URLClassLoader$3.hasMoreElements(URLClassLoader.java:626)
sun.misc.CompoundEnumeration.next(CompoundEnumeration.java:45)
sun.misc.CompoundEnumeration.hasMoreElements(CompoundEnumeration.java:54)
org.aspectj.weaver.loadtime.ClassLoaderWeavingAdaptor.parseDefinitions(ClassLoaderWeavingAdaptor.java:282)
org.aspectj.weaver.loadtime.DefaultWeavingContext.getDefinitions(DefaultWeavingContext.java:130)
org.aspectj.weaver.loadtime.ClassLoaderWeavingAdaptor.initialize(ClassLoaderWeavingAdaptor.java:173)
org.aspectj.weaver.loadtime.Aj$ExplicitlyInitializedClassLoaderWeavingAdaptor.initialize(Aj.java:344)
org.aspectj.weaver.loadtime.Aj$ExplicitlyInitializedClassLoaderWeavingAdaptor.getWeavingAdaptor(Aj.java:349)
org.aspectj.weaver.loadtime.Aj$WeaverContainer.getWeaver(Aj.java:323)
org.aspectj.weaver.loadtime.Aj.preProcess(Aj.java:115) => holding Monitor(org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@879400492})
org.aspectj.weaver.loadtime.ClassPreProcessorAgentAdapter.transform(ClassPreProcessorAgentAdapter.java:51)
sun.instrument.TransformerManager.transform(TransformerManager.java:188)
sun.instrument.InstrumentationImpl.transform(InstrumentationImpl.java:428)
java.lang.ClassLoader.defineClass1(Native Method)
java.lang.ClassLoader.defineClass(ClassLoader.java:763)
java.lang.ClassLoader.defineClass(ClassLoader.java:642)
org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:244)
org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:236)
java.lang.ClassLoader.loadClass(ClassLoader.java:411) => holding Monitor(java.lang.Object@815422741})
java.lang.ClassLoader.loadClass(ClassLoader.java:357)
org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:291) => holding Monitor(org.apache.spark.sql.hive.client.IsolatedClientLoader@1509517497})
org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:492)
org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:352)
org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:71) => holding Monitor(org.apache.spark.sql.hive.HiveExternalCatalog@1343720469})
org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:70) {code}
Because 
{code:java}
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.requestNewShm(DfsClientShmManager.java:181){code}
calling allocSlot Twice , when the free slot was just 1

 "
hadoop distcp needs support to filter by file/directory attribute,13550451,Open,Major,,13/Sep/23 06:16,,3.3.6,"In some circumstances, we need to filter file/directory by file/directroy. For example, we need to filter out them by file modified time, isDir attrs, etc.

So, *should we introduce a new method  public boolean shouldCopy(CopyListingFileStatus fileStatus)* ? 
by this approach, we can introduce a more fluent way to do things than  public abstract boolean shouldCopy(Path path)."
[Java17] Create maven profile for running unit tests,13550719,Open,Major,,14/Sep/23 16:03,,,Added build plugin to use JAVA17_HOME for testing while using JDK 17 profile
Upgrade mockito to 4.11.0,13548316,Open,Major,,23/Aug/23 04:51,,3.3.6,Upgrading mockito in hadoop-project
Added yarn.app.mapreduce.container.jvm.jre17.args as a config,13550721,Open,Major,,14/Sep/23 16:26,,,
KMS server not running on Java 17 by default,13550720,Open,Major,,14/Sep/23 16:03,,,
Add JAVA 17 in hadoop-env.sh,13550718,Open,Major,,14/Sep/23 16:02,,,
Configure add opens in setup JVM launch command,13550717,Open,Major,,14/Sep/23 16:00,,,
Add argLine configuration to maven-surefire-plugin,13550716,Open,Major,,14/Sep/23 15:58,,,Child jvms spawned by the maven do not have add-opens command line options.
Recommended Docker config file missing environment variable,13549443,Open,Major,,01/Sep/23 20:09,,3.3.6,"Docker config is missing {*}HADOOP_HOME=/opt/hadoop{*}, docker environment referencing this variable and docker container cant run examples and behaves erratically

currently not set in environment and config uses it eg

MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=$HADOOP_HOME

but its not set in docker container"
S3ACachingInputStream.ensureCurrentBuffer(): lazy seek means all reads look like random IO,13547608,Open,Major,,16/Aug/23 17:16,,3.3.6,"noticed in HADOOP-18184, but I think it's a big enough issue to be dealt with separately.

# all seeks are lazy; no fetching is kicked off after an open
# the first read is treated as an out of order read, so cancels any active reads (don't think there are any) and then only asks for 1 block

{code}
    if (outOfOrderRead) {
      LOG.debug(""lazy-seek({})"", getOffsetStr(readPos));
      blockManager.cancelPrefetches();

      // We prefetch only 1 block immediately after a seek operation.
      prefetchCount = 1;
    }

{code}

* for any read fully we should prefetch all blocks in the range requested
* for other reads, we may want a bigger prefech count than 1, depending on: split start/end, file read policy (random, sequential, whole-file)
* also, if a read is in a block other than the current one, but which is already being fetched or cached, is this really an OOO read to the extent that outstanding fetches should be cancelled?

"
Upgrade guava due to CVE,13548117,Resolved,Major,Duplicate,21/Aug/23 23:57,22/Aug/23 09:34,3.3.6,"Update guava to 32.0.1 or higher due to CVE: [https://nvd.nist.gov/vuln/detail/CVE-2023-2976]

hadoop-shaded-guava 1.1.1 is currently using 30.1.1-jre per security scanning tools"
Upgrade protobuf to 3.15.0 or newer,13547000,Open,Major,,11/Aug/23 17:11,,3.3.5,"Hadoop includes a shaded version of protobuf-java (currently uses protobuf-java 3.7.1), however, [CVE-2021-22570|https://nvd.nist.gov/vuln/detail/CVE-2021-22570] is a HIGH vulnerability that can be fixed by upgrading to protobuf-java 3.15.0.

Please consider upgrading hadoop-shaded-protobuf to this newer version.

 

Relates to HADOOP-13363 and HADOOP-16821"
Rebuild Exceptions on Client side to get genuine exceptions,13544262,Open,Major,,20/Jul/23 09:03,,,"In current's RPC design, if Server sends an exception back, Client can only rebuild the exception according to the original exception's error message, if the exceptions has some fields containing important information, they will be discarded since we can not rebuild them based on message string easily.

This ticket is to introduce a new interface for Exceptions which supports reconstructing. If Clients want to rebuild the exception, they can just implement the methods and the reconstruction will be done automatically.

The interface uses String[] as parameter for simplicity.  I thought of using Protobuf to store all the exceptions or fields, but the generacity can not be perfectlly met. So we need Client to support it by accepting ""String[]"" and transform the String to it's original type."
Upgrade version of aws-java-sdk-bundle to 1.12.368 avoid verify error. ,13544294,Resolved,Major,Cannot Reproduce,20/Jul/23 13:00,30/Jul/23 15:19,3.3.6,"The compilation failed when I packaged through the maven command 

 
{code:java}
mvn clean install -DskipTests -Dtar -Pdist -Pnative  {code}
 

 

report an error: 

 
{code:java}
[WARNING]
Dependency convergence error for com.amazonaws:aws-java-sdk-simpleworkflow:1.12.367 paths to dependency are:
+-org.apache.hadoop:hadoop-aws:3.3.6
  +-com.amazonaws:aws-java-sdk-bundle:1.12.367
    +-com.amazonaws:aws-java-sdk:1.12.367
      +-com.amazonaws:aws-java-sdk-simpleworkflow:1.12.367
and
+-org.apache.hadoop:hadoop-aws:3.3.6
  +-com.amazonaws:aws-java-sdk-bundle:1.12.367
    +-com.amazonaws:aws-java-sdk:1.12.367
      +-com.amazonaws:aws-java-sdk-swf-libraries:1.11.22
        +-com.amazonaws:aws-java-sdk-simpleworkflow:1.11.22
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Failed while enforcing releasability. See above detailed error message. {code}
 
com.amazonaws:aws-java-sdk-swf-libraries are not required
 "
Hdfs client will easily to oom when enable hedged read,13545417,Open,Major,,30/Jul/23 08:34,,3.3.3,"In the same workload, when I disable hedged read, JVM heap is:

```bash

Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 32178700288 (30688.0MB)
   NewSize                  = 1363144 (1.2999954223632812MB)
   MaxNewSize               = 19306381312 (18412.0MB)
   OldSize                  = 5452592 (5.1999969482421875MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 4194304 (4.0MB)
Heap Usage:
G1 Heap:
   regions  = 7672
   capacity = 32178700288 (30688.0MB)
   used     = 794118192 (757.3301239013672MB)
   free     = 31384582096 (29930.669876098633MB)
   2.467837994986207% used
G1 Young Generation:
Eden Space:
   regions  = 177
   capacity = 1732247552 (1652.0MB)
   used     = 742391808 (708.0MB)
   free     = 989855744 (944.0MB)
   42.857142857142854% used
Survivor Space:
   regions  = 6
   capacity = 25165824 (24.0MB)
   used     = 25165824 (24.0MB)
   free     = 0 (0.0MB)
   100.0% used
G1 Old Generation:
   regions  = 7
   capacity = 1035993088 (988.0MB)
   used     = 26560560 (25.330123901367188MB)
   free     = 1009432528 (962.6698760986328MB)
   2.563777722810444% used

```

 

When I enable hedged read, it easily oom:

```bash

preadDirect: FSDataInputStream#read error:
OutOfMemoryError: Java heap spacejava.lang.OutOfMemoryError: Java heap space
preadDirect: FSDataInputStream#read error:
OutOfMemoryError: Java heap spacejava.lang.OutOfMemoryError: Java heap space
    at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
    at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
    at org.apache.hadoop.hdfs.DFSInputStream.hedgedFetchBlockByteRange(DFSInputStream.java:1292)
    at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1493)
    at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1705)
    at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:259)

```

 

```bash

Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 32178700288 (30688.0MB)
   NewSize                  = 1363144 (1.2999954223632812MB)
   MaxNewSize               = 19306381312 (18412.0MB)
   OldSize                  = 5452592 (5.1999969482421875MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 4194304 (4.0MB)
Heap Usage:
G1 Heap:
   regions  = 7672
   capacity = 32178700288 (30688.0MB)
   used     = 14680397040 (14000.317611694336MB)
   free     = 17498303248 (16687.682388305664MB)
   45.62147292653264% used
G1 Young Generation:
Eden Space:
   regions  = 1
   capacity = 11991515136 (11436.0MB)
   used     = 4194304 (4.0MB)
   free     = 11987320832 (11432.0MB)
   0.03497726477789437% used
Survivor Space:
   regions  = 1
   capacity = 4194304 (4.0MB)
   used     = 4194304 (4.0MB)
   free     = 0 (0.0MB)
   100.0% used
G1 Old Generation:
   regions  = 3500
   capacity = 20182990848 (19248.0MB)
   used     = 14672008432 (13992.317611694336MB)
   free     = 5510982416 (5255.682388305664MB)
   72.69491693523658% used

```

 

Any idea about this?

I look about hedged read metrics, TotalHedgedReadOpsWin/TotalHedgedReadOps is 0, but the TotalHedgedReadOpsInCurThread has a large number(

177117)"
"s3a large file prefetch tests are too slow, don't validate data",13543586,Resolved,Major,Fixed,14/Jul/23 17:47,26/Jul/23 10:33,3.3.9,"the large file prefetch tests (including LRU cache eviction) are really slow.

moving under -scale may hide the problem for most runs, but they are still too slow, can time out, etc etc.


also, and this is very, very important, they can't validate the data.

Better: 
* test on smaller files by setting a very small block size (1k bytes or less) just to force paged reads of a small 16k file.
* with known contents to the values of all forms of read can be validated
* maybe the LRU tests can work with a fake remote object which can then be used in a unit test
* extend one of the huge file tests to read from there -including s3-CSE encryption coverage.

"
s3a prefetch read/write file operations should guard channel close,13543865,Resolved,Major,Fixed,18/Jul/23 02:11,18/Jul/23 13:22,3.3.9,"As per Steve's suggestion from s3a prefetch LRU cache,

s3a prefetch disk based cache file read and write operations should guard against close of FileChannel and WritableByteChannel, close them even if read/write operations throw IOException."
S3A prefetching: switch to prefetching for chosen read policies,13542546,Open,Major,,05/Jul/23 13:23,,3.3.6,"before switching to prefetching input stream everywhere, add an option to list which of the fs.option.openfile.read.policy policies to switch too, e.g
 
fs.s3a.inputstream.prefetch.policies=whole-file, sequential, adaptive

this would leave random and vectored on s3a input stream.
"
s3a prefetching to use split start/end options to limit prefetch range,13542549,In Progress,Major,,05/Jul/23 13:26,,,"the bundled hadoop record readers pass the split start/end down to openFile() -these can be used by the prefetching stream as hints as to where to start and stop prefetching.

Hints only as
* records can extend past split end; that last record will still be read when split end < EOF
* for formats with footers which get read, caching that footer is important"
hadoop distcp -delete sends deleted data to null instead of trash,13543174,Open,Major,,11/Jul/23 16:45,,,"In the docs the -delete option is specified as moving data to Trash when it is enabled:

[https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html#:~:text=%2Ddelete,or%20overwrite%20options].

However, it does not go to trash, it goes to null. I know of two instances where this misunderstanding has caused data loss.

The statement that the data goes to Trash should be removed, and it should be specified that the data is deleted.

An earlier reproduction:

hdfs dfs -mkdir -p /tmp/test1/test2

hdfs dfs -put /tmp/test.img /tmp/

hdfs dfs -put /tmp/test.img /tmp/test2/file1

 

drwxr-xr-x   - root   supergroup          0 2023-04-17 19:07 /tmp/test1

drwxr-xr-x   - hdfs   supergroup          0 2023-04-17 19:06 /tmp/test1/test2

{-}rw-r{-}{-}r{-}-   3 hdfs   supergroup 1073741824 2023-04-17 19:06 /tmp/test1/test2/file1

 

distcp -update -delete /tmp/test.img /tmp/test1

 

{-}rw-r{-}{-}r{-}-   3 root   supergroup 1073741824 2023-04-17 18:52 /tmp/test.img

drwxr-xr-x   - root   supergroup          0 2023-04-17 19:03 /tmp/test1

{-}rw-r{-}{-}r{-}-   3 hdfs   supergroup 1073741824 2023-04-17 19:03 /tmp/test1/test.img

 

2023-04-17 19:08:44,252 INFO FSNamesystem.audit: allowed=true   ugi=hdfs (auth:SIMPLE)  ip=/172.25.41.195       cmd=delete      src=/tmp/test1/test2    dst=null        perm=null       proto

 

[hdfs@c4401-node2 root]$ date

Mon Apr 17 19:11:22 UTC 2023"
Zstd compressor fails with src size is incorrect,13543184,Open,Major,,11/Jul/23 18:57,,3.3.0,"It seems like I've hit an issue similar to https://issues.apache.org/jira/browse/HADOOP-15822. I haven't been able to reproduce the issue though. I did manage to add a little bit of logging to org.apache.hadoop.io.compress.zstd.ZStandardCompressor. I've captured the off and len arguments of compress and the srcOffset and srcLen arguments for deflateBytesDirect:

{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 0 131591}}
{{compress           0 131591}}
{{deflateBytesDirect 131072 519}}

Just after that last line the process dies with a java.lang.InternalError: Src size is incorrect:

{{org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.InternalError: Src size is incorrect}}
{{at org.apache.hadoop.io.compress.zstd.ZStandardCompressor.deflateBytesDirect(Native Method)}}
{{at org.apache.hadoop.io.compress.zstd.ZStandardCompressor.compress(ZStandardCompressor.java:220)}}
{{at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)}}
{{at org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:76)}}
{{at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)}}
{{at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)}}
{{at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.writeBuffer(SequenceFile.java:1569)}}
{{...}}

I have also seen this error: java.lang.InternalError: Error (generic):

{{java.lang.InternalError: Error (generic)}}
{{at org.apache.hadoop.io.compress.zstd.ZStandardCompressor.deflateBytesDirect(Native Method)}}
{{at org.apache.hadoop.io.compress.zstd.ZStandardCompressor.compress(ZStandardCompressor.java:220)}}
{{at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)}}
{{at org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:76)}}
{{at java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)}}
{{at java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)}}
{{at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.writeBuffer(SequenceFile.java:15}}
{{...}}

Note that the arguments `131072 519` are _always_ given to `deflateBytesDirect` in case things go wrong. In other cases the offset argument is zero and the size argument is smaller, but not zero; e.g., 0 and 7772.

As for some context: we're using the compression as part of writing sequence files with data serialised with Kryo to Backblaze using the S3A file system / S3 client with a map-reduce job on YARN. The job has no issues with smaller values, but for larger ones this situation happens. I've seen very larges values being written successfully, but at some point this error is raised all over the place (after a few larger values). Perhaps some buffer is filling up?

Unfortunately, I'm developing using a Mac with M1 processor. So reproducing the issue locally is not a simple feat. If I can somehow produce more leads to investigate this, I'd be happy to.

As an aside: we're considering working around this using the hbase-compression-zstd module. This is an alternative compression codec that uses the zstd-jni library without depending on hadoop native."
remove head bucket request from calls which can be made without audit header,13547400,Open,Minor,,15/Aug/23 14:57,,3.4.0,"once getBucketLocation is done in s3a client, not v1 sdk, the requests MUST have an audit header. Therefore bucket head requests can be removed from the list of ""requests which we allow without a header""

also, why isn't the call to getBucketEncryption in ITestS3AConfiguration failing/being picked up as a failure?"
"s3a client SSLException is raised after very long timeout ""Unsupported or unrecognized SSL message""",13545990,Open,Minor,,03/Aug/23 12:44,,3.3.4,"I've tried to connect from PySpark to Minio running in docker.

Installing PySpark and starting Minio:
{code:bash}
pip install pyspark==3.4.1

docker run --rm -d --hostname minio --name minio -p 9000:9000 -p 9001:9001 -e MINIO_ACCESS_KEY=access -e MINIO_SECRET_KEY=Eevoh2wo0ui6ech0wu8oy3feiR3eicha -e MINIO_ROOT_USER=admin -e MINIO_ROOT_PASSWORD=iepaegaigi3ofa9TaephieSo1iecaesh bitnami/minio:latest
docker exec minio mc mb test-bucket
{code}
Then create Spark session:
{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder\
          .config(""spark.jars.packages"", ""org.apache.hadoop:hadoop-aws:3.3.4"")\
          .config(""spark.hadoop.fs.s3a.endpoint"", ""localhost:9000"")\
          .config(""spark.hadoop.fs.s3a.connection.ssl.enabled"", ""true"")\
          .config(""spark.hadoop.fs.s3a.path.style.access"", ""true"")\
          .config(""spark.hadoop.fs.s3a.access.key"", ""access"")\
          .config(""spark.hadoop.fs.s3a.secret.key"", ""Eevoh2wo0ui6ech0wu8oy3feiR3eicha"")\
          .config(""spark.hadoop.fs.s3a.aws.credentials.provider"", ""org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"")\
          .getOrCreate()
spark.sparkContext.setLogLevel(""debug"")
{code}
And try to access some object in a bucket:
{code:python}
import time

begin = time.perf_counter()
spark.read.format(""csv"").load(""s3a://test-bucket/fake"")
end = time.perf_counter()

py4j.protocol.Py4JJavaError: An error occurred while calling o40.load.
: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://test-bucket/fake: com.amazonaws.SdkClientException: Unable to execute HTTP request: Unsupported or unrecognized SSL message: Unable to execute HTTP request: Unsupported or unrecognized SSL message
...
{code}
[^ssl.log]
{code:python}
>>> print((end-begin)/60, ""min"")
14.72387898775002 min
{code}
I was waiting almost *15 minutes* to get the exception from Spark. The reason was I tried to connect to endpoint with {{fs.s3a.connection.ssl.enabled=true}}, but Minio is configured to listen for HTTP protocol only.

Is there any way to immediately raise exception if SSL connection cannot be established?


If I try to pass wrong endpoint, like {{localhos:9000}}, I'll get exception like this in just 5 seconds:
{code:java}
: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://test-bucket/fake: com.amazonaws.SdkClientException: Unable to execute HTTP request: test-bucket.localhos: Unable to execute HTTP request: test-bucket.localhos
...
{code}
[^host.log]
{code:python}
>>> print(end-begin, ""sec"")
5.700424307000503 sec
{code}
I know about options like {{fs.s3a.attempts.maximum}} and {{{}fs.s3a.retry.limit{}}}, setting them to 1 will cause raising exception just immediately. But this does not look right."
Add enQueue time to RpcMetrics,13546222,Resolved,Minor,Fixed,05/Aug/23 06:32,10/Aug/23 02:40,3.4.0,"Sometimes, the request time observed by the client is much longer than the queue + process time on the RPC server. Perhaps the RPC request 'waiting enQueue' took too long on the RPC server, so we should add enQueue time to RpcMetrics."
ABFS: Fix failing CPK tests on trunk,13548353,Resolved,Minor,Fixed,23/Aug/23 09:35,09/Oct/23 16:40,3.3.6,Fix failing CPK tests on trunk
Upgrade ZooKeeper to 3.6.4,13548667,Resolved,Minor,Fixed,25/Aug/23 14:20,18/Oct/23 01:32,3.3.6,"While ZooKeeper 3.6 is already EOL, we can upgrade to the final release of the ZooKeeper 3.6 as short-term fix until bumping to ZooKeeper 3.7 or later. Dependency convergence error must be addressed on {{-Dhbase.profile=2.0}}.

{noformat}
$ mvn clean install -Dzookeeper.version=3.6.4 -Dhbase.profile=2.0 -DskipTests clean install
Dependency convergence error for org.apache.yetus:audience-annotations:jar:0.13.0:compile paths to dependency are:
+-org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase-common:jar:3.4.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:test-jar:tests:3.4.0-SNAPSHOT:test
    +-org.apache.zookeeper:zookeeper:jar:3.6.4:compile
      +-org.apache.zookeeper:zookeeper-jute:jar:3.6.4:compile
        +-org.apache.yetus:audience-annotations:jar:0.13.0:compile
and
+-org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase-common:jar:3.4.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:test-jar:tests:3.4.0-SNAPSHOT:test
    +-org.apache.zookeeper:zookeeper:jar:3.6.4:compile
      +-org.apache.yetus:audience-annotations:jar:0.13.0:compile
and
+-org.apache.hadoop:hadoop-yarn-server-timelineservice-hbase-common:jar:3.4.0-SNAPSHOT
  +-org.apache.hbase:hbase-common:jar:2.2.4:compile
    +-org.apache.yetus:audience-annotations:jar:0.5.0:compile
{noformat}"
s3a DelegationToken plugin to expand return type of deploy/binding,13542882,Resolved,Minor,Fixed,08/Jul/23 13:10,20/Jul/23 10:37,3.4.0,"Change the binding result of s3a DT services beyond just a credential list

this will the extension point
* return a new signer integrated with the fs
* add something to help with the building the requests to delete objects during paged delete
* gives us an option to add any more stuff

this is incompatible, but the move to AWS v2 breaks DTs bindings anyway..."
ABFS: Adding 100 continue in userAgent String and dynamically removing it if retry is without the header enabled.,13548603,Resolved,Minor,Fixed,25/Aug/23 05:44,16/Jan/24 08:24,3.3.6,Adding 100 continue in userAgent String if enabled in AbfsConfiguration and dynamically removing it if retry is without the header enabled.
S3ARetryPolicy to use sdk exception retryable() if it is valid,13549011,Resolved,Minor,Won't Fix,29/Aug/23 16:07,28/Sep/23 09:50,3.4.0,"S3ARetryPolicy to use sdk exception retryable() if it is appropriate

An initial {{RetryFromAWSClientIOException}} policy has been written, but not turned on as it there's too much risk of suddenly not retrying where we did today."
Add some datanode related metrics to Metrics.md,13551184,Open,Minor,,19/Sep/23 06:46,,,
Refactor @Test(expected) with assertThrows,13548659,Open,Minor,,25/Aug/23 13:41,,,"I am working on research that investigates test smell refactoring in which we identify alternative implementations of test cases, study how commonly used these refactorings are, and assess how acceptable they are in practice.

The smell occurs when exception handling can alternatively be implemented using assertion rather than annotation: using {{assertThrows(Exception.class, () -> \{...});}} instead of {{{}@Test(expected = Exception.class){}}}.

While there are many cases like this, we aim in this pull request to get your feedback on this particular test smell and its refactoring. Thanks in advance for your input."
Spark insertInto with location GCS bucket root not supported,13547976,Open,Minor,,20/Aug/23 06:55,,3.3.3," 
{noformat}
scala> import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.Path
scala> val path: Path = new Path(""gs://test_dd123/"")
path: org.apache.hadoop.fs.Path = gs://test_dd123/

scala> path.suffix(""/num=123"")
java.lang.NullPointerException
  at org.apache.hadoop.fs.Path.<init>(Path.java:150)
  at org.apache.hadoop.fs.Path.<init>(Path.java:129)
  at org.apache.hadoop.fs.Path.suffix(Path.java:450){noformat}
 

Path.suffix throws NPE when writing into GS buckets root. 

 

In our Organisation, we are using GCS bucket root location to point to our Hive table. Dataproc's latest 2.1 uses *Hadoop* *3.3.3* and this needs to be fixed in 3.3.3.

Spark Scala code to reproduce this issue
{noformat}
val DF = Seq((""test1"", 123)).toDF(""name"", ""num"")
DF.write.option(""path"", ""gs://test_dd123/"").mode(SaveMode.Overwrite).partitionBy(""num"").format(""orc"").saveAsTable(""schema_name.table_name"")


val DF1 = Seq((""test2"", 125)).toDF(""name"", ""num"")
DF1.write.mode(SaveMode.Overwrite).format(""orc"").insertInto(""schema_name.table_name"")


java.lang.NullPointerException
  at org.apache.hadoop.fs.Path.<init>(Path.java:141)
  at org.apache.hadoop.fs.Path.<init>(Path.java:120)
  at org.apache.hadoop.fs.Path.suffix(Path.java:441)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.$anonfun$getCustomPartitionLocations$1(InsertIntoHadoopFsRelationCommand.scala:254) {noformat}
 

 "
Multiple token access is supported in HA noZK KMS environment,13548509,Open,Minor,,24/Aug/23 12:23,,3.3.6,"When there is no configuration of ZooKeeper synchronization for secrets on the KMS server (hadoop.kms.authentication.signer.secret.provider is null), the YARN MapReduce job fails to run successfully. The reason is that the KMS returns an error ""invalidToken"" as it cannot find the corresponding token in the cache.

!image-2023-08-24-19-53-24-665.png!

Is it possible to achieve fault tolerance by accessing multiple KMS services without configuring zookeeper for synchronization in KMS?"
Address Netty 4.x / CWE-295 by configuring hostname verification,13544846,Open,Minor,,25/Jul/23 18:18,,3.3.6,"Our SAST tool has picked up that the version of Netty 4.x used by Hadoop is vulnerable to [Security Vulnerability - Common Weakness Enumeration (CWE) CWE-295 · Issue #9930 · netty/netty (github.com)|https://github.com/netty/netty/issues/9930]. Until Netty 5 is released (which will enable it by default), the remediation is to enable host name verification ([SslContext (Netty API Reference (4.1.95.Final))|https://netty.io/4.1/api/io/netty/handler/ssl/SslContext.html#newHandler-io.netty.buffer.ByteBufAllocator-java.util.concurrent.Executor-])."
AWS SDK v2 build complaints,13544315,Resolved,Minor,Duplicate,20/Jul/23 16:24,23/Aug/23 10:35,3.4.0,"rebase branches hightlight spotbugs and javadoc issues,  plus style.

nothing major but should be addressed before the merge, especially the spotbugs one


{code}
hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ProgressableProgressListener.java:80: warning: no @param for upload

{code}

and something that needs review, probably a spotbugs disable if we are happy its a false alarm

{code}

Code	Warning
IS	Inconsistent synchronization of org.apache.hadoop.fs.s3a.S3AFileSystem.s3AsyncClient; locked 60% of time
Bug type IS2_INCONSISTENT_SYNC (click for details)
In class org.apache.hadoop.fs.s3a.S3AFileSystem
Field org.apache.hadoop.fs.s3a.S3AFileSystem.s3AsyncClient
Synchronized 60% of the time
Unsynchronized access at S3AFileSystem.java:[line 1764]
Unsynchronized access at S3AFileSystem.java:[line 989]
Synchronized access at S3AFileSystem.java:[line 4179]
Synchronized access at S3AFileSystem.java:[line 4184]
Synchronized access at S3AFileSystem.java:[line 1002]

{code}


{code}
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/audit/impl/ActiveAuditManagerS3A.java:413:    //  https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/core/interceptor/ExecutionInterceptor.html: Line is longer than 100 characters (found 115). [LineLength]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java:128:  private <BuilderT extends S3BaseClientBuilder<BuilderT, ClientT>, ClientT> BuilderT configureClientBuilder(: Line is longer than 100 characters (found 109). [LineLength]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:24:public interface AWSHeaders {: interfaces should describe a type and hence have methods. [InterfaceIsType]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:46:  /** S3's version ID header */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:49:  /** Header describing what class of storage a user wants */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:52:  /** Header describing what archive tier the object is in, if any */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:55:  /** Header for optional server-side encryption algorithm */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:58:  /** Range header for the get object request */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:68:  /** JSON-encoded description of encryption materials used during encryption */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AWSHeaders.java:71:  /** Header for the optional restore information of an object */: First sentence should end with a period. [JavadocStyle]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentS3ClientFactory.java:68:   FailureInjectionInterceptor(FailureInjectionPolicy policy) {: 'ctor def modifier' has incorrect indentation level 3, expected level should be 4. [Indentation]
./hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/BlockingEnumeration.java:57:  private final Signal<T> END_SIGNAL = new Signal<>((Throwable)null);:27: Name 'END_SIGNAL' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MemberName]
./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3AMockTest.java:57:  protected S3Client s3;:22: Variable 's3' must be private and have accessor methods. [VisibilityModifier]
./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/audit/AbstractAuditingTest.java:28:import java.util.function.Consumer;:8: Unused import - java.util.function.Consumer. [UnusedImports]
./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/StreamPublisher.java:38:  public StreamPublisher(Stream<T> data, Executor executor) {:3: Redundant 'public' modifier. [RedundantModifier]
./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/StreamPublisher.java:43:  public StreamPublisher(Stream<T> data) {:3: Redundant 'public' modifier. [RedundantModifier]
./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/TestSelectEventStreamPublisher.java:159:        .map(e -> { throw SdkException.create(""error!"", null); }));:19: '{' at column 19 should have line break after. [LeftCurly]

{code}
"
TestRPCCallBenchmark timeout,13546750,Open,Minor,,10/Aug/23 03:05,,,"The UT TestRPCCallBenchmark failed in pr [5926|https://github.com/apache/hadoop/pull/5926], this issue will track it.

{code:java}
[ERROR] testBenchmarkWithProto(org.apache.hadoop.ipc.TestRPCCallBenchmark)  Time elapsed: 20.006 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 20000 milliseconds
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1257)
	at java.lang.Thread.join(Thread.java:1331)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.stop(MultithreadedTestUtil.java:164)
	at org.apache.hadoop.ipc.RPCCallBenchmark.run(RPCCallBenchmark.java:326)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
	at org.apache.hadoop.ipc.TestRPCCallBenchmark.testBenchmarkWithProto(TestRPCCallBenchmark.java:30)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)
{code}
"
Some fs.s3a.* config values are different in sources and documentation,13545989,Open,Minor,,03/Aug/23 12:24,,3.3.6,"For config option {{fs.s3a.retry.throttle.interval}} default value in source code is {{500ms}}:
{code:java}
public static final String RETRY_THROTTLE_INTERVAL_DEFAULT = ""500ms"";
{code}
https://github.com/apache/hadoop/blob/rel/release-3.3.6/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java#L921

In {{core-default.xml}} it has value {{100ms}}, but in the description {{500ms}}:
{code:xml}
<property>
  <name>fs.s3a.retry.throttle.interval</name>
  <value>100ms</value>
  <description>
    Initial between retry attempts on throttled requests, +/- 50%. chosen at random.
    i.e. for an intial value of 3000ms, the initial delay would be in the range 1500ms to 4500ms.
    Backoffs are exponential; again randomness is used to avoid the thundering heard problem.
    500ms is the default value used by the AWS S3 Retry policy.
  </description>
</property>
{code}
https://github.com/apache/hadoop/blob/rel/release-3.3.6/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L1750
This change introduced in HADOOP-16823.

In Hadoop-AWS module documentation it has value {{1000ms}}:
{code:xml}
<property>
  <name>fs.s3a.retry.throttle.interval</name>
  <value>1000ms</value>
  <description>
    Interval between retry attempts on throttled requests.
  </description>
</property>
{code}
https://github.com/apache/hadoop/blob/rel/release-3.3.6/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md?plain=1#L1223
File was created in HADOOP-13786, and value is left unchanged since when.

In performance tuning page it has up-to-date value {{500ms}}:
{code:xml}
<property>
  <name>fs.s3a.retry.throttle.interval</name>
  <value>500ms</value>
  <description>
    Interval between retry attempts on throttled requests.
  </description>
</property>
{code}
https://github.com/apache/hadoop/blob/rel/release-3.3.6/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/performance.md?plain=1#L435
This change introduced in HADOOP-15076.

The same issue with:
* {{fs.s3a.retry.throttle.limit}} - in source code it has value {{20}}, but in some documents still old value ${fs.s3a.attempts.maximum}
* {{fs.s3a.connection.establish.timeout}} - in source code it has value {{50_000}}, in config file & documentation {{5_000}}
* {{fs.s3a.attempts.maximum}} - in source code it has value {{10}}, in config file & documentation {{20}}
* {{fs.s3a.threads.max}} - in source & documentation code it has value {{10}}, in config file {{64}}
* {{fs.s3a.max.total.tasks}} - in source code & config it has value {{32}}, in documentation {{5}}
* {{fs.s3a.connection.maximum}} - in source code & config it has value {{96}}, in documentation {{15}} or {{30}}

Please sync these values, outdated documentation is very painful to work with.
As an idea, is it possible to use {{core-default.xml}} directly in documentation, or generate this documentation from docstrings in Java code?"
Hadoop UGI doesn't work with KCM based kerberos tickets,13544947,Open,Minor,,26/Jul/23 10:14,,,
Document missing property (ipc.server.read.threadpool.size) in core-default.xml,13543635,Open,Minor,,15/Jul/23 05:19,,,
LogExactlyOnce to add a debug() method,13543804,Resolved,Minor,Fixed,17/Jul/23 14:30,18/Jul/23 13:24,3.3.9,"to help with some of this i want  LogExactlyOnce to add a debug() method along with the info/warn/error etc.

"
S3A StagingCommitter does not clean up staging-uploads directory,13542678,Resolved,Minor,Fixed,06/Jul/23 10:39,08/Jul/23 13:04,3.2.2,"When setting up StagingCommitter and its internal FileOutputCommitter, a temporary directory that holds MPU information will be created on the default FS, which by default is to be /user/${USER}/tmp/staging/${USER}/${UUID}/staging-uploads.

On a successful job commit, its child directory (_temporary) will be [cleaned up|https://github.com/apache/hadoop/blob/a36d8adfd18e88f2752f4387ac4497aadd3a74e7/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java#L516] properly, but ${UUID}/staging-uploads will remain.

This will result in having too many empty ${UUID}/staging-uploads directories under /user/${USER}/tmp/staging/${USER}, and will eventually cause an issue in an environment where the max number of items in a directory is capped (e.g. by dfs.namenode.fs-limits.max-directory-items in HDFS).

{noformat}
The directory item limit of /user/${USER}/tmp/staging/${USER} is exceeded: limit=1048576 items=1048576
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1205)
{noformat}"
AWS SDK v2 typo in aws.evenstream.version,13548234,Resolved,Trivial,Duplicate,22/Aug/23 13:52,23/Aug/23 14:40,3.4.0,the pom version property aws.evenstream.version should be aws.eventstream.version
ITestS3APrefetchingCacheFiles teardown failure if setup() fails,13551068,Open,Trivial,,18/Sep/23 12:24,,3.4.0,"
if the ITestS3APrefetchingCacheFiles setup fails before conf is set, teardown fails...need to wrap the cleanup code with a check for conf!=null

{code}
[ERROR] testCacheFileExistence(org.apache.hadoop.fs.s3a.ITestS3APrefetchingCacheFiles)  Time elapsed: 0.156 s  <<< ERROR!
java.lang.NullPointerException
        at org.apache.hadoop.fs.s3a.ITestS3APrefetchingCacheFiles.teardown(ITestS3APrefetchingCacheFiles.java:85)

{code}

"
get local file system fails with a casting error,13550735,Open,Trivial,,14/Sep/23 18:36,,3.3.6,"h2. What happened

After setting {{{}fs.file.impl=org.apache.hadoop.fs.RawLocalFileSystem{}}}, trying to acquire local file system using the {{getLocal}} in {{org.apache.hadoop.fs}} fails with {{java.lang.ClassCastException}}
h2. Where's the bug

In the function {{getLocal}} of {{FileSystem}} in HCommon:
{code:java}
  public static LocalFileSystem getLocal(Configuration conf)
    throws IOException {
    return (LocalFileSystem)get(LocalFileSystem.NAME, conf);
  } {code}
the returned file system is directly cast to LocalFileSystem without checking. If the user set the implementation of the local filesystem to be Raw rather than Checksum, this type cast would fail.
h2. How to reproduce
 # Set {{fs.file.impl=org.apache.hadoop.fs.RawLocalFileSystem}}
 # Run the following test in HBase: {{org.apache.hadoop.hbase.TestHBaseTestingUtility#testMiniDFSCluster}}
and the following exception should be observed:

{code:java}
java.lang.ClassCastException: class org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to class org.apache.hadoop.fs.LocalFileSystem (org.apache.hadoop.fs.RawLocalFileSystem and org.apache.hadoop.fs.LocalFileSystem are in unnamed module of loader 'app')
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:441)
	at org.apache.hadoop.hbase.HBaseTestingUtility.getNewDataTestDirOnTestFS(HBaseTestingUtility.java:550)
...{code}
Or simply set the configuration parameter and call the method using a {{Configuration}} object and the exception would be triggered."
S3A committer NPE in spark job abort,13476432,Resolved,Blocker,Fixed,11/Aug/22 17:31,05/Dec/22 13:34,3.3.5,"NPE happening in spark {{HadoopMapReduceCommitProtocol.abortJob}} when jobID is null


{code}
- save()/findClass() - non-partitioned table - Overwrite *** FAILED ***
  java.lang.NullPointerException:
  at org.apache.hadoop.fs.s3a.commit.impl.CommitContext.<init>(CommitContext.java:159)
  at org.apache.hadoop.fs.s3a.commit.impl.CommitOperations.createCommitContext(CommitOperations.java:652)
  at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.initiateJobOperation(AbstractS3ACommitter.java:856)
  at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.abortJob(AbstractS3ACommitter.java:909)
  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.abortJob(HadoopMapReduceCommitProtocol.scala:252)
  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:268)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:191)
  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
  ...

{code}
"
S3AInputStream.unbuffer() async drain not releasing http connections,13477576,Resolved,Blocker,Fixed,19/Aug/22 10:00,05/Sep/22 13:23,3.3.5,"Impala tcp-ds setup to s3 is hitting problems with timeout fetching http connections from the s3a fs pool. Disabling s3a async drain makes this problem *go away*. assumption, either those async ops are blocking, or they are not releasing references properly.

"
NullPointerException in ObjectListingIterator's constructor,13482086,Resolved,Blocker,Fixed,19/Sep/22 02:31,23/Sep/22 09:10,3.3.5,"We saw NullPointerExceptions in Impala's S3 tests: IMPALA-11592. It's thrown from the hadoop jar:
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator.<init>(Listing.java:621)
        at org.apache.hadoop.fs.s3a.Listing.createObjectListingIterator(Listing.java:163)
        at org.apache.hadoop.fs.s3a.Listing.createFileStatusListingIterator(Listing.java:144)
        at org.apache.hadoop.fs.s3a.Listing.getListFilesAssumingDir(Listing.java:212)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListFiles(S3AFileSystem.java:4790)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listFiles$37(S3AFileSystem.java:4732)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:543)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:524)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:445)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2363)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2382)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.listFiles(S3AFileSystem.java:4731)
        at org.apache.impala.common.FileSystemUtil.listFiles(FileSystemUtil.java:754)
        ... {noformat}
We are using a private build of the hadoop jar. Version: CDP 3.1.1.7.2.16.0-164
Code snipper of where the NPE throws:
{code:java}
604     @Retries.RetryRaw
605     ObjectListingIterator(
606         Path listPath,
607         S3ListRequest request,
608         AuditSpan span) throws IOException {
609       this.listPath = listPath;
610       this.maxKeys = listingOperationCallbacks.getMaxKeys();
611       this.request = request;
612       this.objectsPrev = null;
613       this.iostats = iostatisticsStore()
614           .withDurationTracking(OBJECT_LIST_REQUEST)
615           .withDurationTracking(OBJECT_CONTINUE_LIST_REQUEST)
616           .build();
617       this.span = span;
618       this.s3ListResultFuture = listingOperationCallbacks
619           .listObjectsAsync(request, iostats, span);
620       this.aggregator = IOStatisticsContext.getCurrentIOStatisticsContext()
621           .getAggregator();           // <---- thrown here
622     }
{code}"
Upgrade Maven Surefire plugin to 3.0.0-M7,13478269,Patch Available,Blocker,,23/Aug/22 19:06,,3.3.5,"The Maven Surefire plugin 3.0.0-M1 doesn't always include the launcher as part of it's setup, which can cause problems with Yarn tests. Some of the Yarn modules use Jupiter, which may be a complicating factor.  Switching to 3.0.0-M7 fixes the issue.

This is currently blocking MAPREDUCE-7386"
Release hadoop 3.3.5,13483486,Resolved,Blocker,Fixed,27/Sep/22 15:39,23/Mar/23 18:56,3.3.5,
s3a FS init logs at warn if fs.s3a.create.storage.class is unset,13473762,Resolved,Blocker,Fixed,27/Jul/22 11:00,16/Aug/22 18:50,3.3.5,"if you don't have an s3a storage class set in {{fs.s3a.create.storage.class}}, then whenever you create an S3A FS instance, it logs at warn

{code}

bin/hadoop s3guard bucket-info $BUCKET

2022-07-27 11:53:11,239 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption.key is deprecated. Instead, use fs.s3a.encryption.key
2022-07-27 11:53:11,240 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption-algorithm is deprecated. Instead, use fs.s3a.encryption.algorithm
2022-07-27 11:53:11,396 [main] WARN  s3a.S3AFileSystem (S3AFileSystem.java:createRequestFactory(1004)) - Unknown storage class property fs.s3a.create.storage.class: ; falling back to default storage class
2022-07-27 11:53:11,839 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy(189)) - Directory markers will be kept
Filesystem s3a://stevel-london
Location: eu-west-2


{code}

note, this is why part of quaifying an sdk update involves looking at the logs and running the CLI commands by hand...you see if new messages have crept in"
 Fix file split duplicating records from a succeeding split when reading BZip2 text files ,13476265,Resolved,Critical,Fixed,11/Aug/22 01:24,19/Sep/22 04:52,3.3.3,"Fix data correctness issue with TextInputFormat that can occur when reading BZip2 compressed text files. When a file split's range does not include the start position of a BZip2 block, then it is expected to contain no records (i.e. the split is empty). However, if it so happens that the end of this split (exclusive) is at the start of a BZip2 block, then LineRecordReader ends up returning all the records for that BZip2 block. This ends up duplicating records read by a job because the next split would also end up returning all the records for the same block (since its range would include the start of that block).

This bug does not get triggered when the file split's range does include the start of at least one block and ends just before the start of another block. The reason for this has to do with when BZip2CompressionInputStream updates its position when using the BYBLOCK READMODE. Using this read mode, the stream's position while reading only gets updated when reading the first byte past an end of a block marker. The bug is that if the stream, when initialized, was adjusted to be at the end of one block, then we don't update the position after we read the first byte of the next block. Rather, we keep the position to be equal to the next block marker we've initialized to. If the exclusive end position of the split is equal to stream's position, LineRecordReader will continue to read lines until the position is updated (an an additional record in the next block is read if needed)."
JavaKeyStoreProvider should throw FileNotFoundException in renameOrFail,13481037,Open,Critical,,12/Sep/22 01:43,,3.3.5,"Attempting to create a key a KMS is configured with the JavaKeystoreProvider and an HDFS store.  The calls to:
{noformat}
renameOrFail(Path src, Path dest) throws IOException {noformat}
... fails with an IOException when it attempts to rename a file.  The calling code catches FileNotFoundException since the src file may not exist.

 

Example:
{noformat}
$ hadoop key create sample
java.io.IOException: Rename unsuccessful : 'hdfs://mycluster/security/kms.jks_NEW' to 'hdfs://mycluster/security/kms.jks_NEW_ORPHANED_1662946593691'{noformat}
 

Update the implementation to check for the file, throwing a FileNotFoundException."
unable to run hdfs dfs -mkdir ,13482676,Open,Critical,,22/Sep/22 02:52,,3.3.3,
Don't fail when using -DNoUnitTests,13478452,Resolved,Critical,Duplicate,24/Aug/22 16:10,24/Aug/22 21:50,3.4.0,"There are commands in hadoop.sh that use -Dtest=NoUnitTests.  I believe this is intended to replace -DskipTest, by not matching any tests.  This causes an issue since the default for surefire.failIfNoSpecifiedTests is true.

Updating those commands to override the default will address the issue."
Suggest to update the IP address of ConnectionId when server of Connection got updated,13472129,Open,Critical,,18/Jul/22 19:04,,3.3.1,"When use ProtobufRpcEngine to get RPC proxy, we apply the InetSocketAddress addr, and this address is then kept in the ConnectionId (see [here|https://github.com/apache/hadoop/blob/rel/release-3.3.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java#L139] and [here|https://github.com/apache/hadoop/blob/rel/release-3.3.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L1681]).

When the destination IP of the connection changes, we have code to update the server variable in the connection (see [here|https://github.com/apache/hadoop/blob/rel/release-3.3.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L641]). This makes the retry of the connection to succeed. 

However, the above logic only temporary fix the connection. After the RPC call of the client, the connection will be closed. Next time when we use the same proxy to do the RPC call, we will again to fail the first connection and succeed the second one after the server IP got updates. 

My suggestion is to not only update the server variable in the connection, but also update the address of the ConnectionId in the Invoker. This way, we can reuse the proxy with the latest IP address.

The use case for this is: in Apache ozone project, we initialize the RPC proxy with a fixed InetSocketAddress (for example, datanode needs to set up the RPC proxy to scm). But if lately the server IP changes (e.g. in k8s, the IP of a pod is very likely to change), every time the proxy needs two calls (1st fails and the 2nd succeeds with the updated IP). By using the above solution, we can eliminate this issue."
Improve Pre-Commit Time,13471138,Open,Critical,,12/Jul/22 07:08,,,"As of now the complete build time has reached ~24 hours, which makes tracking Jira with root level changes very tough.

Even at module level, it is on the higher side, kind of 6 hours or more, which is typically equivalent to 1 working day hours:

Some areas to explore:
 * Enable Parallel-Test profile for the modules which don't have it.
 * Explore improvements in the area of our Test setup, increase memory, number of threads, or some parallel executions?
 * Remove the modules or atleast disable the test suites of modules which are no longer being used or maintained (Separate Mail Chain is there for this)
 * Spin Two Pre-Commit jobs rather than 1 and split some task between them, kind of one for Java-11 build & Javadoc, Checkstyle and related stuff.
 * Figure out & remove irrelevant or similar tests.
 * Improve existing tests, Like reusing MiniDfsClusters or so, rather than a bunch of test spinning one for themselves.
 * If possible, if there are multiple modules to be tested, run those modules test in parallel (Exploratory: may lead to OOM)"
ABFS: Add correlated metric support for ABFS operations,13469856,Resolved,Major,Fixed,04/Jul/22 05:02,23/May/24 14:10,3.3.3,"Add metrics related to a particular job, specific to number of total requests, retried requests, retry count and others"
Updated addresses are still accessed using the old IP address,13473329,Resolved,Major,Fixed,25/Jul/22 13:42,22/Aug/22 16:57,3.3.5,"When the IPC Client recognizes that an IP address has changed, it updates the server field and logs a message:
Address change detected. Old: journalnode-1.journalnode.hdfs.svc.cluster.local/10.1.0.178:8485 New: journalnode-1.journalnode.hdfs.svc.cluster.local/10.1.0.182:8485
Although the change is detected, the client will continue to connect to the old IP address, resulting in repeated log messages.  This is seen in managed environments when JournalNode syncing is enabled and a JournalNode is restarted, with the remaining nodes in the set repeatedly logging this message when syncing to the restarted JournalNode.

The source of the problem is that the remoteId.address is not updated."
Remove fs.s3a.executor.capacity,13479568,Open,Major,,01/Sep/22 00:35,,,"When s3guard was part of s3a, DynamoDBMetadataStore was the only consumer of StoreContext that used throttled executor provided by StoreContext, which internally uses fs.s3a.executor.capacity to determine executor capacity for SemaphoredDelegatingExecutor. With the removal of s3guard from s3a, we should also remove fs.s3a.executor.capacity and it's usages as it's no longer being used by any StoreContext consumers. The config's existence and its description can be really confusing for the users."
Optimise S3A’s recursive delete to drop successful S3 keys on retry of S3 DeleteObjects,13478456,Open,Major,,24/Aug/22 16:20,,,"S3A users with large filesystems performing renames or deletes can run into throttling when S3A performs a bulk delete on keys. These are currently batches of 250 ([https://github.com/apache/hadoop/blob/c1d82cd95e375410cb0dffc2931063d48687386f/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java#L319-L323]).

When the bulk delete ([S3 DeleteObjects|https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObjects.html]) fails, it provides a list of keys that failed and why. Today, S3A recovers from throttles by sending the DeleteObjects request again with no change. This can result in additional deletes and counts towards throttling limits.

Instead, S3A should retry only the keys that failed, limiting the number of mutations against the S3 bucket, and hopefully mitigate errors when deleting a large number of objects."
Add XMLUtils methods to centralise code that creates secure XML parsers,13483427,Resolved,Major,Fixed,27/Sep/22 10:30,07/Oct/22 10:04,3.3.4,"Relates to HDFS-16766

There are other places in the code where DocumentBuilderFactory instances are created that could benefit from the same changes as HDFS-16766

h3. sonatype-2022-5820

If anyone is landing on this page following the sonatype-2022-5820 alert, know that there is no known issue here, just a centralisation of all construction of XML parsers with lockdown of all the features."
Remove rs-api dependency by downgrading jackson to 2.12.7,13470996,Resolved,Major,Fixed,11/Jul/22 11:56,17/Jul/22 16:05,3.3.4,"This jsr311-api jar seems to conflict with newly added rs-api jar dependency - they have many of the same classes (but conflicting copies) - jersey-core 1.19 needs jsr311-api to work properly (and fails if rs-api used instead)

* https://mvnrepository.com/artifact/javax.ws.rs/jsr311-api
* https://mvnrepository.com/artifact/javax.ws.rs/javax.ws.rs-api

Seems we will need to downgrade jackson to 2.12.7 because of jax-rs compatibility issues in jackson 2.13 (see https://github.com/FasterXML/jackson-jaxrs-providers/issues/134)"
Hadoop 3.3.2 has CVEs coming from dependencies,13475284,Resolved,Major,Duplicate,05/Aug/22 05:23,09/Aug/22 12:12,3.3.2,"Hi Team,

 

Hadoop version 3.3.1 which is compatible for our application have Vulnerebilities:

Is there any plan to fix this

CVE-2021-37404 hadoop versions < 3.3.2 Apache Hadoop potential heap buffer overflow in libhdfs.
CVE-2020-10650 jackson < 2.9.10.4
CVE-2021-33036 hadoop < 3.3.2
CVE-2022-31159 aws xfer manager download < 1.12.262"
Upgrade to Avro 1.11.1,13472053,Resolved,Major,Fixed,18/Jul/22 10:38,28/Jan/24 09:44,thirdparty-1.2.0,Latest version of Avro. Aimed only at trunk as there is no security concern addressed here.
S3A supports S3 on Outposts,13470442,Resolved,Major,Fixed,07/Jul/22 02:48,31/Aug/23 14:03,3.3.9,"Currently, the endpoint for using S3 accesspoint is set as ""s3-accesspoint.%s.amazonaws.com"" as follows.
[https://github.com/apache/hadoop/blob/3ec4b932c179d9ec6c4e465f25e35b3d7eded08b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ArnResource.java#L29]

However, ""s3-outposts.%s.amazonaws.com"" is the preferred endpoint when accessing S3 on Outposts bucket by accesspoint.
This ticket improves them."
Add support for IBM Semeru OE JRE 11.0.15.0 and greater,13470668,Resolved,Major,Fixed,08/Jul/22 10:18,22/Mar/23 17:55,3.0.0,"There are checks within the PlatformName class that use the Vendor property of the provided runtime JVM specifically looking for `IBM` within the name. Whilst this check worked for IBM's [java technology edition|https://www.ibm.com/docs/en/sdk-java-technology] it fails to work on [Semeru|https://developer.ibm.com/languages/java/semeru-runtimes/] since 11.0.15.0 due to the following change:
h4. java.vendor system property

In this release, the {{java.vendor}} system property has been changed from ""International Business Machines Corporation"" to ""IBM Corporation"".

Modules such as the below are not provided in these runtimes.
com.ibm.security.auth.module.JAASLoginModule"
hadoop-client-runtime impact by CVE-2022-2047 CVE-2022-2048 due to shaded jetty,13471087,Resolved,Major,Fixed,11/Jul/22 23:26,24/Aug/22 00:17,3.3.3,"CVE-2022-2047 and CVE-2022-2048 is recently found for Eclipse Jetty, and impacts 9.4.0 thru 9.4.46.

In latest 3.3.3 of hadoop-client-runtime, it shaded 9.4.43.v20210629 version jetty which is impacted.

In Trunk, Jetty is in version 9.4.44.v20210927, which is still impacted.

Need to upgrade Jetty Version. "
S3A storage class option only picked up when buffering writes to disk,13471758,Resolved,Major,Fixed,15/Jul/22 10:20,01/Sep/22 17:19,3.3.5,"when you switch s3a output stream buffering to heap or byte buffer, the storage class option isn't added to the put request


{code}

  <property>
    <name>fs.s3a.fast.upload.buffer</name>
    <value>bytebuffer</value>
  </property>

{code}

and the ITestS3AStorageClass tests fail.
{code}

java.lang.AssertionError: [Storage class of object s3a://stevel-london/test/testCreateAndCopyObjectWithStorageClassGlacier/file1] 
Expecting:
 <null>
to be equal to:
 <""glacier"">
ignoring case considerations

	at org.apache.hadoop.fs.s3a.ITestS3AStorageClass.assertObjectHasStorageClass(ITestS3AStorageClass.java:215)
	at org.apache.hadoop.fs.s3a.ITestS3AStorageClass.testCreateAndCopyObjectWithStorageClassGlacier(ITestS3AStorageClass.java:129)


{code}

we noticed this in a code review; the request factory only sets the option when the source is a file, not memory.

proposed: parameterize the test suite on disk/byte buffer, then fix
"
upgrade commons-configuration2 to 2.8.0 and commons-text to 1.9,13472038,Resolved,Major,Fixed,18/Jul/22 09:28,22/Sep/22 02:15,3.3.5,"Current version 2.1.1 has no CVEs but all higher versions have CVEs except for the latest release 2.8.0. Still feels like it would be safer to upgrade.

Currently, causes issues - that will need to be fixed:

```
[ERROR] testBlockReaderLocalWithMlockChanges(org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal)  Time elapsed: 0.414 s  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.commons.configuration2.interpol.ConfigurationInterpolator$DefaultPrefixLookupsHolder
	at org.apache.commons.configuration2.interpol.ConfigurationInterpolator.getDefaultPrefixLookups(ConfigurationInterpolator.java:290)
	at org.apache.commons.configuration2.AbstractConfiguration.installDefaultInterpolator(AbstractConfiguration.java:375)
	at org.apache.commons.configuration2.AbstractConfiguration.<init>(AbstractConfiguration.java:122)
	at org.apache.commons.configuration2.BaseConfiguration.<init>(BaseConfiguration.java:37)
	at org.apache.commons.configuration2.PropertiesConfiguration.<init>(PropertiesConfiguration.java:1059)
	at org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:114)
	at org.apache.hadoop.metrics2.impl.MetricsConfig.create(MetricsConfig.java:97)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:482)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1780)
```"
AWS SDK update to 1.12.262 to address jackson  CVE-2018-7489 and AWS CVE-2022-31159,13472057,Resolved,Major,Fixed,18/Jul/22 11:06,28/Jul/22 10:43,3.3.4,"The CVE [CVE-2022-31159|https://nvd.nist.gov/vuln/detail/CVE-2022-31159] is a vulnerability in path resolution in the AWS SDK transfer manager during downloads.

*the s3a client is not exposed to this*. it uses the class for local file upload and for object copying, but not download.

it may affect downstream use by other applications.

 yet another jackson CVE in aws sdk
https://github.com/apache/hadoop/pull/4491/commits/5496816b472473eb7a9c174b7d3e69b6eee1e271

maybe we need to have a list of all shaded jackson's we get on the CP and have a process of upgrading them all at the same time"
Enhance client protocol to propagate last seen state IDs for multiple nameservices.,13472119,Resolved,Major,Fixed,18/Jul/22 16:52,23/Aug/22 19:45,3.4.0,"The RPCHeader in the client protocol currently contains a single value to indicate the last seen state ID for a namenode.
{noformat}
optional int64 stateId = 8; // The last seen Global State ID
{noformat}
When there are multiple namenodes, such as in router based federation, the headers need to carry the state IDs for each of these nameservices that are part of the federation.

This change is a prerequisite for HDFS-13522: RBF: Support observer node from Router-Based Federation"
Update commons-cli from 1.2 to 1.5.	,13473153,Resolved,Major,Fixed,25/Jul/22 02:25,09/May/23 20:13,3.4.0,
Fix bug preventing hadoop-metrics2 from emitting metrics to > 1 Ganglia servers.,13473324,Resolved,Major,Fixed,25/Jul/22 13:12,04/Aug/22 12:57,3.2.4,"AbstractGangliaSink is used by the hadoop-metrics2 package to emit metrics to Ganglia. Currently, this class uses the apache commons-configuration package to read from the hadoop-metrics2.properties file. commons-configuration is outdated, and has a bug where the .getString function drops everything after the first comma. "
All method metrics related to the RPC protocol should be initialized,13473267,Resolved,Major,Fixed,25/Jul/22 10:19,04/Aug/22 05:02,3.4.0,"When an RPC protocol is used, the metric of protocol-related methods should be initialized; otherwise, metric information will be incomplete. For example, when we call HAServiceProtocol#monitorHealth(), only the metric of monitorHealth() are initialized, and the metric of transitionToStandby() are still not reported. This incompleteness caused a little trouble for our monitoring system.
The root cause is that the parameter passed by RpcEngine to MutableRatesWithAggregation#init(java.lang.Class<?>)  is always XXXProtocolPB, which is inherited from BlockingInterface and does not implement any methods. We should fix this bug."
hadoop-aws maven build to add a prefetch profile to run all tests with prefetching,13474005,Resolved,Major,Fixed,28/Jul/22 13:35,20/Sep/22 09:27,3.4.0,"Add a prefetch profile to the hadoop-aws build so tests run with prefetching on, similar to how the markers option does

makes it easy to test everything with prefetching on/off without editing xml files."
fs.s3a.prefetch.block.size to be read through longBytesOption,13474016,Resolved,Major,Fixed,28/Jul/22 14:02,23/Aug/22 09:49,3.3.6,"use   {{longBytesOption(fs.s3a.prefetch.block.size)}}

this allows for unitgs like M to be used, and is consistent with the rest of the s3a size params. also sets a minimum size to prevent negatives values"
Codecs with @DoNotPool annotation are not closed causing memory leak,13474518,Resolved,Major,Fixed,01/Aug/22 12:18,12/Aug/22 23:11,3.3.2,"Compressors and Decompressions with a @DoNotPool annotation are not closed when they are returned to the CodecPool, which causes a native memory leak.

 

I have included a link to a [Demo Project|https://github.com/kevins-29/hadoop-gzip-memory-leak] demonstrating the leak"
Allow dynamic groupSearchFilter in LdapGroupsMapping,13474886,Resolved,Major,Fixed,03/Aug/22 12:31,07/Sep/22 12:54,3.4.0,"As of now the lookupGroup() method doesn't allow to have placeholders in 

groupSearchFilter, so that can not be dynamically adjusted.

If we have placeholders for groupSearchFilter like: (&(|(XYZ=\{0})(ABC=\{1}))(objectClass=posixGroup))

This fails here:

 
{code:java}
groupResults =
    c.search(groupbaseDN,
        ""(&"" + groupSearchFilter + ""("" + groupMemberAttr + ""={0}))"",
        new Object[]{userDn},
        SEARCH_CONTROLS); {code}
With 

 

 
{noformat}
javax.naming.directory.InvalidSearchFilterException: number exceeds argument list: 1; remaining name {noformat}
 

>>Dropped off or changed the details above which I thought won't be safe to disclose.

 "
Shutdown AWSSecurityTokenService when its resources are no longer in use,13475858,Resolved,Major,Fixed,09/Aug/22 05:56,12/Aug/22 14:36,3.3.5,"AWSSecurityTokenService resources can be released whenever they are no longer in use. The documentation of AWSSecurityTokenService#shutdown says while it is not important for client to compulsorily shutdown the token service, client can definitely perform early release whenever client no longer requires token service resources. We achieve this by making STSClient closable, so we can certainly utilize it in all places where it's suitable."
S3A Prefetch - SingleFilePerBlockCache to use LocalDirAllocator,13476176,Resolved,Major,Fixed,10/Aug/22 12:46,18/Apr/23 16:06,3.4.0,"prefetching stream's SingleFilePerBlockCache uses Files.tempFile() to allocate a temp file.

it should be using LocalDirAllocator to allocate space from a list of dirs, taking a config key to use. for s3a we will use the Constants.BUFFER_DIR option, which on yarn deployments is fixed under the env.LOCAL_DIR path, so automatically cleaned up on container exit"
Fix broken link to wiki help page in org.apache.hadoop.util.Shell,13476712,Resolved,Major,Fixed,14/Aug/22 13:27,14/Aug/22 14:00,3.4.0,"When the old wiki was phased out, the link in org.apache.hadoop.util.Shell wasn't updated. Fixing the link avoids the ""Page Not Found"" error."
Adds alignment context to call path for creating RPC proxy with multiple connections per user.,13477107,Resolved,Major,Fixed,16/Aug/22 18:50,24/Aug/22 23:53,3.3.5,"HDFS-13274 (RBF: Extend RouterRpcClient to use multiple sockets) gets the RPC proxy using methods which do not allow using an alignment context. These methods were added in HADOOP-13144 (Enhancing IPC client throughput via multiple connections per user).

This change adds an alignment context as an argument for methods in the call path for creating the proxy."
Improve the accuracy of MutableStat mean,13478845,Resolved,Major,Fixed,26/Aug/22 15:36,07/Sep/22 05:51,3.4.0,"The current MutableStat mean calculation method is more prone to loss accuracy because the sum of samples is too large. 
Storing large integers in the double type results in a loss of accuracy. For example, 9223372036854775707 and 9223372036854775708 are both stored as doubles as 9223372036854776000. Therefore, we should try to avoid using the cumulative total sum method to calculate the average, but update the average every time we sample. All in all, we can process each sample on its own to improve mean accuracy."
Parameterize platform toolset version,13478971,Resolved,Major,Fixed,28/Aug/22 12:40,30/Aug/22 17:12,3.4.0,"The *winutils*, *libwinutils* and *native* project structures are currently defined in *.vcxproj* and *.sln* files. For building on Windows, a key parameter is the *PlatformToolsetVersion*. This gets added by the build system by running [dev-support/bin/win-vs-upgrade.cmd|https://github.com/apache/hadoop/blob/c60a900583d6a8d0494980f4bbbf4f95438b741b/dev-support/bin/win-vs-upgrade.cmd]. This essentially runs the following command to detect the PlatformToolsetVersion of the currently installed Visual Studio and uses the same for compilation - https://github.com/apache/hadoop/blob/c60a900583d6a8d0494980f4bbbf4f95438b741b/dev-support/bin/win-vs-upgrade.cmd#L38

{code}
devenv %%f /upgrade
{code}

However, when building with *Dockerfile_windows_10*, only Visual Studio 2019 Build Tools are available (and not the full IDE). The Visual Studio 2019 Build Tools distribution doesn't contain *devenv* and thus, the above command fails to run stating that it couldn't find devenv.

To fix this issue, we need the ability to specify the PlatformToolsetVersion as a Maven option, at which point the *win-vs-upgrade.cmd* won't run and would use the speicified PlatformToolsetVersion against MSBuild."
MutableGaugeFloat#incr(float) get stuck in an infinite loop,13479081,Resolved,Major,Fixed,29/Aug/22 11:34,17/Nov/22 09:51,3.4.0,"The current implementation converts the value from int to float, causing the compareAndSet method to get stuck.
{code:java}
private final boolean compareAndSet(float expect, float update) {
  return value.compareAndSet(Float.floatToIntBits(expect),
      Float.floatToIntBits(update));
}

private void incr(float delta) {
  while (true) {
    float current = value.get();
    float next = current + delta;
    if (compareAndSet(current, next)) {
      setChanged();
      return;
    }
  }
} {code}
 

Perhaps it could be:
{code:java}
private void incr(float delta) {
  while (true) {
    float current = Float.intBitsToFloat(value.get());
    float next = current + delta;
    if (compareAndSet(current, next)) {
      setChanged();
      return;
    }
  }
} {code}
 

The unit test looks like this
{code:java}
MutableGaugeFloat mgf = new MutableGaugeFloat(Context,3.2f);
assertEquals(3.2f, mgf.value(), 0.0);
mgf.incr();
assertEquals(4.2f, mgf.value(), 0.0); {code}
 "
Fix main thread name.,13479448,Resolved,Major,Fixed,31/Aug/22 08:32,09/Nov/22 11:20,3.2.1,"The server's main thread is ""Listener at ${hostname}/9000"", it is confused easily. We can see the main thread like below.
{code:java}
""Listener at ${hostname}/9000"" #1 prio=5 os_prio=0 tid=0x00007f8068016000 nid=0x5c086 in Object.wait() [0x00007f806f1d4000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00007f7552553010> (a org.apache.hadoop.ipc.ProtobufRpcEngine$Server)
    at java.lang.Object.wait(Object.java:502)
    at org.apache.hadoop.ipc.Server.join(Server.java:3449)
    - locked <0x00007f7552553010> (a org.apache.hadoop.ipc.ProtobufRpcEngine$Server)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.join(NameNodeRpcServer.java:613)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.join(NameNode.java:1014)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1774)
 {code}"
Remove org.apache.hadoop.maven.plugin.shade.resource.ServicesResourceTransformer,13480038,Resolved,Major,Fixed,04/Sep/22 21:06,07/Sep/22 11:41,3.4.0,"It's in hadoop-maven-plugins.

maven-shade-plugin has ServicesResourceTransformer now - seems better to use that"
Upgrade snakeyaml to 1.32,13480163,Resolved,Major,Fixed,05/Sep/22 17:24,25/Sep/22 14:51,3.3.3,Upgrade snakeyaml to 1.32 to mitigate CVE-2022-25857 and [CVE-2022-38752|https://github.com/advisories/GHSA-9w3m-gqgf-c4p9]
Add Support for localized trash for ViewFileSystem in Trash.moveToAppropriateTrash,13480332,Resolved,Major,Fixed,06/Sep/22 16:59,23/Sep/22 18:05,3.3.5,"Trash.moveToAppropriateTrash is used by _hadoop cli -rm_ and hive, to move files to trash. However, its current implementation does not support localized trash policy we added to ViewFileSystem in HADOOP-18144.

The reason is in moveToAppropriateTrash, it first resolves a path and then uses the resolvedFs, to initialize the trash. As a result, it uses getTrashRoot() implementation from targetFs, not ViewFileSystem. The new localized trash policy we implemented in ViewFileSystem is not invoked.

With the new localized trash policy for ViewFileSystem, the trash root would be local to a mount point, thus, for ViewFileSystem with this flag turned on, there is no need to resolve the path in moveToAppropriateTrash. Rename in ViewFileSystem can resolve the logical paths correctly and be able to move a file to trash within a mount point. 

Code section of current moveToAppropriateTrash implementation.
{code:java}
public static boolean moveToAppropriateTrash(FileSystem fs, Path p,
    Configuration conf) throws IOException {
  Path fullyResolvedPath = fs.resolvePath(p);
  FileSystem fullyResolvedFs =
      FileSystem.get(fullyResolvedPath.toUri(), conf);
  ...
  Trash trash = new Trash(fullyResolvedFs, conf);
  return trash.moveToTrash(fullyResolvedPath);
}{code}"
Fix TestKMS#testKMSHAZooKeeperDelegationToken Failed By Hadoop-18427,13481402,Resolved,Major,Fixed,13/Sep/22 22:58,14/Sep/22 16:14,3.4.0,"The reason for the error is that the Znode is created directly without checking the status of the Znode.

 "
s3a prefetching Executor should be closed,13481935,Resolved,Major,Fixed,16/Sep/22 19:58,21/Sep/22 18:57,3.4.0,"This is the follow-up work for HADOOP-18186. The new executor service we use for s3a prefetching should be closed while shutting down the file system.

Follow up with HADOOP-18466"
ABFS: Support for account level throttling,13482094,Resolved,Major,Fixed,19/Sep/22 03:16,30/Nov/22 14:48,3.3.4,To add support for throttling at account level
AliyunOSS: AliyunOSSBlockOutputStream to support heap/off-heap buffer before uploading data to OSS,13482259,Resolved,Major,Fixed,20/Sep/22 02:12,29/Mar/23 08:04,2.10.2,"Recently, our customers raise a requirement: AliyunOSSBlockOutputStream should support heap/off-heap buffer before uploading data to OSS.

Currently, AliyunOSSBlockOutputStream buffers data in local directory before uploading to OSS, it is not efficient compared to memory.

Changes:
 # Adds heap/off-heap buffers
 # Adds limitation of memory used, and fallback to disk"
InstrumentedWriteLock should consider Reentrant case ,13482587,Resolved,Major,Fixed,21/Sep/22 13:10,17/Oct/22 04:48,3.4.0,"During looking into the InstrumentedWriteLock and InstrumentedReadLock, I found that InstrumentedReadLock consider reentrant case when checking log, but InstrumentedWriteLock does not consider reentrant case. 

So I think InstrumentedWriteLock should consider it."
upgrade jettison json jar due to fix CVE-2022-40149,13483078,Resolved,Major,Fixed,25/Sep/22 16:30,10/Oct/22 10:15,2.10.2,"A fix for [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-40149]

 

[https://github.com/jettison-json/jettison/releases/tag/jettison-1.5.1]

[https://github.com/advisories/GHSA-56h3-78gp-v83r]"
Restrict vectoredIO threadpool to reduce memory pressure,13472151,Resolved,Major,Fixed,18/Jul/22 22:09,28/Sep/22 17:48,,"https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java#L964-L967

Currently, it fetches all the ranges with unbounded threadpool. This will not cause memory pressures with standard benchmarks like TPCDS. However, when large number of ranges are present with large files, this could potentially spike up memory usage of the task. Limiting the threadpool size could reduce the memory usage."
Vectored IO: Threadpool should be closed on interrupts or during close calls,13480567,Resolved,Major,Duplicate,08/Sep/22 03:58,07/Sep/23 16:29,3.3.5,"Vectored IO threadpool should be closed on any interrupts or during S3AFileSystem/S3AInputStream close() calls.

E.g Query which got cancelled in the middle of the run. However, in background (e.g LLAP) vectored IO threads continued to run.

 

!Screenshot 2022-09-08 at 9.22.07 AM.png|width=537,height=164!"
Fix missing package-info in hadoop-common moudle,13473685,Resolved,Major,Won't Fix,27/Jul/22 06:46,08/Aug/23 02:33,3.3.4,"When reading the code, I found that some packages are missing package-info, fix this problem."
Remove the hadoop-openstack module,13480140,Resolved,Major,Fixed,05/Sep/22 13:41,07/Oct/22 11:32,3.3.5," the openstack module doesn't get tested or maintained; it's just something else to keep up to date security wise. As nobody ever files bugs on it it is clearly not being used either.
 
 On-prem object stores support the S3 APIs and/or provide their own hadoop connectors (ozone, IBM).
 
 Let's just cut it completely. As someone who co-authored a lot of it I am happy to do the duty. I will do a quick review of all test to see if there are any left which we could pull into hadoop common...the FS contract tests were initially derived from the ones I did here.
 "
ITestS3AIOStatisticsContext failure,13478258,Resolved,Major,Fixed,23/Aug/22 17:18,28/Sep/22 08:48,3.3.5,"test failure running the new ITestS3AIOStatisticsContext. attaching the stack and log file.

This happened on a large (12 thread) test run, but i can get it to come back intermittently on repeated runs of the whole suite, but never when i just run the single test case.

{code}
[ERROR] testThreadIOStatisticsForDifferentThreads(org.apache.hadoop.fs.s3a.ITestS3AIOStatisticsContext)  Time elapsed: 3.616 s  <<< FAILURE!
java.lang.AssertionError: 
[Counter named stream_write_bytes] 
Expecting actual not to be null
        at org.apache.hadoop.fs.statistics.IOStatisticAssertions.lookupStatistic(IOStatisticAssertions.java:160)
        at org.apache.hadoop.fs.statistics.IOStatisticAssertions.assertThatStatisticLong(IOStatisticAssertions.java:291)
        at org.apache.hadoop.fs.statistics.IOStatisticAssertions.assertThatStatisticCounter(IOStatisticAssertions.java:306)
        at org.apache.hadoop.fs.s3a.ITestS3AIOStatisticsContext.assertThreadStatisticsForThread(ITestS3AIOStatisticsContext.java:367)
        at org.apache.hadoop.fs.s3a.ITestS3AIOStatisticsContext.testThreadIOStatisticsForDifferentThreads(ITestS3AIOStatisticsContext.java:260)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:750)
{code}

I'm suspecting some race condition *or* gc pressure is releasing that reference in the worker thread.

proposed test changes
* worker thread changes its thread ID for the logs
* stores its thread context into a field, so there's guarantee of no GC
* logs more as it goes along."
Fix org.apache.hadoop.io.ArrayWritable,13478744,Open,Major,,26/Aug/22 06:17,,3.3.4,"When we store an array of string with ""new ArrayWritable(String[] strings)"", the ""valueClass"" will be ""Text"" while the actual value type is ""UTF8"". This will cause an error on deserialization of ArrayWritable."
Support EKS for IAM service account,13472380,Resolved,Major,Duplicate,20/Jul/22 06:16,19/Apr/23 09:00,,"Unable to use for authenticating AWS.
*AWS_WEB_IDENTITY_TOKEN_FILE:  /var/run/secrets/[eks.amazonaws.com/serviceaccount/token|http://eks.amazonaws.com/serviceaccount/token]*"
Issues running in dynamic / managed environments,13475801,In Progress,Major,,08/Aug/22 18:54,,3.3.4,"Running in dynamic or managed environments is a challenge because we can't assume that all services will have DNS entries, will be started in a specific order, will maintain constant IP addresses, etc.  I'm using the following assumptions to guide the changes necessary to operate in this kind of environment:
 # The configuration files are an expression of desired state
 # If a referenced service instance is not resolvable or reachable at a moment in time, it will be eventually and should be able to participate in the future, as if it had been there originally, without requiring manual intervention
 # IP address changes should be handled in a way that no only allows distributed calls to continue to function, but avoids having to re-resolve the address over and over
 # Code that requires resolved names (Kerberos and DataNode registration) should fall back to DNS reverse lookups to work around temporary issues caused by caching.  Example: The DataNode registration is only performed at startup, and yet the extra check that allows it to succeed in registering with the NameNode isn’t performed
 # If an HA system is supposed to only require a quorum, then we shouldn’t require the full set, allowing the called service to bring the remaining instances into compliance
 # Managing a service should be independent of other services.  Example: You should be able to perform a rolling restart of JournalNodes without worrying about causing an issue with NameNodes as long as a quorum is present.

A proof of these concepts would be the ability to:
 * Start with less that the full replica count of a service, while still providing the required quorum or minimal count, should still allow a cluster to start and function.  Example: 2 out of 3 configured JournalNodes should still allow the NameNode to format, function, rollover to the standby, etc.
 * Introduce missing instances should join the existing cluster without manual intervention.  Example: Starting the 3rd JournalNode should automatically be formatted and brought up to date
 * Perform rolling restarts of individual services without negatively impacting other services (causing failures, restarts, etc.).  Example: Rolling restarts of JournalNodes shouldn't cause problems in NameNodes; Rolling restarts of NameNodes shouldn't cause problems with DataNodes
 * Logs should only report updated IP addresses once (per dependent), avoiding costly re-resolution"
Update previous index properly while validating overlapping ranges. ,13472970,Resolved,Major,Fixed,22/Jul/22 19:07,11/Aug/22 21:15,,[https://github.com/apache/hadoop/blob/a55ace7bc0c173f609b51e46cb0d4d8bcda3d79d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/VectoredReadUtils.java#L201]
Propagate vectored s3a input stream stats to file system stats.,13475224,Resolved,Major,Fixed,04/Aug/22 19:42,11/Aug/22 21:14,,
Improve VectoredReadUtils#readVectored() for direct buffers,13475209,Resolved,Major,Fixed,04/Aug/22 17:14,31/Aug/22 16:47,3.3.5,"harden the VectoredReadUtils methods for consistent and more robust use, especially in those filesystems which don't have the api.

VectoredReadUtils.readInDirectBuffer should allocate a max buffer size, .e.g 4mb, then do repeated reads and copies; this ensures that you don't OOM with many threads doing ranged requests. other libs do this.

readVectored to call validateNonOverlappingAndReturnSortedRanges before iterating

this ensures the abfs/s3a requirements are always met, and that because ranges will be read in order, prefetching by other clients will keep their performance good.

readVectored to add special handling for 0 byte ranges"
AliyunOSS: AliyunOSSFileSystemStore deleteObjects interface won't return the objects that failed to delete,13479792,Open,Major,,02/Sep/22 07:26,,2.10.2,"AliyunOSSFileSystemStore deleteObjects interface used ""simple mode"" to do batch delete, which  won't return the objects that failed to delete. So, all the failed objects won't be real deleted in the following retries.

  !hadoop fs oss issue.jpg|width=520,height=571!"
DistCP: Aggregate IOStatistics Counters in MapReduce Counters,13473825,Open,Major,,27/Jul/22 16:54,,3.3.5,"Distcp can collect IOStatisticsContext counter values and report them to the console. it can't do the timings in min/mean/max though, as there's no way to aggregate them properly.

# Publish statistics to MapReduce counters in the tasks within CopyMapper.copyFileWithRetry(). 
# The counters will be automatically logged in Job.monitorAndPrintJob() when DistCp is executed with the -verbose option; no need for changes there.
# We could also publish the iOStatistic means by publishing sample count and total sum as two separate counters
# In AbstractContractDistCpTest, add an override point for subclasses to list which metrics they will issue; assert that values are generated.
"
TestRegistryDNS.testMissingReverseLookup Failing,13478598,Open,Major,,25/Aug/22 08:26,,3.3.3,"TestRegistryDNS.testMissingReverseLookup Failing with 

 
|java.lang.AssertionError: Missing record should be: expected:<3> but was:<0>|
|at org.junit.Assert.fail(Assert.java:89)|
|at org.junit.Assert.failNotEquals(Assert.java:835)|
|at org.junit.Assert.assertEquals(Assert.java:647)|
|at org.apache.hadoop.registry.server.dns.TestRegistryDNS.testMissingReverseLookup(TestRegistryDNS.java:351)|
|at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)|
|at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)|
|at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)|
|at java.lang.reflect.Method.invoke(Method.java:498)|
|at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)|
|at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)|
|at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)|
|at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)|
|at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)|
|at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)|
|at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)|
|at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)|
|at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)|
|at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)|
|at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)|
|at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)|
|at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)|
|at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)|
|at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)|
|at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)|
|at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)|
|at org.junit.runners.ParentRunner.run(ParentRunner.java:413)|
|at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)|
|at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)|
|at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)|
|at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)|
|at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)|
|at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)|
|at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)|
|at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)|"
"Running org.apache.hadoop.ha.TestZKFailoverController when ""hadoop.security.groups.cache.secs"" is zero or negative numbers will throw ambiguous exception",13473280,Open,Major,,25/Jul/22 11:00,,2.10.2,"{quote}
{code:java}
<property>
  <name>hadoop.security.groups.cache.secs</name>
  <value>300</value>
  <description>
    This is the config controlling the validity of the entries in the cache
    containing the user->group mapping. When this duration has expired,
    then the implementation of the group mapping provider is invoked to get
    the groups of the user and then cached back.
  </description>
</property>{code}
{quote}
As we see  in core-default.xml of hadoop.security.groups.cache.secs,  the default value is 300. But when we set it to zero or negative number and  then run org.apache.hadoop.ha.TestZKFailoverController#testGracefulFailoverMultipleZKfcs, it will throw NullPointerException as below：
{quote}
{code:java}
[INFO] Running org.apache.hadoop.ha.TestZKFailoverController
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.932 s <<< FAILURE! - in org.apache.hadoop.ha.TestZKFailoverController
[ERROR] testGracefulFailoverMultipleZKfcs(org.apache.hadoop.ha.TestZKFailoverController)  Time elapsed: 0.799 s  <<< ERROR!
java.lang.NullPointerException
        at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:188)
        at org.apache.hadoop.ha.MiniZKFCCluster.start(MiniZKFCCluster.java:116)
        at org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailoverMultipleZKfcs(TestZKFailoverController.java:581)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:53)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:748){code}
{quote}
 "
Add the ability to support multiple threads to zstd,13481862,Open,Major,,16/Sep/22 10:22,,,Add the ability to support multiple threads to Zstd
Replace Google Analytics with ASF Matomo in website,13479985,Open,Major,,03/Sep/22 18:21,,,"We currently use Google Analytics (https://github.com/apache/hadoop-site/blob/asf-site/layouts/partials/footer.html#L37) in the website but it's not recommended.

Also we should link the Privacy Policy to https://privacy.apache.org/policies/privacy-policy-public.html instead of Hadoop's one https://hadoop.apache.org/privacy_policy.html

For more details, please check: https://privacy.apache.org/faq/committers.html and https://www.apache.org/foundation/marks/pmcs.html#navigation"
s3a endpoint per bucket configuration in pyspark is ignored,13480683,Resolved,Major,Invalid,08/Sep/22 14:50,19/Sep/22 11:32,,"I'm using EMR emr-6.5.0 cluster in us-east-1 with ec2 instances. cluster is running spark application using pyspark 3.2.1
 EMR is using Hadoop distribution:Amazon 3.2.1
my spark application is reading from one bucket in us-west-2 and writing to a bucket in us-east-1.
since I'm processing a large amount of data I'm paying a lot of money for the network transport . in order to reduce the cost I have create a vpc interface to s3 endpoint in us-west-2. inside the spark application I'm using aws cli for reading the file names from us-west-2 bucket and it is working through the s3 interface endpoint but when I use pyspark to read the data it is using the us-east-1 s3 endpoint instead of the us-west-2 endpoint.
 I tried to use per bucket configuration but it is being ignored although I added it to the defualt configuration and to spark submit call.
I tried to set the following configuration but they are ignored:
 '--conf', ""spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain"",
 '--conf', ""spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"",
 '--conf', ""spark.hadoop.fs.s3a.bucket.<us-west-2-bucket -name>.endpoint=<my vpc endpoint>"",
 '--conf', ""spark.hadoop.fs.s3a.bucket.<us-west-2-bucket -name>.endpoint.region=us-west-2"",
 '--conf', ""spark.hadoop.fs.s3a.bucket.<us-east-1-bucket -name>.endpoint=<vpc gateway endpoint>"",
 '--conf', ""spark.hadoop.fs.s3a.bucket.<us-east-1-bucket -name>.endpoint.region=us-east-1"",
 '--conf', ""spark.hadoop.fs.s3a.path.style.access=false"""
Add an integration test to process data asynchronously during vectored read.,13482672,Resolved,Major,Fixed,22/Sep/22 02:24,28/Sep/22 17:48,,
TestDelegatingSSLSocketFactory.testOpenSSL failing on ARM aarch64,13482527,Open,Major,,21/Sep/22 08:16,,3.3.4,i run hadoop maven test in ARM aarch64 and throw Error constructing implementation，problem class is org.wildfly.openssl.OpenSSLContextSPI$OpenSSLTLSContextSpi. i have tested this problem didnt show in the x86_64.
Update hsqldb.version from 2.3.4 to 2.5.2,13481193,Resolved,Major,Fixed,13/Sep/22 00:49,20/Sep/22 18:11,3.4.0,"I plan to upgrade the version of hsqldb from 2.3.4 to 2.5.2 for the following reasons:

1.Current version 2.3.4 is almost ~6 years old，Upgrading to new release to keep up for new features and bug fixes.

2.I plan to increase the verification of the table building statement, which needs to use the compatibility mode of Mysql and SqlServer. 
The 2.5.2 version of hsqldb does better.

3.We are temporarily unable to upgrade hsqldb to version 2.6.0 because version 2.6.0 depends on JDK11."
Upgrade hadolint to 2.10.0,13480957,Open,Major,,10/Sep/22 04:43,,3.4.0,"The current version of hadolint (1.11.1) is only suitable for linting Linux commands in Dockerfile. It fails to recognize Windows command syntax.
HADOOP-18133 adds Dockerfile for Windows. Thus, it's essential to upgrade hadolint to 2.10.0 which has the ability to recognize Windows command syntax."
Fix VectoredIO for LocalFileSystem when checksum is enabled.,13479929,Resolved,Major,Fixed,02/Sep/22 19:09,09/Sep/22 16:41,3.3.5,"While merging the ranges in CheckSumFs, they are rounded up based on the value of checksum bytes size
which leads to some ranges crossing the EOF thus they need to be fixed else it will cause EOFException during actual reads."
Migrate to new azure-storage SDKs,13480466,Open,Major,,07/Sep/22 11:42,,,"The last time the {{azure-storage}} sdk was updated was more than 2 years ago:
[https://github.com/apache/hadoop/blame/trunk/hadoop-project/pom.xml#L1545]

The sdk have since been updated and have gone through major design changes, the reasoning for which are documented here:
[https://github.com/Azure/azure-storage-java/blob/master/V12%20Upgrade%20Story.md]

Upgrading to the latest sdk will bring many improvements, performance improvements and bug fixes - too many to list or count.
In order the to move forward with time, and avoid being stuck when the service API versions start getting deprecated and removed, this needs to be addressed.

The needed changes would be mostly in the following modules:
 # [https://github.com/apache/hadoop/tree/trunk/hadoop-tools/hadoop-azure]
 # [https://github.com/apache/hadoop/tree/trunk/hadoop-tools/hadoop-azure-datalake]

 "
TestArnResource.parseAccessPointFromArn failing intermittently,13478606,Resolved,Major,Duplicate,25/Aug/22 08:33,05/Sep/22 13:24,3.3.3,"TestArnResource.parseAccessPointFromArn failing with

 
{code}

|org.junit.ComparisonFailure: Endpoint does not match expected:<s3[-accesspoint.]eu-west-1.amazonaws....> but was:<s3[.accesspoint-]eu-west-1.amazonaws....>|
|at org.junit.Assert.assertEquals(Assert.java:117)|
|at org.apache.hadoop.fs.s3a.TestArnResource.parseAccessPointFromArn(TestArnResource.java:60)|
|at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)|
|at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)|
|at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)|
|at java.lang.reflect.Method.invoke(Method.java:498)|
|at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)|
|at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)|
|at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)|
|at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)|
|at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)|
|at java.util.concurrent.FutureTask.run(FutureTask.java:266)|
|at java.lang.Thread.run(Thread.java:748)|
| |

{code}

"
List of test failing in 3.3.3,13478613,Resolved,Major,Duplicate,25/Aug/22 08:41,01/Sep/22 14:04,3.3.3,"# org.apache.hadoop.fs.s3a.TestArnResource

|org.junit.ComparisonFailure: Endpoint does not match expected:<s3[-accesspoint.]eu-west-1.amazonaws....> but was:<s3[.accesspoint-]eu-west-1.amazonaws....>|
|at org.junit.Assert.assertEquals(Assert.java:117)|
|at org.apache.hadoop.fs.s3a.TestArnResource.parseAccessPointFromArn(TestArnResource.java:60)|
|at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)|
|at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)|
|at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)|
|at java.lang.reflect.Method.invoke(Method.java:498)|
|at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)|
|at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)|
|at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)|
|at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)|
|at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)|
|at java.util.concurrent.FutureTask.run(FutureTask.java:266)|
|at java.lang.Thread.run(Thread.java:748)|
| |
|[System Output]|
|2022-08-25 05:43:12,254 [JUnit-parseAccessPointFromArn] INFO s3a.TestArnResource (TestArnResource.java:describe(73)) - Parse AccessPoint ArnResource from arn string|

 #  "
Proxy users do not share RPC connections,13479523,Open,Major,,31/Aug/22 16:34,,,"When the Hive MetaStore uses Storage-Based Authorization, it needs to perform checks against the NameNode as the query's user. Unfortunately, RPC's ConnectionId uses the UGI's equal & hash functions, which check for the subject's object equality.

 

Thus, we've seen the HMS spawn thousands of threads before they go idle and are eventually closed. If the peak goes over 10k threads the HMS becomes unstable."
hadoop 3.3.4 doesn't have a binary-aarch64 download link,13479403,Resolved,Major,Duplicate,31/Aug/22 03:17,31/Aug/22 15:45,3.3.4,"[Apache Hadoop|https://hadoop.apache.org/releases.html] !image-2022-08-31-11-16-56-994.png!

the link is empty

 

!image-2022-08-31-11-17-31-812.png!"
Replace Sets#newHashSet() and newTreeSet() with constructors directly hadoop-mapreduce,13477797,Open,Major,,21/Aug/22 05:10,,,
Replace Sets#newHashSet() and newTreeSet() with constructors directly hadoop-tool,13477798,Open,Major,,21/Aug/22 05:11,,,
Upgrade bundled Tomcat to 8.5.82,13478435,Resolved,Major,Fixed,24/Aug/22 14:46,25/Aug/22 04:08,2.10.2,"h4.  

Currently we are using 8.5.81 which is affected by CVE-2022-34305

More Details - [https://github.com/advisories/GHSA-6j88-6whg-x687]

Lets upgrade  to 8.5.82

 "
Replace Sets#newHashSet() and newTreeSet() with constructors directly in hadoop-client-modules,13477795,Resolved,Major,Invalid,21/Aug/22 05:05,24/Aug/22 19:11,,
Replace Sets#newHashSet() and newTreeSet() with constructors directly in hadoop-cloud-storage-project,13477796,Resolved,Major,Invalid,21/Aug/22 05:06,24/Aug/22 19:11,,
Support @Metric annotation on String fields similarly as with methods returning String,13477571,Open,Major,,19/Aug/22 09:41,,,"In the Metrics2 framework, if a method is annotated with Metric annotation, and if it returns a String, then the String is understood as a TAG.

A field that is annotated with the Metric annotation on the other hand is not understood as a tag, even if the type of the annotation is set to Metric.Type.TAG, and gets ignored if the field type is String.

It would be great if Metric annotated String fields would have the same default behaviour as Metric annotated methods that return String value.

This has come up as part of HDDS-7120 (discussion is in the PR for that ticket)."
HEAD OBJECT returns only 400 BAD REQUEST when token is expired,13472510,Resolved,Major,Information Provided,20/Jul/22 17:42,05/Aug/22 16:04,3.3.3,"I tried reproducing this today by changing this test [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ATemporaryCredentials.java#L116] . Getting a session token for 15 mins and trying every one mins and finally it fails after 15 mins. 
 
Looks like the AWS SDK is not having the Expired Token error message as I could see the same in access logs but I see BadRequest on the SDK logs. 
 
*S3A Connector logs with SDK debug enabled.*

2022-07-13 15:44:15,318 [JUnit-testSTS] DEBUG s3a.AWSCredentialProviderList (AWSCredentialProviderList.java:getCredentials(184)) - Using credentials from TemporaryAWSCredentialsProvider
2022-07-13 15:44:15,319 [JUnit-testSTS] DEBUG amazonaws.request (AmazonHttpClient.java:executeOneRequest(1285)) - Sending Request: HEAD [https://mthakur-us-west-1.s3.us-west-1.amazonaws.com|https://mthakur-us-west-1.s3.us-west-1.amazonaws.com/] /test/testSTS/040112e1-d954-46d9-9def-aedd297bd42e Headers: (amz-sdk-invocation-id: 41e6e504-1c2b-2701-09bb-ae692dff2515, Content-Type: application/octet-stream, Referer: [https://audit.example.org/hadoop/1/op_create/ca2778f8-085e-4d1f-aef3-73794869f275-00000098/?op=op_create&p1=test/testSTS/040112e1-d954-46d9-9def-aedd297bd42e&pr=mthakur&ps=46c6d232-80aa-4405-9e39-5df880932fdc&id=ca2778f8-085e-4d1f-aef3-73794869f275-00000098&t0=11&fs=ca2778f8-085e-4d1f-aef3-73794869f275&t1=11&ts=1657745055318], User-Agent: Hadoop 3.4.0-SNAPSHOT, aws-sdk-java/1.12.132 Mac_OS_X/10.15.7 Java_HotSpot(TM)_64-Bit_Server_VM/25.161-b12 java/1.8.0_161 kotlin/1.4.10 vendor/Oracle_Corporation cfg/retry-mode/legacy, )
2022-07-13 15:44:15,623 [JUnit-testSTS] DEBUG amazonaws.request (AmazonHttpClient.java:handleErrorResponse(1846)) - \{*}Received error response: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID{*}: WMGQ0KC4MHEMZTQC; S3 Extended Request ID: IztdwNq71aWBYavfaj8rV5b/Y0GzV4tqJBEVDSdZH+RRR3B1vUVIMV0qWez9ulBrjDM1GQxeT1Q=; Proxy: null), S3 Extended Request ID: IztdwNq71aWBYavfaj8rV5b/Y0GzV4tqJBEVDSdZH+RRR3B1vUVIMV0qWez9ulBrjDM1GQxeT1Q=
2022-07-13 15:44:15,624 [JUnit-testSTS] DEBUG s3a.S3AFileSystem (S3AFileSystem.java:close(3814)) - Filesystem s3a://mthakur-us-west-1 is closed


*AWS access logs*

183c9826b45486e485693808f38e2c4071004bf5dfd4c3ab210f0a21a4235ef8 mthakur-us-west-1 [13/Jul/2022:20:44:15 +0000] 67.79.115.98 - WMGQ0KC4MHEMZTQC REST.HEAD.OBJECT test/testSTS/040112e1-d954-46d9-9def-aedd297bd42e ""HEAD /test/testSTS/040112e1-d954-46d9-9def-aedd297bd42e HTTP/1.1"" *400 ExpiredToken* 556 - 5 - ""[https://audit.example.org/hadoop/1/op_create/ca2778f8-085e-4d1f-aef3-73794869f275-00000098/?op=op_create&p1=test/testSTS/040112e1-d954-46d9-9def-aedd297bd42e&pr=mthakur&ps=46c6d232-80aa-4405-9e39-5df880932fdc&id=ca2778f8-085e-4d1f-aef3-73794869f275-00000098&t0=11&fs=ca2778f8-085e-4d1f-aef3-73794869f275&t1=11&ts=1657745055318]"" ""Hadoop 3.4.0-SNAPSHOT, aws-sdk-java/1.12.132 Mac_OS_X/10.15.7 Java_HotSpot(TM)_64-Bit_Server_VM/25.161-b12 java/1.8.0_161 kotlin/1.4.10 vendor/Oracle_Corporation cfg/retry-mode/legacy"" - IztdwNq71aWBYavfaj8rV5b/Y0GzV4tqJBEVDSdZH+RRR3B1vUVIMV0qWez9ulBrjDM1GQxeT1Q= SigV4 ECDHE-RSA-AES128-SHA AuthHeader [mthakur-us-west-1.s3.us-west-1.amazonaws.com|http://mthakur-us-west-1.s3.us-west-1.amazonaws.com/] TLSv1.2 -
 
I tested by running repeatedly ITestCustomSigner in S3A, and also just ListObjectsV2 on loop… I did just notice your test is failing with HEAD, and *I can reproduce* by running this after credential expiry.

 

aws s3api head-object --bucket djonesoa-us-west-2 --region us-west-2 --key test-object –debug

 
To summarise:
 * If I run ListObjectsV2, I get “400 ExpiredToken”{+}{+}{+}{+}
 * If I run HeadObject, I get “400 Bad Request”{+}{+}{+}{+}
 * If I run GetObject, I get “400 ExpiredToken”"
Unable to Find LoginModel Class using IBM Java openJ9 version 8.0.332.0,13477600,Open,Major,,19/Aug/22 12:26,,3.3.4,"Hi,

I am using Spark v.3.3.0 and Java version IBM Semeru 8.0.332.0.

When I run my Spark Job I get the following exception:

org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: unable to find LoginModule class: 
com.ibm.security.auth.module.JAASLoginModule
    at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
    at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
    at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2561)
    at scala.Option.getOrElse(Option.scala:138)
    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2561)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:316)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
    at scala.Option.getOrElse(Option.scala:138)

This looks similar to a previously reported error that has been fixed: https://issues.apache.org/jira/browse/HADOOP-17971

N.B. The exception I am getting does not contain 'org.apache.hadoop.shaded"" in the package name, whereas in HADOOP-17971 it does: (org.apache.hadoop.shaded.com.ibm.security.auth.module.JAASLoginModule).

 

Spark spark-core_2.12 library contains two Hadoop dependencies:

hadoop-client-api:jar:3.3.2:compile

hadoop-client-runtime:jar:3.3.2:compile

After getting the exception, I tried excluding those components from the Spark dependency in my pom.xml, and explicitly defined them as dependencies. I tried versions 3.3.4 and 3.3.3 but I still get the same error.

 

N.B. I don't get this exception with Java version IBM Semeru 8.0.312.0

I can move this to a Spark issue if this isn't the correct place to post it.

Thanks,

Steve

 "
Prevent AvroRecord*.class from being included non-test jar,13476143,Resolved,Major,Fixed,10/Aug/22 09:54,22/Aug/22 09:55,3.3.3,"bq. 
{code}
[WARNING] Rule 1: org.apache.maven.plugins.enforcer.BanDuplicateClasses failed with message:
Duplicate classes found:

Found in:
org.apache.hadoop:hadoop-client-minicluster:jar:3.3.4:compile
org.apache.hadoop:hadoop-client-api:jar:3.3.4:compile
Duplicate classes:
org/apache/hadoop/io/serializer/avro/AvroRecord.class
org/apache/hadoop/io/serializer/avro/AvroRecord$Builder.class
org/apache/hadoop/io/serializer/avro/AvroRecord$1.class

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M1:enforce (enforce-banned-dependencies) on project hadoop-client-check-test-invariants: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1] 
{code}


When building Hadoop, AvroRecord*.class are included in hadoop-client-api jar and I think it is wrong.
It is caused by ""protobuf-maven-plugin"" as a side effect.

https://github.com/apache/hadoop/blob/rel/release-3.3.4/hadoop-common-project/hadoop-common/pom.xml#L1118


{code:xml}
              <execution>
                <id>src-test-compile-protoc-legacy</id>
                <phase>generate-test-sources</phase>
                <goals>
                  <goal>compile</goal>
                </goals>
                <configuration>
                  <skip>false</skip>
                  <!--Generating with old protobuf version for backward compatibility-->
                  <protocArtifact>
                    com.google.protobuf:protoc:${protobuf.version}:exe:${os.detected.classifier}
                  </protocArtifact>
                  <includeDependenciesInDescriptorSet>false</includeDependenciesInDescriptorSet>
                  <protoSourceRoot>${basedir}/src/test/proto</protoSourceRoot>
                  <outputDirectory>${project.build.directory}/generated-test-sources/java</outputDirectory>
                  <clearOutputDirectory>false</clearOutputDirectory>
                  <includes>
                    <include>test_legacy.proto</include>
                    <include>test_rpc_service_legacy.proto</include>
                  </includes>
                </configuration>
              </execution>
{code}

""src-test-compile-protoc-legacy"" 's goal is written ""compile"" wrongly. It makes outputDirectory (${project.build.directory}/generated-test-sources/java) to be added to ""compileSourceRoots"" of maven-compiler-plugin.
""src-test-compile-protoc-legacy"" 's goal should be ""test-compile"".
"
ITestS3ACannedACLs failure; not in a span,13474703,Resolved,Major,Fixed,02/Aug/22 13:17,18/Aug/22 15:56,,"seen in a test of the prefetcn feature branch, but it looks more like this has been lurking for a long time, or just that some code change has moved the api call out of a span.


{code}
[INFO] Running org.apache.hadoop.fs.s3a.ITestS3ACannedACLs
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.592 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3ACannedACLs
[ERROR] testCreatedObjectsHaveACLs(org.apache.hadoop.fs.s3a.ITestS3ACannedACLs)  Time elapsed: 0.591 s  <<< ERROR!
org.apache.hadoop.fs.s3a.audit.AuditFailureException: dbb71c86-e022-4b76-99cf-c1f64dd21389-00013058 unaudited operation executing a request outside an audit span {com.amazonaws.services.s3.model.GetObjectAclRequest size=0, mutating=true}
        at org.apache.hadoop.fs.s3a.ITestS3ACannedACLs.assertObjectHasLoggingGrant(ITestS3ACannedACLs.java:94)
        at org.apache.hadoop.fs.s3a.ITestS3ACannedACLs.testCreatedObjectsHaveACLs(ITestS3ACannedACLs.java:69)
{code}


fix is trivial: do the operation within a span"
ITestS3SelectLandsat timeout after 10 minutes,13474719,Resolved,Major,Duplicate,02/Aug/22 14:51,05/Aug/22 15:32,3.4.0,"timeout doing a full read of the s3 select file through the gzip codec.

{code}
[ERROR] testSelectSeekFullLandsat(org.apache.hadoop.fs.s3a.select.ITestS3SelectLandsat)  Time elapsed: 600.006 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 600000 milliseconds
	at java.lang.Throwable.getStackTraceElement(Native Method)
	at java.lang.Throwable.getOurStackTrace(Throwable.java:828)
	at java.lang.Throwable.getStackTrace(Throwable.java:817)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.log4j.spi.LocationInfo.<init>(LocationInfo.java:139)
	at org.apache.log4j.spi.LoggingEvent.getLocationInformation(LoggingEvent.java:253)
	at org.apache.log4j.helpers.PatternParser$LocationPatternConverter.convert(PatternParser.java:500)
	at org.apache.log4j.helpers.PatternConverter.format(PatternConverter.java:65)
	at org.apache.log4j.PatternLayout.format(PatternLayout.java:506)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.log(Category.java:856)
	at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:230)
	at org.apache.hadoop.util.DurationInfo.close(DurationInfo.java:101)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:123)
	at org.apache.hadoop.fs.s3a.select.SelectInputStream.read(SelectInputStream.java:246)
	at org.apache.hadoop.fs.s3a.select.SelectInputStream.seek(SelectInputStream.java:324)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:73)
	at org.apache.hadoop.fs.s3a.select.AbstractS3SelectTest.seek(AbstractS3SelectTest.java:701)
	at org.apache.hadoop.fs.s3a.select.ITestS3SelectLandsat.testSelectSeekFullLandsat(ITestS3SelectLandsat.java:427)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j

{code}
"
Limit stacked call of one connection in client to avoid possible oom in server,13474915,Open,Major,,03/Aug/22 16:26,,,"In our prod environment, we encountered an accident that JN OOM because Server#Connection#responseQueue used 97% memory.

After analyzed the memory of JN and found that there are 2w+ called stacked in one Server#Connection#responseQueue, because the network between NN and JN jitters with some tcp packet loss.

!image-2022-08-04-00-22-28-865.png|width=561,height=254!

!image-2022-08-04-00-23-18-427.png|width=559,height=356!

 

In this case, I think Client.java should support limit the stacked calls of one connection to avoid the possible OOM in Server.  When the number of stacked calls is more than the limit size, we can just throw one IOException to the method caller."
rebase feature/HADOOP-18028-s3a-prefetch to trunk,13474009,Resolved,Major,Done,28/Jul/22 13:42,03/Aug/22 15:04,,"rebase to trunk, fix conflicts and tests, force push"
Fix/Improve DistCp doc,13474300,Open,Major,,30/Jul/22 05:37,,,"Fix/Improve stuff in DistCp doc:
 * Missing Headings from the index like (DistCp and Object Stores)
 * Add about -delete along with -update in example for Object Store usage.
 * Check the usage description for -update. It says it matches blocksize as well, which isn't true unless you specify -pb and if you do so, for cloud storages it would overwrite always.(AFAIK) need to double check.

And if possible other improvements"
Upgrade reload4j to 1.2.22 due to XXE vulnerability,13472788,Resolved,Major,Fixed,21/Jul/22 22:33,24/Jul/22 10:32,,https://github.com/qos-ch/reload4j/issues/53 fixed in reload4j 1.2.22
Retarget solution file to VS2019,13473078,Resolved,Major,Abandoned,23/Jul/22 18:43,28/Jul/22 17:22,3.4.0,The Visual Studio version used by winutils and native components in Hadoop common are quite old. We need to retarget the solution and vcxproj files to use the latest version (Visual Studio 2019 as of this writing).
Support for hadoop-aws with aws-java-sdk-bundle with version greater than 1.12.220,13472420,Resolved,Major,Duplicate,20/Jul/22 08:37,27/Jul/22 10:35,,"There are CVEs like  CVE-2021-37137  and many, listed from aws-java-sdk-bundle with version 1.11.375 and the fix is available in versions higher than 1.12.220. It will be great if we have a hadoop-aws with aws-java-sdk-bundle.jar with latest version. Will you be able to provide the same? If so may I know approximately when can I expect it?"
Fix failure of instanciation of s3gatewayrequestlog appender due to lack of HttpRequestLogAppender,13473933,Resolved,Major,Invalid,28/Jul/22 07:49,28/Jul/22 08:05,,"{noformat}
$ bin/hadoop version
log4j:ERROR Could not instantiate class [org.apache.hadoop.http.HttpRequestLogAppender].
java.lang.ClassNotFoundException: org.apache.hadoop.http.HttpRequestLogAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
        at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.hadoop.util.VersionInfo.<clinit>(VersionInfo.java:37)
log4j:ERROR Could not instantiate appender named ""s3gatewayrequestlog"".
Hadoop 3.4.0-SNAPSHOT
Source code repository Unknown -r c92ff0b4f1135e84527e25087d9cc65250bd0222
Compiled by rocky on 2022-07-27T23:17Z
Compiled with protoc 3.7.1
From source with checksum 486fac66d9b4349cc0fd3d5e8bc38e40
This command was run using /home/rocky/dist/hadoop-3.4.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.4.0-SNAPSHOT.jar
{noformat}
"
Enable TLS in RPC Client / Server - Work Description,13473672,Open,Major,,27/Jul/22 04:56,,,"This Jira has been created to describe the work happening in HADOOP-15980. 

 

In acknowledgement of [~daryn] 's initial contribution that I used to build the final TLS implementation, I have retained HADOOP-15980 in his name.

 

However this results in me not being able to upload design documents to the JIRA. Hence I have created this subtask in which I will upload the design documents and link them to HADOOP-15980.

 

[^Securing Hadoop RPC using SSL.pdf]"
S3A prefetching to update IOStatisticsContext,13473532,Open,Major,,26/Jul/22 11:07,,,"Once HADOOP-17461 is in, the S3A prefetching stream should update the IOStatisticsContext of the thread in which it was constructed (doing so in close() is sufficient).

"
Echo java process's parent pid to the pid file intermediate state,13472212,Patch Available,Major,,19/Jul/22 08:03,,3.3.1,"In hadoop-function.sh file，there is hadoop_start_daemon and hadoop_start_daemon_wrapper functions.

hadoop_start_daemon_wrapper invoke hadoop_start_daemon and put it to background.

 

In  hadoop_start_daemon function, echo $$ > pidfile，cause this scenario

because hadoop_start_daemon is in a subshell by ampersand, and $ expands to the process ID of the current shell, not the subshell.

 "
upgrade to jetty 9.4.48 due to CVE,13472056,Resolved,Major,Duplicate,18/Jul/22 10:52,18/Jul/22 11:09,,"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-2047

Use 9.4.48 (latest current v9 release) to pick up newer bug fixes"
Unable to access data from S3 bucket over a vpc endpoint - 400 bad request,13471385,Resolved,Major,Not A Problem,13/Jul/22 11:00,13/Jul/22 11:49,,"We are trying to write to S3 bucket which has policy with specific IAM Users, SSE and endpoint.  So this bucket has 2 endpoints mentioned in policy : gateway endpoint and interface endpoint.

 

When we use gateway endpoint which is general one: [https://s3.us-east-1.amazonaws.com|https://s3.us-east-1.amazonaws.com/] => spark code executes successfully and writes to S3 bucket

But when we use interface endpoint (which we have to use ideally): [https://bucket.vpce-<>.s3.us-east-1.vpce.amazonaws.com|https://bucket.vpce-%3C%3E.s3.us-east-1.vpce.amazonaws.com/] => spark code throws an error as :

 

py4j.protocol.Py4JJavaError: An error occurred while calling o91.save.

: org.apache.hadoop.fs.s3a.AWSBadRequestException: doesBucketExist on <BUCKET NAME>: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: BA67GFNR0Q127VFM; S3 Extended Request ID: BopO6Cn1hNzXdWh89hZlnl/QyTJef/1cxmptuP6f4yH7tqfMO36s/7mF+q8v6L5+FmYHXbFdEss=; Proxy: null), S3 Extended Request ID: BopO6Cn1hNzXdWh89hZlnl/QyTJef/1cxmptuP6f4yH7tqfMO36s/7mF+q8v6L5+FmYHXbFdEss=:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: BA67GFNR0Q127VFM; S3 Extended Request ID: BopO6Cn1hNzXdWh89hZlnl/QyTJef/1cxmptuP6f4yH7tqfMO36s/7mF+q8v6L5+FmYHXbFdEss=; Proxy: null)

 

Attaching the pyspark code and exception trace

  [^spark_s3.txt]

^[^spark_s3_vpce_error.txt]^"
Fix create-release to address removal of GPG_AGENT_INFO in branch-3.2,13471118,Resolved,Major,Fixed,12/Jul/22 04:08,12/Jul/22 11:44,3.2.3,"gpg v2.1 and above does not export GPG_AGENT_INFO. create-release script need to export the info by itself to make {{--sign}} work. It was addressed as part of HADOOP-16797 in branch-3.3 and trunk. Since we can not backport aarch64 support to branch-3.2, I filed this issue for branch-3.2 only."
Shouldn't relocate org/wildfly/openssl in shaded client,13470812,Resolved,Major,Duplicate,09/Jul/22 15:38,09/Jul/22 15:41,3.3.2,"We shouldn't shade {{org.wildfly.openssl}} since it's provided dependency and will give error like the following:
{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/org/wildfly/openssl/OpenSSLProvider
        at org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.bindToOpenSSLProvider(DelegatingSSLSocketFactory.java:196)
        at org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.initializeSSLContext(DelegatingSSLSocketFactory.java:169)
        at org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.<init>(DelegatingSSLSocketFactory.java:134)
        at org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory.initializeDefaultFactory(DelegatingSSLSocketFactory.java:105)
        at org.apache.hadoop.fs.s3a.impl.NetworkBinding.bindSSLChannelMode(NetworkBinding.java:82)
        at org.apache.hadoop.fs.s3a.S3AUtils.initProtocolSettings(S3AUtils.java:1347)
        at org.apache.hadoop.fs.s3a.S3AUtils.initConnectionSettings(S3AUtils.java:1302)
        at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1259)
        at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:846)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:503)
{code}"
Need to introduce OSS scan for security vulnerabilities during builds,13470235,Open,Major,,06/Jul/22 05:03,,3.3.3,Early detection/resolution of OSS security vulnerabilities should be part of build/delivery pipelines.
Support AWS IAM Identity Centre (prev. AWS SSO) for providing credentials to S3A,13472470,Open,Minor,,20/Jul/22 12:18,,3.3.3,"HADOOP-18073 was opened regarding upgrading AWS SDK to V2 which supports a credential provider for AWS SSO. Opening this ticket to track that feature explicitly separately from the SDK upgrade.

Related SDK issue for AWS SSO support in AWS SDK for Java V1: [https://github.com/aws/aws-sdk-java/issues/2434]

In particular, from [https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html], when to set 'fs.s3a.aws.credentials.provider', it must be ""com.amazonaws.auth.AWSCredentialsProvider"". We would like to support ""software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider"" which supports AWS SSO, so users only need to authenticate once."
abfs testReadAndWriteWithDifferentBufferSizesAndSeek failure,13476818,Open,Minor,,15/Aug/22 12:18,,3.4.0,"(possibly transient) failure of testReadAndWriteWithDifferentBufferSizesAndSeek on a parallel test run.

this was a run done with a VPN enabled; this may be causing problems. certainly the run was slow

{code}
[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 701.903 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek
[ERROR] testReadAndWriteWithDifferentBufferSizesAndSeek[Size=104,857,600](org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek)  Time elapsed: 673.614 s  <<< FAILURE!
org.junit.ComparisonFailure: [Retry was required due to issue on server side] expected:<[0]> but was:<[1]>
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at org.apache.hadoop.fs.azurebfs.utils.TracingHeaderValidator.validateBasicFormat(TracingHeaderValidator.java:136)
        at org.apache.hadoop.fs.azurebfs.utils.TracingHeaderValidator.validateTracingHeader(TracingHeaderValidator.java:77)
        at org.apache.hadoop.fs.azurebfs.utils.TracingHeaderValidator.callTracingHeaderValidator(TracingHeaderValidator.java:46)
        at org.apache.hadoop.fs.azurebfs.utils.TracingContext.constructHeader(TracingContext.java:172)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:249)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:217)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)
        at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)
        at org.apache.hadoop.fs.azurebfs.services.AbfsClient.read(AbfsClient.java:853)
        at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:544)
        at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:510)
        at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:317)
        at org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:263)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek.testReadWriteAndSeek(ITestAbfsReadWriteAndSeek.java:110)
        at org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek(ITestAbfsReadWriteAndSeek.java:69)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:750)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek:69->testReadWriteAndSeek:110 [Retry was required due to issue on server side] expected:<[0]> but was:<[1]>
[INFO] 
[ERROR] Tests run: 332, Failures: 1, Errors: 0, Skipped: 41

{code}
 
build params {{-Dparallel-tests=abfs -DtestsThreadCount=8 -Dscale}}"
Update commons-math3 from 3.1.1 to 3.6.1.,13473101,Resolved,Minor,Fixed,24/Jul/22 04:54,01/Aug/22 20:19,3.4.0,"I found that commons-math3 can be upgraded from 3.1.1 to 3.6.1. Try to upgrade, local compilation and verification are correct."
tag FSDataInputStream.getWrappedStream() @Public/@Stable,13471203,Resolved,Minor,Fixed,12/Jul/22 11:08,13/Jul/22 11:58,3.3.3,"PARQUET-2134 shows external code is calling FSDataInputStream.getWrappedStream()

tag ase @Public/@Stable as it has been stable and we should acknowledge that use & know not to break it"
S3A prefetching: Error logging during reads,13472451,Resolved,Minor,Fixed,20/Jul/22 10:37,15/Feb/23 18:29,3.4.0,"Look at how errors during read are logged, current implementation could flood logs with stack traces on failures.

proposed
 * errors in prefetch only logged at info with error text but not stack
 * full stack logged at debug

but: we do want the most recent failure to be raised on the next read() on the stream when there is no data in the cache."
Update commons-csv from 1.0 to 1.9.0.,13473154,Resolved,Minor,Fixed,25/Jul/22 02:27,13/Oct/22 06:41,3.4.0,"commons-csv 1.0 is a very old jar, mvnrepository shows this jar as the 2014 version, I have compiled and tested locally, I think this jar can be upgraded to commons-csv 1.9 version.

The link to the release note is as follows:
[https://commons.apache.org/proper/commons-csv/changes-report.html]

We can see that the new version fixes some issues.

I read the code used, we use header related methods. We found that many header-related methods have been upgraded.

*Release 1.1 – 2014-11-16*
CSVFormat#withHeader doesn't work well with #printComment, add withHeaderComments(String...).
CSVFormat.EXCEL should ignore empty header names.

*Release 1.2 – 2015-08-24*
CSVFormat.with* methods clear the header comments.

*Release 1.3 – 2016-05-09*
Add shortcut method for using first record as header to CSVFormat.
Add withHeader(Class<? extends Enum>) to CSVFormat.
CSVPrinter doesn't skip creation of header record if skipHeaderRecord is set to true.
Add IgnoreCase option for accessing header names.

*Release 1.5 – 2017-09-03*
Fix incorrect method name 'withFirstRowAsHeader' in user guide.

*Release 1.7 – 2019-06-01*
Cannot get headers in column order from CSVRecord.

*Release 1.8 – 2020-02-01*
CSVFormat#validate() does not account for allowDuplicateHeaderNames.
A single empty header is allowed when not allowing empty column headers.

*Release 1.9.0 – 2020-07-24*
Add possibility to use ResultSet header meta data as CSV header."
Update commons-net from 3.6 to 3.8.0.	,13473155,Resolved,Minor,Fixed,25/Jul/22 02:28,24/Aug/22 14:35,3.4.0,"Current version 3.6 is almost ~5 years old

Upgrading to new release to keep up for new features and bug fixes."
Fix failure of shelltest for hadoop_add_ldlibpath,13473898,Resolved,Minor,Fixed,28/Jul/22 03:32,30/Aug/22 10:34,3.3.5,Test cases in hadoop_add_ldlibpath.bats failed.
"Implement readFully(long position, byte[] buffer, int offset, int length)",13474008,Resolved,Minor,Fixed,28/Jul/22 13:41,06/Oct/22 11:01,3.4.0,"Implement readFully(long position, byte[] buffer, int offset, int length) in PrefetchingInputStream, as it currently uses FSInputStream's [readFully|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSInputStream.java#L136] which calls read(long position, byte[] buffer, int offset, int length).

This read then seeks to the position (which is ok), but then seeks back to the original starting position at the end (so always seeking back to 0). this is pretty bad for the prefetching implementation as it means lots of caching to disk and getting blocks from disk. 

when backporting. follow with HADOOP-18531"
Upgrade AWS SDK to V2 - Prerequisites ,13474515,Resolved,Minor,Fixed,01/Aug/22 12:15,05/Oct/22 14:11,3.3.5,"We want to update the AWS SDK to V2, before we do this we should warn on things that will no longer supported. The following changes should be made:

 
 * [getAmazonS3Client()|https://github.com/apache/hadoop/blob/221eb2d68d5b52e4394fd36cb30d5ee9ffeea7f0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L1174] - Warn that this method will be removed 
 * [initCustomSigners()|https://github.com/apache/hadoop/blob/03cfc852791c14fad39db4e5b14104a276c08e59/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/SignerManager.java#L65] - Warn that the interface is changing, any custom signers will need to be updated

 * [bindAWSClient|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L840] - If DT is enabled, warn that credential providers interface is changing, any custom cred providers used in binding classes will need to be updated
 *  [buildAWSProviderList|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java#L618] - if any SDK V1 cred providers are in this list, warn that these will be removed
 * [S3ClientFactory|https://github.com/apache/hadoop/blob/221eb2d68d5b52e4394fd36cb30d5ee9ffeea7f0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ClientFactory.java] - Update javadocs to say this interface will be replaced by a V2 client factory, mark interface deprecated?"
Fix incorrect placeholder in hadoop-common,13474775,Resolved,Minor,Fixed,02/Aug/22 23:16,07/Aug/22 21:07,3.4.0,
Fix out of sync import for HADOOP-18321,13474952,Resolved,Minor,Fixed,03/Aug/22 21:37,06/Aug/22 12:52,3.3.5,Fix out of sync import for added as part of HADOOP-18321
[ABFS]: ITestAbfsManifestCommitProtocol  fails on nonHNS configuration,13477353,Resolved,Minor,Fixed,18/Aug/22 05:51,29/Nov/22 16:08,3.3.5,"ITestAbfsRenameStageFailure fails for NonHNS-SharedKey configuration.

Failure:
[ERROR] ITestAbfsRenameStageFailure>TestRenameStageFailure.testResilienceAsExpected:126 [resilient commit support] expected:<[tru]e> but was:<[fals]e>

RCA:
ResilientCommit looks for whether etags are preserved in rename, if not then it throws an exception and the flag for resilientCommitByRename stays null, leading ultimately to the test failure

Mitigation:
Since, etags are not preserved in the case of rename in nonHNS account, required value for rename resilience should be False, as resilient commits cannot be made. Thus, requiring a True value for requireRenameResilience for nonHNS account is not a valid case. Hence, as part of this task, we shall set correct value of False for requireRenameResilience for nonHNS account."
Add a re-queue metric to RpcMetrics.java to quantify the number of re-queue RPCs,13480549,Resolved,Minor,Fixed,08/Sep/22 01:23,16/Sep/22 17:09,3.4.0,"Add a reQueue metric to RpcMetrics.java to quantify the number of RPCs reQueued.

Because Observer NameNode will re-queue the rpc if the call processing should be postponed.

 

There is no any metric to quantify the number of re-queue RPCs, so I think we should do it."
S3A server-side encryption tests fail before checking encryption tests should skip,13482770,Resolved,Minor,Fixed,22/Sep/22 13:05,19/Oct/22 15:04,3.3.5,"When setting {{test.fs.s3a.encryption.enabled}} to {{{}false{}}}, this is not respected by ITestS3AEncryptionSSEKMSDefaultKey. See failure below.

 
{code:java}
------------------------------------------------------------------------------
Test set: org.apache.hadoop.fs.s3a.ITestS3AEncryptionSSEKMSDefaultKey
-------------------------------------------------------------------------------
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 6.053 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3AEncryptionSSEKMSDefaultKey
testEncryptionOverRename(org.apache.hadoop.fs.s3a.ITestS3AEncryptionSSEKMSDefaultKey)  Time elapsed: 3.063 s  <<< ERROR!
org.apache.hadoop.fs.s3a.AWSBadRequestException: PUT 0-byte object  on fork-0002/test: com.amazonaws.services.s3.model.AmazonS3Exception: SSE unavailable (Service: Amazon S3; Status Code: 400; Proxy: null)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:242)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)
        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:4394)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.createFakeDirectory(S3AFileSystem.java:4379)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.access$1800(S3AFileSystem.java:268)
        at org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.createFakeDirectory(S3AFileSystem.java:3469)
        at org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:159)
        at org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:57)
        at org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2441)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2460)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3435)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2456)
        at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.mkdirs(AbstractFSContractTestBase.java:363)
        at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:205)
        at org.apache.hadoop.fs.s3a.AbstractS3ATestBase.setup(AbstractS3ATestBase.java:111)
        at org.apache.hadoop.fs.s3a.AbstractTestS3AEncryption.setup(AbstractTestS3AEncryption.java:94)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:748)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)
        at com.amazonaws.services.s3.AmazonS3Client.access$300(AmazonS3Client.java:421)
        at com.amazonaws.services.s3.AmazonS3Client$PutObjectStrategy.invokeServiceCall(AmazonS3Client.java:6531)
        at com.amazonaws.services.s3.AmazonS3Client.uploadObject(AmazonS3Client.java:1861)
        at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1821)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$putObjectDirect$18(S3AFileSystem.java:2937)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier(IOStatisticsBinding.java:651)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:2934)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$createEmptyObject$31(S3AFileSystem.java:4396)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)
        ... 37 more
 {code}
What I believe is happening is it performs the superclass setup method which asserts that it can create a directory. If the S3-compatible endpoint does not support encryption, this check will fail causing the test to fail before skipping. 

 "
Limit the findbugs suppression IS2_INCONSISTENT_SYNC to S3AFileSystem field,13482817,Resolved,Minor,Fixed,22/Sep/22 19:19,26/Sep/22 17:59,3.4.0,Limit the findbugs suppression IS2_INCONSISTENT_SYNC to S3AFileSystem field futurePool to avoid letting it discover other synchronization bugs.
An unhandled ArrayIndexOutOfBoundsException in DefaultStringifier.storeArray() if provided with an empty input,13484056,Resolved,Minor,Fixed,30/Sep/22 20:39,20/Oct/22 17:15,3.3.4,The code throws an unhandled ArrayIndexOutOfBoundsException when method _storeArray_ of DefaultStringifier.java is called with an empty array as input.
Fix FileSystem leak in ITestS3AAWSCredentialsProvider,13476486,Resolved,Minor,Fixed,12/Aug/22 03:52,18/Aug/22 23:00,3.3.5,ITestS3AAWSCredentialsProvider#testAnonymousProvider has FileSystem leak that should be fixed.
ITestS3AFileSystemStatistic failure in prefetch feature branch,13474701,Open,Minor,,02/Aug/22 13:11,,3.4.0,"testing the rebased prefetch feature branch; got a failure in ITestS3AFileSystemStatistic
 
{code}
tics.ITestS3AFileSystemStatistic
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.489 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic
[ERROR] testBytesReadWithStream(org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic)  Time elapsed: 1.489 s  <<< FAILURE!
java.lang.AssertionError: Mismatch in number of FS bytes read by InputStreams expected:<2048> but was:<69537130>
        at org.apache.hadoop.fs.s3a.statistics.ITestS3AFileSystemStatistic.testBytesReadWithStream(ITestS3AFileSystemStatistic.java:72)


{code}
that;s 64MB + ~237 kb, the kind of values you would get from prefetching

but, prefetch was disabled in this test run.

maybe its just the fs stats aren't being reset between test cases"
scan detected CVE-2021-37136 and CVE-2021-37137 in netty.io_netty_codec,13482299,Resolved,Minor,Done,20/Sep/22 06:35,16/Jun/23 10:51,,"Our security scan detected CVE-2021-37136 and CVE-2021-37137 in io.netty_netty

 
|Component|Version|CVE|Fixed in|
|io.netty_netty|3.10.6|CVE-2021-37136|4.1.68|
|io.netty_netty|3.10.6|CVE-2021-37137|4.1.68|
|org.yaml_snakeyaml|1.26|CVE-2022-25857|1.31|"
[ABFS]: RenameFilePath Source File Not Found (404) error in retry loop,13478820,Resolved,Minor,Duplicate,26/Aug/22 12:38,31/Mar/23 14:31,,"RenameFilePath on its first try receives a Request timed out error with code 500. On retrying the same operation, a Source file not found (404) error is received. 

Possible mitigation: Check whether etags remain the same before and after the retry and accordingly send an Operation Successful result, instead of source file not found. "
TestServiceInterruptHandling.testRegisterAndRaise failing intermittently,13478599,Open,Minor,,25/Aug/22 08:26,,3.3.3,"TestServiceInterruptHandling.testRegisterAndRaise faling with

 
|java.lang.AssertionError: interrupt data|
|at org.junit.Assert.fail(Assert.java:89)|
|at org.junit.Assert.assertTrue(Assert.java:42)|
|at org.junit.Assert.assertNotNull(Assert.java:713)|
|at org.apache.hadoop.service.launcher.TestServiceInterruptHandling.testRegisterAndRaise(TestServiceInterruptHandling.java:48)|
|at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)|
|at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)|
|at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)|
|at java.lang.reflect.Method.invoke(Method.java:498)|
|at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)|
|at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)|
|at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)|
|at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)|
|at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)|
|at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)|
|at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)|
|at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)|
|at java.util.concurrent.FutureTask.run(FutureTask.java:266)|
|at java.lang.Thread.run(Thread.java:748)|
| |"
S3AFileSystem removes Path when calling createS3Client,13470718,Resolved,Minor,Fixed,08/Jul/22 14:36,26/Jul/22 11:00,3.3.0,"when using hadoop and spark to read/write data from an s3 bucket like -> s3a://bucket/path and using a custom Credentials Provider, the path is removed from the s3a URI and the credentials provider fails because the full path is gone.

In Spark 3.2,
It was invoked as -> s3 = ReflectionUtils.newInstance(s3ClientFactoryClass, conf)
.createS3Client(name, bucket, credentials); 

But In spark 3.3.3
It is invoked as s3 = ReflectionUtils.newInstance(s3ClientFactoryClass, conf).createS3Client(getUri(), parameters);
the getUri() removes the path from the s3a URI"
ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer failing,13482378,Resolved,Minor,Fixed,20/Sep/22 12:20,10/Oct/22 12:37,3.3.5,seeing a test failure in both parallel and single test case runs of {{ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer))
Improve vectored IO api spec. ,13477130,Resolved,Minor,Fixed,16/Aug/22 21:38,31/Aug/22 16:48,,"Let's add more details to the vectored IO api spec for better clarity. 
 * the position returned by getPos(); is undefined afterwards.
 * note that if a file is changed during a read, the output is again undefined. some ranges may be old data, some may be new, *and some may be both
 * note that while reads are active, normal fs api calls may block."
Fix eval expression in hadoop-functions.sh,13470433,Open,Minor,,07/Jul/22 01:08,,3.3.3,"Need to fix the eval expression.

1. Prefix exec by eval in Hadoop bin scripts Prior to this change, if HADOOP_OPTS contains any arguments that include a space, the command is not parsed correctly. For example, if HADOOP_OPTS=""... -XX:OnOutOfMemoryError=\""kill -9 %p\"" ..."", the bin/hadoop script will fail with the error ""Unrecognized option: -9"". No amount of clever escaping of the quotes or spaces in the ""kill -9 %p"" command will fix this. The only alternative appears to be to use 'eval'. Switching to use 'eval' *instead of* 'exec' also works, but it results in an intermediate bash process being left alive throughout the entire lifetime of the Java proces being started. Using 'exec' prefixed by 'eval' as has been done in this commit gets the best of both worlds, in that options with spaces are parsed correctly, and you don't end up with an intermediate bash process as the parent of the Java process.

2. We can replace single quote with escape-char and a double quote.

 

 "
No ARM binaries in branch-3.3.x releases,13476310,Resolved,Minor,Fixed,11/Aug/22 06:54,29/Nov/22 15:30,3.3.2,"release files miss hadoop-3.3.4-aarch64.tar.gz

!image-2022-08-11-14-54-15-490.png!"
Improve ZKDelegationTokenSecretManager#startThead With recommended methods.,13478927,Resolved,Minor,Fixed,27/Aug/22 11:46,08/Sep/22 18:41,3.4.0,"When reading the code, I found a deprecated method to use. In ZKDelegationTokenSecretManager#startThead, the code here uses the Curator's EnsurePath,
But EnsurePath is deprecated, use the recommended method instead

public class EnsurePath
Deprecated.
Since 2.9.0 - Prefer CuratorFramework.create().creatingParentContainersIfNeeded() or CuratorFramework.exists().creatingParentContainersIfNeeded()"
Could not find artifact org.apache.ftpserve:ftplet-api:jar:1.0.0,13483062,Open,Minor,,25/Sep/22 09:39,,3.3.4,"Hi,

i have a problem with fetch POM file for ftplet-api for Haddop.

OS: FreeBSD  13.1-RELEASE-p1 FreeBSD 13.1-RELEASE-p1 GENERIC amd64

 
{code:java}
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ftpserver/ftpserver-parent/1.0.0/ftpserver-parent-1.0.0.pom (14 kB at 141 kB/s)
Downloading from apache.snapshots.https: https://repository.apache.org/content/repositories/snapshots/org/apache/ftpserve/ftplet-api/1.0.0/ftplet-api-1.0.0.pom
Downloading from repository.jboss.org: https://repository.jboss.org/nexus/content/groups/public/org/apache/ftpserve/ftplet-api/1.0.0/ftplet-api-1.0.0.pom
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/ftpserve/ftplet-api/1.0.0/ftplet-api-1.0.0.pom
[WARNING] The POM for org.apache.ftpserve:ftplet-api:jar:1.0.0 is missing, no dependency information available{code}
next 

 
{code:java}
INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Skipping Apache Hadoop Main
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Main 3.3.4:
[INFO] 
[INFO] Apache Hadoop Main ................................. SUCCESS [ 33.104 s]
[INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 14.536 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 10.055 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  4.319 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.062 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 11.683 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 29.699 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 11.518 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [ 58.978 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  1.696 s]
[INFO] Apache Hadoop Common ............................... FAILURE [ 38.983 s]
[INFO] Apache Hadoop NFS .................................. SKIPPED
[INFO] Apache Hadoop KMS .................................. SKIPPED
[INFO] Apache Hadoop Registry ............................. SKIPPED
[INFO] Apache Hadoop Common Project ....................... SKIPPED
[INFO] Apache Hadoop HDFS Client .......................... SKIPPED
[INFO] Apache Hadoop HDFS ................................. SKIPPED
[INFO] Apache Hadoop HDFS Native Client ................... SKIPPED
[INFO] Apache Hadoop HttpFS ............................... SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................. SKIPPED
[INFO] Apache Hadoop HDFS-RBF ............................. SKIPPED
[INFO] Apache Hadoop HDFS Project ......................... SKIPPED
[INFO] Apache Hadoop YARN ................................. SKIPPED
[INFO] Apache Hadoop YARN API ............................. SKIPPED
[INFO] Apache Hadoop YARN Common .......................... SKIPPED
[INFO] Apache Hadoop YARN Server .......................... SKIPPED
[INFO] Apache Hadoop YARN Server Common ................... SKIPPED
[INFO] Apache Hadoop YARN NodeManager ..................... SKIPPED
[INFO] Apache Hadoop YARN Web Proxy ....................... SKIPPED
[INFO] Apache Hadoop YARN ApplicationHistoryService ....... SKIPPED
[INFO] Apache Hadoop YARN Timeline Service ................ SKIPPED
[INFO] Apache Hadoop YARN ResourceManager ................. SKIPPED
[INFO] Apache Hadoop YARN Server Tests .................... SKIPPED
[INFO] Apache Hadoop YARN Client .......................... SKIPPED
[INFO] Apache Hadoop YARN SharedCacheManager .............. SKIPPED
[INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Backend ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Common .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Client .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Servers ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase tests ..... SKIPPED
[INFO] Apache Hadoop YARN Router .......................... SKIPPED
[INFO] Apache Hadoop YARN TimelineService DocumentStore ... SKIPPED
[INFO] Apache Hadoop YARN Applications .................... SKIPPED
[INFO] Apache Hadoop YARN DistributedShell ................ SKIPPED
[INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SKIPPED
[INFO] Apache Hadoop MapReduce Client ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Core ....................... SKIPPED
[INFO] Apache Hadoop MapReduce Common ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Shuffle .................... SKIPPED
[INFO] Apache Hadoop MapReduce App ........................ SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer .............. SKIPPED
[INFO] Apache Hadoop MapReduce JobClient .................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ......................... SKIPPED
[INFO] Apache Hadoop YARN Services ........................ SKIPPED
[INFO] Apache Hadoop YARN Services Core ................... SKIPPED
[INFO] Apache Hadoop YARN Services API .................... SKIPPED
[INFO] Apache Hadoop YARN Application Catalog ............. SKIPPED
[INFO] Apache Hadoop YARN Application Catalog Webapp ...... SKIPPED
[INFO] Apache Hadoop YARN Application Catalog Docker Image  SKIPPED
[INFO] Apache Hadoop YARN Application MaWo ................ SKIPPED
[INFO] Apache Hadoop YARN Application MaWo Core ........... SKIPPED
[INFO] Apache Hadoop YARN Site ............................ SKIPPED
[INFO] Apache Hadoop YARN Registry ........................ SKIPPED
[INFO] Apache Hadoop YARN UI .............................. SKIPPED
[INFO] Apache Hadoop YARN CSI ............................. SKIPPED
[INFO] Apache Hadoop YARN Project ......................... SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SKIPPED
[INFO] Apache Hadoop MapReduce NativeTask ................. SKIPPED
[INFO] Apache Hadoop MapReduce Uploader ................... SKIPPED
[INFO] Apache Hadoop MapReduce Examples ................... SKIPPED
[INFO] Apache Hadoop MapReduce ............................ SKIPPED
[INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED
[INFO] Apache Hadoop Distributed Copy ..................... SKIPPED
[INFO] Apache Hadoop Client Aggregator .................... SKIPPED
[INFO] Apache Hadoop Dynamometer Workload Simulator ....... SKIPPED
[INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SKIPPED
[INFO] Apache Hadoop Dynamometer Block Listing Generator .. SKIPPED
[INFO] Apache Hadoop Dynamometer Dist ..................... SKIPPED
[INFO] Apache Hadoop Dynamometer .....................INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Skipping Apache Hadoop Main
[INFO] This project has been banned from the build due to previous failures.
[INFO] ------------------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Apache Hadoop Main 3.3.4:
[INFO] 
[INFO] Apache Hadoop Main ................................. SUCCESS [ 33.104 s]
[INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 14.536 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [ 10.055 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  4.319 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.062 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 11.683 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 29.699 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 11.518 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [ 58.978 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  1.696 s]
[INFO] Apache Hadoop Common ............................... FAILURE [ 38.983 s]
[INFO] Apache Hadoop NFS .................................. SKIPPED
[INFO] Apache Hadoop KMS .................................. SKIPPED
[INFO] Apache Hadoop Registry ............................. SKIPPED
[INFO] Apache Hadoop Common Project ....................... SKIPPED
[INFO] Apache Hadoop HDFS Client .......................... SKIPPED
[INFO] Apache Hadoop HDFS ................................. SKIPPED
[INFO] Apache Hadoop HDFS Native Client ................... SKIPPED
[INFO] Apache Hadoop HttpFS ............................... SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................. SKIPPED
[INFO] Apache Hadoop HDFS-RBF ............................. SKIPPED
[INFO] Apache Hadoop HDFS Project ......................... SKIPPED
[INFO] Apache Hadoop YARN ................................. SKIPPED
[INFO] Apache Hadoop YARN API ............................. SKIPPED
[INFO] Apache Hadoop YARN Common .......................... SKIPPED
[INFO] Apache Hadoop YARN Server .......................... SKIPPED
[INFO] Apache Hadoop YARN Server Common ................... SKIPPED
[INFO] Apache Hadoop YARN NodeManager ..................... SKIPPED
[INFO] Apache Hadoop YARN Web Proxy ....................... SKIPPED
[INFO] Apache Hadoop YARN ApplicationHistoryService ....... SKIPPED
[INFO] Apache Hadoop YARN Timeline Service ................ SKIPPED
[INFO] Apache Hadoop YARN ResourceManager ................. SKIPPED
[INFO] Apache Hadoop YARN Server Tests .................... SKIPPED
[INFO] Apache Hadoop YARN Client .......................... SKIPPED
[INFO] Apache Hadoop YARN SharedCacheManager .............. SKIPPED
[INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Backend ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Common .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Client .... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Servers ... SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SKIPPED
[INFO] Apache Hadoop YARN TimelineService HBase tests ..... SKIPPED
[INFO] Apache Hadoop YARN Router .......................... SKIPPED
[INFO] Apache Hadoop YARN TimelineService DocumentStore ... SKIPPED
[INFO] Apache Hadoop YARN Applications .................... SKIPPED
[INFO] Apache Hadoop YARN DistributedShell ................ SKIPPED
[INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SKIPPED
[INFO] Apache Hadoop MapReduce Client ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Core ....................... SKIPPED
[INFO] Apache Hadoop MapReduce Common ..................... SKIPPED
[INFO] Apache Hadoop MapReduce Shuffle .................... SKIPPED
[INFO] Apache Hadoop MapReduce App ........................ SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer .............. SKIPPED
[INFO] Apache Hadoop MapReduce JobClient .................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ......................... SKIPPED
[INFO] Apache Hadoop YARN Services ........................ SKIPPED
[INFO] Apache Hadoop YARN Services Core ................... SKIPPED
[INFO] Apache Hadoop YARN Services API .................... SKIPPED
[INFO] Apache Hadoop YARN Application Catalog ............. SKIPPED
[INFO] Apache Hadoop YARN Application Catalog Webapp ...... SKIPPED
[INFO] Apache Hadoop YARN Application Catalog Docker Image  SKIPPED
[INFO] Apache Hadoop YARN Application MaWo ................ SKIPPED
[INFO] Apache Hadoop YARN Application MaWo Core ........... SKIPPED
[INFO] Apache Hadoop YARN Site ............................ SKIPPED
[INFO] Apache Hadoop YARN Registry ........................ SKIPPED
[INFO] Apache Hadoop YARN UI .............................. SKIPPED
[INFO] Apache Hadoop YARN CSI ............................. SKIPPED
[INFO] Apache Hadoop YARN Project ......................... SKIPPED
[INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SKIPPED
[INFO] Apache Hadoop MapReduce NativeTask ................. SKIPPED
[INFO] Apache Hadoop MapReduce Uploader ................... SKIPPED
[INFO] Apache Hadoop MapReduce Examples ................... SKIPPED
[INFO] Apache Hadoop MapReduce ............................ SKIPPED
[INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED
[INFO] Apache Hadoop Distributed Copy ..................... SKIPPED
[INFO] Apache Hadoop Client Aggregator .................... SKIPPED
[INFO] Apache Hadoop Dynamometer Workload Simulator ....... SKIPPED
[INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SKIPPED
[INFO] Apache Hadoop Dynamometer Block Listing Generator .. SKIPPED
[INFO] Apache Hadoop Dynamometer Dist ..................... SKIPPED
[INFO] Apache Hadoop Dynamometer .......................... SKIPPED
[INFO] Apache Hadoop Archives ............................. SKIPPED
[INFO] Apache Hadoop Archive Logs ......................... SKIPPED
[INFO] Apache Hadoop Rumen ................................ SKIPPED
[INFO] Apache Hadoop Gridmix .............................. SKIPPED
[INFO] Apache Hadoop Data Join ............................ SKIPPED
[INFO] Apache Hadoop Extras ............................... SKIPPED
[INFO] Apache Hadoop Pipes ................................ SKIPPED
[INFO] Apache Hadoop OpenStack support .................... SKIPPED
[INFO] Apache Hadoop Amazon Web Services support .......... SKIPPED
[INFO] Apache Hadoop Kafka Library support ................ SKIPPED
[INFO] Apache Hadoop Azure support ........................ SKIPPED
[INFO] Apache Hadoop Aliyun OSS support ................... SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED
[INFO] Apache Hadoop Resource Estimator Service ........... SKIPPED
[INFO] Apache Hadoop Azure Data Lake support .............. SKIPPED
[INFO] Apache Hadoop Image Generation Tool ................ SKIPPED
[INFO] Apache Hadoop Tools Dist ........................... SKIPPED
[INFO] Apache Hadoop Tools ................................ SKIPPED
[INFO] Apache Hadoop Client API ........................... SKIPPED
[INFO] Apache Hadoop Client Runtime ....................... SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED
[INFO] Apache Hadoop Client Test Minicluster .............. SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED
[INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED
[INFO] Apache Hadoop Distribution ......................... SKIPPED
[INFO] Apache Hadoop Client Modules ....................... SKIPPED
[INFO] Apache Hadoop Cloud Storage ........................ SKIPPED
[INFO] Apache Hadoop Tencent COS Support .................. SKIPPED
[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  04:03 min
[INFO] Finished at: 2022-09-25T11:24:14+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hadoop-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-common:jar:3.3.4: Could not find artifact org.apache.ftpserve:ftplet-api:jar:1.0.0 in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-common..... SKIPPED
[INFO] Apache Hadoop Archives ............................. SKIPPED
[INFO] Apache Hadoop Archive Logs ......................... SKIPPED
[INFO] Apache Hadoop Rumen ................................ SKIPPED
[INFO] Apache Hadoop Gridmix .............................. SKIPPED
[INFO] Apache Hadoop Data Join ............................ SKIPPED
[INFO] Apache Hadoop Extras ............................... SKIPPED
[INFO] Apache Hadoop Pipes ................................ SKIPPED
[INFO] Apache Hadoop OpenStack support .................... SKIPPED
[INFO] Apache Hadoop Amazon Web Services support .......... SKIPPED
[INFO] Apache Hadoop Kafka Library support ................ SKIPPED
[INFO] Apache Hadoop Azure support ........................ SKIPPED
[INFO] Apache Hadoop Aliyun OSS support ................... SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED
[INFO] Apache Hadoop Resource Estimator Service ........... SKIPPED
[INFO] Apache Hadoop Azure Data Lake support .............. SKIPPED
[INFO] Apache Hadoop Image Generation Tool ................ SKIPPED
[INFO] Apache Hadoop Tools Dist ........................... SKIPPED
[INFO] Apache Hadoop Tools ................................ SKIPPED
[INFO] Apache Hadoop Client API ........................... SKIPPED
[INFO] Apache Hadoop Client Runtime ....................... SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED
[INFO] Apache Hadoop Client Test Minicluster .............. SKIPPED
[INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED
[INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED
[INFO] Apache Hadoop Distribution ......................... SKIPPED
[INFO] Apache Hadoop Client Modules ....................... SKIPPED
[INFO] Apache Hadoop Cloud Storage ........................ SKIPPED
[INFO] Apache Hadoop Tencent COS Support .................. SKIPPED
[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  04:03 min
[INFO] Finished at: 2022-09-25T11:24:14+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hadoop-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-common:jar:3.3.4: Could not find artifact org.apache.ftpserve:ftplet-api:jar:1.0.0 in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :hadoop-common{code}"
Fix typo: a os-native -> an os-native,13481503,Open,Minor,,14/Sep/22 13:20,,,"There is a comment in FileUtil.java:

>  Convert a os-native filename to a path that works for the shell.

That ""a os-native"" should be ""an os-native"".

Several more comments from other files are fixed in the same way."
Enable parallel building in pre-commit jobs,13475424,Open,Minor,,05/Aug/22 13:30,,,"Once MAPREDUCE-7386 is merged, we can update the pre-commit jobs to take advantage of the parallel builds."
deleteOnExit does not work with S3AFileSystem,13471854,Resolved,Minor,Fixed,16/Jul/22 02:33,11/Aug/22 19:38,3.3.3,"When deleteOnExit is set on some paths, they are not removed when file system object is closed. The following exception is logged when printing out the exception in info log.
{code:java}
2022-07-15 19:29:12,552 [main] INFO  fs.FileSystem (FileSystem.java:processDeleteOnExit(1810)) - Ignoring failure to deleteOnExit for path /file, exception {}
java.io.IOException: s3a://mock-bucket: FileSystem is closed!
        at org.apache.hadoop.fs.s3a.S3AFileSystem.checkNotClosed(S3AFileSystem.java:3887)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2333)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2355)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4402)
        at org.apache.hadoop.fs.FileSystem.processDeleteOnExit(FileSystem.java:1805)
        at org.apache.hadoop.fs.FileSystem.close(FileSystem.java:2669)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.close(S3AFileSystem.java:3830)
        at org.apache.hadoop.fs.s3a.TestS3AGetFileStatus.testFile(TestS3AGetFileStatus.java:87)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
 {code}"
IOStatisticsContext tuning,13473793,Resolved,Minor,Fixed,27/Jul/22 13:34,08/Aug/22 13:38,3.3.5,"Tuning of the IOStatisticsContext code


h2. change property name  to fs.iostatistics....

there are other fs.iostatistics options, the new one needs consistent naming

h2. enable in hadoop-aws

edit core-site.xml in hadoop-aws/test/resources to always collect context iOStatistics

This helps qualify the code
{code}
	  <property>
	    <name>fs.thread.level.iostatistics.enabled</name>
	    <value>true</value>
	  </property>

{code}

h3.  IOStatisticsContext to add add static probe to see if it is enabled.

lets apps know not to bother collecting/reporting"
ITestS3Select.testSelectSeekFullLandsat is timing out,13473338,Resolved,Minor,Fixed,25/Jul/22 14:18,05/Aug/22 13:46,,"ITestS3Select.testSelectSeekFullLandsat is timing out. When I run it separately on my IDE, I get the following output repeatedly till I stop the test manually:

 
{code:java}
2022-07-25 15:15:37,463 [JUnit-testSelectSeekFullLandsat] DEBUG s3a.Invoker (DurationInfo.java:close(101)) - read(): duration 0:00.000s
2022-07-25 15:15:37,463 [JUnit-testSelectSeekFullLandsat] DEBUG s3a.Invoker (DurationInfo.java:<init>(80)) - Starting: read()
2022-07-25 15:15:37,463 [JUnit-testSelectSeekFullLandsat] DEBUG s3a.Invoker (DurationInfo.java:close(101)) - read(): duration 0:00.000s
2022-07-25 15:15:37,463 [JUnit-testSelectSeekFullLandsat] DEBUG s3a.Invoker (DurationInfo.java:<init>(80)) - Starting: read()
2022-07-25 15:15:37,463 [JUnit-testSelectSeekFullLandsat] DEBUG s3a.Invoker (DurationInfo.java:close(101)) - read(): duration 0:00.000s {code}
When running the entire test suite on my EC2, I get:
{code:java}
Time elapsed: 600.01 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 600000 milliseconds {code}"
ITestCustomSigner fails when access point name has '-' ,13473556,Resolved,Minor,Fixed,26/Jul/22 12:50,01/Aug/22 20:44,,"If you create an access point name which has hyphens, eg custom-signer-access-point, the ITestCustomSigner fails. This is because the array is split on ""-"" [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestCustomSigner.java#L231] and then joined, so access point name becomes ""customsigneraccesspoint"". "
ILoadTestS3ABulkDeleteThrottling failing,13473769,Resolved,Minor,Fixed,27/Jul/22 11:23,27/Jul/22 17:05,3.4.0,the test ILoadTestS3ABulkDeleteThrottling; looks like the fs config is being set up too late in the test suite. it should be moved from setup to createConf
Update jackson from 2.12.7 to 2.13.3,13473053,Resolved,Minor,Duplicate,23/Jul/22 09:43,23/Jul/22 13:07,3.4.0,Update jackson from 2.12.7 to 2.13.3
Supporting performant output committers for Aliyun,13471355,Open,Minor,,13/Jul/22 08:24,,3.3.3,"Due to the non-performant rename on OSS (similar to s3), zero-rename output committer (similar to HADOOP-13786 which provides this for S3) can be added for Aliyun as well. I see [similar output committer provided by Alibaba's E MapReduce platform|https://www.alibabacloud.com/help/en/e-mapreduce/latest/use-jindo-job-committer] but it's not available in open source "
Performance improvement in org.apache.hadoop.io.Text#find,13475578,Patch Available,Trivial,,07/Aug/22 07:18,,,"The current implementation reset src and tgt to the mark and continues searching when tgt has remaining and src expired first. which is probably not necessary.
{code:java}
public int find(String what, int start) {
  try {
    ByteBuffer src = ByteBuffer.wrap(this.bytes, 0, this.length);
    ByteBuffer tgt = encode(what);
    byte b = tgt.get();
    src.position(start);

    while (src.hasRemaining()) {
      if (b == src.get()) { // matching first byte
        src.mark(); // save position in loop
        tgt.mark(); // save position in target
        boolean found = true;
        int pos = src.position()-1;
        while (tgt.hasRemaining()) {
          if (!src.hasRemaining()) { // src expired first
            tgt.reset();
            src.reset();
            found = false;
            break;
          }
          if (!(tgt.get() == src.get())) {
            tgt.reset();
            src.reset();
            found = false;
            break; // no match
          }
        }
        if (found) return pos;
      }
    }
    return -1; // not found
  } catch (CharacterCodingException e) {
    throw new RuntimeException(""Should not have happened"", e);
  }
} {code}
For example, when q is searched, it is found that src has no remaining, and src is reset to d to continue searching. But the remaining length of src is always smaller than tgt, at this point we can return -1 directly.
{code:java}
@Test
public void testFind() throws Exception {
  Text text = new Text(""abcd\u20acbdcd\u20ac"");
  assertThat(text.find(""cd\u20acq"")).isEqualTo(-1);
} {code}
Perhaps it could be:
{code:java}
public int find(String what, int start) {
  try {
    ByteBuffer src = ByteBuffer.wrap(this.bytes, 0, this.length);
    ByteBuffer tgt = encode(what);
    byte b = tgt.get();
    src.position(start);

    while (src.hasRemaining()) {
      if (b == src.get()) { // matching first byte
        src.mark(); // save position in loop
        tgt.mark(); // save position in target
        boolean found = true;
        int pos = src.position()-1;
        while (tgt.hasRemaining()) {
          if (!src.hasRemaining()) { // src expired first
            return -1;
          }
          if (!(tgt.get() == src.get())) {
            tgt.reset();
            src.reset();
            found = false;
            break; // no match
          }
        }
        if (found) return pos;
      }
    }
    return -1; // not found
  } catch (CharacterCodingException e) {
    throw new RuntimeException(""Should not have happened"", e);
  }
}{code}"
Refactor CallerContext's constructor to eliminate duplicate code,13479348,Resolved,Trivial,Fixed,30/Aug/22 17:54,26/Oct/22 13:39,3.3.4,Refactor CallerContext's constructor to eliminate duplicate code
Leak of S3AInstrumentation instances via hadoop Metrics references,13500874,Resolved,Blocker,Fixed,10/Nov/22 10:19,15/Dec/22 11:52,3.3.4,"A heap dump of a process running OOM shows that if a process creates then destroys lots of S3AFS instances, you seem to run out of heap due to references to S3AInstrumentation and the IOStatisticsStore kept via the hadoop metrics registry

It doesn't look like S3AInstrumentation.close() is being invoked in S3AFS.close(). it should -with the IOStats being snapshotted to a local reference before this happens. This allows for stats of a closed fs to be examined.

If you look at org.apache.hadoop.ipc.DecayRpcScheduler.MetricsProxy it uses a WeakReference to refer back to the larger object. we should do the same for abfs/s3a bindings. ideally do some template proxy class in hadoop common they can both use.

"
disable purging list of in progress reads in abfs stream closed,13507481,Resolved,Blocker,Fixed,30/Nov/22 14:27,15/Dec/22 17:14,3.3.2,"turn off the prune of in progress reads in ReadBufferManager::purgeBuffersForStream

this will ensure active prefetches for a closed stream complete. they wiill then get to the completed list and hang around until evicted by timeout, but at least prefetching will be safe.

When backporting
* include the followup test fix patch
* include HADOOP-18577."
AvroFSInput opens a stream twice and discards the second one without closing,13509904,Resolved,Blocker,Fixed,05/Dec/22 15:55,06/Dec/22 10:10,3.3.5,"late breaking blocker for 3.3.5; AvroFsinput can leak input streams because the change of HADOOP-16202 failed to comment out the original open() call.

noticed during a code review with [~harshit.gupta]"
CVE-2021-37533 on commons-net is included in hadoop common and hadoop-client-runtime,13510132,Resolved,Blocker,Fixed,06/Dec/22 18:28,19/Dec/22 11:25,3.3.4,"Latest 3.3.4 version of hadoop-common and hadoop-client-runtime includes commons-net in version 3.6, which has vulnerability CVE-2021-37533. Need to upgrade it to 3.9 to fix. 

This is a due diligence patch only; by the time the caller encounters the CVE they must have already provided their username and password to a malicious ftp server."
NFS Gateway may release buffer too early,13513152,Resolved,Blocker,Fixed,12/Dec/22 09:53,14/Dec/22 15:00,3.2.5,"After upgrading Netty from 4.1.68 to 4.1.77 (HADOOP-18079), NFS Gateway started crashing when writing data (can be easily reproduced by a few 10MB+ files).  The problem was triggered by [reduced default chunk size in PooledByteBufAllocator|https://github.com/netty/netty/commit/f650303911] (in 4.1.75), but it turned out to be caused by a buffer released too early in NFS Gateway."
Improve error reporting on non-standard kerberos names,13513595,Resolved,Blocker,Fixed,14/Dec/22 12:18,15/Dec/22 11:51,3.3.4,"The kerberos RFC does not declare any restriction on
characters used in kerberos names, though
implementations MAY be more restrictive.

If the kerberos controller supports use non-conventional
user names *and the kerberos admin chooses to use them*
this can confuse some of the parsing.

The obvious solution is for the enterprise admins to ""not do that""
as a lot of things break, bits of hadoop included.

Harden the hadoop code slightly so at least we fail more gracefully,
so people can then get in touch with their sysadmin and tell them
to stop it.

Note: given the kerberos admin is implicitly a superuser, being
able to create malformed principal names.
doesn't give them any privileges, just offers a different way
to stop the cluster working."
Abfs and S3A FileContext bindings to close wrapped filesystems in finalizer,13484454,Resolved,Blocker,Fixed,04/Oct/22 14:26,18/Oct/22 15:16,3.3.4,"if you use the FileContext APIs to talk to abfs or s3a, it creates a new wrapped FileSystem implementation, and, because there is no close() call, never cleans up.

proposed: add finalizers for these two classes, which we know create helper threads, especially if plugins are added"
ChecksumFileSystem::readVectored might return byte buffers not positioned at 0,13502891,Resolved,Blocker,Fixed,16/Nov/22 12:36,29/Nov/22 14:53,3.3.5,"Checksystem::readVectored method returns the byte buffers that are not positioned at 0, which might be the underlying assumption for the readers like ORC.

 

cc:/ [~mthakur] [~rbalamohan] [~stevel@apache.org]"
ABFS ReadBufferManager buffer sharing across concurrent HTTP requests,13496586,Resolved,Critical,Fixed,05/Nov/22 15:00,19/Dec/22 11:11,3.3.2,"{{AbfsInputStream.close()}} can trigger the return of buffers used for active prefetch GET requests into the ReadBufferManager free buffer pool.

A subsequent prefetch by a different stream in the same process may acquire this same buffer. This can lead to risk of corruption of its own prefetched data, data which may then be returned to that other thread.

The full analysis in in the document attached to this JIRA.

The issue is fixed in Hadoop 3.3.5 

h2. Emergency fix through site configuration

On releases without the fix for this (3.3.2-3.3.4), the bug can be avoided by disabling all prefetching
{code:java}
fs.azure.readaheadqueue.depth = 0
{code}

h2. Automated probes for risk of exposure

The [cloudstore|https://github.com/steveloughran/cloudstore] diagnostics JAR has a command [safeprefetch|https://github.com/steveloughran/cloudstore/blob/trunk/src/main/site/safeprefetch.md] which probes an abfs client for being vulnerable. It does this through {{PathCapabilities.hasPathCapability()}} probes. It can be invoked on the command line to validate the version/configuration

Consult [the source|https://github.com/steveloughran/cloudstore/blob/trunk/src/main/java/org/apache/hadoop/fs/store/abfs/SafePrefetch.java#L96] to see how to do this programmatically.

Note also that the tool's [mkcsv|https://github.com/steveloughran/cloudstore/blob/trunk/src/main/site/mkcsv.md] command can be used to generate the multi-GB CSV files needed to trigger the condition and so verify that the issue exists.


h2. Microsoft Announcement

{code}

From: Sneha Vijayarajan
Subject: RE: Alert ! ABFS Driver - Possible data corruption on read path

Hi,

One of the contributions made to ABFS Driver has a potential to cause data corruption on read
path.

Please check if the below change is part of any of your releases:

HADOOP-17156. Purging the buffers associated with input streams during close() by mukund-thakur
· Pull Request #3285 · apache/hadoop (github.com)

RCA: Scenario that can lead to data corruption:

Driver allocates a bunch of prefetch buffers at init and are shared by different instances of
InputStreams created within that process. These prefetch buffers could be in 3 stages –

* In ReadAheadQueue : request for prefetch logged
* In ProgressList : Work has begun to talk to backend store to get the requested data
* In CompletedList: Prefetch data is now available for consumption.

When multiple InputStreams have prefetch buffers across these states and close is triggered on
any InputStream/s, the commit above will remove buffers allotted to respective stream from all
the 3 lists and also declare that the buffers are available for new prefetches to happen, but
no action to cancel/prevent buffer from being updated with ongoing network request is done.
Data corruption can happen if one such freed up buffer from InProgressList is allotted to a new
prefetch request and then the buffer got filled up with the previous stream’s network request.

Mitigation: If this change is present in any release, kindly help communicate to your customers
to immediately set below config to 0 in their clusters. This will disable prefetches which can
have an impact on perf but will prevent the possibility of data corruption.

fs.azure.readaheadqueue.depth: Sets the readahead queue depth in AbfsInputStream. In case the
set value is negative the read ahead queue depth will be set as
Runtime.getRuntime().availableProcessors(). By default the value will be 2. To disable
readaheads, set this value to 0. If your workload is doing only random reads (non-sequential)
or you are seeing throttling, you may try setting this value to 0.

Next steps: We are getting help to post the notifications for this in Apache groups. Work on
HotFix is also ongoing. Will update this thread once the change is checked in.

Please reach out for any queries or clarifications.

Thanks,
Sneha Vijayarajan

{code}
 "
ViewFileSystem major bug can cause entire subtrees to effectively disappear,13500737,Open,Critical,,09/Nov/22 15:30,,3.3.4,"{{ViewFileSystem}} allows a federated view of a file system, so that for example under the path {{foo/}} I might have {{foo/bar1}} mapped to some other file system, {{foo/bar2}} mapped to some different file system, etc. using the ViewFS mount table.

Consider a situation where I have 1,000 subdirectories {{foo/bar000}} to {{foo/bar999}} mapped to 1,000 different cloud providers (e.g. AWS S3 buckets or whatever). Let's say that for whatever reason the mapping for {{foo/bar123}} was incorrect (maybe there was a corrupted mount table or a race condition in creating the destination cloud storage), so that when we we try to get the status of {{foo/bar123}} it returns an HTTP {{404}}, throwing an exception.

But let's say that we were instead _listing the status of {{foo/}} itself_, in order to return all 1,000 children. Look what would happen in the {{ViewFileSystem.listStatus(Path f)}} code when we call {{ViewFileSystem.listStatus(new Path(""…/foo""))}}. We expect it to return 999 child paths instead of 1,000 child (because one of the mounted paths is misconfigured and returns {{404}})):

{code:java}
      for (Entry<String, INode<FileSystem>> iEntry :
          theInternalDir.getChildren().entrySet()) {
…
          try {
            FileStatus status =
                ((ChRootedFileSystem)link.getTargetFileSystem())
                .getMyFs().getFileStatus(new Path(linkedPath));
            linkStatuses.add(
                new FileStatus(status.getLen(), status.isDirectory(),
                    status.getReplication(), status.getBlockSize(),
                    status.getModificationTime(), status.getAccessTime(),
                    status.getPermission(), status.getOwner(),
                    status.getGroup(), null, path));
          } catch (FileNotFoundException ex) {
            LOG.warn(""Cannot get one of the children's("" + path
                + "")  target path("" + link.getTargetFileSystem().getUri()
                + "") file status."", ex);
            throw ex;
          }
{code}

For each particular child that is mapped in the map table, a {{((ChRootedFileSystem)link.getTargetFileSystem()).getMyFs().getFileStatus(new Path(linkedPath))}} is performed on the underlying federated file system and the resulting `FileSystatus` is added to the list. But in the case of {{foo/bar123}}, it throws an exception. The code above appropriately catches the exception and warns, ""Cannot get one of the children's … file status"" That part is perfectly fine. *But then the code rethrows the exception, which is incorrect.*

Rethrowing the exception with {{throw ex}} breaks the directory listing; it will result in an exception for the entire directory listing of {{foo/}}, not just the child. If the child mapping for {{foo/bar123}} has somehow disappeared (maybe it's just a race condition, and that the mapping table was stale when the directory listing started so that the mapping was never current) and {{foo/bar123}} returns a {{404}}, suddenly the entire directory listing, instead of returning 999 entries as expected doesn't return any entries because the file status listing of {{foo/}} itself returns {{404}}!

This bug essentially causes an entire subtree to disappear merely because of a problem accessing one of the _children_. In a distributed environment (which is what ViewFs was intended for), with thousands of mappings to various HTTP-based cloud storage accounts, it's not unexpected that one of them might be temporarily unavailable. But this bug would cause the _parent_ directory to seem unavailable, essentially making it appear that e.g. {{/users}} simply did not exist simply because {{/users/fulano}} happened to be missing.

And if we happen to have {{/missing-mount}} mounted under the root and it was temporarily unavailable, and we did a {{listStatus()}} on the root directory {{/}} itself? Yes, it would _appear as if the root directory itself was missing_, i.e. the entire federated file system.

I have seen this bug in practice. In fact I had thought I had already filed a ticket for this, but maybe it was at some organization's internal bug tracking system instead of on the public Apache Hadoop bug tracking system.

You can verify this bug simply by adding a unit/integration test that mocks {{foo/bar1}}, {{foo/bar2}}, and {{foo/bar3}} as {{ChRootedFileSystem}} in a {{ViewFileSystem}} via {{ViewFileSystem.getMyFs()}}. Perform a {{ViewFileStatus.listStatus()}} on {{foo/}} and see that it returns 3 children. Then have {{getMyFs().getFileStatus()}} return a {{404}} error only for {{foo/bar2}}. Do a {{ViewFileStatus.listStatus()}} on {{foo/}} again, and instead of returning 2 children, it will claim that {{foo/}} itself does not exist.

Fixing this bug is very simple: remove the {{throw ex}} altogether on line 1449."
hadoop-aws tests to take a configurable subdir in the test bucket,13485413,Resolved,Major,Duplicate,10/Oct/22 10:55,15/Jan/25 12:21,3.3.5,"For parallel jenkins runs (now possible with s3guard cut) we need to be able to run the failsafe test suites either in separate buckets brackets (which doesn't scale) or in subdirectories of a single bucket. That needs a way to pass in a base directory for the suites and to disable cleanup of the entire bucket afterwards. Option pick up is straightforward; we will do it the way we have done with the others –an auth-keys option which can be overridden on the command line. 

{{S3ATestUtils.createTestPath(Path defVal)}} is already used to create test paths; it can be extended to pick up a unique process id as well as thread id system property, and use that for the path it constructs. 

another system property/config option is needed to disable root dir tests (ITestMarkerToolRootOperations, ITestS3AContractRootDir). a config/property picked up by the S3AContract returning false when probed for ""test.root-tests-enabled"" should do most of that.
"
Über-jira: S3A Hadoop 3.3.9-3.4.1 features,13484475,Resolved,Major,Fixed,04/Oct/22 16:21,03/Dec/24 11:29,3.3.5,"Changes related to s3a in the next branch-3.3 release. 
Presence in this list != any commitment to implement, unless there's active dev"
S3A: add option to disable probe for dir marker recreation on delete/rename.,13506721,Open,Major,,28/Nov/22 17:48,,3.3.4,"In applications which do many single-file deletions on the same dir, a lot of time is wasted in {{maybeCreateFakeParentDirectory()}}.

Proposed: add an option to disable the probe, for use by applications which are happy for parent dirs to sometimes disappear after a cleanup.

file by file delete is still woefully inefficient because of the HEAD request on every file, but there's no need to amplify the damage."
hadoop checknative fails to load openssl 3.x,13515231,Resolved,Major,Fixed,22/Dec/22 12:56,05/Nov/24 17:16,3.3.4,"After building Hadoop 3.3.4 from source on Ubuntu 22.04, `hadoop checknative` reports
{code:java}
$ hadoop checknative
2022-12-21 22:12:02,106 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
2022-12-21 22:12:02,107 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2022-12-21 22:12:02,130 INFO nativeio.NativeIO: The native code was built without PMDK support.
Native library checking:
hadoop:  true /hadoop/lib/native/libhadoop.so.1.0.0
zlib:    true /lib/x86_64-linux-gnu/libz.so.1
zstd  :  true /lib/x86_64-linux-gnu/libzstd.so.1
bzip2:   true /lib/x86_64-linux-gnu/libbz2.so.1
openssl: false EVP_CIPHER_CTX_block_size
ISA-L:   true /lib/x86_64-linux-gnu/libisal.so.2
PMDK:    false The native code was built without PMDK support.{code}
The issue seems to be at least two symbols that were removed from ABI in OpenSSL 3.x releases:
 * EVP_CIPHER_CTX_block_size (new name: EVP_CIPHER_CTX_get_block_size)
 * EVP_CIPHER_CTX_encrypting (new name: EVP_CIPHER_CTX_is_encrypting)

The attached patch [^100-hadoop-3.3.4-openssl-3.patch] works around the issue."
Upgrade ojalgo to  51.4.1,13506131,Open,Major,,27/Nov/22 16:15,,3.4.0,Upgrade ojalgo to  51.4.1 to resolve CWE-327: [Use of a Broken or Risky Cryptographic Algorithm|https://cwe.mitre.org/data/definitions/327.html] 
Make protobuf 2.5 an optional runtime dependency.,13485461,Resolved,Major,Fixed,10/Oct/22 15:26,29/Aug/24 14:01,3.3.4,"uses of protobuf 2.5 and RpcEnginej have been deprecated since 3.3.0 in HADOOP-17046

while still keeping those files around (for a long time...), how about we make the protobuf 2.5.0 export off hadoop common and hadoop-hdfs *provided*, rather than *compile*

that way, if apps want it for their own apis, they have to explicitly ask for it, but at least our own scans don't break.

i have no idea what will happen to the rest of the stack at this point, it will be ""interesting"" to see"
Azure Token provider requires tenant and client IDs despite being optional,13506258,Resolved,Major,Fixed,28/Nov/22 12:30,21/Aug/24 13:17,3.3.2,"The `AbfsConfiguration` class requires that we provide a tenant and client ID when using the `MsiTokenProvider` class to fetch an authentication token. The bug is that those fields are not required by the Azure API, which can infer those fields when the call is made from an Azure instance.

The fix is to make tenant and client ID optional when getting an Azure token from the Azure Metadata Service.

A fix has been submitted here: [https://github.com/apache/hadoop/pull/4262]

The bug was introduced with HADOOP-17725  ([https://github.com/apache/hadoop/pull/3041/files])"
Upgrade Huawei OBS client to 3.22.3.1,13485308,Open,Major,,09/Oct/22 09:22,,3.3.4,
Upgrade to snakeyaml 1.33,13484109,Resolved,Major,Fixed,01/Oct/22 10:39,29/Oct/22 17:44,3.3.6,"Recent snakeyaml fixes missed a use case. Relates to HADOOP-18443

[https://bitbucket.org/snakeyaml/snakeyaml/wiki/Changes]
 * Fix [#553|https://bitbucket.org/snakeyaml/snakeyaml/issues/553/loaderoptionssetcodepointlimit-not-honored]: LoaderOptions.setCodePointLimit() not honored by loadAll() (thanks to Robert Patrick)"
upgrade  AWS SDK to 1.12.316,13484611,Resolved,Major,Fixed,05/Oct/22 12:59,10/Oct/22 12:36,3.3.5,"go up to the latest sdk through the usual qualification process.

no doubt it'll be bigger..."
Exclude Dockerfile_windows_10 from hadolint,13485216,Resolved,Major,Fixed,08/Oct/22 00:33,11/Oct/22 16:24,3.3.4,"HADOOP-18133 tries to add Dockerfile for Windows 10 for building Hadoop. However, hadolint fails to run on *Dockerfile_windows_10* since the version of hadolint (1.1.1) used in Hadoop CI doesn't support parsing of the Windows command syntax.

HADOOP-18449 tries to upgrade the version of hadolint to the latest (2.10.0). However, it runs into some GPG issues on Centos 8.

Thus, we're going to exclude Dockerfile_windows_10 from getting hadolint-ed for the time being. There's a bug in Yetus that prevents exclusion of the file if the exclusion rule and the Dockerfile are added in the same PR - https://github.com/apache/yetus/pull/289#issuecomment-1263813381. Thus, this PR adds the exclusion rule first and then the Dockerfile will be added in another PR."
upgrade hsqldb to v2.7.1 due to CVE,13485330,Resolved,Major,Fixed,09/Oct/22 17:25,16/Nov/22 10:59,3.3.5,https://github.com/advisories/GHSA-77xx-rxvh-q682
Upgrade commons-text version to fix CVE-2022-42889,13486472,Resolved,Major,Fixed,16/Oct/22 07:49,18/Oct/22 03:43,3.2.4,"Upgrade commons-text version to ensure downstream applications are not at risk from CVE-2022-42889.

https://nvd.nist.gov/vuln/detail/CVE-2022-42889

The CVE is related to variable expansion through the utility class {{org.apache.commons.text.lookup.StringLookup}}.

# Hadoop does not use this in its codebase, and never has. Therefore it is not at direct risk from this. 
# We are not aware of any uses in its dependent libraries. Assuming this is true, hadoop is not at indirect risk from this.

Applications built using the hadoop libraries may be at risk if they use the class and get their version of commons-text set transitively from the hadoop build. Upgrading the dependency declared by hadoop ensures that these applications are not vulnerable.

"
S3A to support HTTPS web proxies,13487096,Resolved,Major,Fixed,19/Oct/22 14:12,27/Oct/22 14:49,3.3.4,"Currently, we cannot set the protocol for a proxy in S3A. The proxy protocol is set to ""http"" by default and thus we lack the support for HTTPS proxy in S3A."
Hadoop metrics should return 0 when there is no change,13488325,Resolved,Major,Fixed,21/Oct/22 06:41,09/Nov/22 02:24,3.4.0,"When we try to switch active NN to standby, we find that the getContentSummary average time is always a very high value even if there is no more query. For us, the metrics return 0 is more reasonable. The monitor is as below:

!image-2022-10-21-14-41-43-105.png!

 "
 An unhandled NullPointerException in class KeyProvider,13489057,Resolved,Major,Fixed,22/Oct/22 10:09,10/Nov/22 20:43,3.3.4,The code throws an unhandled NullPointerException when the method *getBaseName* of KeyProvider.java is called with a null as input.
Update build instructions for Windows using VS2019,13489151,Resolved,Major,Fixed,22/Oct/22 16:54,24/Oct/22 16:29,3.4.0,"With HADOOP-18133, we're finally able to build Hadoop on Windows using Visual Studio 2019. We now need to update the documentation with the latest instructions."
upgrade woodstox-core to 5.4.0 for security fix,13492599,Resolved,Major,Fixed,27/Oct/22 21:05,01/Nov/22 18:53,3.3.4,"Per [issue|https://github.com/FasterXML/woodstox/issues/157], woodstox-core 5.3.0 has security vulnerability and need to upgrade to 5.4.0 for fix.

The Hadoop Configuration classes uses woodstox to parse the XML format (core-site.xml, ...) but

* people don't normally put in DTDs
* the XML format is not the wire format used when applications submit jobs to the yarn resource manager.
* when parsing untrusted XML configuration files in restricted mode (eg. oozie workflows), DTD support is already disabled"
ABFS: Add fs.azure.enable.readahead option to disable readahead,13494362,Resolved,Major,Fixed,02/Nov/22 16:22,08/Nov/22 13:50,3.3.4,"Add an option fs.azure.enable.readahead to allow the abfs readahead to be disabled.

This has been shipping in cloudera releases, because workloads with heavy random IO don't always benefit from prefetching.

Pulling this in to ASF hadoop allows for
* everyone to experiment with/without prefetching
* ability to disable prefetching if you have problems with it

"
Deploy Hadoop trunk version website,13499673,Resolved,Major,Fixed,08/Nov/22 12:09,14/Feb/23 13:06,3.4.0,"Have a link to the running trunk documentation in the hadoop site.

A lot of projects like ozone, iceberg have a running documentation for the master branch version.

Good to have for Hadoop as well"
Disable abfs prefetching by default,13502176,Resolved,Major,Fixed,14/Nov/22 13:30,15/Nov/22 14:38,3.3.4,"After the addition of HADOOP-18517, we should disable readAhead by default to mitigate inconsistent read results caused by ABFS prefetching, HADOOP-18521.

As an urgent fix: Disable readAhead/prefetch, tracked for 3.3.5.
Long-term fix: HADOOP-18521, tracked for 3.3.6.
"
assertion failure in ITestS3APrefetchingInputStream,13503208,Resolved,Major,Fixed,17/Nov/22 16:47,23/Nov/22 17:48,3.4.0,"assert failure in {{ITestS3APrefetchingInputStream.testReadLargeFileFullyLazySeek}}; looks like the executor was acquired faster than the test expected.

{code}
java.lang.AssertionError: 
[Maxiumum named action_executor_acquired.max] 
Expecting:
 <0L>
to be greater than:
 <0L> 

{code}

proposed: cut that assert as it doesn't seem needed
"
Implement token storage solution based on MySQL,13505173,Resolved,Major,Fixed,22/Nov/22 01:16,22/Feb/23 19:24,3.3.6,"Hadoop RBF supports custom implementations of secret managers. At the moment, the only available implementation is ZKDelegationTokenSecretManagerImpl, which stores tokens and delegation keys in Zookeeper.

During our investigation, we found that the performance of routers is limited by the writes to the Zookeeper token store, which impacts requests for token creation, renewal and cancellation. An alternative secret manager implementation has been created, based on MySQL, to handle a higher number of writes.

We measured the throughput of each token operation (create/renew/cancel) on different setups and obtained the following results:
 # Sending requests directly to Namenode (no RBF):
Token creations: 290 reqs per sec
Token renewals: 86 reqs per sec
Token cancellations: 97 reqs per sec

 # Sending requests to routers using Zookeeper based secret manager:
Token creations: 31 reqs per sec
Token renewals: 29 reqs per sec
Token cancellations: 40 reqs per sec
 # Sending requests to routers using SQL based secret manager:
Token creations: 241 reqs per sec
Token renewals: 103 reqs per sec
Token cancellations: 114 reqs per sec

We noticed a significant improvement when using a SQL secret manager, comparable to the throughput offered by Namenodes."
Upgrade kafka to 2.8.2,13506123,Resolved,Major,Fixed,27/Nov/22 15:05,26/Dec/22 18:26,3.4.0,Upgrade kafka to 2.8.2 to resolve [CVE-2022-34917|https://nvd.nist.gov/vuln/detail/CVE-2022-34917] 
Upgrade Bouncy Castle to 1.70,13506134,Resolved,Major,Fixed,27/Nov/22 17:27,01/Jan/24 19:04,3.4.0,"Upgrade Bouncycastle to 1.70 to resolve

 
|[[sonatype-2021-4916] CWE-327: Use of a Broken or Risky Cryptographic Algorithm|https://ossindex.sonatype.org/vulnerability/sonatype-2021-4916?component-type=maven&component-name=org.bouncycastle/bcprov-jdk15on]|

|[[sonatype-2019-0673] CWE-400: Uncontrolled Resource Consumption ('Resource Exhaustion')|https://ossindex.sonatype.org/vulnerability/sonatype-2019-0673?component-type=maven&component-name=org.bouncycastle/bcprov-jdk15on]|"
Hadoop Archive tool (HAR) should acquire delegation tokens from source and destination file systems,13509155,Resolved,Major,Fixed,02/Dec/22 04:28,29/Mar/23 23:12,3.4.0,"Running Hadoop Archive tool (HAR) against source/dest that are not HDFS does not work today.

Example:
{noformat}
hadoop archive -archiveName foo.har -p ofs://ozone1/vol2/bucket2/src/ ofs://ozone1/vol2/bucket2/dst/
{noformat}
The command fails because it's essentially a MapReduce job and for some reason it does not obtain Ozone delegation token (only HDFS and KMS)

The failure:
{noformat}
22/11/30 18:12:10 INFO mapreduce.Job: Job job_1669745757520_0001 failed with state FAILED due to: Job setup failed : java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:789)
        at java.security.AccessController.doPrivileged(Native Method)
...
{noformat}

The workaround is to add the ozone path as the default file system:
{noformat}
export HADOOP_OPTS=""$HADOOP_OPTS -Dfs.defaultFS=ofs://ozone1/vol2/bucket2""
{noformat}

A proper fix should make sure HAR requests delegation tokens from src/dest file system too."
LogThrottlingHelper: the dependent recorder is not triggered correctly,13513117,Resolved,Major,Fixed,12/Dec/22 06:46,16/Dec/22 17:26,3.3.4,"The current implementation of {{LogThrottlingHelper}} works well most of the time, but it missed out one case, which appears quite common in the production codes:
- if the dependent recorder was not suppressed before the primary one is triggered on the next period, then the next logging of the dependent recorder will be unexpectedly suppressed.

{code:java}
    helper = new LogThrottlingHelper(LOG_PERIOD, ""foo"", timer);

    assertTrue(helper.record(""foo"", 0).shouldLog());
    assertTrue(helper.record(""bar"", 0).shouldLog());

    // Both should log once the period has elapsed
    // <pos1>
    assertTrue(helper.record(""foo"", LOG_PERIOD).shouldLog());
    assertTrue(helper.record(""bar"", LOG_PERIOD).shouldLog()); <--- This assertion will now fail
{code}

Note if we call {{helper.record(""bar"", LOG_PERIOD * 2)}} in <pos1>, as the existing test cases do, it will work as expected."
AWS SDK V2 - Qualify the upgrade. ,13513155,Resolved,Major,Done,12/Dec/22 10:02,11/Sep/23 13:32,3.4.0,Run tests as per [qualifying aws ask update|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md#-qualifying-an-aws-sdk-update]
Changing log level of IOStatistics increment to make the DEBUG logs less noisy,13513621,Resolved,Major,Fixed,14/Dec/22 15:30,15/Dec/22 11:53,3.3.5,During IOStatistics increment we log the incremented value and the final value at DEBUG level causing a lot of noise to the DEBUG logs. Changing that log level to TRACE.
Make XML transformer factory more lenient,13513649,Resolved,Major,Fixed,14/Dec/22 18:21,18/Dec/22 12:29,3.3.5,"Issues raised today in HADOOP-18469 about particular attributes that are not supported by Saxon transformer.

 

note: there are two PRs here, the initial patch and a followup which improves performance on incompatible transformers. backports/cherrypicks should include both."
Java 11 JavaDoc fails due to missing package comments,13514035,Resolved,Major,Fixed,15/Dec/22 11:37,06/Feb/23 18:18,3.4.0,"yetus is failing javadoc builds with import issues on package-info.java files related to @InterfaceAudience/stability annotations

We need to change how they are imported to fix this. Other modules have the same issue

This is happening because java11 javadoc requires a doc comment for all packages with annotations. "
ABFS: add probes of readahead fix,13514080,Resolved,Major,Fixed,15/Dec/22 17:04,16/Dec/22 16:20,3.3.5,"Followup patch as to  HADOOP-18456 to
aid in probing for read correctness of
hadoop ABFS client across different releases.

* ReadBufferManager constructor logs the fact it is safe at TRACE
* AbfsInputStream declares it is fixed in toString()
  by including fs.azure.capability.readahead.safe"" in the
  result.

The ABFS FileSystem hasPathCapability(""fs.azure.capability.readahead.safe"")
probe returns true to indicate the client's readahead manager has been fixed
to be safe when prefetching.

All Hadoop releases for which probe this returns false
and for which the probe ""fs.capability.etags.available""
returns true at risk of returning invalid data when reading
ADLS Gen2/Azure storage data.
"
Handle Server KDC re-login when Server and Client run in same JVM.,13514848,Resolved,Major,Fixed,20/Dec/22 10:01,08/Jan/23 18:26,3.1.1,"Handle re-login in Server when client, server running in same JVM and client trying to re-login, but it fails.

For example, NameNode is server but in same JVM journal node client also running to push to edit logs. When JN client try to re-login and it fails, it will destroy server service ticket also and NameNode not able to server client request. We can see the below error logs in NameNode log file.

 
{noformat}
Auth failed for x.x.x.x:42199:null (GSS initiate failed) with true cause: (GSS initiate failed)
Auth failed for x.x.x.x:42199:null (GSS initiate failed) with true cause: (GSS initiate failed)
Auth failed for x.x.x.x:42199:null (GSS initiate failed) with true cause: (GSS initiate failed){noformat}
Same discussion happened in HADOOP-17996."
No need to clean tmp files in distcp direct mode,13515018,Resolved,Major,Fixed,21/Dec/22 04:33,23/Feb/23 04:51,3.3.4,"it not necessary to do `cleanupTempFiles`  while ditcp commit job in direct  mode, because it there is no temp files in direct mode.

This clean operation will increase the task execution time, because it will get the list of files in the target path. When the number of files in the target path is very large, this operation will be very slow.

*note* there are two patches which need to be cherrypicked when picking this up; the original patch and a followup, both with HADOOP-18582 in the title


{code}
3b7b79b37ae HADOOP-18582. skip unnecessary cleanup logic in distcp (#5251)
e8a6b2c2c4e HADOOP-18582. Addendum: Skip unnecessary cleanup logic in DistCp.
{code}
"
[NFS GW] Fix regression after netty4 migration,13515267,Resolved,Major,Fixed,22/Dec/22 17:53,01/Feb/23 13:27,3.4.0,"Two bugs were found recently. Both related to hadoop portmap utility.

(1) The ChannelHandler that can be added to a ChannelPipeline in Netty4 must be Shareable. However, IdleStateHandler, used by Portmap is not. We need to fix it otherwise hadoop portmap may fail to start.

(2) RpcUdpResponseStage does not get the request from client (hadoop mountd) because of response type mismatch, and therefore it does not respond back to client request.

This is a regression from HADOOP-11245 but I consider it not a blocker, because most recent Linux distro supports rpcbind and there's little reason to use Hadoop's portmap."
Update the year to 2023,13516105,Resolved,Major,Fixed,31/Dec/22 23:22,01/Jan/23 17:46,2.10.3,Update the year to 2023
update jackson-databind 2.12.7.1 due to CVE fixes,13486069,Resolved,Major,Fixed,13/Oct/22 08:17,17/Oct/22 04:44,3.3.5,"* [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-42003]
 * [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-42004]
 * both fixes have been backported (the CVEs themselves need to be updated to reflect this)
 * [https://github.com/FasterXML/jackson-databind/pull/3622]"
Azure RefreshTokenBasedTokenProvider is only supporting public client,13492392,Open,Major,,27/Oct/22 08:07,,3.3.4,"The Azure RefreshTokenBasedTokenProvider is assuming the client is public, meaning it's not exchanging the refresh token to an access token with the client secret.

 

This limitation is not really justify and the RefreshTokenBasedTokenProvider should use the client secret if present.

 

From my understanding, there is no particular reason to think that hadoop is not able to store secrets securely, especially as I see the client credential flow, which require a confidential client, is supported by the library.

 

The fix is to simply inject the client secret in the request, using client basic auth method or client Post auth method, when the client secret is present.

 

https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/RefreshTokenBasedTokenProvider.java#L61"
Backport HADOOP-18427 and HADOOP-18452 to branch-3.3,13494478,Resolved,Major,Fixed,03/Nov/22 00:59,10/Nov/22 05:11,3.3.5,"This is a sub-task of HADOOP-18518 to upgrade zk&curator on 3.3 branches.

It is a clean cherry pick from [https://github.com/apache/hadoop/pull/4812] which solved the deprecation of EnsurePath in new Curator. Note, as this change contained a bug, fixed by [https://github.com/apache/hadoop/pull/4885] , we thus need to cherry pick this bug fix PR as well. "
Bump Zookeeper to 3.6.3 and Curator to 5.2.0 in branch 3.3,13494475,Open,Major,,03/Nov/22 00:39,,,"This is the parent ticket. We plan to backport the upgrade from trunk to branch-3.3 in 3 PRs:
PR1(HADOOP-18515). Cherry pick [https://github.com/apache/hadoop/pull/3241] which simply bump the version.

PR2(HADOOP-18519). Cherry pick [https://github.com/apache/hadoop/pull/3266] which solved the deprecation of PathChildrenCache/TreeCache in new ZK. Note, we need to backport https://issues.apache.org/jira/browse/HDFS-15383 along because PR-3266 is based on this earlier change, specifically isTokenWatcherEnabled was introduced in [ZKDelegationTokenSecretManager.java|https://github.com/apache/hadoop/pull/2047/files#diff-f65a8ac81e253e85af159ba041fbad62fbb34b5bd909c1e9fc93d58222f406b9]

PR3(HADOOP-18520).Cherry pick [https://github.com/apache/hadoop/pull/4812] which solved the deprecation of EnsurePath in new Curator. Note, as this change contained a bug, fixed by [https://github.com/apache/hadoop/pull/4885] , we thus need to cherry pick this bug fix PR as well. "
upgrade kotlin-stdlib due to CVEs,13486195,Open,Major,,13/Oct/22 18:54,,,"I'm not an expert on Kotlin but dependabot show these 2 CVEs with the version of kotlin-stdlib used in Hadoop.
 * [https://github.com/advisories/GHSA-cqj8-47ch-rvvq]
 * [https://github.com/advisories/GHSA-2qp4-g3q3-f92w]

kotlin-stlib 1.6.0 is the minimum version needed to fix both. It might be better to use latest v1.6 jar (currently 1.6.21) or even use latest jar altogether (currently 1.7.20).

 "
Bump netty to the latest 4.1.86,13514151,Resolved,Major,Duplicate,16/Dec/22 07:19,17/Mar/23 13:19,3.2.4,"Netty 4.1.86 fixes the following vulnerabilities.
 * HAProxyMessageDecoder Stack Exhaustion DoS (CVE-2022-41881)
 * HTTP Response splitting from assigning header value iterator (CVE-2022-41915)

For more details: https://netty.io/news/2022/12/12/4-1-86-Final.html"
AWS SDK V2 - Complete outstanding items,13511091,Resolved,Major,Fixed,09/Dec/22 15:34,24/Apr/23 16:30,3.4.0,"The following work remains to complete the SDK upgrade work:
 * S3A allows users configure to custom signers, add in support for this.
 * Remove SDK V1 bundle dependency
 * Update `getRegion()` logic to use retries. 
 * Add in progress listeners for `S3ABlockOutputStream`
 * Fix any failing tests."
AWS SDK V2 - Fix failing tests,13513575,Resolved,Major,Fixed,14/Dec/22 10:21,17/May/23 15:32,3.4.0,"We have a few failing tests for various reasons. Some are dependent on the TM, but others can be looked into and fixed. 


|TestS3AExceptionTranslation|test301ContainsEndpoint|Missing endpoint in SDK exception ([aws/aws-sdk-java-v2#3048|https://github.com/aws/aws-sdk-java-v2/issues/3048])|
|TestStreamChangeTracker|testCopyETagRequired, testCopyVersionIdRequired|Transfer Manager response does not yet have {{CopyObjectResult}}|
|ITestS3AFileContextStatistics|testStatistics|ProgressListeners not attached to non-TM uploads|
|ITestS3AEncryptionSSEC|multiple tests (14 out of 24)|Transfer Manager issue with SSE-C|
|ITestXAttrCost|testXAttrRoot.|{{headObject()}} with empty key fails|
|ITestSessionDelegationInFileystem|testDelegatedFileSystem|Succeeds, but {{headObject()}} with empty key commented out|
|ITestS3ACannedACLs|testCreatedObjectsHaveACLs|AWSCannedACL.LogDeliveryWrite not supported in SDK v2|"
AWS SDK V2 - Update region logic,13513154,Resolved,Major,Fixed,12/Dec/22 10:01,17/May/23 15:32,3.4.0,"SDK V2 will no longer resolve a buckets region if it is not set when initialising the client. 

 

Current logic will always make a head bucket call on FS initialisation. We should review this. Possible solution:
 * Warn if region is not set.
 * If no region, try and resolve. If resolution fails, throw an exception. Cache the region to optimise for short lived FS. "
Web UI has no session controller.There is a risk of session hijacking.,13509532,Open,Major,,03/Dec/22 09:56,,3.3.1,"When I browse the webui,and keep this page connect. the session which jetty given me.And session will keep alive as long as page alive."
Upgrade hikari.version from 4.0.3 to 5.0.1,13485355,Resolved,Major,Won't Fix,10/Oct/22 01:42,15/Feb/23 14:38,3.4.0,Upgrade hikari.version from 4.0.3 to 5.0.1
Upgrade grizzly version to 2.4.4,13506139,Open,Major,,27/Nov/22 19:07,,3.4.0,"Upgrade grizzly version to 2.4.4 to resolve
|[[sonatype-2016-0415] CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')|https://ossindex.sonatype.org/vulnerability/sonatype-2016-0415?component-type=maven&component-name=org.glassfish.grizzly/grizzly-http-server]|

[CVE-2014-0099|https://nvd.nist.gov/vuln/detail/CVE-2014-0099], [CVE-2014-0075|https://nvd.nist.gov/vuln/detail/CVE-2014-0075], [CVE-2017-1000028|https://nvd.nist.gov/vuln/detail/CVE-2017-1000028]"
"VectorIO FileRange type to support a ""reference"" field",13491347,Resolved,Major,Fixed,25/Oct/22 17:00,09/Jan/23 18:40,3.3.5,"to use in libraries, it is really good to be able to connect a FileRange back to the application/library level structure (chunk/split data, usually). 

Proposed: add an {{Object reference)) field which can be given arbitrary data or null, and queried for by app. it is not used in the API at all"
Set Cache-Control no-store max-age as 0 header on all dynamic content,13511508,Open,Major,,10/Dec/22 09:59,,,
Use file-level checksum by default when copying between two different file systems,13510831,Open,Major,,08/Dec/22 18:42,,,"h2. Goal

Reduce user friction

h2. Background

When distcp'ing between two different file systems, distcp still uses block-level checksum by default, even though the two file systems can be very different in how they manage blocks, so that a block-level checksum no longer makes sense between these two.

e.g. distcp between HDFS and Ozone without overriding {{dfs.checksum.combine.mode}} throws IOException because the blocks of the same file on two FSes are different (as expected):

{code}
$ hadoop distcp -i -pp /test o3fs://buck-test1.vol1.ozone1/
java.lang.Exception: java.io.IOException: File copy failed: hdfs://duong-1.duong.root.hwx.site:8020/test/test.bin --> o3fs://buck-test1.vol1.ozone1/test/test.bin
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552)
Caused by: java.io.IOException: File copy failed: hdfs://duong-1.duong.root.hwx.site:8020/test/test.bin --> o3fs://buck-test1.vol1.ozone1/test/test.bin
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:262)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:219)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:48)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://duong-1.duong.root.hwx.site:8020/test/test.bin to o3fs://buck-test1.vol1.ozone1/test/test.bin
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:258)
	... 11 more
Caused by: java.io.IOException: Checksum mismatch between hdfs://duong-1.duong.root.hwx.site:8020/test/test.bin and o3fs://buck-test1.vol1.ozone1/.distcp.tmp.attempt_local1346550241_0001_m_000000_0.Source and destination filesystems are of different types
Their checksum algorithms may be incompatible You can choose file-level checksum validation via -Ddfs.checksum.combine.mode=COMPOSITE_CRC when block-sizes or filesystems are different. Or you can skip checksum-checks altogether  with -skipcrccheck.
{code}

And it works when we use a file-level checksum like {{COMPOSITE_CRC}}:

{code:title=With -Ddfs.checksum.combine.mode=COMPOSITE_CRC}
$ hadoop distcp -i -pp /test o3fs://buck-test2.vol1.ozone1/ -Ddfs.checksum.combine.mode=COMPOSITE_CRC
22/10/18 19:07:42 INFO mapreduce.Job: Job job_local386071499_0001 completed successfully
22/10/18 19:07:42 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=219900
		FILE: Number of bytes written=794129
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=0
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=13
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
		HDFS: Number of bytes read erasure-coded=0
		O3FS: Number of bytes read=0
		O3FS: Number of bytes written=0
		O3FS: Number of read operations=5
		O3FS: Number of large read operations=0
		O3FS: Number of write operations=0
..
{code}

h2. Alternative

(if changing global defaults could potentially break distcp'ing between HDFS/S3/etc. Also [~weichiu] mentioned COMPOSITE_CRC is only added in Hadoop 3.1.1. So this might be the only way.)

Don't touch the global default, and make it a client-side config.

e.g. add a config to allow automatically usage of COMPOSITE_CRC (dfs.checksum.combine.mode) when distcp'ing between HDFS and Ozone, which would be the equivalent of specifying {{-Ddfs.checksum.combine.mode=COMPOSITE_CRC}} on the distcp command but the end user won't have to specify it every single time.


cc [~duongnguyen] [~weichiu]"
ABFS: Add thread-level IOStatistics in ABFS streams,13509842,Open,Major,,05/Dec/22 10:04,,,ABFS streams to capture thread-level IOStatsitsics for better metrics collection for worker threads.
"AliyunOSS: AliyunOSSFileSystem#open(Path path, int bufferSize) should use buffer size as its downloadPartSize",13506405,Open,Major,,28/Nov/22 13:57,,,"In our application, different components have their own suitable buffer size to download.

But currently, AliyunOSSFileSystem#open(Path path, int bufferSize) just get downloadPartSize from configuration.

We cannnot use different value for different components in our programs.

I think we should the method should use the buffer size from the paramater.

AliyunOSSFileSystem#open(Path path) could have default value as current default downloadPartSize."
hadoop-azure integration tests are failing on the trunk branch against non-HNS stores,13507296,Open,Major,,29/Nov/22 16:52,,3.4.0,"I am getting errors when running the hadoop-azure integration test from the trunk branch (commit 0ef572abed624011ad9a9e69d725774d6d00b75b).

This is running on an Azure instance, with HNS disabled and using a Shared Key (NonHNS-SharedKey).

Here are the logs when running those tests:

 
{code:java}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsHttpOperation
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsClient
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsInputStream
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAzureADAuthenticator
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestShellDecryptionKeyProvider
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsOutputStream
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAccountConfiguration
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.976 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsHttpOperation
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.174 s - in org.apache.hadoop.fs.azurebfs.services.TestShellDecryptionKeyProvider
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsPerfTracker
[ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.921 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.TestAccountConfiguration
[ERROR] testConfigPropNotFound(org.apache.hadoop.fs.azurebfs.TestAccountConfiguration)  Time elapsed: 0.102 s  <<< FAILURE!
java.lang.AssertionError: Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider""
        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:499)
        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:384)
        at org.apache.hadoop.fs.azurebfs.TestAccountConfiguration.testMissingConfigKey(TestAccountConfiguration.java:399)
        at org.apache.hadoop.fs.azurebfs.TestAccountConfiguration.testConfigPropNotFound(TestAccountConfiguration.java:386)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.396 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsClient
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestTextFileBasedIdentityHandler
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.043 s - in org.apache.hadoop.fs.azurebfs.services.TestTextFileBasedIdentityHandler
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestQueryParams
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.2 s - in org.apache.hadoop.fs.azurebfs.services.TestQueryParams
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestAbfsRenameRetryRecovery
[INFO] Running org.apache.hadoop.fs.azurebfs.services.TestExponentialRetryPolicy
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.088 s - in org.apache.hadoop.fs.azurebfs.services.TestAzureADAuthenticator
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsInputStreamStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.utils.TestCachedSASToken
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.651 s - in org.apache.hadoop.fs.azurebfs.utils.TestCachedSASToken
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.133 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsPerfTracker
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.278 s - in org.apache.hadoop.fs.azurebfs.services.TestExponentialRetryPolicy
[INFO] Running org.apache.hadoop.fs.azurebfs.utils.TestUriUtils
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.713 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsOutputStream
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.861 s - in org.apache.hadoop.fs.azurebfs.TestAbfsInputStreamStatistics
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.668 s - in org.apache.hadoop.fs.azurebfs.utils.TestUriUtils
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsOutputStreamStatistics
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.598 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsRenameRetryRecovery
[INFO] Running org.apache.hadoop.fs.azurebfs.diagnostics.TestConfigurationValidators
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsNetworkStatistics
[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.322 s - in org.apache.hadoop.fs.azurebfs.diagnostics.TestConfigurationValidators
[INFO] Running org.apache.hadoop.fs.azurebfs.TestTracingContext
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsConfigurationFieldsValidation
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsErrorTranslation
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.61 s - in org.apache.hadoop.fs.azurebfs.TestAbfsConfigurationFieldsValidation
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsStatistics
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.534 s - in org.apache.hadoop.fs.azurebfs.TestAbfsErrorTranslation
[INFO] Running org.apache.hadoop.fs.azurebfs.extensions.TestCustomOauthTokenProvider
[INFO] Running org.apache.hadoop.fs.azurebfs.extensions.TestDTManagerLifecycle
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.441 s - in org.apache.hadoop.fs.azurebfs.TestAbfsOutputStreamStatistics
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.849 s - in org.apache.hadoop.fs.azurebfs.extensions.TestCustomOauthTokenProvider
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.786 s - in org.apache.hadoop.fs.azurebfs.TestAbfsNetworkStatistics
[WARNING] Tests run: 3, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 6.956 s - in org.apache.hadoop.fs.azurebfs.TestTracingContext
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.59 s - in org.apache.hadoop.fs.azurebfs.extensions.TestDTManagerLifecycle
[INFO] Running org.apache.hadoop.fs.azurebfs.TestAbfsCrc64
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.061 s - in org.apache.hadoop.fs.azurebfs.TestAbfsCrc64
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.686 s - in org.apache.hadoop.fs.azurebfs.TestAbfsStatistics
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.877 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsClientThrottlingAnalyzer
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 55.459 s - in org.apache.hadoop.fs.azurebfs.services.TestAbfsInputStream
[INFO]
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider""
[INFO]
[ERROR] Tests run: 107, Failures: 1, Errors: 0, Skipped: 2
[INFO]
[ERROR] There are test failures.

Please refer to /hadoop/hadoop-tools/hadoop-azure/target/surefire-reports for the individual test results.
Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[INFO]
[INFO] --- maven-jar-plugin:2.5:jar (default-jar) @ hadoop-azure ---
[INFO]
[INFO] --- maven-jar-plugin:2.5:test-jar (default) @ hadoop-azure ---
[INFO] Building jar: /hadoop/hadoop-tools/hadoop-azure/target/hadoop-azure-3.4.0-SNAPSHOT-tests.jar
[INFO]
[INFO] --- maven-site-plugin:3.11.0:attach-descriptor (attach-descriptor) @ hadoop-azure ---
[INFO] Skipping because packaging 'jar' is not pom.
[INFO]
[INFO] --- maven-failsafe-plugin:3.0.0-M1:integration-test (integration-test-abfs-parallel-classesAndMethods) @ hadoop-azure ---
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit47/3.0.0-M1/surefire-junit47-3.0.0-M1.jar
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit47/3.0.0-M1/surefire-junit47-3.0.0-M1.jar (93 kB at 2.4 MB/s)
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit47/3.0.0-M1/surefire-junit47-3.0.0-M1.pom
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-junit47/3.0.0-M1/surefire-junit47-3.0.0-M1.pom (7.3 kB at 292 kB/s)
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/common-junit48/3.0.0-M1/common-junit48-3.0.0-M1.pom
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/common-junit48/3.0.0-M1/common-junit48-3.0.0-M1.pom (2.4 kB at 70 kB/s)
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-grouper/3.0.0-M1/surefire-grouper-3.0.0-M1.pom
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-grouper/3.0.0-M1/surefire-grouper-3.0.0-M1.pom (2.6 kB at 146 kB/s)
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/common-junit48/3.0.0-M1/common-junit48-3.0.0-M1.jar
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-grouper/3.0.0-M1/surefire-grouper-3.0.0-M1.jar
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/common-junit48/3.0.0-M1/common-junit48-3.0.0-M1.jar (22 kB at 1.3 MB/s)
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/surefire/surefire-grouper/3.0.0-M1/surefire-grouper-3.0.0-M1.jar (40 kB at 1.8 MB/s)
[INFO]
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsRestOperationException
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.294 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsRestOperationException
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRenameUnicode
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.288 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRenameUnicode
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsOutputStream
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.765 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsOutputStream
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCLI
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.222 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCLI
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.588 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsNetworkStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsPositionedRead
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.635 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsPositionedRead
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemBackCompat
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.634 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemBackCompat
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsUnbuffer
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.496 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsUnbuffer
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCopy
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.503 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCopy
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAppend
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.634 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAppend
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAuthorization
[WARNING] Tests run: 29, Failures: 0, Errors: 0, Skipped: 29, Time elapsed: 2.638 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAuthorization
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestOauthOverAbfsScheme
[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.277 s - in org.apache.hadoop.fs.azurebfs.ITestOauthOverAbfsScheme
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemInitAndCreate
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.317 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemInitAndCreate
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsDurationTrackers
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.97 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsDurationTrackers
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemDelete
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.973 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemDelete
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsClient
[WARNING] Tests run: 5, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 19.652 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsClient
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFilesystemAcl
[WARNING] Tests run: 66, Failures: 0, Errors: 0, Skipped: 56, Time elapsed: 11.749 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFilesystemAcl
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestFileSystemProperties
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.013 s - in org.apache.hadoop.fs.azurebfs.ITestFileSystemProperties
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemOauth
[WARNING] Tests run: 2, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 0.099 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemOauth
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemMkDir
[WARNING] Tests run: 5, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.549 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemMkDir
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsOutputStreamStatistics
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.074 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsOutputStreamStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsIdentityTransformer
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.745 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsIdentityTransformer
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFlush
[INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.404 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFlush
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemE2E
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.549 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemE2E
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemStoreListStatusWithRange
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.767 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemStoreListStatusWithRange
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCheckAccess
[ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 10, Time elapsed: 1.521 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCheckAccess
[ERROR] testCheckAccessForAccountWithoutNS(org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCheckAccess)  Time elapsed: 1.15 s  <<< FAILURE!
java.lang.AssertionError: Expecting org.apache.hadoop.security.AccessControlException with text ""This request is not authorized to perform this operation using this permission."", 403 but got : ""void""
        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:499)
        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:529)
        at org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCheckAccess.testCheckAccessForAccountWithoutNS(ITestAzureBlobFileSystemCheckAccess.java:181)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.lang.Thread.run(Thread.java:829)

[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsInputStreamStatistics
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.827 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsInputStreamStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsStatistics
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.649 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCreate
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.393 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemCreate
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemMainOperation
[INFO] Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.373 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemMainOperation
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFinalize
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFinalize
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRandomRead
[WARNING] Tests run: 10, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 3.196 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRandomRead
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRename
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.622 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemRename
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsHugeFiles
[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.34 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsHugeFiles
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestWasbAbfsCompatibility
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.802 s - in org.apache.hadoop.fs.azurebfs.ITestWasbAbfsCompatibility
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestClientUrlScheme
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.604 s - in org.apache.hadoop.fs.azurebfs.ITestClientUrlScheme
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestFileSystemRegistration
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.775 s - in org.apache.hadoop.fs.azurebfs.ITestFileSystemRegistration
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestSharedKeyAuth
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.186 s - in org.apache.hadoop.fs.azurebfs.ITestSharedKeyAuth
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestFileSystemInitialization
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.474 s - in org.apache.hadoop.fs.azurebfs.ITestFileSystemInitialization
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestCustomerProvidedKey
[WARNING] Tests run: 28, Failures: 0, Errors: 0, Skipped: 28, Time elapsed: 0.882 s - in org.apache.hadoop.fs.azurebfs.ITestCustomerProvidedKey
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAttributes
[WARNING] Tests run: 3, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 0.402 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemAttributes
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemDelegationSAS
[WARNING] Tests run: 14, Failures: 0, Errors: 0, Skipped: 14, Time elapsed: 0.41 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemDelegationSAS
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsMsiTokenProvider
[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.298 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsMsiTokenProvider
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFileStatus
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.921 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemFileStatus
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestGetNameSpaceEnabled
[WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 6.37 s - in org.apache.hadoop.fs.azurebfs.ITestGetNameSpaceEnabled
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsListStatusRemoteIterator
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.053 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsListStatusRemoteIterator
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemPermission
[WARNING] Tests run: 128, Failures: 0, Errors: 0, Skipped: 128, Time elapsed: 9.17 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemPermission
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStreamSmallFileReads
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 48.622 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStreamSmallFileReads
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStream
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 48.918 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStream
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStreamReadFooter
[INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 58.717 s - in org.apache.hadoop.fs.azurebfs.services.ITestAbfsInputStreamReadFooter
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemLease
[INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 75.544 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemLease
[INFO]
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   ITestAzureBlobFileSystemCheckAccess.testCheckAccessForAccountWithoutNS:181 Expecting org.apache.hadoop.security.AccessControlException with text ""This request is not authorized to perform this operation using this permission."", 403 but got : ""void""
[INFO]
[ERROR] Tests run: 567, Failures: 1, Errors: 0, Skipped: 277
[INFO]
[INFO]
[INFO] --- maven-failsafe-plugin:3.0.0-M1:integration-test (integration-test-abfs-parallel-classes) @ hadoop-azure ---
[INFO]
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsLoadManifestsStage
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestSmallWriteOptimization
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek
[INFO] Running org.apache.hadoop.fs.azurebfs.services.ITestReadBufferManager
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCleanupStage
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemE2EScale
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsRenameStageFailure
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsJobThroughManifestCommitter
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.918 s - in org.apache.hadoop.fs.azurebfs.services.ITestReadBufferManager
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCommitTaskStage
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.879 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsRenameStageFailure
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.657 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCleanupStage
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTaskManifestFileIO
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCreateOutputDirectoriesStage
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.991 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsLoadManifestsStage
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.569 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemE2EScale
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.159 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCommitTaskStage
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort
[INFO] Running org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestStoreOperations
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.197 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTaskManifestFileIO
[ERROR] Tests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 32.672 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsJobThroughManifestCommitter
[ERROR] test_0420_validateJob(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsJobThroughManifestCommitter)  Time elapsed: 0.74 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0002/test/TestJobThroughManifestCommitter/out%20put/dir-03-00/dir-02-00/dir-01-00/part-00-0033': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0002/test/TestJobThroughManifestCommitter/out put/_temporary/202211291549220355_0002/01/tasks/attempt_202211291549220355_0002_m_000000_1/dir-03-00/dir-02-00/dir-01-00/part-00-0033 with etag 0x8DAD22153D4F458 and length 2 but found a file with etag 0x8DAD22159994F7E and length 2
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.371 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsReadWriteAndSeek
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAbfsStreamStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractRename
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAzureBlobFileSystemBasics
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.86 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsCreateOutputDirectoriesStage
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractRootDirectory
[WARNING] Tests run: 3, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 8.559 s - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestStoreOperations
[WARNING] Tests run: 48, Failures: 0, Errors: 0, Skipped: 24, Time elapsed: 41.978 s - in org.apache.hadoop.fs.azurebfs.ITestSmallWriteOptimization
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractOpen
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.96 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsStreamStatistics
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractMkdir
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractSetTimes
[WARNING] Tests run: 9, Failures: 0, Errors: 0, Skipped: 9, Time elapsed: 9.624 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractRootDirectory
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.207 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractRename
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractEtag
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsContractUnbuffer
[ERROR] Tests run: 17, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 25.661 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol
[ERROR] testCommitterWithDuplicatedCommit(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 10.143 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testCommitterWithDuplicatedCommit/part-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testCommitterWithDuplicatedCommit/_temporary/job_202211291549500881_0004/01/tasks/attempt_202211291549500881_0004_m_000000_0/part-m-00000 with etag 0x8DAD2215D8BA025 and length 40 but found a file with etag 0x8DAD2215F25DA02 and length 40
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testMapFileOutputCommitter(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 1.857 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testMapFileOutputCommitter/part-m-00000/data': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testMapFileOutputCommitter/_temporary/job_202211291550000055_0004/01/tasks/attempt_202211291550000055_0004_m_000000_0/part-m-00000/data with etag 0x8DAD2215FF58EA2 and length 296 but found a file with etag 0x8DAD221603B8E0D and length 296
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testCommitLifecycle(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 1.462 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testCommitLifecycle/part-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testCommitLifecycle/_temporary/job_202211291550020019_0004/01/tasks/attempt_202211291550020019_0004_m_000000_0/part-m-00000 with etag 0x8DAD22160F41493 and length 40 but found a file with etag 0x8DAD22161466E35 and length 40
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testConcurrentCommitTaskWithSubDir(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 1.262 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testConcurrentCommitTaskWithSubDir/SUB_DIR/part-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testConcurrentCommitTaskWithSubDir/_temporary/job_202211291550030237_0004/00/tasks/attempt_202211291550030237_0004_m_000000_1/SUB_DIR/part-m-00000 with etag 0x8DAD22161DB6C66 and length 40 but found a file with etag 0x8DAD22162100941 and length 40
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testParallelJobsToAdjacentPaths(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 1.401 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testParallelJobsToAdjacentPaths/part-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testParallelJobsToAdjacentPaths/_temporary/job_202211291550080667_0004/01/tasks/attempt_202211291550080667_0004_m_000000_0/part-m-00000 with etag 0x8DAD221646E60AB and length 40 but found a file with etag 0x8DAD22164B4AE25 and length 40
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testOutputFormatIntegration(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 0.925 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testOutputFormatIntegration/part-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testOutputFormatIntegration/_temporary/job_202211291550110055_0004/01/tasks/attempt_202211291550110055_0004_m_000000_0/part-m-00000 with etag 0x8DAD221662398A8 and length 4 but found a file with etag 0x8DAD221664D1399 and length 4
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[ERROR] testTwoTaskAttemptsCommit(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsManifestCommitProtocol)  Time elapsed: 1.291 s  <<< ERROR!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.OutputValidationException: `abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testTwoTaskAttemptsCommit/attempt2-m-00000': Expected the file renamed from abfs://****@****.dfs.core.windows.net/fork-0004/test/ITestAbfsManifestCommitProtocol-testTwoTaskAttemptsCommit/_temporary/job_202211291550130427_0004/01/tasks/attempt_202211291550130427_0004_m_000000_1/attempt2-m-00000 with etag 0x8DAD221676CB12D and length 40 but found a file with etag 0x8DAD22167ACBE08 and length 40
        at org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage.validateOneFile(ValidateRenamedFilesStage.java:166)
        at org.apache.hadoop.util.functional.TaskPool$Builder.lambda$runParallel$0(TaskPool.java:410)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 7.58 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractSetTimes
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.177 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractMkdir
[INFO] Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.624 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractOpen
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractDistCp
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractAppend
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractConcat
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractDelete
[INFO] Tests run: 45, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.35 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAzureBlobFileSystemBasics
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.322 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsContractUnbuffer
[WARNING] Tests run: 4, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 11.125 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractEtag
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractCreate
[WARNING] Tests run: 5, Failures: 0, Errors: 0, Skipped: 5, Time elapsed: 6.888 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractConcat
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractGetFileStatus
[INFO] Running org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractSeek
[WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 9.429 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractAppend
[INFO] Running org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemListStatus
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.189 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractDelete
[INFO] Running org.apache.hadoop.fs.azurebfs.extensions.ITestAbfsDelegationTokens
[INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.352 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractCreate
[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.206 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractSeek
[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.014 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractGetFileStatus
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.823 s - in org.apache.hadoop.fs.azurebfs.extensions.ITestAbfsDelegationTokens
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.969 s - in org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemListStatus
[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.734 s - in org.apache.hadoop.fs.azurebfs.contract.ITestAbfsFileSystemContractDistCp
[ERROR] Tests run: 7, Failures: 1, Errors: 0, Skipped: 2, Time elapsed: 100.527 s <<< FAILURE! - in org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort
[ERROR] test_110_teragen(org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort)  Time elapsed: 25.881 s  <<< FAILURE!
java.lang.AssertionError: teragen(1000, abfs://****@****.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:89)
        at org.junit.Assert.failNotEquals(Assert.java:835)
        at org.junit.Assert.assertEquals(Assert.java:647)
        at org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort.executeStage(ITestAbfsTerasort.java:211)
        at org.apache.hadoop.fs.azurebfs.commit.ITestAbfsTerasort.test_110_teragen(ITestAbfsTerasort.java:244)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.lang.Thread.run(Thread.java:829)

[INFO]
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   ITestAbfsTerasort.test_110_teragen:244->executeStage:211->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 teragen(1000, abfs://****@****.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected:<0> but was:<1>
[ERROR] Errors:
[ERROR]   ITestAbfsJobThroughManifestCommitter.test_0420_validateJob ? OutputValidation ...
[ERROR]   ITestAbfsManifestCommitProtocol.testCommitLifecycle ? OutputValidation `abfs:/...
[ERROR]   ITestAbfsManifestCommitProtocol.testCommitterWithDuplicatedCommit ? OutputValidation
[ERROR]   ITestAbfsManifestCommitProtocol.testConcurrentCommitTaskWithSubDir ? OutputValidation
[ERROR]   ITestAbfsManifestCommitProtocol.testMapFileOutputCommitter ? OutputValidation ...
[ERROR]   ITestAbfsManifestCommitProtocol.testOutputFormatIntegration ? OutputValidation
[ERROR]   ITestAbfsManifestCommitProtocol.testParallelJobsToAdjacentPaths ? OutputValidation
[ERROR]   ITestAbfsManifestCommitProtocol.testTwoTaskAttemptsCommit ? OutputValidation `...
[INFO]
[ERROR] Tests run: 335, Failures: 1, Errors: 8, Skipped: 46 {code}
 

And here are the commands I run on a new instance to get there:

 
{code:java}
cd /hadoop/hadoop-maven-plugins
mvn install

cd /hadoop/hadoop-tools/hadoop-azure
cp ./src/test/resources/azure-auth-keys.xml.template ./src/test/resources/azure-auth-keys.xml

az login --identity

ACCOUNT_NAME=****
CONTAINER_NAME=****
RESOURCE_GROUP=****
ACCOUNT_ACCESS_KEY=$(az storage account keys list \
      --resource-group ""$RESOURCE_GROUP"" \
      --account-name ""$ACCOUNT_NAME"" \
      --output tsv \
      --query '[].{value:value} | [0]')

cat <<EOT > src/test/resources/azure-auth-keys.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<configuration>
  <!-- Account names without domain -->
  <property>
    <name>fs.azure.hnsTestAccountName</name>
    <value>$ACCOUNT_NAME</value>
  </property>

  <property>
    <name>fs.azure.nonHnsTestAccountName</name>
    <value>$ACCOUNT_NAME</value>
  </property>
</configuration>
EOT
cp ./src/test/resources/accountSettings/accountName_settings.xml.template ./src/test/resources/accountSettings/${ACCOUNT_NAME}_settings.xml

sed -i -e ""s/ACCOUNTNAME/$ACCOUNT_NAME/g"" ./src/test/resources/accountSettings/${ACCOUNT_NAME}_settings.xml
sed -i -e ""s/CONTAINER_NAME/$CONTAINER_NAME/g"" ./src/test/resources/accountSettings/${ACCOUNT_NAME}_settings.xml
sed -i -e ""s/IS_NAMESPACE_ENABLED/false/g"" ./src/test/resources/accountSettings/${ACCOUNT_NAME}_settings.xml
sed -i -e ""s#ACCOUNT_KEY#$ACCOUNT_ACCESS_KEY#g"" ./src/test/resources/accountSettings/${ACCOUNT_NAME}_settings.xml

dev-support/testrun-scripts/runtests.sh{code}
 

 

 "
KeyVault CrednetialsProvider,13501136,Open,Major,,11/Nov/22 15:05,,,"Azure KeyVault is a secure key management tool that keys, secrets , etc could securely store in it. Also azure KeyVault is a secure place to store storage account access key.

By providing a new CredentialsProvider, it's possible to easily fetch access key from KeyVault and this would helps a lot of developer especially in the local environment"
"S3A create(Path f, FsPermission permission)  need create target middle directories?",13505876,Resolved,Major,Information Provided,25/Nov/22 08:22,25/Nov/22 11:34,,"I see the code of s3a create(Path f, etc) which does not create target middle directories,

So whether exist this one case in which I create only one file  ""/a/b/c.txt"", It can get ""b"" as a directory by list ""/a/"", but If we delete this key ""/a/b/c.txt"",  ""a"" and ""b"" directories are all gone disappear.  

In the end, I want to ask two questions

1) Whether HCFS interface create(Path f, etc) need to create target middle directories inside?(sorry did not find doc to say about this)

2) If not create target middle directories inside, Is the object storage(s3a) scenario described above plausible?"
Allow to retrieve an object from MinIO (S3 API) with a very restrictive policy,13499634,Resolved,Major,Won't Fix,08/Nov/22 09:00,22/Nov/22 11:35,,"Hello,

We're using Spark ({{{}""org.apache.spark:spark-[catalyst|core|sql]_2.12:3.2.2""{}}}) and Hadoop ({{{}""org.apache.hadoop:hadoop-common:3.3.3""{}}}) and want to retrieve an object stored in a MinIO bucket (MinIO implements the S3 API). Spark relies on Hadoop for this operation.

The MinIO bucket (that we don't manage) is configured with a very restrictive policy that only allows us to retrieve the object (and nothing else). Something like:
{code:java}
{
  ""statement"": [
    {
      ""effect"": ""Allow"",
      ""action"": [ ""s3:GetObject"" ],
      ""resource"": [ ""arn:aws:s3:::minio-bucket/object"" ]
    }
  ]
}{code}
And using the AWS CLI, we can well retrieve the object.

When we try with Spark's {{{}DataFrameReader{}}}, we receive an HTTP 403 response (access denied) from MinIO:
{code:java}
java.nio.file.AccessDeniedException: s3a://minio-bucket/object: getFileStatus on s3a://minio-bucket/object: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied. (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; ...
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)
    at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
    at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
    at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:571)
    at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:481)
    at com.soprabanking.dxp.pure.bf.dataaccess.S3Storage.loadDataset(S3Storage.java:55)
    at com.soprabanking.dxp.pure.bf.business.step.DatasetLoader.lambda$doLoad$3(DatasetLoader.java:148)
    at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:125)
    at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)
    at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:151)
    at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)
    at reactor.core.publisher.MonoFlatMap$FlatMapInner.onNext(MonoFlatMap.java:249)
    at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1816)
    at reactor.core.publisher.MonoZip$ZipCoordinator.signal(MonoZip.java:251)
    at reactor.core.publisher.MonoZip$ZipInner.onNext(MonoZip.java:336)
    at reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2398)
    at reactor.core.publisher.MonoZip$ZipInner.onSubscribe(MonoZip.java:325)
    at reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)
    at reactor.core.publisher.Mono.subscribe(Mono.java:4400)
    at reactor.core.publisher.MonoZip.subscribe(MonoZip.java:128)
    at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:157)
    at reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:74)
    at reactor.core.publisher.FluxFilter$FilterSubscriber.onNext(FluxFilter.java:113)
    at reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:74)
    at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118)
    at reactor.core.publisher.MonoPeekTerminal$MonoTerminalPeekSubscriber.onNext(MonoPeekTerminal.java:180)
    at reactor.core.publisher.FluxPeekFuseable$PeekFuseableConditionalSubscriber.onNext(FluxPeekFuseable.java:503)
    at reactor.core.publisher.FluxHide$SuppressFuseableSubscriber.onNext(FluxHide.java:137)
    at reactor.core.publisher.Operators$MonoInnerProducerBase.complete(Operators.java:2664)
    at reactor.core.publisher.MonoSingle$SingleSubscriber.onComplete(MonoSingle.java:180)
    at com.jakewharton.retrofit2.adapter.reactor.BodyFlux$BodySubscriber.onComplete(BodyFlux.java:80)
    at reactor.core.publisher.StrictSubscriber.onComplete(StrictSubscriber.java:123)
    at reactor.core.publisher.FluxCreate$BaseSink.complete(FluxCreate.java:439)
    at reactor.core.publisher.FluxCreate$LatestAsyncSink.drain(FluxCreate.java:945)
    at reactor.core.publisher.FluxCreate$LatestAsyncSink.complete(FluxCreate.java:892)
    at reactor.core.publisher.FluxCreate$SerializedFluxSink.drainLoop(FluxCreate.java:240)
    at reactor.core.publisher.FluxCreate$SerializedFluxSink.drain(FluxCreate.java:206)
    at reactor.core.publisher.FluxCreate$SerializedFluxSink.complete(FluxCreate.java:197)
    at com.jakewharton.retrofit2.adapter.reactor.EnqueueSinkConsumer$DisposableCallback.onResponse(EnqueueSinkConsumer.java:52)
    at retrofit2.OkHttpCall$1.onResponse(OkHttpCall.java:161)
    at brave.okhttp3.TraceContextCall$TraceContextCallback.onResponse(TraceContextCall.java:95)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:519)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source){code}

The credentials are well set but under the hood Hadoop calls MinIO to check whether the object is a directory (which we don't want), and this results in a failure.

We can well retrieve the object by changing MinIO's policy - but this isn't an option to us - to something like:
{code:java}
{
  ""statement"": [
    {
      ""effect"": ""Allow"",
      ""action"": [ ""s3:GetObject"" ],
      ""resource"": [ ""arn:aws:s3:::minio-bucket/object"" ]
    },
    {
      ""effect"": ""Allow"",
      ""action"": [ ""s3:ListBucket"" ],
      ""resource"": [ ""arn:aws:s3:::minio-bucket/"" ],
      ""condition"": {
        ""StringLike"": {
          ""s3:prefix"": [ ""object"", ""object/"" ]
        }
      }
    }
  ]
}{code}

We couldn't find any way to configure Hadoop so that it just attempts to retrieve the object. Reading HADOOP-17454, it feels like it could be possible to provide options to fine-tune Hadoop's behaviour.

Are there such options? If not, is it a reasonable behaviour to put in place?

Regards,

Sébastien

Please note this is my first time here: I hope I picked the right project, issue type and priority (I tried my best looking around). If not, I'm very sorry about that."
Backport HDFS-15383 and HADOOP-17835 to branch-3.3,13494477,Resolved,Major,Fixed,03/Nov/22 00:55,09/Nov/22 17:51,3.3.5,"This is a sub-task of HADOOP-18518 to upgrade zk&curator on 3.3 branches.

It contains clean cherry pick from [https://github.com/apache/hadoop/pull/3266] which solved the deprecation of PathChildrenCache/TreeCache in new ZK. 
Also clean cherry pick from https://issues.apache.org/jira/browse/HDFS-15383 because PR-3266 is based on this earlier change, specifically isTokenWatcherEnabled was introduced in [ZKDelegationTokenSecretManager.java|https://github.com/apache/hadoop/pull/2047/files#diff-f65a8ac81e253e85af159ba041fbad62fbb34b5bd909c1e9fc93d58222f406b9]"
Update jackson-databind to mitigate CVE-2022-42003,13484522,Resolved,Major,Duplicate,04/Oct/22 22:03,31/Oct/22 07:55,3.3.4,Update jackson-databind to mitigate CVE-2022-42003
Backport HADOOP-17612 to branch-3.3,13494194,Resolved,Major,Fixed,02/Nov/22 00:55,07/Nov/22 17:27,3.3.5,"This is a sub-task of HADOOP-18518 to upgrade zk&curator on 3.3 branches. 

It is a clean cherry pick from [https://github.com/apache/hadoop/pull/3241] ."
Remove the legacy Ozone website,13493538,Open,Major,,28/Oct/22 20:35,,,"Let's remove the old Ozone website: https://hadoop.apache.org/ozone/

Since Ozone has moved to a separate TLP long ago with its own website."
ABFS: Support client-side encryption,13493472,Open,Major,,28/Oct/22 13:15,,3.3.4,"ABFS currently supports [customer-provided keys|https://azure.microsoft.com/en-us/blog/customer-provided-keys-with-azure-storage-service-encryption/], but does not support [client-side encryption|https://learn.microsoft.com/en-us/azure/storage/blobs/client-side-encryption?tabs=dotnet]. With client-side encryption, the encryption key never leaves the client."
Convert build instructions file to markdown,13492088,Open,Major,,26/Oct/22 17:40,,3.4.0,Need to convert https://github.com/apache/hadoop/blob/trunk/BUILDING.txt to markdown.
ArrayIndexOutOfBoundsException in TestCoderBase.java,13489059,Open,Major,,22/Oct/22 10:54,,3.3.4,The code throws an unhandled ArrayIndexOutOfBoundsException in the method _backupAndEraseChunks_ of _TestCoderBase_ class when it tries to access the out of bound indexes in _dataChunks_ and _parityChunks_ array.
The check logic for erasedIndexes in XORRawDecoder is buggy,13485650,Open,Major,,11/Oct/22 11:59,,3.3.4,"In the method _doDecode_ of class {_}XORRawDecoder{_}, the code does not handle all the erased and null marked locations in the array ({_}inputs{_}) but only skips the first erased location ({_}erasedIndexes[0]{_}). The missing handling results in an unhandled NullPointerException."
upgrade commons-text to 1.10.0,13485915,Resolved,Major,Duplicate,12/Oct/22 14:31,18/Oct/22 03:43,,"Extends HADOOP-18341

[https://commons.apache.org/proper/commons-text/changes-report.html#a1.10.0]

StringInterpolator prior to v1.10.0 allowed scripting that could be problematic – a similar issue to one that led to CVE in commons-configuration2. "
AWS v2 SDK upgrade log to not warn of use standard AWS Credential Providers,13484621,Resolved,Major,Fixed,05/Oct/22 14:10,14/Oct/22 10:54,3.4.0,"
looking at test output with the sdk warnings enabled, it is now always warning of a v1 provider reference, even if the user hasn't set any fs.s3a.credential.provider option


{code}
2022-10-05 14:09:09,733 [setup] DEBUG s3a.S3AUtils (S3AUtils.java:createAWSCredentialProvider(691)) - Credential provider class is org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
2022-10-05 14:09:09,733 [setup] DEBUG s3a.S3AUtils (S3AUtils.java:createAWSCredentialProvider(691)) - Credential provider class is org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
2022-10-05 14:09:09,734 [setup] WARN  s3a.SDKV2Upgrade (LogExactlyOnce.java:warn(39)) - Directly referencing AWS SDK V1 credential provider com.amazonaws.auth.EnvironmentVariableCredentialsProvider. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2
2022-10-05 14:09:09,734 [setup] DEBUG s3a.S3AUtils (S3AUtils.java:createAWSCredentialProvider(691)) - Credential provider class is com.amazonaws.auth.EnvironmentVariableCredentialsProvider
2022-10-05 14:09:09,734 [setup] DEBUG s3a.S3AUtils (S3AUtils.java:createAWSCredentialProvider(691)) - Credential provider class is org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider

{code}

This is because the EnvironmentVariableCredentialsProvider provider is on the default list of providers.

Everybody who is using the S3 a connector and who has not explicitly declared a new set of providers excluding this one will be seeing the error message.

Proposed:

Don't warn on this provider. Instead with the v2 move the classname can be patched to switch to a modified one.

The alternative would be to provide an s3a specific env var provider subclass of this; and while that is potentially good in future it is a bit more effort for the forthcoming 3.3.5 release.
And especially because and it will not be in previous versions people cannot explicitly switch to it in their configs and be confident it will always be there,

"
Configuration Resolver not working on env.vars through namenode/conf or spark,13486094,Resolved,Major,Not A Bug,13/Oct/22 09:57,13/Oct/22 10:09,3.3.4,"- Following [Configuration javadoc|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java]
- Using property value as 
{code:xml}
${env.ENV_VARIABLE} 
{code}
- Cluster Startup well, and property is correctly replaced

Launching a Spark Job or accessing configuration throught http://namenode/conf property value is not replaced and stile provided as value ${env.ENV_VARIABLE}
"
Cherrypick HADOOP-11245 to branch-3.3,13485481,Resolved,Major,Fixed,10/Oct/22 17:32,10/Oct/22 21:29,,
RawErasureCoderBenchmark.configure() returns negative values of data and throughput in the output,13485658,Open,Major,,11/Oct/22 12:20,,3.3.4,"The method _configure_ of class BenchData inside RawErasureCoderBenchmark.java, when provided with a negative value for its input parameter ({_}int dataSizeMB{_}) returns negative values of data and throughput instead of logging an error or throwing an exception."
Cherrypick HADOOP-11245 to branch-3.2,13485506,Resolved,Major,Fixed,10/Oct/22 22:34,11/Oct/22 15:37,,
ClassNotFoundException due to org.apache.hadoop.shaded prepended to class name,13484314,Open,Major,,03/Oct/22 19:00,,3.3.1,"I have a scenario where I am doing Dynamic Class loading . I do not have any dependencies in pom.xml.  Class is supposed to be loaded when hadoop runs. I am facing class not found exception. This is happening on Hadoop 3.x. It used to work fine on hadoop 2.9

java.io.IOException: ClassNotFoundExceptionjava.lang.ClassNotFoundException: org.apache.hadoop.shaded.com.microsoft.MyClustomClass"
Upgrade jettison to 1.5.1 to mitigate CVE-2022-40149,13484520,Resolved,Major,Duplicate,04/Oct/22 22:00,04/Oct/22 22:56,3.3.4,Upgrade jettison to 1.5.1 to mitigate CVE-2022-40149
S3A: support custom S3 and STS headers,13510487,Open,Minor,,07/Dec/22 18:29,,,"Some users have the use case where for a particular S3A filesystem, they would like to set one or more headers to a static value.

We might imagine a set of configurations properties with a common prefix, where the suffix determines the header name and the value of the property determines the value of the header.

Per-filesystem headers can be set using the per-bucket configuration."
Warn when no region is configured,13514209,Open,Minor,,16/Dec/22 11:36,,3.3.9,"The AWS Java SDK V1 allows for cross region access. This means that even if you instantiate the S3 client with US_EAST_1 (or any region different to your actual bucket's region), the SDK will figure out the region. 

 

With the upgrade to SDK V2, this is no longer supported and the region should be set explicitly. Requests with the incorrect region will fail. To prepare for this change, S3A should warn when a region is not set via fs.s3a.endpoint.region. 

 

We should warn even if fs.s3a.endpoint is set and region can be parsed from this. This is because it is recommended to let the SDK V2 figure out the endpoint to use from the region, and so S3A should discourage from setting the endpoint unless absolutely required (eg for third party stores). 

 

Ideally rename fs.s3a.endpoint.region to fs.s3a.region, but not sure if this is ok to do. "
support multiple s3a integration test runs on same bucket in parallel,13491808,Resolved,Minor,Fixed,26/Oct/22 13:25,14/Jun/24 18:49,3.3.9,"It is now possible to provide a job ID in the maven ""job.id"" property
hadoop-aws test runs to isolate paths under a the test bucket
under which all tests will be executed.

This will allow independent builds *in different source trees*
to test against the same bucket in parallel, and is designed for
CI testing.

Example:

mvn verify -Dparallel-tests -Droot.tests.enabled=false -Djob.id=1
mvn verify -Droot.tests.enabled=false -Djob.id=2

- Root tests must be be disabled to stop them cleaning up
  the test paths of other test runs.
- Do still regularly run the root tests just to force cleanup
  of the output of any interrupted test suites.  

"
ITestS3AInputStreamPerformance#testDecompressionSequential128K NPE if no CSV file available,13484433,Open,Minor,,04/Oct/22 12:23,,,"I am running the integration tests against a store with no CSV file available for scale testing. (scale enabled, csv empty).

NPE is here: https://github.com/apache/hadoop/blob/682931a6ace460d829954398eddecefeeac82b34/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AInputStreamPerformance.java#L397

We should add a call to this somewhere - not entirely sure where right now: https://github.com/apache/hadoop/blob/682931a6ace460d829954398eddecefeeac82b34/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AInputStreamPerformance.java#L166-L171

Stack trace:
{code:java}
[ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 6, Time elapsed: 43.985 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.scale.ITestS3AInputStreamPerformance
[ERROR] testDecompressionSequential128K(org.apache.hadoop.fs.s3a.scale.ITestS3AInputStreamPerformance)  Time elapsed: 1.413 s  <<< ERROR!
java.lang.NullPointerException
    at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodec(CompressionCodecFactory.java:202)
    at org.apache.hadoop.fs.s3a.scale.ITestS3AInputStreamPerformance.executeDecompression(ITestS3AInputStreamPerformance.java:397)
    at org.apache.hadoop.fs.s3a.scale.ITestS3AInputStreamPerformance.testDecompressionSequential128K(ITestS3AInputStreamPerformance.java:383)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.lang.Thread.run(Thread.java:750) {code}"
[ABFS]: Support fixed SAS token config in addition to Custom SASTokenProvider Implementation,13494225,Resolved,Minor,Fixed,02/Nov/22 06:31,07/Jun/24 13:34,3.4.0,"This PR introduces a new configuration for Fixed SAS Tokens: *""fs.azure.sas.fixed.token""*

Using this new configuration, users can configure a fixed SAS Token in the account settings files itself. Ideally, this should be used with SAS Tokens that are scoped at a container or account level (Service or Account SAS), which can be considered to be a constant for one account or container, over multiple operations.

The other method of using a SAS Token remains valid as well, where a user provides a custom implementation of the SASTokenProvider interface, using which a SAS Token are obtained.

When an Account SAS Token is configured as the fixed SAS Token, and it is used, it is ensured that operations are within the scope of the SAS Token.

The code checks for whether the fixed token and the token provider class implementation are configured. In the case of both being set, preference is given to the custom SASTokenProvider implementation. It must be noted that if such an implementation provides a SAS Token which has a lower scope than Account SAS, some filesystem and service level operations might be out of scope and may not succeed."
ITestS3APrefetchingInputStream does not skip if no CSV test file available,13485112,Resolved,Minor,Fixed,07/Oct/22 09:46,31/Oct/22 21:20,3.3.6,"We should use S3ATestUtils.getCSVTestFile(conf) to skip if the property is empty (single space).

Today, when I set _fs.s3a.scale.test.csvfile_ to empty space, all but this test that rely on the file are skipped."
[ABFS]: Error introduced when SAS Token containing '?' prefix is passed,13486895,Resolved,Minor,Fixed,18/Oct/22 18:32,28/Nov/22 11:51,3.3.5,"Error Description:
At present, SAS Tokens generated from the Azure Portal may or may not contain a ? as a prefix. SAS Tokens that contain the ? prefix will lead to an error in the driver due to a clash of query parameters. This leads to customers having to manually remove the ? prefix before passing the SAS Tokens.

Mitigation:
After receiving the SAS Token from the provider, check if any prefix ? is present or not. If present, remove it and pass the SAS Token."
Upgrade maven-shade-plugin to 3.3.0,13487157,Resolved,Minor,Fixed,19/Oct/22 19:35,20/Oct/22 17:48,3.4.0,"Maven-shade-plugin rewrites classes when moving them into {{hadoop-client}} JARs. That's true even when it doesn't actually need to modify the byte code of the classes, say for shading.

We use a tool that checks for classpath duplicates that don't have equal byte code. This tool flags classes brought in via Hadoop. The classes it flagged came on one side from 
a JAR containing relocated classes ({{hadoop-client-api}} or {{-runtime}}) and the other from the relocated JAR ({{hadoop-common}} or {{hadoop-shaded-guava}}). We checked and the byte code for the same class is indeed different between the relocated and non-relocated JARs.

This is because maven-shade-plugin, before 3.3.0, was rewriting class files even when the relocation was a ""no-op"". See MSHADE-391 and [apache/maven-shade-plugin#95|https://github.com/apache/maven-shade-plugin/pull/95].
{quote}Maven Shade internally uses [ASM's {{ClassRemapper}}|https://asm.ow2.io/javadoc/org/objectweb/asm/commons/ClassRemapper.html] and defines a custom {{Remapper}} subclass, which takes care of relocation, partially doing the work by itself and partially delegating to the ASM parent class. An ASM {{ClassReader}} reads each class file from the original JAR and *unconditionally* writes it into a {{{}ClassWriter{}}}, plugging in the transformer.

This transformation, even if not a single relocation (package name mapping) takes place, often leads to binary differences between original class and transformed class, because constant pool or stack map frames have been adjusted, not changing the functionality of the class, but making it look like something changed when comparing class files before and after the relocation process.
{quote}
Upgrading to maven-shade-plugin 3.3.0 fixes the unnecessary rewrite of classes."
RPC Client performance improvement,13503733,Open,Minor,,19/Nov/22 07:03,,,"  The current implementation copies the rpcRequest and header to a ByteArrayOutputStream in order to calculate the total length of the sent request, and then writes it to the socket buffer.

  But if the rpc engine is ProtobufRpcEngine2, we can pre-calculate the request size, and then send the request directly to the socket buffer, reducing a memory copy. And avoid allocating 1024 bytes of ResponseBuffer each time a request is sent."
Propose a mechanism to free the direct memory occupied by RPC Connections,13504750,Open,Minor,,21/Nov/22 05:22,,,"  In the RPC Client, a thread called RpcRequestSender is responsible for writing the connection request to the socket. Every time a request is sent, a direct memory is applied for in sun.nio.ch.IOUtil#write() and cached.

  If Connection and RpcRequestSender are promoted to the old generation, they will not be recycled when full gc is not performed, resulting in the DirectByteBuffer cached in sun.nio.ch.Util not being recycled. When the memory occupied by DirectByteBuffer is too large, the jvm process may not have the opportunity to do full gc and is killed.

  Unfortunately, there is no easy way to free these DirectByteBuffers. Perhaps, we can manually free these DirectByteBuffers by the following methods when the Connection is closed.
{code}
private void freeDirectBuffer() {
  try {
    DirectBuffer buffer = (DirectBuffer) Util.getTemporaryDirectBuffer(1);
    buffer.cleaner().clean();
  } catch (Throwable t) {
    LOG.error(""free direct memory error, connectionId: "" + remoteId, t);
  }
}{code}"
Magic Committer optional clean up ,13513138,Open,Minor,,12/Dec/22 08:53,,3.3.3,"It seems that deleting the `__magic` folder, depending on the number of tasks/partitions used on a given spark job, can take really long time. I'm having the following behavior on a given Spark job (processing ~30TB, with ~420k tasks) using the magic committer:
{code:java}
2022-12-10T21:25:19.629Z pool-3-thread-32 INFO MagicS3GuardCommitter: Starting: Deleting magic directory s3a://my-bucket/random_hash/__magic
2022-12-10T21:52:03.250Z pool-3-thread-32 INFO MagicS3GuardCommitter: Deleting magic directory s3a://my-bucket/random_hash/__magic: duration 26:43.620s {code}
I don't see a way out of it since the deletion of s3 objects needs to list all objects under a prefix and this is what may be taking too much time. Could we somehow make this cleanup optional? (the idea would be to delegate it through s3 lifecycle policies in order to not create this overhead on the commit phase)."
Special characters handling in Azure file system is different from others,13514219,Open,Minor,,16/Dec/22 12:55,,3.2.0,"Special characters handling in [AzureBlobFileSystem|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java] looks different from other file systems. e.g. GoogleHadoopFileSystem.

For instance, FileSystem.open with "".../part=a%25percent/test.parquet"" works fine in other file systems, but ABFS requires "".../part=a%percent/test.parquet"" (%25 → %) because the path is URL encoded by AbfsClient#createRequestUrl internally. Can we change the behavior? Or can I request to add javadoc to explain the behavior if this is the expected behavior? "
[ABFS]: Partial Read should add to throttling metric,13487234,Open,Minor,,20/Oct/22 09:29,,3.3.4,"Error Description:
For partial read (due to account backend throttling), the ABFS driver retry but doesn't add up in the throttling metrics.
In case of partial read with connection-reset exception, ABFS driver retry for the full request and doesn't add up in throttling metrics.

Mitigation:
In case of partial read, Abfs Driver should retry for the remaining bytes and it should be added in throttling metrics."
Check if config value is not empty string in AbfsConfiguration.getMandatoryPasswordString(),13507745,Open,Minor,,01/Dec/22 06:22,,3.3.4,"The method `getMandatoryPasswordString` is called in `AbfsConfiguration.getTokenProvider()' to check if following configs are non-null (diff keys applicable for different implementation of AccessTokenProvider):

1. fs.azure.account.oauth2.client.endpoint: in ClientCredsTokenProvider
2. fs.azure.account.oauth2.client.id: in ClientCredsTokenProvider, MsiTokenProvider, RefreshTokenBasedTokenProvider
3. fs.azure.account.oauth2.client.secret: in ClientCredsTokenProvider
4. fs.azure.account.oauth2.client.endpoint: in UserPasswordTokenProvider
5. fs.azure.account.oauth2.user.name: in UserPasswordTokenProvider
6. fs.azure.account.oauth2.user.password: in  UserPasswordTokenProvider
7. fs.azure.account.oauth2.msi.tenant: in MsiTokenProvider
8. fs.azure.account.oauth2.refresh.token: in RefreshTokenBasedTokenProvider

Right now, this method checks if its non-null and not non-empty. This task needs to add check on non-empty config values."
"DataNode's internal infoserver redirects with http scheme, not https when https enabled.",13515410,Open,Minor,,24/Dec/22 09:28,,3.3.4,"After HADOOP-16314, WebServlet.java was added. On WebServlet#doGet, it redirects '/' to '/index.html'. However, if a client connects to DataNode with https scheme, it fails to connect because it responds 302 with Location header which has http scheme.

(Hostname is modified.)

{code:java}
$ curl https://dn1.example.com:50475/ -v 2>&1 | grep Location
< Location: http://dn1.example.com:50475/index.html
{code}

I can't ensure that which solution is the best among:

- Use DefaultServlet instead of WebServlet. DataNode can answer with index.html when accessed in '/'.
- According to ""dfs.http.policy"" in hdfs-site.xml, run internal infoserver as https or http server each.
- Make redirection on URLDispatcher.java"
RPC Client Improvement,13505212,Resolved,Minor,Not A Problem,22/Nov/22 05:09,23/Nov/22 12:34,,
Upgrade jackson-databind to a version with fixes for CVE-2022-42003 and CVE-2022-42004,13502654,Resolved,Minor,Duplicate,15/Nov/22 08:35,15/Nov/22 12:00,3.3.4,"|CVE-2022-42003|
|CVE-2022-42004|

These HIGH severity CVEs are reported against hadoop-client-runtime jars of hadoop 3.3.4. These are from Twistlock security scans"
Remove usage of System.out.println from Hadoop Codebase ,13497432,Open,Minor,,06/Nov/22 05:33,,,"In Hadoop codebase , there exists usage of System.out.println used by developers during development . 

This PR tries to remove those as a part of technical debt"
Update command usage in FileSystemShell.md,13503272,Resolved,Trivial,Fixed,17/Nov/22 23:27,21/Nov/22 06:57,3.3.4,Fixt typos in FileSystemShell.md
 Remove useless variable bindAddr from Client#setupConnection,13486072,Open,Trivial,,13/Oct/22 08:20,,3.4.0,The variable bindAddr is always null in org.apache.hadoop.ipc.Client#setupConnection
TestDecayRpcScheduler.testPriority is a flaky test,13492577,Open,Trivial,,27/Oct/22 19:08,,," 
{code:java}
org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority{code}
This is a flaky test, it can pass maven-test while showing non-deterministic behavior under [NonDex|[https://github.com/TestingResearchIllinois/NonDex]] and thus failed.

The test shows below:
{code:java}
java.lang.AssertionError: Get expected JMX of CallVolumeSummary before decay
    at org.junit.Assert.fail(Assert.java:89)
    at org.junit.Assert.assertTrue(Assert.java:42)
    at org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority(TestDecayRpcScheduler.java:295){code}
*Steps to reproduce the failure:*
 # install [NonDex|https://github.com/TestingResearchIllinois/NonDex]
 # run the following command in hadoop: 
{code:java}
MODULE=hadoop-common-project/hadoop-common

TEST=org.apache.hadoop.ipc.TestDecayRpcScheduler#testPriority   

mvn install -pl $MODULE -am -DskipTests 

mvn -pl $MODULE edu.illinois:nondex-maven-plugin:1.1.2:nondex -Dtest=$TEST{code}

      3. the result will be saved under the package folder in .nondex

 "
Misleading AWS SDK S3 timeout configuration comment,13510782,Resolved,Trivial,Fixed,08/Dec/22 13:10,08/Dec/22 15:09,3.3.4,"[https://github.com/apache/hadoop/blob/branch-3.3.5/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java#L254]
{code:java}
// seconds until we give up trying to establish a connection to s3
  public static final String ESTABLISH_TIMEOUT =
      ""fs.s3a.connection.establish.timeout"";
  public static final int DEFAULT_ESTABLISH_TIMEOUT = 50000;

  // seconds until we give up on a connection to s3
  public static final String SOCKET_TIMEOUT = ""fs.s3a.connection.timeout"";
  public static final int DEFAULT_SOCKET_TIMEOUT = 200000;{code}
 
{code:java}
public static void initConnectionSettings(Configuration conf,
      ClientConfiguration awsConf) throws IOException {
    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,
        DEFAULT_MAXIMUM_CONNECTIONS, 1));
    initProtocolSettings(conf, awsConf);
    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,
        DEFAULT_MAX_ERROR_RETRIES, 0));
    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,
        DEFAULT_ESTABLISH_TIMEOUT, 0));
    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,
        DEFAULT_SOCKET_TIMEOUT, 0));
    int sockSendBuffer = intOption(conf, SOCKET_SEND_BUFFER,
{code}
AWS SDK S3 client actually consumes timeout in milliseconds
[https://github.com/aws/aws-sdk-java/blob/1.11.600/aws-java-sdk-core/src/main/java/com/amazonaws/ClientConfiguration.java#L185]"
when exporting table from mysql to hive using Sqoop it gets stuck,13410470,Resolved,Blocker,Not A Bug,07/Nov/21 12:24,19/Aug/24 10:52,,when exporting table from mysql to hive it gets stuck without giving any error and also when checked in hive the table is not present there as well
Apache Hadoop support for log4j >= 2.17,13418660,Resolved,Blocker,Duplicate,20/Dec/21 16:41,22/Dec/21 03:00,2.10.1,"Federal Agencies are being given [CISA|https://www.cisa.gov/] directives requiring all  agencies to upgrade log4j 1.x applications to versions supporting log4j version 2.16.0 or higher (as of last Friday) or remove the jar files from our machines.

1.x versions of log4j are EOL, are vulnerable to multiple existing CVEs (9.8 Critical severity RCE<[https://nvd.nist.gov/vuln/detail/CVE-2019-17571]> and 8.1 High severity RCE<[https://nvd.nist.gov/vuln/detail/CVE-2021-4104]>), and due to increased scrutiny have already had a new CVE reported this week ([https://nvd.nist.gov/vuln/detail/CVE-2021-4104]<(https:/nvd.nist.gov/vuln/detail/CVE-2021-4104>).

The CISA guidance will continue to grow and improve overtime, and as of Friday 12/17/2021 CISA stated that log4j needs to be upgraded to 2.16.0 or higher.

I'm afraid Apache's statement <https://hadoop.apache.org/news/2021-12-17-log4jshell.html> will not meet the federal requirement. Please consider this an urgent request to release updated versions of Hadoop 2.x / 3.x which support log4j 2.17 or higher. Patches or workarounds would be helpful in the short term."
Document Hadoop's stance on the log4jshell vulnerability,13418249,Resolved,Blocker,Done,17/Dec/21 13:50,20/Dec/21 07:34,,"As of today, Hadoop relies on log4j-1, not log4j2. It is understood that the log4jshell vulnerability (CVE-2021-44228) does not impact log4j-1. Given the widespread attention to the incidence, we should make it clear that Hadoop is not susceptible to the attack."
Disable JIRA plugin for YETUS on Hadoop,13409722,Resolved,Critical,Fixed,02/Nov/21 18:05,09/Nov/21 16:36,3.3.5,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
Disable JIRA plugin for YETUS on Hadoop,13410294,Resolved,Critical,Duplicate,05/Nov/21 16:32,23/May/22 10:43,2.10.2,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
Disable JIRA plugin for YETUS on Hadoop,13410311,Resolved,Critical,Fixed,05/Nov/21 18:22,12/Nov/21 14:27,3.2.3,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
Disable JIRA plugin for YETUS on Hadoop,13409720,Resolved,Critical,Abandoned,02/Nov/21 17:46,12/Nov/21 15:00,3.3.1,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
Disable JIRA plugin for YETUS on Hadoop,13409721,Resolved,Critical,Abandoned,02/Nov/21 17:57,12/Nov/21 15:00,3.3.0,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
Disable JIRA plugin for YETUS on Hadoop,13409131,Resolved,Critical,Fixed,29/Oct/21 13:57,31/Oct/21 17:22,3.4.0,"I’ve been noticing an issue with Jenkins CI where a file jira-json goes missing all of a sudden – jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)
{code}
[2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588
[2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from
[2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588
[2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021
[2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021
[2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory)
[2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"".
[2021-10-27T17:52:59.814Z]
{code}
This causes the pipeline run to fail. I’ve seen this in my multiple attempts to re-run the CI on my PR –
 # After 45 minutes – [jenkins / hadoop-multibranch / PR-3588 / #1 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/1/pipeline/]
 # After 1 minute – [jenkins / hadoop-multibranch / PR-3588 / #2 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/2/pipeline/]
 # After 17 minutes – [jenkins / hadoop-multibranch / PR-3588 / #3 (apache.org)|https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3588/3/pipeline/]

The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the *jira* plugin to fix this issue."
ABFS: Enable config control for default connection timeout ,13411721,Resolved,Major,Fixed,15/Nov/21 07:55,16/May/24 15:46,3.3.1,"ABFS driver has a default connection timeout and read timeout value of 30 secs. For jobs that are time sensitive, preference would be quick failure and have shorter HTTP connection and read timeout. 

This Jira is created enable config control over the default connection and read timeout. 

New config name:

fs.azure.http.connection.timeout

fs.azure.http.read.timeout"
ABFS: add cloud trash policy with per-schema policy selection,13412556,In Progress,Major,,18/Nov/21 14:37,,3.3.2,"Add a custom TrashPolicy for azure which

* is patched in to the config automatically if ""fs.trash.classname"" is unset
* checks the source for existing at all before trying anything
* considers itself enabled even if the trash interval is 0
* if interval is 0, just does a delete (issue: what about 90s timeouts)
* downgrades exceptions/failures during rename. If anything is raised and the source dir isn't there, all is good.
"
ABFS:Support for secondary accounts on ABFS Driver for SharedKey Auth,13415007,Open,Major,,02/Dec/21 10:08,,3.3.2,"Support for secondary accounts on ABFS Driver for SharedKey Auth.

PR Link :- [Hadoop-18032: ABFS: Support for secondary accounts on ABFS Driver for SharedKey Auth by anmolanmol1234 · Pull Request #3901 · apache/hadoop (github.com)|https://github.com/apache/hadoop/pull/3901]"
ABFS: Enable config controlled ETag check for Rename idempotency,13411867,Resolved,Major,Fixed,15/Nov/21 17:39,06/Apr/23 14:55,3.3.2,"To support recovery of comms failure during rename, the abfs client fetches the etag of the source file, and when recovering from a failure uses this tag to determine whether the rename succeeded *before the failure happened*

The relevant configuration option is {{fs.azure.enable.rename.resilience}}; default value is: {{true}}

# This works for files, but not directories
# this adds the overhead of a HEAD request before each rename.
# the option can be disabled by setting ""fs.azure.enable.rename.resilience"" to false

Note: the manifest committer collects etags during task commit and supplies them to the abfs client for the rename, which avoids the need for a HEAD call. "
CallerContext should not include some characters,13412699,Resolved,Major,Fixed,19/Nov/21 05:39,25/Nov/21 05:06,3.3.5,
Bump netty to the latest 4.1.68,13405369,Resolved,Major,Fixed,07/Oct/21 10:00,08/Oct/21 01:51,3.2.3,"Netty 4.1.68 fixes the following vulnerabilities.
 * Bzip2Decoder doesn't allow setting size restrictions for decompressed data (#CVE-2021-37136)
 * SnappyFrameDecoder doesn't restrict chunk length any may buffer skippable chunks in an unnecessary way (#CVE-2021-37137)

For more details: [https://netty.io/news/2021/09/09/4-1-68-Final.html]"
unguava: remove Preconditions from hadoop-tools modules,13412835,Resolved,Major,Fixed,19/Nov/21 18:51,23/Nov/21 04:40,3.4.0,Replace guava Preconditions by internal implementations that rely on java8+ APIs in the hadoop.util for all modules in hadoop-tools.
unguava: remove Preconditions from hdfs-projects module,13407345,Resolved,Major,Fixed,19/Oct/21 17:14,25/Oct/21 14:08,3.4.0,"Replace guava Preconditions by internal implementations that rely on java8+ APIs in the hadoop.util for all modules in hadoop-hdfs-project

"
Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-common-project modules,13404930,Resolved,Major,Fixed,05/Oct/21 08:06,07/Oct/21 02:43,2.10.2,
Fix typo: validateEncrytionSecrets -> validateEncryptionSecrets,13419365,Resolved,Major,Fixed,24/Dec/21 13:49,27/Dec/21 08:59,3.4.0,"Fixing typo validateEncrytionSecrets to validateEncryptionSecrets in 
{code:java}
hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractTestS3AEncryption.java{code}"
Stale record should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson,13410595,Resolved,Major,Fixed,08/Nov/21 11:29,25/Nov/21 02:22,3.4.0,"As [HADOOP-16947|https://issues.apache.org/jira/browse/HADOOP-16947] problem with description, 
Stale SumAndCount also should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson. 
Ensure the DataNode JMX get SendPacketDownstreamAvgInfo Metrics is accurate"
No-op implementation of setWriteChecksum and setVerifyChecksum in ViewFileSystem,13410758,Resolved,Major,Fixed,09/Nov/21 06:38,17/Nov/21 06:53,2.10.2,"Currently setVerifyChecksum and setWriteChecksum initializes all target file systems which causes delay in hadoop shell copy commands such as get, put, copyFromLocal etc.

This also eventually causes OOM."
Hadoop - Upgrade to JQuery 3.6.0,13416935,Resolved,Major,Fixed,13/Dec/21 03:27,12/Jan/22 03:41,3.3.4,"jQuery 3.6.0 has been released few months ago - [http://blog.jquery.com/2021/03/02/jquery-3-6-0-released/  |http://blog.jquery.com/2021/03/02/jquery-3-6-0-released/,]

We can upgrade jquery-3.5.1.min.js to jquery-3.6.0.min.js in hadoop project. "
DistCp: Filter duplicates in the source paths,13419330,Resolved,Major,Fixed,24/Dec/21 08:58,05/Jan/22 18:29,3.3.2,"Add a basic filtering to remove the exact duplicate paths exposed for copying.

In case two same srcPath say /tmp/file1 is passed in the list twice. DistCp fails with DuplicateFileException, post building the listing.

Would be better if we do a basic filtering of duplicate paths. "
Fix jetty version in LICENSE-binary,13416528,Resolved,Major,Fixed,10/Dec/21 10:39,13/Dec/21 01:48,3.4.0,"We upgraded jetty version in https://issues.apache.org/jira/browse/HADOOP-18001, we also need to modify jetty version in LICENSE-binary."
Update to Jetty 9.4.44,13411022,Resolved,Major,Fixed,10/Nov/21 09:28,09/Dec/21 07:47,3.4.0,
Bump mina-core from 2.0.16 to 2.1.5 in /hadoop-project ,13415376,Resolved,Major,Fixed,04/Dec/21 06:16,08/Dec/21 18:40,3.4.0,Raised from github depandot 
Allow cp command to run with multi threads.,13413752,Resolved,Major,Fixed,25/Nov/21 03:53,29/Nov/21 15:48,3.2.3,Allow _*hadoop fs -cp*_ command to  run with multi-thread to improve copy speed.
Async Profiler endpoint for Hadoop daemons,13419041,Resolved,Major,Fixed,22/Dec/21 15:10,06/Jan/22 09:57,3.4.0,"Async profiler ([https://github.com/jvm-profiling-tools/async-profiler]) is a low overhead sampling profiler for Java that does not suffer from Safepoint bias problem. It features HotSpot-specific APIs to collect stack traces and to track memory allocations. The profiler works with OpenJDK, Oracle JDK and other Java runtimes based on the HotSpot JVM.

Async profiler can also profile heap allocations, lock contention, and HW performance counters in addition to CPU.

We have an httpserver based servlet stack hence we can use HIVE-20202 as an implementation template to provide async profiler as servlet for Hadoop daemons. Ideally we achieve these requirements:
 * Retrieve flamegraph SVG generated from latest profile trace.
 * Online enable and disable of profiling activity. (async-profiler does not do instrumentation based profiling so this should not cause the code gen related perf problems of that other approach and can be safely toggled on and off while under production load.)
 * CPU profiling.
 * ALLOCATION profiling."
Provide replacement for deprecated APIs of commons-io IOUtils,13404700,Resolved,Major,Fixed,03/Oct/21 18:28,07/Oct/21 04:49,3.3.2,Replace deprecated API usage of commons-io IOUtils after we have upgraded commons-io as part of HADOOP-17683.
Replace all default Charset usage with UTF-8,13405389,Resolved,Major,Fixed,07/Oct/21 12:37,14/Oct/21 04:08,3.3.5,"As discussed on PR#3515, creating this sub-task to replace all default charset with UTF-8 as default charset has some potential problems (e.g. HADOOP-11379, HADOOP-11389).

FYI [~aajisaka]"
Use maven.test.failure.ignore instead of ignoreTestFailure,13416174,Resolved,Major,Fixed,09/Dec/21 00:43,09/Dec/21 16:55,2.10.2,"In HADOOP-16596, ""ignoreTestFailure"" variable was introduced to ignore unit test failure, however, Maven property ""maven.test.failure.ignore"" can be used instead and it can simplify the pom.xml."
Disable TestDynamometerInfra,13416937,Resolved,Major,Fixed,13/Dec/21 04:29,28/Dec/21 04:24,3.3.2,This test is broken and there is no fix provided for a long time. Let's disable the test to reduce the noise in the daily qbt job.
Support Apple Silicon in start-build-env.sh,13418449,Resolved,Major,Fixed,19/Dec/21 15:29,23/Dec/21 09:14,3.3.2,"start-build-env.sh uses Dockerfile for x86 in M1 Mac, and the Dockerfile sets wrong JAVA_HOME. Dockerfile_aarch64 should be used instead."
Allow get command to run with multi threads.,13410740,Resolved,Major,Fixed,09/Nov/21 03:22,22/Nov/21 16:17,3.3.1,"CopyFromLocal/Put is enabled to run with multi-thread with HDFS-11786 and HADOOP-14698， and make put dirs or multiple files faster.
So, It's necessary to enable get and copyToLocal command run with multi-thread."
Exclude IBM Java security classes from being shaded/relocated,13407374,Resolved,Major,Fixed,19/Oct/21 19:39,20/Oct/21 05:32,3.3.1,"IBM Java classes are shaded in Hadoop libraries, e.g. hadoop-client-api. When loaded by Spark, UserGroupInformation has exception:
{noformat}
org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: unable to find LoginModule class: org.apache.hadoop.shaded.com.ibm.security.auth.module.JAASLoginModule
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2609)
	at org.apache.spark.util.Utils$$$Lambda$1388/0x00000000338e9c30.apply(Unknown Source)
	at scala.Option.getOrElse(Option.scala:189)
{noformat}
When I manually compile UserGroupInformation.java without maven (aka relocation) and inject the class files into hadoop-client-api jar; it works.

 "
Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-cloud-storage-project and hadoop-mapreduce-project modules,13405670,Resolved,Major,Fixed,08/Oct/21 18:45,11/Oct/21 07:23,3.4.0,
Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-hdfs-project modules,13405397,Resolved,Major,Fixed,07/Oct/21 13:03,11/Oct/21 06:34,3.4.0,
Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-tools modules,13405997,Resolved,Major,Fixed,11/Oct/21 17:59,14/Oct/21 09:02,3.4.0,
Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-yarn-project modules,13405998,Resolved,Major,Fixed,11/Oct/21 17:59,14/Oct/21 09:03,3.4.0,
Keep restrict-imports-enforcer-rule for Guava VisibleForTesting in hadoop-main pom,13406625,Resolved,Major,Fixed,14/Oct/21 19:16,21/Oct/21 07:58,3.4.0,
Migrate checkstyle module illegalimport to maven enforcer banned-illegal-imports,13406637,Resolved,Major,Fixed,14/Oct/21 19:53,28/Oct/21 06:58,3.4.0,"As discussed on PR [3503|https://github.com/apache/hadoop/pull/3503], we should migrate existing imports provided in IllegalImport tag in checkstyle.xml to maven-enforcer-plugin's banned-illegal-imports enforcer rule so that build never succeeds in the presence of any of the illegal imports."
abfs rename idempotency broken -remove recovery,13411090,Resolved,Major,Fixed,10/Nov/21 15:09,16/Nov/21 11:18,3.3.2,"rename idempotency logic of HADOOP-17105   is broken as modtimes aren't uodated on rename.

remove, with the changes from the PR of HADOOP-17981.
also fix delete recovery test after HADOOP-17934"
maven-enforcer-plugin's execution of banned-illegal-imports gets overridden in child poms,13411400,Resolved,Major,Fixed,12/Nov/21 05:18,15/Nov/21 13:58,3.4.0,"When we specify any maven plugin with execution tag in the parent as well as child modules, child module plugin overrides parent plugin. For instance, when {{banned-illegal-imports}} is applied for any child module with only one banned import (let’s say {{{}Preconditions{}}}), then only that banned import is covered by that child module and all imports defined in parent module (e.g Sets, Lists etc) are overridden and they are no longer applied.
After this [commit|https://github.com/apache/hadoop/commit/62c86eaa0e539a4307ca794e0fcd502a77ebceb8], hadoop-hdfs module will not complain about {{Sets}} even if i import it from guava banned imports but on the other hand, hadoop-yarn module doesn’t have any child level {{banned-illegal-imports}} defined so yarn modules will fail if {{Sets}} guava import is used.
So going forward, it would be good to replace guava imports with Hadoop’s own imports module-by-module and only at the end, we should add new entry to parent pom {{banned-illegal-imports}} list."
unguava: remove Preconditions from hadoop-yarn-project modules,13412829,Resolved,Major,Fixed,19/Nov/21 18:21,23/Nov/21 04:41,3.4.0,Replace guava Preconditions by internal implementations that rely on java8+ APIs in the hadoop.util for all modules in hadoop-yarn-project.
Add restrict-imports-enforcer-rule for Guava Preconditions in hadoop-main pom,13413374,Resolved,Major,Fixed,23/Nov/21 11:41,29/Nov/21 08:39,3.4.0,Add restrict-imports-enforcer-rule for Guava Preconditions in hadoop-main pom to restrict any new import in future. Remove any remaining usages of Guava Preconditions from the codebase.
Upgrade HBase version to 1.7.1 for hbase1 profile,13413831,Resolved,Major,Fixed,25/Nov/21 11:33,02/Dec/21 03:09,3.4.0,
Include static imports in the maven plugin rules,13414264,Resolved,Major,Fixed,29/Nov/21 10:00,01/Dec/21 07:49,3.4.0,Maven enforcer plugin to ban illegal imports require explicit mention of static imports in order to evaluate whether any publicly accessible static entities from the banned classes are directly imported by Hadoop code.
Upgrade hbase2 version and fix TestTimelineWriterHBaseDown,13416081,Resolved,Major,Fixed,08/Dec/21 14:50,13/Dec/21 05:04,3.4.0,"As mentioned on the parent Jira, we can't upgrade hbase2 profile version beyond 2.2.4 until we either have hbase 2 artifacts available that are built with hadoop 3 profile by default or hbase 3 is rolled out (hbase 3 is compatible with hadoop 3 versions only).

Let's upgrade hbase2 profile version to 2.2.4 as part of this Jira and also fix TestTimelineWriterHBaseDown to create connection only after mini cluster is up."
Use mina-core 2.0.22 to fix LDAP unit test failures,13416929,Resolved,Major,Fixed,13/Dec/21 01:57,13/Dec/21 08:49,3.4.0,"TestMultiSchemeAuthenticationHandler fails on trunk
{code:java}
[INFO] Running org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler
[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.957 s <<< FAILURE! - in org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler
[ERROR] testRequestWithLdapAuthorization(org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler)  Time elapsed: 1.663 s  <<< ERROR!
org.apache.hadoop.security.authentication.client.AuthenticationException: Error validating LDAP user
	at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateWithoutTlsExtension(LdapAuthenticationHandler.java:310)
	at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateUser(LdapAuthenticationHandler.java:240)
	at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticate(LdapAuthenticationHandler.java:202)
	at org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler.authenticate(MultiSchemeAuthenticationHandler.java:197)
	at org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler.testRequestWithLdapAuthorization(TestMultiSchemeAuthenticationHandler.java:161)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.naming.NamingException: LDAP connection has been closed
	at com.sun.jndi.ldap.LdapRequest.getReplyBer(LdapRequest.java:133)
	at com.sun.jndi.ldap.Connection.readReply(Connection.java:469)
	at com.sun.jndi.ldap.LdapClient.ldapBind(LdapClient.java:365)
	at com.sun.jndi.ldap.LdapClient.authenticate(LdapClient.java:214)
	at com.sun.jndi.ldap.LdapCtx.connect(LdapCtx.java:2897)
	at com.sun.jndi.ldap.LdapCtx.<init>(LdapCtx.java:347)
	at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxFromUrl(LdapCtxFactory.java:225)
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURL(LdapCtxFactory.java:189)
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURLs(LdapCtxFactory.java:243)
	at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxInstance(LdapCtxFactory.java:154)
	at com.sun.jndi.ldap.LdapCtxFactory.getInitialContext(LdapCtxFactory.java:84)
	at javax.naming.spi.NamingManager.getInitialContext(NamingManager.java:695)
	at javax.naming.InitialContext.getDefaultInitCtx(InitialContext.java:313)
	at javax.naming.InitialContext.init(InitialContext.java:244)
	at javax.naming.InitialContext.<init>(InitialContext.java:216)
	at javax.naming.directory.InitialDirContext.<init>(InitialDirContext.java:101)
	at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateWithoutTlsExtension(LdapAuthenticationHandler.java:305)
	... 16 more {code}
 "
High performance S3A input stream with prefetching & caching,13414345,Open,Major,,29/Nov/21 16:12,,,"I work for Pinterest. I developed a technique for vastly improving read throughput when reading from the S3 file system. It not only helps the sequential read case (like reading a SequenceFile) but also significantly improves read throughput of a random access case (like reading Parquet). This technique has been very useful in significantly improving efficiency of the data processing jobs at Pinterest. 
 
I would like to contribute that feature to Apache Hadoop. More details on this technique are available in this blog I wrote recently:
[https://medium.com/pinterest-engineering/improving-efficiency-and-reducing-runtime-using-s3-read-optimization-b31da4b60fa0]
 "
Upgrade fasterxml Jackson to 2.13.0,13415301,Resolved,Major,Fixed,03/Dec/21 14:54,13/Dec/21 04:53,,"Spark 3.2.0 depends on Jackson 2.12.3. Let's upgrade to 2.12.5 (2.12.x latest as of now) or upper.

h2. this has been reverted.

we had to revert this as it broke tez."
Unable to load AWS credentials from any provider in the chain,13418894,Resolved,Major,Invalid,21/Dec/21 19:29,24/Dec/21 05:26,3.3.1,"Hello everybody, please help with this issue. I have a job running with spark over kubernetes (AWS EKS) and I get this error:
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling o83.csv.
: com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain
    at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297) {code}"
Spark application stuck at ACCEPTED state (unset port issue),13408754,Resolved,Major,Invalid,27/Oct/21 17:40,28/Oct/21 11:19,3.2.2,"Hello guys! 

 

I am using Hadoop 3.3.2 to set up a cluster of 2 nodes. I was able to start manually both hadoop (through hdfs namenode -regular & hdfs datanode -regular one command on each machine) and yarn (yarn resourcemanager (master) yarn nodemanager (on the slave)) But when i issue a spark-submit command to run my application it gets stuck in the ACCEPTED STATUS and the log of the slave machine shows the following error : 

 

 

 
{noformat}
2021-10-26 19:51:40,359 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1914cad9{/executors/json,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,359 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,360 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1778f2da{/executors/threadDump,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,361 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,362 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22a2a185{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,362 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,383 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74a801ad{/static,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,384 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,385 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bcbe54{/,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,386 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,390 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19646f00{/api,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,390 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,391 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f7ec9ca{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,391 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,394 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33a1fb05{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-10-26 19:51:40,396 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://slaveVM1:64888
2021-10-26 19:51:40,486 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
2021-10-26 19:51:40,664 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64902.
2021-10-26 19:51:40,664 INFO netty.NettyBlockTransferService: Server created on slaveVM1:64902
2021-10-26 19:51:40,666 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-10-26 19:51:40,679 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, slaveVM1, 64902, None)
2021-10-26 19:51:40,685 INFO storage.BlockManagerMasterEndpoint: Registering block manager slaveVM1:64902 with 366.3 MiB RAM, BlockManagerId(driver, slaveVM1, 64902, None)
2021-10-26 19:51:40,688 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, slaveVM1, 64902, None)
2021-10-26 19:51:40,689 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, slaveVM1, 64902, None)
2021-10-26 19:51:40,925 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2021-10-26 19:51:40,926 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@97b0a9c{/metrics/json,null,AVAILABLE,@Spark}
2021-10-26 19:51:41,029 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030
2021-10-26 19:51:41,096 INFO yarn.YarnRMClient: Registering the ApplicationMaster
2021-10-26 19:51:43,156 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:51:45,158 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:23,098 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:25,100 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:27,102 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:29,103 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:31,106 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:56:32,110 INFO retry.RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see: http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ApplicationMasterProtocolPBClientImpl.registerApplicationMaster over null after 6 failover attempts. Trying to failover after sleeping for 30360ms.
2021-10-26 19:57:04,472 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:06,473 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:08,476 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:10,478 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:12,481 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:14,481 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:16,484 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:18,488 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:20,489 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:22,490 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2021-10-26 19:57:23,492 INFO retry.RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see: http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ApplicationMasterProtocolPBClientImpl.registerApplicationMaster over null after 7 failover attempts. Trying to failover after sleeping for 38816ms.
{noformat}
 I set resourcemanager properties (datanode side) but it's like Hadoop not reading the address and is returning the default one 0.0.0.0:8030 (scheduler):

i check the Hadoop Yarn code and i find that the method returning `0.0.0.0:8030` (the resourcemanager address according to the log (""`...client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030...`"")) is actually using a default address (that of the scheduler) and not using any of my property values set in slave nor master: 
From `hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ClientRMProxy.java`

 
{code:java}
protected InetSocketAddress getRMAddress(YarnConfiguration conf,
 Class<?> protocol) throws IOException {
 if (protocol == ApplicationClientProtocol.class) {
 return conf.getSocketAddr(YarnConfiguration.RM_ADDRESS,
 YarnConfiguration.DEFAULT_RM_ADDRESS,
 YarnConfiguration.DEFAULT_RM_PORT);
 } else if (protocol == ResourceManagerAdministrationProtocol.class) {
 return conf.getSocketAddr(
 YarnConfiguration.RM_ADMIN_ADDRESS,
 YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,
 YarnConfiguration.DEFAULT_RM_ADMIN_PORT);
 } else if (protocol == ApplicationMasterProtocol.class) {
 setAMRMTokenService(conf);
 return conf.getSocketAddr(YarnConfiguration.RM_SCHEDULER_ADDRESS,
 YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS,
 YarnConfiguration.DEFAULT_RM_SCHEDULER_PORT);
 } else {
 String message = ""Unsupported protocol found when creating the proxy "" +
 ""connection to ResourceManager: "" +
 ((protocol != null) ? protocol.getClass().getName() : ""null"");
 LOG.error(message);
 throw new IllegalStateException(message);
 }
 }{code}
Any explanation ?

What configuration am i missing here, could it be related to my Hadoop version as i am setting the ""right"" config ? 

Thanks for clarifying guys !

Cheers!"
Interface EtagSource to allow FileStatus subclasses to provide etags,13408503,Resolved,Major,Fixed,26/Oct/21 16:04,29/Nov/21 16:22,3.3.1,"Various objects stores provide etags in their FileStatus implementations

Make these values accessible
* new interface {{EtagFromFileStatus}} to be implemented when provided
* filesystem.md to declare requirements of etags (constant between LIST and HEAD)...
* path capabilities for (a) etag and (b) etags consistent across rename

Add implementation for abfs, later s3a (and google gcs)

This is initially to handle recovery from certain failures in job commit against abfs, but it would allow a cloud-ready version of distcp to track etags of uploaded files, so diff properly."
RBF: Namespace usage of mount table with multi subclusters can exceed quota,13410734,Resolved,Major,Won't Fix,09/Nov/21 02:24,17/Jan/23 06:17,,"# mkdir on each subcluster /test10
 # mount table: hdfs dfsrouteradmin -add /test10 ns1,ns2 /test10
 # set quota: hdfs dfsrouteradmin -setQuota /test10 -nsQuota 4
 # touch two files under hdfs://\{fed}/test10, now the namespace usage is 4(2 directories and 2 files). 
 # refresh and put a new file, the put is successfull, and the namespace usage comes to 5.
!1636424307319.jpg!


The router checks quota without considering the increments, It is difficult to limit quota precisely without increments , but throw exception when usage >= quota will be better."
UserGroupInformation#unprotectedRelogin sets the last login time before logging in,13410670,Resolved,Major,Won't Fix,08/Nov/21 17:06,08/Jan/23 18:33,3.3.1,"UserGroupInformation#unprotectedRelogin sets the last login time before logging in. IPC#Client does reloginFromKeytab when there is a connection reset failure from AD which does logout and set the last login time to now and then tries to login. The login also fails as not able to connect to AD. Then the reattempts does not happen as kerberosMinSecondsBeforeRelogin check fails. All Client and Server operations fails with *GSS initiate failed*

{code}
2021-10-31 09:50:53,546 WARN  ha.EditLogTailer - Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: org.apache.hadoop.security.KerberosAuthException:  DestHost:destPort namenode0:8020 , LocalHost:localPort namenode1/1.2.3.4:0. Failed on local exception: org.apache.hadoop.security.KerberosAuthException: Login failure for user: nn/namenode1@EXAMPLE.COM javax.security.auth.login.LoginException: Connection reset
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:382)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:441)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:410)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:427)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:360)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1712)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:480)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:423)
Caused by: org.apache.hadoop.security.KerberosAuthException:  DestHost:destPort namenode0:8020 , LocalHost:localPort namenode1/1.2.3.4:0. Failed on local exception: org.apache.hadoop.security.KerberosAuthException: Login failure for user: nn/namenode1@EXAMPLE.COM javax.security.auth.login.LoginException: Connection reset
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1501)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.Client.call(Client.java:1353)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy21.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:150)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:367)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:364)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:514)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.security.KerberosAuthException: Login failure for user: nn/namenode1@EXAMPLE.COM javax.security.auth.login.LoginException: Connection reset
	at org.apache.hadoop.security.UserGroupInformation.unprotectedRelogin(UserGroupInformation.java:1193)
	at org.apache.hadoop.security.UserGroupInformation.relogin(UserGroupInformation.java:1159)
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1128)
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1110)
	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:734)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)
	at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813)
	at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1389)
	... 12 more
Caused by: javax.security.auth.login.LoginException: Connection reset
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:812)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:618)
	at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
	at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:1928)
	at org.apache.hadoop.security.UserGroupInformation.unprotectedRelogin(UserGroupInformation.java:1187)
	... 24 more
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at sun.security.krb5.internal.TCPClient.readFully(NetClient.java:130)
	at sun.security.krb5.internal.TCPClient.receive(NetClient.java:82)
	at sun.security.krb5.KdcComm$KdcCommunication.run(KdcComm.java:404)
	at sun.security.krb5.KdcComm$KdcCommunication.run(KdcComm.java:364)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.security.krb5.KdcComm.send(KdcComm.java:348)
	at sun.security.krb5.KdcComm.sendIfPossible(KdcComm.java:253)
	at sun.security.krb5.KdcComm.send(KdcComm.java:229)
	at sun.security.krb5.KdcComm.send(KdcComm.java:200)
	at sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:345)
	at sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:498)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:780)
	... 37 more
2021-10-31 09:50:53,576 WARN  security.UserGroupInformation - Not attempting to re-login since the last re-login was attempted less than 60 seconds before. Last Login=1635673853525
2021-10-31 09:50:53,576 WARN  security.UserGroupInformation - Not attempting to re-login since the last re-login was attempted less than 60 seconds before. Last Login=1635673853525
2021-10-31 09:50:53,576 WARN  security.UserGroupInformation - Not attempting to re-login since the last re-login was attempted less than 60 seconds before. Last Login=1635673853525
2021-10-31 09:50:56,085 WARN  security.UserGroupInformation - Not attempting to re-login since the last re-login was attempted less than 60 seconds before. Last Login=1635673853525

2021-11-02 13:28:08,750 WARN  ipc.Server - Auth failed for 10.25.35.45:37849:null (GSS initiate failed) with true cause: (GSS initiate failed)
2021-11-02 13:28:08,767 WARN  ipc.Server - Auth failed for 10.25.35.46:35919:null (GSS initiate failed) with true cause: (GSS initiate failed)

{code}"
Build arm64 (aarch64) and x86_64 image with the same Dockerfile,13415004,Open,Major,,02/Dec/21 10:01,,,"-Support building Linux arm64 (aarch64) Docker images. And bump up some dependency versions.-

-Note: This only provides a arm64 *runtime* environment for Hadoop 3. But not a full environment for compiling Hadoop 3 in arm64 yet. For the latter, gRPC may well need to be compiled from source because it hasn't started distributing Linux arm64 binaries yet.-

-The patch for branch-3.3 is ready. I developed this patch on branch-3.3.1 when I was trying to build arm64 Linux Hadoop Docker image. For trunk (3.4.0), due to HADOOP-17727, I need to post a different PR.-

Just realized we already had {{Dockerfile_aarch64}}. Will try it out.

My approach builds the Docker images for both architectures (x86_64 and aarch64) with the same {{Dockerfile}}.

We should push the built arm64 image to Docker Hub. I only see amd64 [there|https://hub.docker.com/r/apache/hadoop/tags] so I assumed we didn't have arm64 Docker image, hmm."
AuxService should not use class name as default system classes,13418371,Resolved,Major,Duplicate,18/Dec/21 12:33,18/Dec/21 12:36,3.3.1,"Following Apache Spark to configure Spark Shuffle Service as YARN AuxService,

[https://spark.apache.org/docs/3.2.0/running-on-yarn.html#running-multiple-versions-of-the-spark-shuffle-service]

 
{code:java}
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>spark_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.classpath</name>
    <value>/opt/apache/spark/yarn/*</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>  </property> {code}
 but failed with exception
{code:java}
2021-12-02 15:34:00,886 INFO util.ApplicationClassLoader: classpath: [file:/opt/apache/spark/yarn/spark-3.2.0-yarn-shuffle.jar]
2021-12-02 15:34:00,886 INFO util.ApplicationClassLoader: system classes: [org.apache.spark.network.yarn.YarnShuffleService]
2021-12-02 15:34:00,887 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices failed in state INITED
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.ClassNotFoundException: org.apache.spark.network.yarn.YarnShuffleService
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.initAuxService(AuxServices.java:482)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:761)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:109)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:327)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:109)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:494)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:962)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:1042)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.network.yarn.YarnShuffleService
        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
        at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)
        at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:165)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.createAuxServiceFromLocalClasspath(AuxServices.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.createAuxService(AuxServices.java:271)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.initAuxService(AuxServices.java:452)
        ... 10 more
{code}

An workaround is add 
{code:java}
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.system-classes</name>
    <value>not.existed.class</value>
  </property>
{code}
 "
Support etag-assisted renames in FileOutputCommitter,13408935,Resolved,Major,Won't Fix,28/Oct/21 15:24,10/Nov/21 16:00,3.4.0,"To deal with some throttling/retry issues in object stores,
pass the FileStatus entries retrieved during listing
into a private interface ResilientCommitByRename which filesystems
may implement to use extra attributes in the listing (etag, version)
to constrain and validate the operation.

Although targeting azure, GCS and others could use. no point in S3A as they shouldn't use this committer.

# And we are not going to do any changes to FileSystem as there are explicit guarantees of public use and stability.
I am not going to make that change as the hive thing that will suddenly start expecting it to work forever.
# I'm not planning to merge this in, as the manifest committer is going to include this and more (MAPREDUCE-7341)

However, I do need to get this in on a branch, so am doing this work on trunk for dev & test and for others to review"
S3AFileSystem.s3GetFileStatus() doesn't find dir markers on minio,13412852,Resolved,Major,Won't Fix,19/Nov/21 21:17,20/Jun/22 08:58,3.3.0,"Repro code:
{code:java}
val conf = new Configuration()  
conf.set(""fs.s3a.endpoint"", ""http://127.0.0.1:9000"") conf.set(""fs.s3a.path.style.access"", ""true"") 
conf.set(""fs.s3a.access.key"", ""user_access_key"") 
conf.set(""fs.s3a.secret.key"", ""password"")  

val path = new Path(""s3a://comcast-test"")  
val fs = path.getFileSystem(conf)  
fs.mkdirs(new Path(""/testdelta/_delta_log""))  
fs.getFileStatus(new Path(""/testdelta/_delta_log"")){code}
Fails with *FileNotFoundException fails* on Minio. The same code works in real S3.
It also works in Hadoop 3.2 with Minio and earlier versions.

Only fails on 3.3 and newer Hadoop branches.

The reason as discovered by [~sadikovi] is actually a more fundamental one - Minio does not have empty directories (sort of), see [https://github.com/minio/minio/issues/2423].

This works in Hadoop 3.2 because of this infamous ""Is this necessary?"" block of code
[https://github.com/apache/hadoop/blob/branch-3.2.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L2204-L2223]

that was removed in Hadoop 3.3 -
[https://github.com/apache/hadoop/blob/branch-3.3.0/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L2179]

and this causes the regression"
Provide a public wrapper of Configuration#substituteVars,13413323,Resolved,Major,Fixed,23/Nov/21 07:20,03/Dec/21 16:50,3.4.0,"YARN-10838 and YARN-10911 introduced an unfortunate change in Configuration that could potentially be backward incompatible (visibility of substituteVars). Since it is easily circumvented, my proposal is to avoid breaking changes if possible.

One found issue is that Oozie defines substituteVars as a private method.
https://github.com/apache/oozie/blob/master/core/src/main/java/org/apache/oozie/util/XConfiguration.java#L186"
Use platform specific endpoints for CI report,13411512,Open,Major,,12/Nov/21 16:04,,3.4.0,"Consider the Github comment by hadoop-yetus for different platform runs -

||Platform||hadoop-yetus report||
|Centos 7|https://github.com/apache/hadoop/pull/3563#issuecomment-947151110|
|Centos 8|https://github.com/apache/hadoop/pull/3563#issuecomment-947239385|
|Debian 10|https://github.com/apache/hadoop/pull/3563#issuecomment-947340406|
|Ubuntu Focal|https://github.com/apache/hadoop/pull/3563#issuecomment-947464004|

 
Notice that the Docker subsystem points to the same file for all the platforms -

!image-2021-11-12-21-13-18-997.png!

To illustrate the issue here, upon clicking on the link to Dockerfile in [Centos 7's hadoop-yetus report|https://github.com/apache/hadoop/pull/3563#issuecomment-947151110] will navigate to Ubuntu Focal's Dockerfile (since Ubuntu Focal wass the most recently run platform for that pipeline run).
Please note that this issue applies to all the links that contain ""out/"" that appear in the Yetus summary.

As a side note, this issue doesn't happen when the Yetus run fails. Consider https://github.com/apache/hadoop/pull/3563#issuecomment-946215556. Here the Yetus run has failed for Centos 7. The link to Dockerfile points to Centos 7's Dockerfile since Centos 7 is the last platform that was run for that pipeline.


----


To fix this issue, we need to do the following two things -
*+Archive the artifacts even when the Yetus run is successful+*
Currently, we're archiving the artifacts only when the run fails - https://github.com/apache/hadoop/blob/bccf2f3ef4c8f09f010656f9061a4e323daf132b/dev-support/Jenkinsfile#L142-L148 (Hence, we're able to navigate to the right Dockerfile when the Yetus run fails for a platform). We need to archive the artifacts even when the run succeeds so that we're able to navigate to it irrespective of success/failure of the Yetus run.

*+Expose different endpoints for each platform+*
At the end of the Yetus's pre-commit run, we see the following table -

|| Subsystem || Report/Notes ||
|  cc  | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3650/1/*artifact/out*/results-compile-cc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkUbuntu-11.0.11+9-Ubuntu-0ubuntu2.20.04.txt |
|  unit  | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3650/1/*artifact/out*/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |

We need a way to change the URL to /artifact/*centos-7*/out/, /artifact/*ubuntu-focal*/out/ and so on."
Fallback to simple auth does not work for a secondary DistributedFileSystem instance,13407954,Resolved,Major,Fixed,22/Oct/21 12:35,24/Nov/21 11:33,,"The following code snippet demonstrates what is necessary to cause a failure in connection to a non secure cluster with fallback to SIMPLE auth allowed from a secure cluster.
{code:java}
    Configuration conf = new Configuration();

    conf.setBoolean(""ipc.client.fallback-to-simple-auth-allowed"", true);
    URI fsUri = new URI(""hdfs://<nn_uri>"");

    conf.setBoolean(""fs.hdfs.impl.disable.cache"", true);
    FileSystem fs = FileSystem.get(fsUri, conf);
    FSDataInputStream src = fs.open(new Path(""/path/to/a/file""));
    FileOutputStream dst = new FileOutputStream(File.createTempFile(""foo"", ""bar""));
    IOUtils.copyBytes(src, dst, 1024);

    // The issue happens even if we re-enable cache at this point
    //conf.setBoolean(""fs.hdfs.impl.disable.cache"", false);
    // The issue does not happen when we close the first FileSystem object
    // before creating the second.
    //fs.close();
    FileSystem fs2 = FileSystem.get(fsUri, conf);
    FSDataInputStream src2 = fs2.open(new Path(""/path/to/a/file""));
    FileOutputStream dst2 = new FileOutputStream(File.createTempFile(""foo"", ""bar""));
    IOUtils.copyBytes(src2, dst2, 1024);
{code}


The problem is that when the DfsClient is created it creates an instance of AtomicBoolean, which is propagated down into the IPC layer, where the Client.Connection instance in setupIOStreams sets its value. This connection object is cached and re-used to multiplex requests against the same DataNode.

In case of creating a second DfsClient, the AtomicBoolean reference in the client is a new AtomicBoolean, but the Client.Connection instance is the same, and as it has a socket already open to the DataNode, it returns immediatelly from setupIOStreams, leaving the fallbackToSimpleAuth AtomicBoolean false as it is created in the DfsClient.
This AtomicBoolean on the other hand controls how the SaslDataTransferClient handles the connection in the above level, and with this value left on the default false, the SaslDataTransferClient of the second DfsClient will not fall back to SIMPLE authentication but will try to send a SASL handshake when connecting to the DataNode.
 
The access to the FileSystem via the second DfsClient fails with exceptions like the following one, then fails the read with a BlockMissingException like below:
{code}
WARN hdfs.DFSClient: Failed to connect to /<dn_ip>:<dn_port> for file <file> for block BP-531773307-<nn_ip>-1634685133591:blk_1073741826_1002, add to deadNodes and continue. 
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:552)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessage(DataTransferSaslUtil.java:215)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:455)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getSaslStreams(SaslDataTransferClient.java:393)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:267)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:215)
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)
	at org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:648)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2980)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:822)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:747)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:380)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:658)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:589)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:771)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:840)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)
	at DfsClientTest3.main(DfsClientTest3.java:30)
{code}

{code}
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-813026743-<nn_ip>-1495248833293:blk_1139767762_66027405 file=/path/to/file
{code}
 
The DataNode in the meantime logs the following:
{code}
ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: <dn_host>:<dn_port>:DataXceiver error processing unknown operation  src: /<client_ip>:<client_port> dst: /<dn_ip>:<dn_port>
java.io.IOException: Version Mismatch (Expected: 28, Received: -8531 )
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:70)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:222)
        at java.lang.Thread.run(Thread.java:748)
{code}

This happens only if the second client is connecting to the same DataNode as the first one did, so might seem intermittent in case the clients are reading different files, but happens always if the two client reads the same file with replication factor 1.

We ran into this issue during running HBase ExportSnapshot tool to move a snapshot from a non-secure to a secure cluster, the issue is loosely related to HBASE-12819 and HBASE-20433 and similar problems, I am linking these so that HBase team will see how this is relevant for them."
TestIPC#testIOEOnListenerAccept fails,13416967,Resolved,Major,Not A Problem,13/Dec/21 08:05,08/Feb/22 14:40,,"{code}
[ERROR] testIOEOnListenerAccept(org.apache.hadoop.ipc.TestIPC)  Time elapsed: 0.007 s  <<< FAILURE!
java.lang.AssertionError: Expected an EOFException to have been thrown
	at org.junit.Assert.fail(Assert.java:89)
	at org.apache.hadoop.ipc.TestIPC.testIOEOnListenerAccept(TestIPC.java:652)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}"
Backport HADOOP-17796 for branch-3.2,13415769,Resolved,Major,Duplicate,07/Dec/21 09:43,02/Feb/22 11:44,3.2.2,
SocketChannel is not closed when IOException happens in Server$Listener.doAccept,13413758,Reopened,Major,,25/Nov/21 05:19,,3.2.2,"This is a follow-up of HADOOP-17552.

When the symptom described in HADOOP-17552 happens, the client may time out in 2min, according to the default RPC timeout configuration specified in HADOOP-17552. Before this timeout, the client just waits, and does not know this issue happens.

However, we recently found that actually the client doesn’t need to waste this 2min, and the server’s availability can be also improved. If the IOException happens in line 1402 or 1403 or 1404, we can just close this problematic `SocketChannel` and continue to accept new socket connections. The client side can also be aware of the close socket immediately, instead of waiting 2min.

The old implementation:
{code:java}
//hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java

   public void run() {
      while (running) {
        // ...
        try {
          // ...
          while (iter.hasNext()) {
            // ...
            try {
              if (key.isValid()) {
                if (key.isAcceptable())
                  doAccept(key);                              // line 1348
              }
            } catch (IOException e) {                         // line 1350
            }
            // ...
          }
        } catch (OutOfMemoryError e) {
          // ...
        } catch (Exception e) {
          // ...
        }
      }
    } {code}
{code:java}
//hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java

    void doAccept(SelectionKey key) throws InterruptedException, IOException, 
        OutOfMemoryError {
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) {           // line 1400

        channel.configureBlocking(false);                     // line 1402
        channel.socket().setTcpNoDelay(tcpNoDelay);           // line 1403
        channel.socket().setKeepAlive(true);                  // line 1404

        Reader reader = getReader();
        Connection c = connectionManager.register(channel,
            this.listenPort, this.isOnAuxiliaryPort);
        // If the connectionManager can't take it, close the connection.
        if (c == null) {
          if (channel.isOpen()) {
            IOUtils.cleanup(null, channel);
          }
          connectionManager.droppedConnections.getAndIncrement();
          continue;
        }
        key.attach(c);  // so closeCurrentConnection can get the object
        reader.addConnection(c);
      }
    } {code}
 

We propose that the following implementation is better:
{code:java}
    void doAccept(SelectionKey key) throws InterruptedException, IOException, 
        OutOfMemoryError {
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) {           // line 1400

        try {
          channel.configureBlocking(false);                   // line 1402
          channel.socket().setTcpNoDelay(tcpNoDelay);         // line 1403
          channel.socket().setKeepAlive(true);                // line 1404
        } catch (IOException e) {
          LOG.warn(...);
          try {
            channel.socket().close();
            channel.close();
          } catch (IOException ignored) { }
          continue;
        }

        // ...
      }
    }{code}
The advantages include:
 # {*}In the old implementation{*}, the `ServerSocketChannel` was abandoned due to the single exception in this single `SocketChannel`, because the exception handler is in line 1350. {*}In the new implementation{*}, we use a try-catch to handle the exception in line 1402 or 1403 or 1404, then the `ServerSocketChannel` can continue to accept new connections, and don’t need to go back to the line 1348 in the next while loop in the run method.
 # {*}In the old implementation{*}, the client (another endpoint of this `SocketChannel`) is not aware of this issue, because the `SocketChannel` is accepted and not closed. {*}In the new implementation{*}, we close the `SocketChannel` when the IOException happens, then the client will immediately get EOF from the socket. Then the client can choose to retry or throw an exception, by the client’s discretion.

 

We have confirmed that this patch works as expected, in our local machine.

 

This code pattern was adopted by other communities. For example, in Kafka [https://github.com/apache/kafka/blob/23e9818e625976c22fe6d4297a5ab76b01f92ef6/core/src/main/scala/kafka/network/SocketServer.scala#L714-L740]:
{code:java}
   /**
   * Accept a new connection
   */
  private def accept(key: SelectionKey): Option[SocketChannel] = {
    val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]
    val socketChannel = serverSocketChannel.accept()
    try {
      connectionQuotas.inc(endPoint.listenerName, socketChannel.socket.getInetAddress, blockedPercentMeter)
      configureAcceptedSocketChannel(socketChannel)
      Some(socketChannel)
    } catch {
      case e: TooManyConnectionsException =>
        info(...)
        close(endPoint.listenerName, socketChannel)
        None
      case e: ConnectionThrottledException =>
        // ...
        None
      case e: IOException =>
        error(...)
        close(endPoint.listenerName, socketChannel)
        None
    }
  }

  /**
   * Close `channel` and decrement the connection count.
   */
  def close(listenerName: ListenerName, channel: SocketChannel): Unit = {
    if (channel != null) {
      // ...
      closeSocket(channel)
    }
  }

  protected def closeSocket(channel: SocketChannel): Unit = {
    CoreUtils.swallow(channel.socket().close(), this, Level.ERROR)
    CoreUtils.swallow(channel.close(), this, Level.ERROR)
  }
{code}"
org.apache.spark.SparkException: Task failed while writing rows S3,13405190,Resolved,Major,Cannot Reproduce,06/Oct/21 12:36,10/Jan/22 13:01,2.6.0,"I am trying to run spark job (1.6.0) which reads rows from HBASE and does some transformation and finally writes to S3 .

Some time i can notice error because of time out .

Task is able to write to S3 but at last stage it fails 

Here is the error details 

Its intermittent issue but most of the time i see this error .

 
{code:java}
Job aborted due to stage failure: Task 1074 in stage 1.0 failed 4 times, most recent failure: Lost task 1074.3 in stage 1.0 (TID 2162, abcd.ecom.bigdata.int.abcd.com, executor 18): org.apache.spark.SparkException: Task failed while writing rowsJob aborted due to stage failure: Task 1074 in stage 1.0 failed 4 times, most recent failure: Lost task 1074.3 in stage 1.0 (TID 2162, abcd.ecom.bigdata.int.abcd.com, executor 18): org.apache.spark.SparkException: Task failed while writing rows at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:417) at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:148) at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:148) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.hadoop.fs.s3a.AWSS3IOException: saving output on common/hbaseHistory/metadataSept100621/_temporary/_attempt_202110060911_0001_m_001074_3/year=2021/month=09/submitDate=2021-09-08T04%3a58%3a47Z/part-r-01074-205c8b21-7840-4985-bb0e-65ed787c337d.snappy.parquet: com.cloudera.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: 5J85XRNF10W16ZJS), S3 Extended Request ID: 4g08KHEDbFs5jueJqt9Snw7Xlmw5VeS1eCtJyAzp0fzHGinMhBntwIEhddJP7LLaS0teR3EAuOI=: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: 5J85XRNF10W16ZJS) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:143) at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:123) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) at parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:470) at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:112) at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:112) at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetRelation.scala:101) at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply$mcV$sp(WriterContainer.scala:387) at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:343) at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:343) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1278) at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:409) ... 8 more Suppressed: java.lang.NullPointerException at parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:152) at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:111) at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:112) at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetRelation.scala:101) at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$5.apply$mcV$sp(WriterContainer.scala:411) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1287) ... 9 moreCaused by: com.cloudera.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: 5J85XRNF10W16ZJS), S3 Extended Request ID: 4g08KHEDbFs5jueJqt9Snw7Xlmw5VeS1eCtJyAzp0fzHGinMhBntwIEhddJP7LLaS0teR3EAuOI= at com.cloudera.com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182) at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770) at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489) at com.cloudera.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310) at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785) at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1472) at com.cloudera.com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131) at com.cloudera.com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123) at com.cloudera.com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139) at com.cloudera.com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 more
 Driver stacktrace:
 
{code}"
RPCMetrics increases the number of handlers in processing,13418926,Resolved,Major,Fixed,22/Dec/21 04:04,31/Dec/21 08:41,2.9.2,"When using RPC, we recorded a lot of useful information, such as Queue time, Processing time. These are very helpful.
But we can't know how many handlers are actually working now (only those that handle Call), especially when the Call Queue is very high. This is also not conducive to us optimizing the cluster.
It would be very helpful if we can see the number of handlers being processed in RPCMetrics."
Backport HADOOP-17653 for branch-3.2,13415712,Open,Major,,07/Dec/21 04:29,,3.2.2,
Pin python lazy-object-proxy to 1.6.0 in Docker file as newer versions are incompatible with python2.7,13418184,Resolved,Major,Fixed,17/Dec/21 07:55,17/Dec/21 09:03,2.10.2,"Latest version of lazy-object-proxy (dependency of pylint) seems incompatible with python2.7 as per [release notes|https://pypi.org/project/lazy-object-proxy/1.7.1/]

[https://ci-hadoop.apache.org/blue/organizations/jenkins/hadoop-multibranch/detail/PR-3776/2/pipeline]

 
{code:java}
[2021-12-16T12:37:15.710Z] Collecting lazy-object-proxy (from astroid<2.0,>=1.6->pylint==1.9.2)
[2021-12-16T12:37:15.710Z]   Downloading https://files.pythonhosted.org/packages/75/93/3fc1cc28f71dd10b87a53b9d809602d7730e84cc4705a062def286232a9c/lazy-object-proxy-1.7.1.tar.gz (41kB)
[2021-12-16T12:37:16.225Z]     Complete output from command python setup.py egg_info:
[2021-12-16T12:37:16.225Z]     /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'project_urls'
[2021-12-16T12:37:16.225Z]       warnings.warn(msg)
[2021-12-16T12:37:16.225Z]     /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'
[2021-12-16T12:37:16.225Z]       warnings.warn(msg)
[2021-12-16T12:37:16.225Z]     /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'use_scm_version'
[2021-12-16T12:37:16.225Z]       warnings.warn(msg)
[2021-12-16T12:37:16.225Z]     running egg_info
[2021-12-16T12:37:16.225Z]     creating pip-egg-info/lazy_object_proxy.egg-info
[2021-12-16T12:37:16.225Z]     writing pip-egg-info/lazy_object_proxy.egg-info/PKG-INFO
[2021-12-16T12:37:16.225Z]     writing top-level names to pip-egg-info/lazy_object_proxy.egg-info/top_level.txt
[2021-12-16T12:37:16.225Z]     writing dependency_links to pip-egg-info/lazy_object_proxy.egg-info/dependency_links.txt
[2021-12-16T12:37:16.225Z]     writing manifest file 'pip-egg-info/lazy_object_proxy.egg-info/SOURCES.txt'
[2021-12-16T12:37:16.225Z]     warning: manifest_maker: standard file '-c' not found
[2021-12-16T12:37:16.225Z]     
[2021-12-16T12:37:16.225Z]     Traceback (most recent call last):
[2021-12-16T12:37:16.225Z]       File ""<string>"", line 1, in <module>
[2021-12-16T12:37:16.225Z]       File ""/tmp/pip-build-j47m88/lazy-object-proxy/setup.py"", line 146, in <module>
[2021-12-16T12:37:16.225Z]         distclass=BinaryDistribution,
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/distutils/core.py"", line 151, in setup
[2021-12-16T12:37:16.225Z]         dist.run_commands()
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands
[2021-12-16T12:37:16.225Z]         self.run_command(cmd)
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command
[2021-12-16T12:37:16.225Z]         cmd_obj.run()
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 186, in run
[2021-12-16T12:37:16.225Z]         self.find_sources()
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 209, in find_sources
[2021-12-16T12:37:16.225Z]         mm.run()
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 293, in run
[2021-12-16T12:37:16.225Z]         self.add_defaults()
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/dist-packages/setuptools/command/egg_info.py"", line 322, in add_defaults
[2021-12-16T12:37:16.225Z]         sdist.add_defaults(self)
[2021-12-16T12:37:16.225Z]       File ""/usr/lib/python2.7/dist-packages/setuptools/command/sdist.py"", line 131, in add_defaults
[2021-12-16T12:37:16.225Z]         if self.distribution.has_ext_modules():
[2021-12-16T12:37:16.225Z]       File ""/tmp/pip-build-j47m88/lazy-object-proxy/setup.py"", line 70, in has_ext_modules
[2021-12-16T12:37:16.225Z]         return super().has_ext_modules() or not os.environ.get('SETUPPY_ALLOW_PURE')
[2021-12-16T12:37:16.225Z]     TypeError: super() takes at least 1 argument (0 given)
[2021-12-16T12:37:16.225Z]     
[2021-12-16T12:37:16.225Z]     ----------------------------------------
[2021-12-16T12:37:16.225Z] [91mCommand ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-j47m88/lazy-object-proxy/
{code}"
[branch-3.3] Dockerfile_aarch64 build fails with fatal error: Python.h: No such file or directory,13417567,Resolved,Major,Fixed,14/Dec/21 22:48,15/Dec/21 01:58,,See previous discussion: https://issues.apache.org/jira/browse/HADOOP-17723?focusedCommentId=17452329&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17452329
Command to refresh auth to local rules without having to restart the cluster,13416426,Open,Major,,10/Dec/21 03:02,,,"when two kerberos cluster establish cross realm trust, we should set up hadoop.security.auth_to_local parameter in both clusters, and then restart cluster。restart namenode is very challenge thing for large cluster, so we can add admin command to refresh auth to local rules without restart cluster."
Skip unit test failures to run all the unit tests,13415551,Resolved,Major,Fixed,06/Dec/21 10:31,09/Dec/21 16:35,,"In branch-3.3 and upper, unit tests failures are ignored in HADOOP-16596. That way we can run all the unit tests in all the modules by simply modifying a file under the project root directory. Without the feature, if there is a test failure in a module (it is likely happen due to the flaky jobs), the tests in the subsequent modules are not executed. I want to introduce the feature in the other branches."
Authentication cookie will never expire by default after HADOOP-12049,13414948,Open,Major,,02/Dec/21 06:37,,3.1.0,"Whlie create auth cookie for client, AuthenticationFilter will add ""Expires""  attribute for the cookie if needed. But after https://issues.apache.org/jira/browse/HADOOP-12049, it never enter the code block by default.

 

 
{code:java}
// AuthenticationFilter

public static void createAuthCookie(HttpServletResponse resp, String token,
                                    String domain, String path, long expires,
                                    boolean isCookiePersistent,
                                    boolean isSecure) {
  //...
  //By default, isCookiePersistent = false
  if (expires >= 0 && isCookiePersistent) {
    Date date = new Date(expires);
    SimpleDateFormat df = new SimpleDateFormat(""EEE, "" +
            ""dd-MMM-yyyy HH:mm:ss zzz"");
    df.setTimeZone(TimeZone.getTimeZone(""GMT""));
    sb.append(""; Expires="").append(df.format(date));
  }

  //...
}{code}
 

 "
"ITestAzureBlobFileSystemDelete failing ""Operations has null HTTP response""",13410059,Resolved,Major,Fixed,04/Nov/21 13:37,06/Dec/21 11:16,3.3.2,"ITestAzureBlobFileSystemDelete .testDeleteIdempotency failing



looks like caused by HADOOP-17934 and not a bug in that patch, just a mockito based test case which needs fixing now a new method is called.


{code} 

java.lang.IllegalArgumentException: Operations has null HTTP response

	at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144)
	at org.apache.hadoop.fs.azurebfs.services.AbfsClient.deleteIdempotencyCheckOp(AbfsClient.java:830)
	at org.apache.hadoop.fs.azurebfs.ITestAzureBlobFileSystemDelete.testDeleteIdempotency(ITestAzureBlobFileSystemDelete.java:196)

{code}
"
Make certain methods LimitedPrivate in S3AUtils.java ,13412711,Resolved,Major,Fixed,19/Nov/21 07:02,24/Nov/21 08:03,3.4.0,"S3AUtils(Private) have certain methods which can be used in external projects to provide proxy support, since it is private we can't call them directly and have to copy the whole methods to provide the support. Making these methods LimitedPrivate so that these can be called directly with a reason."
Avoid breaking changes in Configuration,13413321,Resolved,Major,Duplicate,23/Nov/21 07:14,23/Nov/21 07:20,,"YARN-10838 and YARN-10911 introduced an unfortunate change in Configuration that could potentially be backward incompatible (visibility of substituteVars). Since it is easily circumvented, my proposal is to avoid breaking changes if possible."
Public Interface to use S3AUtils.java methods in external projects,13412710,Open,Major,,19/Nov/21 06:58,,3.4.0,"S3AUtils is marked Private, this makes it difficult to use the methods in it and the projects have to copy the whole method rather than simply calling them. Having a public Interface helper class would act as a gateway to these methods which can be directly called now."
some time delay (0.3s) for swebhdfs + kerberos + observer setting.,13411687,Resolved,Major,Duplicate,14/Nov/21 22:37,15/Nov/21 14:24,3.3.1,"Settings:

1 master namenode (A), 1 standby namenode (B), 1 observer namenode (C).

following 

[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html]

except that 

dfs.client.failover.observer.auto-msync-period.<nameservice>

is set to -1 (not auto -msync)

 

uable to do curl - - negotiate -u ':' 'https://<observer>:<port>/webhdfs/v1/...'

because it seems like due to the following issue:

https://issues.apache.org/jira/browse/HDFS-14443

using curl --negotiate -u ':' 'https://<master>:<port>/webhdfs/v1/...'

can successfully get 307 redirect with the corresponding Location.

but got 

token (token for xxx HDFS_DELEGATION_TOKEN owner=xxx renewer=xxx masterKeyID=ooo) can't be found in cache""

if redirect the url within 300ms.

 

Not issue if waiting for more than 300ms and then do the redirect.

No issue if changing (C) to Standby (no observers) (and redirect within 10 ms)

 

 "
Failing concurrent FS.initialize commands when fs.azure.createRemoteFileSystemDuringInitialization is enabled on hadoop-azure ABFS,13410102,Open,Major,,04/Nov/21 16:45,,3.3.1,"*Bug description:*

When {{fs.azure.createRemoteFileSystemDuringInitialization}} is enabled, the filesystem will create a container if it does not already exist inside the {{initialize}} method. The current flow of creating the container will fail in the case of concurrent {{initialize}} methods being executed simultaneously (only one request can create the container, the rest will fail instead of moving on). This is happen due to the `checkException` method that is not catching the Hadoop `FileAlreadyExists` exception.

Stacktrace:

{{Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Operation failed: ""The specified filesystem already exists."", 409, PUT, https://<REDACTED>.dfs.core.windows.net/project?resource=filesystem, FilesystemAlreadyExists, ""The specified filesystem already exists. RequestId:<REDACTED> Time:2021-10-18T13:46:05.7504906Z""}}
 {{ {{at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.checkException(AzureBlobFileSystem.java:1182)}}}}
 {{ {{at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.createFileSystem(AzureBlobFileSystem.java:1067)}}}}
 {{ {{at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:126)}}}}
 {{ {{at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)}}}}

*To reproduce:*
 * Set `fs.azure.createRemoteFileSystemDuringInitialization` to `true`
 * Run two concurrent `initialize` commands with the root to the non existing container/filesystem.

 

*Proposed fix:*

[https://github.com/apache/hadoop/pull/3620]"
Hadoop - Upgrade to JQuery 3.6.0,13411002,Open,Major,,10/Nov/21 08:26,,,jQuery version is being upgraded from jquery-3.5.1.min.js to jquery-3.6.0.min.js
Exclude ASF license check for pkg-resolver JSON,13408107,Resolved,Major,Fixed,24/Oct/21 14:14,25/Oct/21 23:14,2.10.0,"There's no way to add comments to a JSON file. Need to exclude the following files from ASF license checks since they're JSON files -

1. dev-support/docker/pkg-resolver/packages.json
2. dev-support/docker/pkg-resolver/platforms.json"
Improve CustomTokenProviderAdapter to import VisibleForTesting,13410265,Open,Major,,05/Nov/21 14:06,,,"Recently, some new features have been added to CustomTokenProviderAdapter, which is of course very good.
Here is the introduction of'org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting'. When using Maven to compile the hadoop project, an error occurred as expected.
E.g:
org.apache.maven.enforcer.rule.api.EnforcerRuleException: 
Banned imports detected:

Reason: Use hadoop-annotation provided VisibleForTesting rather than the one provided by Guava
	in file: org/apache/hadoop/fs/azurebfs/oauth2/CustomTokenProviderAdapter.java
		org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting (Line: 25, Matched by: org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting)

Analysis took 0 seconds

    at de.skuzzle.enforcer.restrictimports.rule.RestrictImports.execute (RestrictImports.java:70)
    at org.apache.maven.plugins.enforcer.EnforceMojo.execute (EnforceMojo.java:202)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)

In the end it was unsuccessful.
In addition, I want to explain that this happened in a Mac environment.

Obviously, we should introduce'org.apache.hadoop.classification.VisibleForTesting'."
S3A SSE-KMS inconsistency issue during rename,13406263,Resolved,Major,Duplicate,13/Oct/21 04:58,04/Nov/21 15:08,3.1.2,"According to the document:
 [https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/encryption.html#S3_Default_Encryption]
 ""Organizations may define a default key in the Amazon KMS; if a default key is set, then it will be used whenever SSE-KMS encryption is chosen and the value of fs.s3a.server-side-encryption.key is empty.""

So basically two conditions to make the object with default KMS: 1. Set SSE-KMS encryption 2. Did not set fs.s3a.server-side-encryption.key

But there is another confusing scenario below:

1. User want to rely on s3 bucket side encryption using their customer KMS key(kms-keyA, for example), so user did not set fs.s3a.server-side-encryption-algorithm or fs.s3a.server-side-encryption.key, and the files uploaded to this bucket will use bucket custom KMS key kms-keyA
 2. Next step, user want to copy the file to other file using s3a, the process will invoke copyFile() in S3AFileSystem, during the copy, s3a will clone the meta data of the source in cloneObjectMetadata(), in the clone, there is copy of SSE algorithm but no specific kms key copy for the SSE-KMS, it will cause the destination using SSE-KMS without any key id, the final file will use account level default key under aws/s3(
 [https://docs.aws.amazon.com/cli/latest/reference/s3api/put-object.html),]
 lets say its kms-keyB.

It means when ever there is a copy, the kms key will be changed from customer key kms-keyA to kms-keyB, which will cause inconsistency, for example:

hdfs dfs -put test s3://ssetest/

During this put, there will be rename processing from test.__COPYING__ to test, it will cause the final test file encrypted with account default key kms-keyB instead of s3 bucket customer key kms-keyA which is expected. Should we consider to clone the KMS key id also to keep the consistency?"
Backport HADOOP-17683 for branch-3.2 ,13407418,Resolved,Major,Fixed,20/Oct/21 05:09,28/Oct/21 02:48,3.2.2,"Our security tool raised the following security flaw on Hadoop 3.2.2: 

CVE-2021-29425: [https://nvd.nist.gov/vuln/detail/CVE-2021-29425]"
[hadoop-azure] Support Azure Active Directory auth with hadoop config (azureblob storage),13407777,Open,Major,,21/Oct/21 13:28,,3.3.1,"For azure blob storage there's 3 auth types: Shared key, SAS and Active Directory (via app, using tenantId, appID and appSecret)

 

Currently it's not possible to configure hadoopConfiguration to use the third way in hadoop-azure driver for azure blob storage. Oauth is supported only for azure data lake storage gen2, but not for wasb(s) containers.

 

Is it possible to implement it either?"
Fix documentation build failure using JDK 7 on branch-2.10,13406162,Resolved,Major,Fixed,12/Oct/21 15:21,19/Oct/21 09:18,,{{mvn site}} by JDK 7 fails due to error related to spotbugs which does not support Java 7.
Increase Java heap size for running Maven in Dockerfile of branch-2.10,13406158,Resolved,Major,Fixed,12/Oct/21 15:02,12/Oct/21 15:55,,"I got OOM on running create-release script in branch-2.10.

{noformat}
$ ./dev-support/bin/create-release --docker --dockercache --native
$ tail -n 5 patchprocess/mvn_install.log
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 07:06 min
Exception in thread ""main""
Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""main""
{noformat}"
hadoop-auth module cannot import non-guava implementation in hatoop util,13405827,Resolved,Major,Won't Fix,10/Oct/21 19:47,11/Oct/21 20:59,,"hadoop-common  provides several util implementations in {{org.apache.hadoop.util.*}}. Since hadoop-common depends on hadoop-auth, all the utility implementations cannot be used within hadoop-auth.

There are several options:
* similar to {{hadoop-annotations}} generic and utility implementations such as maps, Strings, Preconditions, ..etc could be moved to a new common-util module that has no dependency on other modules.
* easier fix is to manually replace the guava calls in hadoop-auth module without importing {{hadoop.util.*}}. Only few calls need to be manually replaced: {{Splitter}}, {{Preconditions.checkNotNull}}, and {{Preconditions.checkArgument}}

CC: [~vjasani] , [~stevel@apache.org], [~tasanuma]

"
Incorrect Class Name supplied to Logger in AbfsRestOperation.java & TracingContext.java,13405406,Patch Available,Major,,07/Oct/21 13:31,,,"The class name provided while initializing org.slf4j.Logger (using LoggerFactory.getLogger(...) method) is incorrect for following classes.
 # [AbfsRestOperation.java|https://github.infra.cloudera.com/CDH/hadoop/blob/cdpd-master/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java#L62]
 # [TracingContext.java|https://github.infra.cloudera.com/CDH/hadoop/blob/cdpd-master/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java#L66]

This makes debugging logs difficult."
Improve logging for abfs rename failures.,13404428,Open,Major,,01/Oct/21 07:42,,,[https://github.infra.cloudera.com/CDH/hadoop/blob/cdpd-master/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L431] should be warn. 
s3 and abfs incremental listing: use SAX parsers to stream results to list iterators,13405971,Open,Minor,,11/Oct/21 15:27,,3.3.2,"With code gradually adopting listStatusIncremental(), asking for a smaller initial batch could permit faster ramp of result processing.

probably most significant on an s3 versioned bucket, as there the need to skip tombstones can result in significantly slower listings -but could benefit ABFS too"
Correct log format for LdapGroupsMapping,13411374,Resolved,Minor,Fixed,12/Nov/21 01:22,12/Nov/21 14:42,3.3.2,"Correct log format for LdapGroupsMapping.

!image-2021-11-12-09-22-12-454.png|width=557,height=101!

 "
Add a method appendIfAbsent for CallerContext,13411169,Resolved,Minor,Fixed,11/Nov/21 01:47,15/Nov/21 13:46,3.3.5,"As we discussed here [#3635.|#discussion_r746873078]

In some cases, when we need to add a _key:value_ to the {_}CallerContext{_}, we need to check whether the _key_ already exists in the outer layer, which is a bit of a hassle. To solve this problem, we can add a new method {_}CallerContext#appendIfAbsent{_}."
abfs etag extraction inconsistent between LIST and HEAD calls,13407975,Resolved,Minor,Fixed,22/Oct/21 15:23,17/Mar/22 11:54,3.3.1,"etag parsing in abfs client needs to strip off the surrounding quotes from etag-header else they don't match those in the LIST call.

Adding in MAPREDUCE-7341 with tests"
CachedDNSToSwitchMapping#reloadCachedMappings is not consistent with CachedDNSToSwitchMapping#resolve,13419675,Open,Minor,,28/Dec/21 08:20,,,"The follow test case failed.

{code}
public class TestCachedDNSToSwitchMapping {

  @Test
  public void testReloadCachedMappings() {
    StaticMapping.resetMap();
    StaticMapping.addNodeToRack(""127.0.0.1"", ""/rack0"");
    StaticMapping.addNodeToRack(""notexisit.host.com"", ""/rack1"");
    CachedDNSToSwitchMapping cacheMapping =
        new CachedDNSToSwitchMapping(new StaticMapping());
    List<String> names = new ArrayList<>();
    names.add(""localhost"");
    names.add(""notexisit.host.com"");
    cacheMapping.resolve(names);
    Assert.assertTrue(cacheMapping.getSwitchMap().containsKey(""127.0.0.1""));
    Assert.assertTrue(cacheMapping.getSwitchMap().containsKey(""notexisit.host.com""));
    cacheMapping.reloadCachedMappings(names);
    // failed here
    Assert.assertEquals(0, cacheMapping.getSwitchMap().keySet().size());
  }
}
{code}"
Fix default value of Magic committer,13413849,Resolved,Minor,Fixed,25/Nov/21 12:23,06/Dec/21 09:19,3.3.1,"`fs.s3a.committer.magic.enabled` was set to true by default after HADOOP-17483, we can improve the doc"
Update CompressionCodecFactory to handle uppercase file extensions,13414645,Resolved,Minor,Fixed,01/Dec/21 00:11,02/Dec/21 00:53,,"I've updated the CompressionCodecFactory to be able to handle filenames with capitalized compression extensions. Two of the three maps internal to the class which are used to store codecs have existing lowercase casts, but it is absent from the call inside getCodec() used for comparing path names.

I updated the corresponding unit test in TestCodecFactory to include intended use cases, and confirmed the test passes with the change. I also updated the error message in the case of a null from an NPE to a rich error message. I've resolved all checkstyle violations within the changed files."
"""hdfs --daemon start"" command may write invalid PID to file",13415867,Open,Minor,,07/Dec/21 16:47,,3.2.2,"Starting a daemon with {{*hdfs --daemon start ...*}} (and also {{{}*yarn --daemon start ...*{}}}) might result in writing invalid PID to PIDfile.

Scenario: run {{*hdfs --daemon start namenode*}} (or any other hadoop daemon).

Expected result: PID of running namenode java process gets written to PID file.

Actual result (non-deterministic): PID of exited bash process get written to PID file.

 

Root cause of the issue is a fact that both daemon launching bash functions - {{*hadoop_start_daemon*}} and {{*hadoop_start_daemon_wrapper*}} - are concurrently writing different PIDs to the same file, and only PID written by {{*hadoop_start_daemon_wrapper*}} is correct. Order of those writes is weakly synchronised (with hardcoded 5s timeout). Under specific circumstances (like heavy CPU load) this ordering might not be preserved resulting in invalid PID ending up in PIDfile.

 

Possible solution: It seems that it's unnecessary for {{*hadoop_start_daemon*}} to write to pidfile if it's being called from {{*hadoop_start_daemon_wrapper*}} - it should skip this step in this scenario."
Reset state after running TestSecureRegistry,13411642,Open,Minor,,13/Nov/21 22:49,,,"If test class TestRegistryOperationUtils runs after TestSecureRegistry in the same JVM, test TestRegistryOperationUtils#testUsernameExtractionEnvVarOverrride fails due to shared state between tests resulting in a wrong returned username. While currently the build process runs every test class in its own, separate JVM, tests could be sped up by running in the same JVM, and the state should be easy to reset."
Add metrics doc for ReadLockLongHoldCount and WriteLockLongHoldCount,13411562,Resolved,Minor,Invalid,12/Nov/21 22:19,12/Nov/21 22:21,,Add metrics doc for ReadLockLongHoldCount and WriteLockLongHoldCount. See [HDFS-15808|https://issues.apache.org/jira/browse/HDFS-15808].
abfs and s3a disk buffer factories to use UUIDs for file prefixes,13411267,Open,Minor,,11/Nov/21 12:55,,3.3.2,"the disk buffers created in s3a and abfs output streams use a simple String.format(""datablock-%04d-"",  index) pattern for the prefix for File.tmpFile

this means there will be contention for filenames across streams, especially across processes. 

if each stream had a uuid prefix there'd be no contention. That'd change the API though. Alternatively: each disk block factory has the uuid, and the index is simply total number blocks created. "
S3AFileSystem to add object tags on a PutObjectRequest to S3,13408990,Open,Minor,,28/Oct/21 20:41,,3.3.1,"AWS S3 lifecycle rules can filter on object tags allowing for data retention policies to be more granular than a bucket or prefix.

 

It would be useful if during write, tags could be applied to the PutObjectRequest.

 

For our use-case, we are simply writing Parquet files through S3A to a target bucket, without a cluster. I believe a Configuration property would be sufficient for our needs.

 

But i'm unsure what the general use case would be for a HDFS cluster and the user wanted unique tags per job."
Hadoop-aws jar is unable to read file from S3 if used with third party like MINIO,13408996,Resolved,Minor,Invalid,28/Oct/21 21:05,29/Oct/21 12:14,3.2.0,"Unable to read a file from S3 from spark if end point url is pointing to MINIO within EKS kubernetes cluster. We are able to do read/write from other clients and minio console. But when we read using spark I see empty data frame coming. If I use dataframe.show() it displays  like below.

 

++
 
++

++

 

*Spark Config:*

.config(""spark.hadoop.fs.s3a.endpoint"", ""http://127.0.0.1:9000"") // minio url or port-forward to local

.config(""spark.hadoop.fs.s3a.access.key"",<myaccesskey>)

.config(""spark.hadoop.fs.s3a.secret.key"",<mysecretkey>)

 

""spark.hadoop.fs.s3a.secret.key""

""spark.hadoop.fs.s3a.secret.key""

.config(""spark.hadoop.fs.s3a.path.style.access"", *true*)

        .config(""spark.hadoop.fs.s3a.impl"", ""org.apache.hadoop.fs.s3a.S3AFileSystem"")

        .config(""spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version"", ""2"")

        .config(""fs.s3a.committer.staging.conflict-mode"", ""replace"")

        .config(""fs.s3a.committer.name"", ""file"")

        .config(""fs.s3a.committer.threads"", ""20"")

        .config(""fs.s3a.threads.max"", ""20"")

        .config(""fs.s3a.fast.upload.buffer"", ""bytebuffer"")

        .config(""fs.s3a.fast.upload.active.blocks"", ""8"")

        .config(""fs.s3a.block.size"", ""128M"")

        .config(""mapred.input.dir.recursive"",""true"")

    .config(""spark.sql.parquet.binaryAsString"", ""true"")

 

 

*JAR files:*

hadoop-aws:3.2.0

aws-java-sdk:1.12.30

spark-core_2.12:3.1.2

spark-sql_2.12:3.1.2

 

*Logs:*

DEBUG S3AFileSystem:2121: Getting path status for s3a://<mybucket>/<myfolder>/2021/test1_2021-03-23_15_21_31.592.csv  (2021/test1_2021-03-23_15_21_31.592.csv)

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: object_metadata_requests += 1  ->  1

21/10/28 16:52:34 DEBUG S3AFileSystem:2189: Found exact file: normal file

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_exists += 1  ->  1

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_get_file_status += 1  ->  2

21/10/28 16:52:34 DEBUG S3AFileSystem:2121: Getting path status for s3a://mybbucket/myfolder/test1_2021-03-23_15_21_31.592.csv  (2021/test1_2021-03-23_15_21_31.592.csv)

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: object_metadata_requests += 1  ->  2

21/10/28 16:52:34 DEBUG S3AFileSystem:2189: Found exact file: normal file

21/10/28 16:52:34 DEBUG S3AFileSystem:1899: List status for path: s3a://mybbucket/myfolder/test1_2021-03-23_15_21_31.592.csv

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_list_status += 1  ->  1

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_get_file_status += 1  ->  3

21/10/28 16:52:34 DEBUG S3AFileSystem:2121: Getting path status for s3a://mybbucket/myfolder//test1_2021-03-23_15_21_31.592.csv  (2021/test1_2021-03-23_15_21_31.592.csv)

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: object_metadata_requests += 1  ->  3

21/10/28 16:52:34 DEBUG S3AFileSystem:2189: Found exact file: normal file

21/10/28 16:52:34 DEBUG S3AFileSystem:1930: Adding: rd (not a dir): s3a://mybbucket/myfolder//test1_2021-03-23_15_21_31.592.csv

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_is_directory += 1  ->  2

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_get_file_status += 1  ->  4

21/10/28 16:52:34 DEBUG S3AFileSystem:2121: Getting path status for s3a://mybbucket/myfolder//test1_2021-03-23_15_21_31.592.csv  (2021/test1_2021-03-23_15_21_31.592.csv)

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: object_metadata_requests += 1  ->  4

21/10/28 16:52:34 DEBUG S3AFileSystem:2189: Found exact file: normal file

21/10/28 16:52:34 DEBUG S3AFileSystem:1899: List status for path: s3a://mybbucket/myfolder//test1_2021-03-23_15_21_31.592.csv

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_list_status += 1  ->  2

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: op_get_file_status += 1  ->  5

21/10/28 16:52:34 DEBUG S3AFileSystem:2121: Getting path status for s3a://mybbucket/myfolder/test1_2021-03-23_15_21_31.592.csv  (2021/test1_2021-03-23_15_21_31.592.csv)

21/10/28 16:52:34 DEBUG S3AStorageStatistics:63: object_metadata_requests += 1  ->  5

21/10/28 16:52:34 DEBUG S3AFileSystem:2189: Found exact file: normal file

 

++

||

++

++"
Fix the import statements in hadoop-aws module,13407713,Resolved,Minor,Fixed,21/Oct/21 08:57,21/Oct/21 17:46,3.3.1,"The install target with the activated clover profile fails with an error on the hadoop-aws module.


{code:java}
mvn -Pclover clean install -DskipTests --projects '!hadoop-client-modules/hadoop-client-check-invariants,!hadoop-client-modules/hadoop-client-check-test-invariants' {code}
{code:java}
com.atlassian.clover.api.CloverException: /Users/tdomok/Work/hadoop/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DeleteOperation.java:31:85:unexpected token: ; {code}"
S3A: ITestS3AFileContextStatistics test to lookup global or per-bucket configuration for encryption algorithm,13404961,Resolved,Minor,Fixed,05/Oct/21 10:37,19/Oct/21 09:59,3.4.0,"ITestS3AFileContextStatistics uses the conf.get() method to get the global configuration for encryption algorithm and keys, but the per-bucket configuration would be ignored in this case."
NFS unable to bind to chosen address,13407099,Open,Minor,,18/Oct/21 15:23,,3.1.1,"[https://lists.apache.org/list.html?user@hadoop.apache.org:2021-10]

 

NFS Gateway appears not have a configuration setting for selecting which IP(s) to which to bind on a multihomed machine. I would like to offer a patch to add this feature in line with other similar settings for HDFS components that can bind to a chosen address. Or even better, I would _love_ to be told I am wrong and that this config setting _does_ exist. :) I will attempt to create a patch, but I have not contributed to Hadoop before, so I will need handholding."
OpensslCipher initialization error should log a WARN message,13408988,Resolved,Trivial,Fixed,28/Oct/21 20:34,10/Dec/21 09:14,,"We spent months troubleshooting a RangerKMS performance problem, only to realize that the openssl library wasn't even loaded properly.

The failure to load openssl lib is currently logged as a debug message during initialization. We really should upgrade it to at least INFO/WARN. 

{code}
static {
    String loadingFailure = null;
    try {
      if (!NativeCodeLoader.buildSupportsOpenssl()) {
        PerformanceAdvisory.LOG.debug(""Build does not support openssl"");
        loadingFailure = ""build does not support openssl."";
      } else {
        initIDs();
      }
    } catch (Throwable t) {
      loadingFailure = t.getMessage();
      LOG.debug(""Failed to load OpenSSL Cipher."", t);
    } finally {
      loadingFailureReason = loadingFailure;
    }
  }
{code}"
AccessPoint verifyBucketExistsV2 always returns false,13404774,Resolved,Trivial,Fixed,04/Oct/21 11:12,04/Oct/21 19:58,3.4.0,"Turns out the implementation added for accesspoints to check that they exist was always returning ""false"". (Is it an improvement from the SDK which always returned ""true""? nope).

The fix for this is trivial as we need to check for the presence of a message ""Could not access through this access point"" or 404 if the AP doesn't exist. Otherwise it does."
hostnmae is shown in LowerCase in SecurityUtil#replacePattern ,13417563,Resolved,Trivial,Invalid,14/Dec/21 22:19,15/Dec/21 07:18,3.3.1,"Leading hostname to lower case causes test failures, when original hostname in uppercase. 

[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java#L230]

What is the intention of using lowercase there?"
"Cut excess dependencies from hadoop-azure, hadoop-aliyun transitive imports; fix LICENSE-binary",13525923,Resolved,Blocker,Fixed,23/Feb/23 17:11,28/Feb/23 14:09,3.3.5,"There's too many dependencies coming through transitively from libraries getting into the hadoop-cloud-storage POM, which, while being resolved in hadoop imports, can cause problems elsewhere. Cut those we know are duplicated in hadoop-common

also, changes in direct dependencies have added new transitive ones which aren't covered in the license file. identify, verify license is valid, remind PR authors of their homework. That change makes this a blocker.

this pr went in with HADOOP-18641 in the commit text"
Fix bin/hadoop usage script terminology,13528231,Resolved,Blocker,Fixed,13/Mar/23 12:20,13/Mar/23 12:25,3.3.4,"Use terminology ""workers"" throughout bin/hadoop script."
Upgrade Kerby to 2.0.3 due to CVE-2023-25613,13527422,Patch Available,Blocker,,07/Mar/23 09:24,,3.3.4,"An LDAP Injection vulnerability exists in the LdapIdentityBackend of Apache Kerby before 2.0.3.

CVSSv3 Score:- 9.8(Critical)

[https://nvd.nist.gov/vuln/detail/CVE-2023-25613]"
Spark application's dependency conflicts with Hadoop's dependency,13529164,Resolved,Blocker,Invalid,20/Mar/23 03:41,20/Mar/23 12:12,3.3.2,"The issue I'm going to describe happens with the distribution: Spark 3.3.2 (git revision 5103e00c4c) built for Hadoop 3.3.2

Based on [this ticket|https://issues.apache.org/jira/browse/HADOOP-11804], as per my understanding, from Hadoop v3, there shouldn't be any conflict between the Hadoop's and Spark app's dependencies. But, I see a runtime failure with my spark app because of this conflict. Pasting the stack trace below:

{{Caused by: java.lang.NoSuchMethodError: com.google.common.collect.Sets.newConcurrentHashSet()Ljava/util/Set;}}
{{    at org.apache.cassandra.config.Config.<init>(Config.java:102)}}
{{    at org.apache.cassandra.config.DatabaseDescriptor.clientInitialization(DatabaseDescriptor.java:288)}}
{{    at org.apache.cassandra.io.sstable.CQLSSTableWriter.<clinit>(CQLSSTableWriter.java:109)}}
{{    at com.<redacted>.spark.cassandra.bulkload.GameRecommendationsSSTWriter.init(GameRecommendationsSSTWriter.java:60)}}
{{    at com.<redacted>.spark.cassandra.bulkload.GameRecommendationsSSTWriter.<init>(GameRecommendationsSSTWriter.java:23)}}
{{    at com.<redacted>.spark.cassandra.bulkload.CassandraBulkLoad.execute(CassandraBulkLoad.java:93)}}
{{    at com.<redacted>.spark.cassandra.bulkload.CassandraBulkLoad.main(CassandraBulkLoad.java:60)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)}}
{{    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)}}
{{    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)}}
{{    at java.lang.reflect.Method.invoke(Method.java:498)}}
{{    at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740)}}

My Spark app has a transitive dependency on Guava library. It depends on cassandra-all lib, which does on guava lib. The jar guava-14.0.1 that comes in ""spark-3.3.2-bin-hadoop3/jars"" directory is a decade old and doesn't have Sets.newConcurrentHashSet() method. I'm able to run the spark app successfully by deleting that old version of guava jar from /jar directory and by including a recent version in my project's pom.xml."
maven site generation doesn't include javadocs,13520081,Resolved,Blocker,Fixed,18/Jan/23 14:55,31/Jan/23 11:31,3.3.5,"the rc0 excluded all the site docs. running mvn site on trunk throws up site plugin issues, which may be related, so start by updating that.

rc validation scripts to include checks for the api/index.html"
ABFS OAuth2 Token Provider to support Azure Workload Identity for AKS,13522687,Resolved,Critical,Fixed,02/Feb/23 04:18,12/Nov/24 11:28,3.3.4,"In Jan 2023, Microsoft Azure AKS replaced its original pod-managed identity with with [Azure Active Directory (Azure AD) workload identities|https://learn.microsoft.com/en-us/azure/active-directory/develop/workload-identities-overview] (preview), which integrate with the Kubernetes native capabilities to federate with any external identity providers. This approach is simpler to use and deploy.

Refer to [https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview|https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview.] and [https://azure.github.io/azure-workload-identity/docs/introduction.html] for more details.

The basic use scenario is to access Azure cloud resources (such as cloud storage) from Kubernetes (such as AKS) workload using Azure managed identity federated with Kubernetes service account. The credential environment variables in pod projected by Azure AD workload identity are like following:

AZURE_AUTHORITY_HOST: (Injected by the webhook, [https://login.microsoftonline.com/])

AZURE_CLIENT_ID: (Injected by the webhook)

AZURE_TENANT_ID: (Injected by the webhook)

AZURE_FEDERATED_TOKEN_FILE: (Injected by the webhook, /var/run/secrets/azure/tokens/azure-identity-token)

The token in the file pointed by AZURE_FEDERATED_TOKEN_FILE is a JWT (JASON Web Token) client assertion token which we can use to request to AZURE_AUTHORITY_HOST (url is  AZURE_AUTHORITY_HOST + tenantId + ""/oauth2/v2.0/token"")  for a AD token which can be used to directly access the Azure cloud resources.

This approach is very common and similar among cloud providers such as AWS and GCP. Hadoop AWS integration has WebIdentityTokenCredentialProvider to handle the same case.

The existing MsiTokenProvider can only handle the managed identity associated with Azure VM instance. We need to implement a WorkloadIdentityTokenProvider which handle Azure Workload Identity case. For this, we need to add one method (getTokenUsingJWTAssertion) in AzureADAuthenticator which will be used by WorkloadIdentityTokenProvider.

 "
CryptoOutputStream::close leak when encrypted zones + quota exceptions,13522839,Resolved,Critical,Fixed,02/Feb/23 23:03,07/Feb/23 12:14,3.3.1,"{color:#172b4d}I would like to report an issue with a resource leak ({color}DFSOutputStream objects) when using the (java) hadoop-hdfs-client

And specifically (at least in my case) when there is a combination of:
 * encrypted zones
 * quota space exceptions (DSQuotaExceededException)

As you know, when encrypted zones are in play, when calling fs.create(path) in the hadoop-hdfs-client it will return a HdfsDataOutputStream stream object which wraps a CryptoOutputStream object which then wraps a DFSOutputStream object.

Even though my code is correctly calling stream.close() on the above I can see from debugging that the underlying DFSOutputStream objects are being leaked. 

Specifically I see the DFSOutputStream objects being leaked in the filesBeingWritten map in DFSClient.  (i.e. the DFSOutputStream objects remain in the map even though I've called close() on the stream object).

I suspect this is due to a bug in CryptoOutputStream::close
{code:java}
  @Override                                                                                                   
  public synchronized void close() throws IOException {                                                       
    if (closed) {                                                                                             
      return;                                                                                                 
    }                                                                                                         
    try {                                                                                                     
      flush();                                                                                                
      if (closeOutputStream) {                                                                                
        super.close();                                                                                        
        codec.close();                                                                                        
      }                                                                                                       
      freeBuffers();                                                                                          
    } finally {                                                                                               
      closed = true;                                                                                          
    }                                                                                                         
  }{code}
... whereby if flush() throws (observed in my case when a DSQuotaExceededException exception is thrown due to quota exceeded) then the super.close() on the underlying DFSOutputStream is skipped.

In my case I had a space quota set up on a given directory which is also in an encrypted zone and so each attempt to create and write to a file failed and leaked as above.

I have attached a speculative patch ([^hadoop_cryto_stream_close_try_finally.diff]) which simply wraps the flush() in a try .. finally.  The patch resolves the problem in my testing.

Thanks."
"Add recoverLease(), setSafeMode(), isFileClosed() APIs to FileSystem",13529330,Resolved,Major,Fixed,20/Mar/23 23:23,03/May/23 10:06,3.3.6,"We are in the midst of enabling HBase and Solr to run on Ozone.

An obstacle is that HBase relies heavily on HDFS APIs and semantics for its Write Ahead Log (WAL) file (similarly, for Solr's transaction log). We propose to push up these HDFS APIs, i.e. recoverLease(), setSafeMode(), isFileClosed() to FileSystem abstraction so that HBase and other applications do not need to take on Ozone dependency at compile time. This work will (hopefully) enable HBase to run on other storage system implementations in the future.

There are other HDFS features that HBase uses, including hedged read and favored nodes. Those are FS-specific optimizations and are not critical to enable HBase on Ozone."
improve s3a committer stats collected,13527033,Resolved,Major,Won't Fix,03/Mar/23 18:22,15/Jan/25 17:36,3.3.5,"we can improve stats collected in the s3a committer and saved to the JSON.

key ones
# of task manifests read; duration of loads
# size of each manifest

I think we would also benefit if we could set the commit thread pools to be big -but then shared across all jobs (i.e. demand-created thread pool in s3a fs). that would allow for a pool size of say, 500, but still support many jobs actively committing at same time (busy spark driver)
finally: should file commit pool size be > size of pool of manifest readers. I think it could be, but the ratio should be fairly low.
"
Add BulkDelete API for paged delete of files and objects,13529988,Resolved,Major,Fixed,24/Mar/23 14:49,28/May/24 21:22,3.3.5,"iceberg and hbase could benefit from being able to give a list of individual files to delete -files which may be scattered round the bucket for better read peformance.

Add some new optional interface for an object store which allows a caller to submit a list of paths to files to delete, where
the expectation is
 * if a path is a file: delete
 * if a path is a dir, outcome undefined
For s3 that'd let us build these into DeleteRequest objects, and submit, without any probes first.

h2. Cherrypicking
{quote}
when cherrypicking, you must include
 * followup commit #6854
 * https://issues.apache.org/jira/browse/HADOOP-19196
 * test fixes HADOOP-19814 and HADOOP-19188"
AWS SDK V2 - Refactor getS3Region & other follow up items ,13529447,Open,Major,,21/Mar/23 14:06,,3.4.0,"* Factor getS3Region into its own ExecutingStoreOperation;
 * Fix issue with getXAttr(""/"")
 * Look at adding flexible checksum support"
define s3a encryption behaviour on copy,13525614,Open,Major,,21/Feb/23 16:20,,,"When doing a copy, S3A always uses encryption configuration of the filesystem, rather than the source object. This behaviour may not have been intended, as in `RequestFactoryImpl.copyEncryptionParameters()`  it does copy source object encryption properties [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java#L336] , but a missing return statement means it ends up using the FS settings anyway. 

 

Proposed:
 * If the copy is called by rename, always preserve source object encryption properties. 
 * For all other copies, use current FS encryption settings. "
"Add ""versions"" tool to s3a command line entry point",13527079,Open,Major,,04/Mar/23 12:54,,3.3.9,"having just implemented some version command support in the cloudstore jar, I can see benefit in actually implementing it in hadoop-aws module

https://github.com/steveloughran/cloudstore/blob/trunk/src/main/site/versioned-objects.md

https://github.com/steveloughran/cloudstore/blob/trunk/src/main/extra/org/apache/hadoop/fs/s3a/extra/) 

this code
* uses v1 sdk by asking the s3a fs for it; this will break with the move to v2 sdk
* doesn't have any tests
* doesn't have any review, maintenance plan
* bypasses audit log/referrer header creation

we could just say ""use the aws CLI"", but there are some benefits in using the s3a connector code
* support for s3a:// urls
* can use the s3a auth/signing chain (knox, etc)
* plus proxy, region settings etc.
* could integrate with other bits of the stack (e.g spark RDD to get at all versions of objects)
* would be really useful to have a tool to purge all directory delete markers down a path, to speed up listing on versioned buckets.
* gets bundled everywhere

For use by downstream code we would want to have a public/evolving API to access operations, e.g. 

# taking an S3AFileStatus for rename/purge/restore operations
# listing all versions of objects under a path within a given time range and mapping to RemoteIterator.
# HADOOP-16387. S3A openFile() options to allow etag/version to be set

Core code straightforward (it takes exactly two days to write, *excluding tests*), public API and tests more work.

note, we should also move the entry point to being ""s3a"" with ""s3guard"" retained for compatibility)"
Move hadoop docker scripts under the main source code,13530061,Resolved,Major,Fixed,25/Mar/23 13:01,04/Nov/24 16:53,,"Exploratory:
Coming from https://github.com/apache/hadoop/pull/5514
We have docker scripts maintained in a different branch. We can explore them to have as part of dev-support in our main source code.

They can be used for new users to try the code without the headache of building and doing crazy stuff, and can help in dev testing as well"
Leaked calls may cause ObserverNameNode OOM.,13523701,Open,Major,,08/Feb/23 03:46,,,"Leaked calls may cause ObserverNameNode OOM.

 

During Observer Namenode tailing edits from JournalNode, it will cancel slow request with an interruptException if there are a majority of successful responses. 

There is a bug in Client.java, it will not clean the interrupted call from the calls. The leaked calls may cause ObserverNameNode OOM."
cyclonedx maven plugin breaks builds on recent maven releases (3.9.0),13525876,Resolved,Major,Fixed,23/Feb/23 11:13,23/Feb/23 18:28,3.3.5,"having upgraded maven to get spark to build, the cyclonedx plugin for HADOOP-18590 is failing. see SPARK-42380 for the same.

the 3.3.5 RCs are building with the older maven version of our docker images, but if we release as is we will ship something which will get harder over time to build.

reverting HADOOP-18590 everywhere. sorry.

*note* commits for HADOOP-18642, _HADOOP-18641. Cloud connector dependency and LICENSE fixup._ have gone in using this JIRA iD."
Publish SBOM artifacts,13517084,Resolved,Major,Fixed,06/Jan/23 20:36,15/Apr/23 16:32,3.4.0,
NPE in LdapAuthenticationHandler as disableHostNameVerification is never initialized,13521209,Open,Major,,25/Jan/23 06:35,,,"Steps to reproduce this issue:

Enable ldap auth with tls by configuring these in core-site.xml
1. hadoop.http.authentication.multi-scheme-auth-handler.schemes = basic
2. hadoop.http.authentication.multi-scheme-auth-handler.schemes.basic.handler = ldap
3. hadoop.http.authentication.ldap.enablestarttls = true

 

Trace:
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateWithTlsExtension(LdapAuthenticationHandler.java:261)
        at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticateUser(LdapAuthenticationHandler.java:238)
        at org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler.authenticate(LdapAuthenticationHandler.java:202)
{noformat}"
ProxyUserAuthenticationFilter add properties 'hadoop.security.impersonation.provider.class'  to enable  load custom ImpersonationProvider class when start namenode,13519746,Open,Major,,16/Jan/23 10:06,,,"h3. h3.  the phenomenon

I made a custom  ImpersonationProvider class and configured in core-site.xml
{code:none}
    <property>
      <name>hadoop.security.impersonation.provider.class</name>
      <value>org.apache.hadoop.security.authorize.MyImpersonationProvider</value>
    </property>
{code}
 

{color:#ff0000}However, when  start namenode, MyImpersonationProvider could't be load automatically, but DefaultImpersonationProvider is loaded.{color}

When execute the following command, custom ImpersonationProvider could be load.
{code:java}
bin/hdfs dfsadmin -refreshSuperUserGroupsConfiguration{code}
h3. h3. what I see else

custom ImpersonationProvider was load in org.apache.hadoop.security.authorize.ProxyUsers#refreshSuperUserGroupsConfiguration
through the property ""hadoop.security.impersonation.provider.class""

[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/ProxyUsers.java#L70]
{code:java}
public static void refreshSuperUserGroupsConfiguration(Configuration conf,
    String proxyUserPrefix) {
  Preconditions.checkArgument(proxyUserPrefix != null && 
      !proxyUserPrefix.isEmpty(), ""prefix cannot be NULL or empty"");
  // sip is volatile. Any assignment to it as well as the object's state
  // will be visible to all the other threads. 
  ImpersonationProvider ip = getInstance(conf);
  ip.init(proxyUserPrefix);
  sip = ip;
  ProxyServers.refresh(conf);
} 


private static ImpersonationProvider getInstance(Configuration conf) {
  Class<? extends ImpersonationProvider> clazz =
      conf.getClass(
          CommonConfigurationKeysPublic.HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS,
          DefaultImpersonationProvider.class, ImpersonationProvider.class);
  return ReflectionUtils.newInstance(clazz, conf);
}{code}
 

when namenode start, refreshSuperUserGroupsConfiguration was called in ProxyUserAuthenticationFilter,

[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java#L56]
{code:java}
  public void init(FilterConfig filterConfig) throws ServletException {
    Configuration conf = getProxyuserConfiguration(filterConfig);
    ProxyUsers.refreshSuperUserGroupsConfiguration(conf, PROXYUSER_PREFIX);
    super.init(filterConfig);
  }
{code}
here is the stack trace
{code:none}
init:70, DefaultImpersonationProvider (org.apache.hadoop.security.authorize)
refreshSuperUserGroupsConfiguration:77, ProxyUsers (org.apache.hadoop.security.authorize)
init:56, ProxyUserAuthenticationFilter (org.apache.hadoop.security.authentication.server)
initialize:140, FilterHolder (org.eclipse.jetty.servlet)
lambda$initialize$0:731, ServletHandler (org.eclipse.jetty.servlet)
accept:-1, 1541075662 (org.eclipse.jetty.servlet.ServletHandler$$Lambda$36)
forEachRemaining:948, Spliterators$ArraySpliterator (java.util)
forEachRemaining:742, Streams$ConcatSpliterator (java.util.stream)
forEach:580, ReferencePipeline$Head (java.util.stream)
initialize:755, ServletHandler (org.eclipse.jetty.servlet)
startContext:379, ServletContextHandler (org.eclipse.jetty.servlet)
doStart:910, ContextHandler (org.eclipse.jetty.server.handler)
doStart:288, ServletContextHandler (org.eclipse.jetty.servlet)
start:73, AbstractLifeCycle (org.eclipse.jetty.util.component)
start:169, ContainerLifeCycle (org.eclipse.jetty.util.component)
doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component)
doStart:97, AbstractHandler (org.eclipse.jetty.server.handler)
start:73, AbstractLifeCycle (org.eclipse.jetty.util.component)
start:169, ContainerLifeCycle (org.eclipse.jetty.util.component)
doStart:117, ContainerLifeCycle (org.eclipse.jetty.util.component)
doStart:97, AbstractHandler (org.eclipse.jetty.server.handler)
start:73, AbstractLifeCycle (org.eclipse.jetty.util.component)
start:169, ContainerLifeCycle (org.eclipse.jetty.util.component)
start:423, Server (org.eclipse.jetty.server)
doStart:110, ContainerLifeCycle (org.eclipse.jetty.util.component)
doStart:97, AbstractHandler (org.eclipse.jetty.server.handler)
doStart:387, Server (org.eclipse.jetty.server)
start:73, AbstractLifeCycle (org.eclipse.jetty.util.component)
start:1276, HttpServer2 (org.apache.hadoop.http)
start:170, NameNodeHttpServer (org.apache.hadoop.hdfs.server.namenode)
startHttpServer:954, NameNode (org.apache.hadoop.hdfs.server.namenode)
initialize:765, NameNode (org.apache.hadoop.hdfs.server.namenode)
<init>:1020, NameNode (org.apache.hadoop.hdfs.server.namenode)
<init>:995, NameNode (org.apache.hadoop.hdfs.server.namenode)
createNameNode:1769, NameNode (org.apache.hadoop.hdfs.server.namenode)
main:1834, NameNode (org.apache.hadoop.hdfs.server.namenode)
{code}
 
{color:#ff0000}but the filterConfig in ProxyUserAuthenticationFilter did't contains properties ''hadoop.security.impersonation.provider.class''{color}
filterConfig in ProxyUserAuthenticationFilter is controled by ProxyUserAuthenticationFilterInitializer or AuthFilterInitializer
filterConfig only put property which start with ""hadoop.proxyuser"", but not put ""hadoop.security.impersonation.provider.class""
{code:java}
  protected Map<String, String> createFilterConfig(Configuration conf) {
    Map<String, String> filterConfig = AuthenticationFilterInitializer
        .getFilterConfigMap(conf, configPrefix);
    //Add proxy user configs
    for (Map.Entry<String, String> entry : conf.getPropsWithPrefix(
        ProxyUsers.CONF_HADOOP_PROXYUSER).entrySet()) {
      filterConfig.put(""proxyuser"" + entry.getKey(), entry.getValue());
    }
    return filterConfig;
  }
{code}
[https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java#L46]

[https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/AuthFilterInitializer.java#L46]

it leads to custome ImpersonationProvider can't be load during namenode start.
 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 
 
 
 

 "
Remove unnecessary dependency on json-smart,13530962,Resolved,Major,Fixed,31/Mar/23 11:16,06/Apr/23 15:01,3.3.5,"hadoop-auth has a dependency on net.minidev:json-smart 2.4.7, but this dependency is never used.

This dependency was originally included because the transitive dependency that nimbus-jose-jwt had did not work properly (see https://issues.apache.org/jira/browse/HADOOP-14903). Since version 9.* nimbus-jose-jwt is using its own shaded version of json-smart, so the version declared in hadoop-auth is never actually used.

json-smart 2.4.7 shows up in CVE scans for CVE-2023-1370. It is still used as a transitive dependency in hadoop-hdfs"
ABFS: Customize and optimize timeouts made based on each separate request,13524803,Open,Major,,15/Feb/23 09:21,,,"In present day ABFS Driver functioning, all API request calls use the same values of default timeouts. This is sub-optimal in the scenarios where a request is failing due to hitting a particular busy node, and would benefit simply by retrying quicker.

For this, the change to be brought in chooses customized timeouts based on which API call is being made. Further, starting with smaller, optimized values of timeouts, the timeout values would increase by a certain incremental factor for subsequent retries to ensure quicker retries and success."
Upgrade ZooKeeper to version 3.8.3,13522810,Resolved,Major,Fixed,02/Feb/23 15:56,19/Dec/23 17:31,3.3.4,
upgrade to jettison 1.5.3 to fix CVE-2022-40150,13516385,Resolved,Major,Fixed,03/Jan/23 22:50,06/Jan/23 23:44,3.3.5,"[https://github.com/advisories/GHSA-x27m-9w8j-5vcw]

 

[https://github.com/jettison-json/jettison/releases]

v1.5.2 is flagged as fixing a CVE but a v1.5.3 was quickly released and appears ti fix some regressions caused by v1.5.2.
Many hadoop tests fail when jettison 1.5.2 is used."
Fix build failure with docs profile,13520293,Resolved,Major,Fixed,20/Jan/23 08:09,31/Jan/23 10:46,3.4.0,"Build with docs profile failed due to depcheck error.

{noformat}
$ mvn clean install -DskipTests -DskipShade -Pdocs
...
[INFO] --- maven-enforcer-plugin:3.0.0:enforce (depcheck) @ hadoop-hdfs ---
[WARNING]
Dependency convergence error for xerces:xercesImpl:jar:2.12.2:provided paths to dependency are:
+-org.apache.hadoop:hadoop-hdfs:jar:3.4.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:jar:3.4.0-SNAPSHOT:provided
    +-xerces:xercesImpl:jar:2.12.2:provided
and
+-org.apache.hadoop:hadoop-hdfs:jar:3.4.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:test-jar:tests:3.4.0-SNAPSHOT:test
    +-xerces:xercesImpl:jar:2.12.2:test
and
+-org.apache.hadoop:hadoop-hdfs:jar:3.4.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-hdfs-client:jar:3.4.0-SNAPSHOT:provided
    +-xerces:xercesImpl:jar:2.12.2:provided
and
+-org.apache.hadoop:hadoop-hdfs:jar:3.4.0-SNAPSHOT
  +-com.google.code.findbugs:findbugs:jar:3.0.1:provided
    +-jaxen:jaxen:jar:1.1.6:provided
      +-xerces:xercesImpl:jar:2.6.2:provided
and
+-org.apache.hadoop:hadoop-hdfs:jar:3.4.0-SNAPSHOT
  +-xerces:xercesImpl:jar:2.12.2:compile

[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Failed while enforcing releasability. See above detailed error message.
[INFO] ------------------------------------------------------------------------
{noformat}
"
Remove netty3 dependency,13521094,Resolved,Major,Fixed,24/Jan/23 09:32,27/Jan/23 15:32,3.4.0,AFAIK netty3 is no longer in use so it can be removed from the dependencies.
Add compile platform in the hadoop version output,13521728,Resolved,Major,Fixed,27/Jan/23 12:33,28/Jan/23 09:04,3.3.6,"Hadoop releases support both x86 and Aarch64, good to have a line indicating this in the hadoop version output.

Inspired by: HDDS-7783"
Avoid using grizzly-http-* APIs,13523309,Resolved,Major,Fixed,06/Feb/23 21:35,09/Feb/23 02:45,3.3.6,"As discussed on the parent Jira HADOOP-15984, we do not have any grizzly-http-servlet version available that uses Jersey 2 dependencies. 

version 2.4.4 contains Jersey 1 artifacts: [https://repo1.maven.org/maven2/org/glassfish/grizzly/grizzly-http-servlet/2.4.4/grizzly-http-servlet-2.4.4.pom]

The next higher version available is 3.0.0-M1 and it contains Jersey 3 artifacts: [https://repo1.maven.org/maven2/org/glassfish/grizzly/grizzly-http-servlet/3.0.0-M1/grizzly-http-servlet-3.0.0-M1.pom]

 

Moreover, we do not use grizzly-http-* modules extensively. We use them only for few tests such that we don't have to implement all the methods of HttpServletResponse for our custom test classes.

We should get rid of grizzly-http-servlet, grizzly-http and grizzly-http-server artifacts of org.glassfish.grizzly and rather implement HttpServletResponse directly to avoid having to depend on grizzly upgrades as part of overall Jersey upgrade."
Upgrade ant to 1.10.13,13523493,Resolved,Major,Fixed,07/Feb/23 12:03,21/Feb/23 11:19,3.3.6,"lnerabilities reported in org.apache.ant:ant:1.10.11
 * [CVE-2022-23437|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-23437]
 * [CVE-2020-14338|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14338]

suggested: org.apache.ant:ant ~> 1.10.13"
Add gh-pages in asf.yaml to deploy the current trunk doc,13524650,Resolved,Major,Fixed,14/Feb/23 10:50,14/Feb/23 13:04,3.4.0,"The docs are now pushed to gh-pages.
Add the entry in asf.yaml to publish as mentioned in 
https://issues.apache.org/jira/browse/INFRA-24202"
Migrate Async appenders to log4j properties,13524777,Resolved,Major,Fixed,15/Feb/23 06:42,17/Mar/23 23:19,3.4.0,"Before we can upgrade to log4j2, we need to migrate async appenders that we add ""dynamically in the code"" to the log4j.properties file. Instead of using core/hdfs site configs, log4j properties or system properties should be used to determine if the given logger should use async appender."
S3A to support upload of files greater than 2 GB using DiskBlocks,13525525,Resolved,Major,Fixed,21/Feb/23 09:05,27/Apr/23 10:01,3.3.6,"Use S3A Diskblocks to support the upload of files greater than 2 GB using DiskBlocks. Currently, the max upload size of a single block is ~2GB. 

cc: [~mthakur] [~stevel@apache.org] [~mehakmeet] "
Add bswap support for LoongArch,13526048,Resolved,Major,Fixed,24/Feb/23 10:08,23/Mar/23 03:21,3.4.0,"The LoongArch architecture (LoongArch) is an Instruction Set Architecture (ISA) that has a RISC style.
Documentations:
ISA:
[https://loongson.github.io/LoongArch-Documentation/LoongArch-Vol1-EN.html]
ABI:
[https://loongson.github.io/LoongArch-Documentation/LoongArch-ELF-ABI-EN.html]
More docs can be found at:
[https://loongson.github.io/LoongArch-Documentation/README-EN.html]"
Upgrade Netty to 4.1.89.Final,13526196,Resolved,Major,Fixed,26/Feb/23 07:12,10/Mar/23 15:27,3.3.4,"h4. Netty version - 4.1.89 has fix  CVEs: [CVE-2022-41881|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41881]
 "
Avoid loading kms log4j properties dynamically by KMSWebServer,13526415,Resolved,Major,Fixed,28/Feb/23 00:19,02/Mar/23 00:02,3.4.0,"Log4j2 does not support loading of log4j properties (/xml/json/yaml) dynamically by applications. It no longer supports overriding the loading of properties using ""log4j.defaultInitOverride"" the way log4j1 does.

For KMS, instead of loading the properties file dynamically, we should add the log4j properties file as part of HADOOP_OPTS."
CLA and CRLA appenders to be replaced with RFA,13526808,Resolved,Major,Fixed,02/Mar/23 07:26,15/Mar/23 16:47,3.4.0,"ContainerLogAppender and ContainerRollingLogAppender both have quite similar functionality as RollingFileAppender. Maintenance of custom appenders for Log4J2 is costly when there is very minor difference in comparison with built-in appender provided by Log4J. 

The goal of this sub-task is to replace both ContainerLogAppender and ContainerRollingLogAppender custom appenders with RollingFileAppender without changing any system properties already being used to determine file name, file size, backup index, pattern layout properties etc."
LogLevel servlet to determine log impl before using setLevel,13527178,Resolved,Major,Fixed,06/Mar/23 06:03,13/Mar/23 12:30,3.4.0,"LogLevel GET API is used to set log level for a given class name dynamically. While we have cleaned up the commons-logging references, it would be great to determine whether slf4j log4j adapter is in the classpath before allowing client to set the log level.

Proposed changes:
 * Use slf4j logger factory to get the log reference for the given class name
 * Use generic utility to identify if the slf4j log4j adapter is in the classpath before using log4j API to update the log level
 * If the log4j adapter is not in the classpath, report error in the output"
Remove unused custom appender TaskLogAppender,13527370,Resolved,Major,Fixed,07/Mar/23 03:12,15/Mar/23 16:48,3.4.0,"TaskLogAppender is no longer being used in codebase. The only past references we have are from old releasenotes (HADOOP-7308, MAPREDUCE-3208, MAPREDUCE-2372, HADOOP-1355).

Before we migrate to log4j2, it would be good to remove TaskLogAppender."
snakeyaml dependency: upgrade to v2.0,13527792,Resolved,Major,Fixed,09/Mar/23 11:09,13/Mar/23 04:38,3.3.6,"* [https://github.com/advisories/GHSA-mjmj-j48q-9wg2]
 * I don't think this needs to go in v3.3.5 - since this CVE affects part of snakeyaml that hadoop doesn't use"
ListFiles with recursive fails with FNF,13528365,Resolved,Major,Fixed,14/Mar/23 08:07,23/Mar/23 03:01,3.3.6,"Problem triggers in HDFS, but the change is in Hadoop-Common, Since the listFiles is defined in Hadoop-Common.

Scenario:

ListFiles With recursive: 
 * Fetches a dir say /dir, which has some /dir/s1...s10
 * Recursive is set to true: It goes and tries on say /dir/s5 and /dir/s5 got deleted by that time
 * The entire operation fails with FNF

Hive Cleaner uses listFiles with recursive true and this impacts that
{noformat}
2023-03-06 07:45:48,331 ERROR org.apache.hadoop.hive.ql.txn.compactor.Cleaner: [Cleaner-executor-thread-12]: Caught exception when cleaning, unable to complete cleaning of id:39762523,dbname:test,tableName:test_table,partName:null,state:,type:MINOR,enqueueTime:0,start:0,properties:null,runAs:hive,tooManyAborts:false,hasOldAbort:false,highestWriteId:989,errorMessage:null,workerId: null,initiatorId: null java.io.FileNotFoundException: File hdfs:/cluster/warehouse/tablespace/managed/hive/test.db/test_table/.hive-staging_hive_2023-03-06_07-45-23_120_4659605113266849995-73550 does not exist.
    at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1275)
    at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1249)
    at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1194)
    at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1190)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:1208)
    at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2144)
    at org.apache.hadoop.fs.FileSystem$5.handleFileStat(FileSystem.java:2332)
    at org.apache.hadoop.fs.FileSystem$5.hasNext(FileSystem.java:2309)
    at org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator.sourceHasNext(RemoteIterators.java:432)
    at org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator.fetch(RemoteIterators.java:581)
    at org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator.hasNext(RemoteIterators.java:602)
    at org.apache.hadoop.hive.ql.io.AcidUtils.getHdfsDirSnapshots(AcidUtils.java:1435)
    at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.removeFiles(Cleaner.java:287)
    at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.clean(Cleaner.java:214)
    at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.lambda$run$0(Cleaner.java:114)
    at org.apache.hadoop.hive.ql.txn.compactor.CompactorUtil$ThrowingRunnable.lambda$unchecked$0(CompactorUtil.java:54)
    at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750){noformat}"
A whitelist of endpoints to skip Kerberos authentication doesn't work for ResourceManager and Job History Server,13528579,Resolved,Major,Fixed,15/Mar/23 07:44,22/Mar/23 04:39,3.4.0,"Thanks to HADOOP-16527, we can add a whitelist of endpoints to skip Kerberos authentication such as {{/isActive}}, {{/jmx}}, {{/prom}}.
However, I found that ResourceManager and Job History Server doesn't repect {{hadoop.http.authentication.kerberos.endpoint.whitelist}}.

To workaround this issue for ResourceManager, set {{yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true}} in yarn-site.xml.
However, there is no workaround for Job History Server.

This bug is caused by {{HttpServer2#initSpnego}} call without proper configurations which starts with ""{{hadoop.http.authentication.}}"".

I will make a PR soon."
Path capability probe for truncate is only honored by RawLocalFileSystem,13528873,Resolved,Major,Fixed,16/Mar/23 22:56,21/Mar/23 02:23,3.3.4,"FileSystem#hasPathCapability returns true for probing ""fs.capability.paths.truncate"" only by RawLocalFileSystem. It should be honored by all file system implementations that support truncate."
Remove Log4Json Layout,13529045,Resolved,Major,Fixed,18/Mar/23 05:39,21/Mar/23 02:07,3.4.0,"Log4Json extends org.apache.log4j.Layout to provide log layout for Json. This utility is not being used anywhere in Hadoop. It is IA.Private (by default).

Log4j2 has introduced drastic changes to the Layout. It also converted it as an interface. Log4j2 also has JsonLayout, it provides options like Pretty vs. compact JSON, Encoding UTF-8 or UTF-16, Complete well-formed JSON vs. fragment JSON, addition of custom fields into generated JSON

[https://github.com/apache/logging-log4j2/blob/2.x/log4j-core/src/main/java/org/apache/logging/log4j/core/layout/JsonLayout.java]

 

This utility is more suitable to be part of log4j project rather than hadoop because the maintenance cost in hadoop would be higher with any more upgrades introducing changes to the Layout format."
Include jettison as direct dependency of hadoop-common,13529823,Resolved,Major,Fixed,23/Mar/23 16:23,27/Mar/23 08:18,3.4.0,"When hadoop common is pulled in outside of hadoop project, the wrong version of jettison is coming as the dependency management of hadoop-project doesn't apply this case.

So it's not enough just to upgrade the version in dependency management. When jettison is coming only as transitive, whoever is pulling in that hadoop library, will still get the wrong jettison.
example:
hadoop-common
{noformat}
org.example:untitled:jar:1.0-SNAPSHOT
\- org.apache.hadoop:hadoop-common:jar:3.4.0-SNAPSHOT:compile
...
   +- com.github.pjfanning:jersey-json:jar:1.20:compile
   |  +- org.codehaus.jettison:jettison:jar:1.1:compile
...
{noformat}
When the module is a library (so it will be used outside of the actual project), the correct dependency must be declared as direct dependency (and optionally excluding from the dependency where it came from originally).


jettison should be added as direct dependency"
S3A filesystem to support binding to other URI schemes,13530627,Resolved,Major,Fixed,29/Mar/23 13:35,05/Apr/23 14:05,3.3.5,"Allow s3a filesystem to be bindable to other filesystem schemas, especially s3://

* FileContext API has hard coded use of ""s3a""
* S3AFileSystem.getScheme() needs to pick up the scheme of the URI passed to initialize()
* plus tests"
S3A audit header to include count of items in delete ops,13530997,Resolved,Major,Fixed,31/Mar/23 15:48,16/May/23 12:37,3.3.5,it would be good to find out how many files were deleted in a DeleteObjects call
Sasl connection failure should log remote address,13517721,Resolved,Major,Fixed,12/Jan/23 02:53,01/Feb/23 18:27,3.3.4,"If Sasl connection fails with some generic error, we miss logging remote server that the client was trying to connect to.

Sample log:
{code:java}
2023-01-12 00:22:28,148 WARN  [20%2C1673404849949,1] ipc.Client - Exception encountered while connecting to the server 
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
    at sun.nio.ch.IOUtil.read(IOUtil.java:197)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
    at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
    at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:141)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
    at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
    at java.io.FilterInputStream.read(FilterInputStream.java:133)
    at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
    at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
    at java.io.DataInputStream.readInt(DataInputStream.java:387)
    at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1950)
    at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:367)
    at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:623)
    at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:414)
...
... {code}
We should log the remote server address."
Ensure that the config writers are closed,13522819,Open,Major,,02/Feb/23 17:11,,3.3.5,Use AutoCloseable to ensure that the config writers are closed between tests.
Expose distcp counters to user via config parameter and distcp contants,13524858,Open,Major,,15/Feb/23 14:19,,3.4.0,"Currently users or application such as Hive cannot access directly the distcp counters such as total number of bytes copied by distcp operation. 

This Jira is to enable this functionality in distcp tool."
replace jsr311-api dependency with rs-api,13523298,Open,Major,,06/Feb/23 20:00,,,"[jsr311-api|https://mvnrepository.com/artifact/javax.ws.rs/jsr311-api] is unmaintained and causes issues when jars bring in a dependency on the newer [rs-api|https://mvnrepository.com/artifact/javax.ws.rs/javax.ws.rs-api/2.1.1] jar - that uses the same package name but has incompatible code

To make things worse, there is now a jakarta fork of rs-api but I suggest we worry about that later.

jersey-core 1.19.x gives us the jsr311-api dependency. 

The upgrade to HADOOP-15984 is currently blocked and looks hard.

HADOOP-15983 is a workaround that allows us to keep jersey 1.x but removes the issue where we end up relying on the unmaintained Jackson 1.9 jars.

We may now need a similar fork of jersey-core 1.19 to build a version of that jar that uses rs-api instead of jsr311.

The main benefit here is get around the fact that jackson jaxrs 2.13+ has dropped support for jsr311 and now only supports rs-api. (see HADOOP-18332)

 "
Distcp -update between different cloud stores to use modification time while checking for file skip.,13519848,Resolved,Major,Fixed,17/Jan/23 05:51,15/Feb/23 11:22,,"Distcp -update currently relies on File size, block size, and Checksum comparisons to figure out which files should be skipped or copied. 
Since different cloud stores have different checksum algorithms we should check for modification time as well to the checks.

This would ensure that while performing -update if the files are perceived to be out of sync we should copy them. The machines between which the file transfers occur should be in time sync to avoid any extra copies.

Improving testing and documentation for modification time checks between different object stores to ensure no incorrect skipping of files."
Java 11 JavaDoc fails due to missing package comments,13522853,Resolved,Major,Duplicate,03/Feb/23 02:14,06/Feb/23 18:16,3.3.5,"Submissions to `hadoop-common` fail in Yetus due to Java 11 JavaDoc errors:
```
[ERROR] /home/builder/src/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/package-info.java:21: error: unknown tag: InterfaceAudience.Private
[ERROR] @InterfaceAudience.Private
[ERROR] ^
[ERROR] /home/builder/src/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/package-info.java:22: error: unknown tag: InterfaceStability.Unstable
[ERROR] @InterfaceStability.Unstable
[ERROR] ^
```"
fix test AbstractContractDistCpTest#testDistCpUpdateCheckFileSkip ,13524809,Resolved,Major,Fixed,15/Feb/23 09:50,22/Feb/23 09:02,,"In the newly introduced test testDistCpUpdateCheckFileSkip, for the first pass of ""distcp -update"", target file should not be present so that the copy takes place and creates the target file. 

Currently, we create both the source and target file with same block size from the start which can lead to flakiness due to race condition causing the modification time of the target file to be greater than/equal to the source and not copy the file at all. This can be seen more in the TestLocalContractDistCp due to no remote calls to create the target.
{code:java}
java.lang.AssertionError: Mismatch in COPY counter value expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:647)
	at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.verifySkipAndCopyCounter(AbstractContractDistCpTest.java:1000)
	at org.apache.hadoop.tools.contract.AbstractContractDistCpTest.testDistCpUpdateCheckFileSkip(AbstractContractDistCpTest.java:919) {code}"
Tune ABFS create() retry logic,13527647,Open,Major,,08/Mar/23 14:23,,3.3.5,"Based on experience trying to debug this happening
# add debug statements when create() fails
# generated exception text to reference string shared with tests, path and error code
# generated exception to include inner exception for full stack trace

Currently the retry logic is
# create(overwrite=false)
# if HTTP_CONFLICT/409 raised; call HEAD
# use etag in create(path, overwrite=true, etag)
# special handling of error HTTP_PRECON_FAILED = 412

There's a race condition here, which is if between 1 and 2 the file which exists is deleted. The retry should succeed, but currently a 404 from the head is escalated to a failure

proposed changes
# if HEAD is 404, leave etag == null and continue
# special handling of 412 also to handle 409"
Upgrade hadoop3 docker scripts to use 3.3.5,13530060,Resolved,Major,Fixed,25/Mar/23 12:49,28/Mar/23 06:25,,"Hadoop 3.3.5 is released, the docker3 still points to 3.3,1, Lets upgrade it to 3.3.5

It doesn't work as of now due to broken link to apache-rat-0.13"
ask: abfs connector to support checksum,13529343,Resolved,Major,Information Provided,21/Mar/23 03:08,27/Mar/23 14:20,,"Hi Hadoop-Azure community,

I cannot find much information on reason why abfs connector file level checksum is not supported, could you share some insights on why it doesn't support and is there plan to support in the future ? 

having this would be helpful for migrating data from on-prem to Azure storage using abfs connector

ref https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html

"
Broken links for arm build of hadoop 3.3.5,13529926,Resolved,Major,Fixed,24/Mar/23 07:57,24/Mar/23 12:16,3.3.5,The 3.3.5 release include arm builds under the link [https://dlcdn.apache.org/hadoop/common/hadoop-3.3.5/hadoop-arm64-3.3.5.tar.gz] notice the `arm64` suffix before the version number. But the download links on the release page [https://hadoop.apache.org/releases.html]  `hadoop/common/hadoop-3.3.5/hadoop-3.3.5-aarch64.tar.gz` using `aarch64` suffix after the version number.  This is also the suffix used the last time an arm build was published in release 3.3.1 [https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/]
CachedSASToken noisy log errors when SAS token has YYYY-MM-DD expiration,13529816,Open,Major,,23/Mar/23 15:59,,,"Error Description:

When using SAS tokens with expiration dates in the format YYYY-MM-DD, a frequent error appears in the logs related to the date format. The error expects an ISO_DATE_TIME. See [existing implementation|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/CachedSASToken.java#LL112-L119]. The error is noisy in the logs, but does not cause issues. 

Example stacktrace:
{code:java}
23/03/23 15:40:06 ERROR CachedSASToken: Error parsing se query parameter (2023-11-05) from SAS.
java.time.format.DateTimeParseException: Text '2023-11-05' could not be parsed at index 10
	at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)
	at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1851)
	at java.time.OffsetDateTime.parse(OffsetDateTime.java:402)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.utils.CachedSASToken.getExpiry(CachedSASToken.java:116)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.utils.CachedSASToken.update(CachedSASToken.java:168)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:670)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.ReadBufferWorker.lambda$run$0(ReadBufferWorker.java:66)
	at com.databricks.common.SparkTaskIOMetrics.withTaskIOMetrics(SparkTaskIOMetrics.scala:43)
	at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.ReadBufferWorker.run(ReadBufferWorker.java:65)
	at java.lang.Thread.run(Thread.java:750) {code}
Desired Resolution:

Expiration code can read YYYY-MM-DD format as well as existing format."
DockerContainerDeletionTask is not removed from the Nodemanager's statestore when the task is completed.,13525749,Open,Major,,22/Feb/23 13:43,,3.1.2,"YARN NodeManager's deletion service has two types of deletion tasks: the FileDeletionTask for deleting log, usercache, appcache files and the DockerContainerDeletionTask for deleting Docker containers.
 
The FileDeletionTask is removed from the statestore when the task is completed, but the DockerContainerDeletionTask is not.
Therefore, the DockerContainerDeletionTask accumulates continuously in the statestore.
 
This causes the NodeManager's deletion service to run the accumulated DockerContainerDeletionTask in the statestore when the NodeManager restarts.

As a result, the FileDeletionTask and DockerContainerDeletionTask are delayed unnecessarily while processing accumulated tasks, which can cause disk full issues in environments where a large number of containers are allocated and released.

I will attach a patch soon"
"Hadoop ""current"" documentation link broken after release 3.3.5.",13529862,Resolved,Major,Fixed,23/Mar/23 20:17,23/Mar/23 20:31,,"From hadoop.apache.org, access Documentation -> Current, leading to:

https://hadoop.apache.org/docs/current/

This results in a Forbidden response, seemingly since completion of the 3.3.5 release. (To see this, you might need to refresh your browser if it's still serving a cached copy of the 3.3.4 docs from this link.)"
IOUtils.wrapWithMessage can't wrap exceptions without string constructor,13528486,Open,Major,,14/Mar/23 18:32,,3.3.4,"When and attempt is made to wrap {{AbfsRestOperationException}}; it fails as there's no string constructor there.


{code}
java.lang.NoSuchMethodException: org.apache.hadoop.fs.azurebfs.contracts.exceptions.AbfsRestOperationException.<init>(java.lang.String)
        at java.lang.Class.getConstructor0(Class.java:3082)
        at java.lang.Class.getConstructor(Class.java:1825)
        at org.apache.hadoop.io.IOUtils.wrapWithMessage(IOUtils.java:507)
        at org.apache.hadoop.io.IOUtils.wrapException(IOUtils.java:491)
        at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.close(AbfsOutputStream.java:495)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)

{code}

proposed: handle NoSuchMethodException and downgrade to wrapping with a simple iOE. reluctant to not wrap in close() as we've had bizarre failures in the past there with try-with-resources failures
"
DecayRpcSchedulerDetailedMetrics display name numbers incremented by 1,13528405,Open,Major,,14/Mar/23 10:58,,,"It should keep names consistent with {color:#00875a}DecayRpcSchedulerMetrics{color}, to avoid confusion.


What might be displayed as Priority.0 and Priority.1 for {color:#00875a}DecayRpcSchedulerMetrics{color}, it's Priority.1 and Priority.2 for {color:#00875a}DecayRpcSchedulerDetailedMetrics{color}."
ABFS to add Auditing support and attach http referrer to requests,13527823,Open,Major,,09/Mar/23 14:19,,3.3.5," the s3a auditing feature of HADOOP-17511 is wonderful in production, as we can
* scan for all IO done by a single user, job, operation
* get a comprehensive view of what was done to the store, in the order it saw it, which doesn't always match the order the client logged it, especially logged outcomes as that only happens when the threads get scheduled.

the abfs TracingContext effectively does most of this. All that is needed is to integrate with the hadoop-common audit interfaces/apis
* TracingContext to pick up global/thread-local audit contexts in construction and so extract information (tool class; principal; spark job id. etc)

The manifest committer already sets task id on the active thread whenever entered; this persists through all the IO done by the spark worker. "
Hadoop DistCp supports specifying favoredNodes for data copying,13524576,Open,Major,,14/Feb/23 03:12,,3.3.4,"When importing large scale data to HBase, we always generate the hfiles with other Hadoop cluster, use the Distcp tool to copy the data to the HBase cluster, and bulkload data to HBase table. However, the data locality is rather low which may result in high query latency. After taking a compaction it will recover. Therefore, we can increase the data locality by specifying the favoredNodes in Distcp.

Could I submit a pull request to optimize it?"
S3A proxy to use proxy protocol to set port,13526011,Open,Major,,24/Feb/23 07:27,,,"In the current code, we use the Cloud store's protocol to set the proxy's port({*}If no port is set{*}).
{code:java}
if (conf.getBoolean(SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) {
          LOG.warn(""Proxy host set without port. Using HTTPS default 443"");
          awsConf.setProxyPort(443);
        } else {
          LOG.warn(""Proxy host set without port. Using HTTP default 80"");
          awsConf.setProxyPort(80);
        } {code}
We should use the proxy protocol instead."
site intro docs to make clear Kerberos is mandatory for secure clusters,13524203,Resolved,Major,Fixed,10/Feb/23 10:35,17/Feb/23 16:33,3.3.4,"make extra clear in the intro docs that you need to turn kerberos on or run a private network where all accessors have unrestricted access to all storage and compute.
"
Add libhdfs APIs for createFile,13524838,Resolved,Major,Duplicate,15/Feb/23 12:19,15/Feb/23 12:20,,"HDFS-14478 introduces builder-based APIs for openFile() based on HADOOP-15229.

We should also add builder-based APIs for createFile() based on HADOOP-14365.

This would be especially useful for object stores to tune performance of file writes."
[Azure FS] Add new property to configure credential provider path for azure storage.,13524152,Open,Major,,10/Feb/23 05:06,,3.1.1,"Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property ""{*}fs.azure.account.credential.provider.path.<account>.blob.core.windows.net{*}"".

 

This Jira utilize new API supported in HADOOP-18618 for external provider path."
Make IOStatisticsStore and binding APIs public for use beyond our code,13522982,Open,Major,,03/Feb/23 19:19,,3.3.5,"it's really useful to be able to collect iostats in things other than the FS classes -we do it in the S3A and manifest committers.

But external code -such as the spark committers can't use the methods in {{org.apache.hadoop.fs.statistics.impl))

Proposed

Make some classes/interfaces public

* IOStatisticsBinding
* IOStatisticsStore
* IOStatisticsStoreBuilder

Ideally we should actually move the IOStatisticsStore interface into org.apache.hadoop.fs.statistics and the builder to match -but we can't do that without causing trauma elsewhere (google gcs).

Strategy there: Add a new interface IOStatisticsCollector in .impl which is then implemented by IOStatisticsStore, and a new builder API which forwards to IOStatisticsStoreBuilder.

Side issue: we don't make any use of the ""clever, elegant functional"" bit of DynamicIOStatisticsBuilder/DynamicIOStatistics, where every counter is mapped to a function which is then invoked to get at the atomic longs. It's used in IOStatisticsStoreImpl, but only with AtomicLong and MeanStatistic instances. If we just move to simple maps we will save on lambda-expressions and on lookup overhead. The original intent was something like coda hale metrics where we could add dynamic lookup to other bits of instrumentation; in practise we measure durations and build counts/min/max."
Fix source code modification after building with docs profile,13522210,Open,Major,,31/Jan/23 11:06,,,MetricsSystem.java and MetricsSystemImpl.java are automatically modified by building with {{-Pdocs}}.
Add validation to check Javadocs are included in site documentation,13522207,Open,Major,,31/Jan/23 10:57,,,We should add validation to avoid an issue like HADOOP-18598 found in release process.
"Expose `listStatus(Path path, String startFrom)` on `AzureBlobFileSystem`",13520071,Open,Major,,18/Jan/23 14:16,,3.3.2,"When working with Azure blob storage listing operations can often be quite slow even on storage accounts with the hierarchical namespace. 

This can be mitigated by listing only a specific subset of directories using a function like [https://hadoop.apache.org/docs/r3.3.4/api/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.html#listStatus-org.apache.hadoop.fs.Path-java.lang.String-org.apache.hadoop.fs.azurebfs.utils.TracingContext-]

Which accepts a `startFrom` argument and lists all files in order starting from there.

I'm wondering if we could add a method to the `AzureBlobFileSystem`

Something like:

```
public FileStatus[] listStatus(final Path f, final String startFrom) throws IOException
```

This exposes the functionality that already exists on the underlying `AzureBlobFileSystemStore`. My understanding from reading a bit of the code is that users should mainly be dealing with `AzureBlobFileSystem`s and `AzureBlobFileSystem` seem easier to use to me hence the benefit of exposing it on the `AzureBlobFileSystem`.

 

I'm very un-familiar with java but I'm told that keeping strictly to interfaces is strongly preferred. However I can see some examples already on `AzureBlobFileSystem` that do not belong to any interface (e.g. `breakLease`) so I'm hoping its acceptable to add a method like I described only for the one `FileSystem` implementation.

 

The specific motivation for this is to unblock [https://github.com/delta-io/delta/issues/1568]

I would be willing to contribute this if maintainers think the plan is reasonable. "
Hadoop 2.x should support s3a committers,13520194,Resolved,Major,Won't Fix,19/Jan/23 12:48,19/Jan/23 16:54,2.10.2,"I think the feature about ""Add S3A committers for zero-rename commits to S3 endpoints"" (https://issues.apache.org/jira/browse/HADOOP-13786)  should be 蜜珀 in the merged in hadoop 2.10.2. 

 

 "
Reduce byte array allocation in DecompressorStreams,13518359,Open,Major,,13/Jan/23 18:07,,,"Compression codecs all implement CompressionCodec interface, which has a createInputStream method which takes an InputStream. These end up creating a class which extends DecompressorStream, which allocates 2 byte arrays of configurable size.

Currently its impossible to re-use these DecompressorStreams. Heavy users of decompression, can end up seeing large amounts of allocations coming from the creation of DecompressorStreams. Realtime systems like HBase have put a lot of work into reducing allocations, either through pooling and/or off-heaping. 

It would be very useful to be able to either re-use DecompressorStreams or add constructors which accept a ByteArrayManager or other source of byte array allocator. Either way could reduce allocations considerably in this case."
Create a new config for ABFS read-ahead,13516611,Open,Major,,05/Jan/23 08:39,,,"Create a new config for ABFS read-ahead to make it easier to enable it and deprecate the existing one simultaneously.

 

CC [~snvijaya] [~mthakur] [~stevel@apache.org] "
S3A Add new store vendor config option,13530238,Open,Minor,,27/Mar/23 11:58,,3.3.5,"Add in a new fs.s3a.store.vendor config, where users can specify the storage vendor they are using (eg: aws, netapp, minio).

This will allow us to configure S3A correctly per vendor. For example, if the vendor is not AWS, you probably want to use ListObjectsV1."
AWS SDK V2 - Add socket factory to Netty Client,13529449,Open,Minor,,21/Mar/23 14:09,,3.4.0,The Java async client uses the netty http client. We should investigate how to add a socket factory to this.
Amazon S3 disabling ACLs on all new buckets,13530794,Open,Minor,,30/Mar/23 12:17,,,"In April 2023, Amazon S3 will be disabling ACLs by default on *all* new buckets. [https://aws.amazon.com/blogs/aws/heads-up-amazon-s3-security-changes-are-coming-in-april-of-2023/.|https://aws.amazon.com/blogs/aws/heads-up-amazon-s3-security-changes-are-coming-in-april-of-2023/] Note, buckets created using the AWS Console are already created with ACLs disabled by default.

In S3A, we have tests that rely on ACLs being enabled. We should either update test documentation to tell developers to enable ACLs on buckets, or we should make the ACL tests opt-in if they are used infrequently by S3A users."
S3A AssumedRole credentials provider should use Instance Role credentials in chain for assuming role,13522016,Open,Minor,,30/Jan/23 09:11,,,"The AssumedRoleCredentialsProvider today creates its own credential provider set to use when getting session credentials from STS. This includes only environment variables and simple credentials provider. [https://github.com/apache/hadoop/blob/952d707240cb7dd088820d8b39e705b77fa3b6c4/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AssumedRoleCredentialProvider.java#L114-L116]

 

It should at least include EC2 Instance Role credentials, possibly others, by default."
ABFS: Support for Pagination in Recursive Directory Delete ,13527577,Resolved,Minor,Fixed,08/Mar/23 05:31,12/Nov/24 11:39,3.4.0,"Today, when a recursive delete is issued for a large directory in ADLS Gen2 (HNS) account, the directory deletion happens in O(1) but in backend ACL Checks are done recursively for each object inside that directory which in case of large directory could lead to request time out. Pagination is introduced in the Azure Storage Backend for these ACL checks.

More information on how pagination works can be found on public documentation of [Azure Delete Path API|https://learn.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/delete?view=rest-storageservices-datalakestoragegen2-2019-12-12].

This PR contains changes to support this from client side. To trigger pagination, client needs to add a new query parameter ""paginated"" and set it to true along with recursive set to true. In return if the directory is large, server might return a continuation token back to the caller. If caller gets back a continuation token, it has to call the delete API again with continuation token along with recursive and pagination set to true. This is similar to directory delete of FNS account.

Pagination is available only in versions ""2023-08-03"" onwards.
PR also contains functional tests to verify driver works well with different combinations of recursive and pagination features for HNS.
Full E2E testing of pagination requires large dataset to be created and hence not added as part of driver test suite. But extensive E2E testing has been performed."
Update solr from 8.8.2 to 8.11.2,13527435,Resolved,Minor,Fixed,07/Mar/23 10:42,22/Jun/23 07:46,3.3.4,"Solr is used in hadoop-yarn-applications-catalog-webapp.
{code:sh}
$ grep solr.version . -r
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml:            <version>${solr.version}</version>
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml:            <version>${solr.version}</version>
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml:            <version>${solr.version}</version>
./hadoop-project/pom.xml:    <solr.version>8.8.2</solr.version>
$
{code}
 

Solr-8.8.2's log4j2 version is 2.13.2.

Solr-8.11.2's log4j2 version is 2.17.1 ( https://issues.apache.org/jira/browse/SOLR-15871 )

Should we update solr to 8.11.2 correspondingly? 
Then maven will not try to download log4j2-2.13.2, which is forbidden by some maven repository.

 "
ABFS: Enabling Client-side Backoff only for new requests,13525858,Open,Minor,,23/Feb/23 09:50,,,"Enabling backoff only for new requests that happen, and disabling for retried requests."
LocalDirAllocator cannot recover from directory tree deletion during the life of a filesystem client,13525194,Resolved,Minor,Fixed,17/Feb/23 11:40,01/Mar/23 13:30,3.3.4,"The  s3a and abfs clients use LocalDirAllocator for allocating files in local (temporary) storage for buffering blocks to write, and, for the s3a staging committer, files being staged. 
When initialized (or when the configuration key value is updated) LocalDirAllocator enumerates all directories in the list and calls {{mkdirs()}} to create them.

when you ask actually for a file, it will look for the parent dir, and will again call {{mkdirs()}}. 

But before it does that, it looks to see if the dir has any space...if not it is excluded from the list of directories with room for data.

And guess what: directories which don't exist report as having no space. So they get excluded -the recreation code doesn't get a chance to run.


"
Fix a typo in Trash,13517419,Resolved,Minor,Fixed,10/Jan/23 08:53,12/Jan/23 21:25,3.3.6,
"Fix ""the the"" and friends typos",13519832,Resolved,Minor,Fixed,17/Jan/23 01:46,17/Jan/23 01:52,3.4.0,"Fix ""the the"" and friends typos #5267 Contributed by @neshkeev"
Add reason in in x-ms-client-request-id on a retry API call.,13522048,Resolved,Minor,Fixed,30/Jan/23 11:28,28/Mar/23 11:01,3.3.6,"In the header, x-ms-client-request-id contains informaiton on what retry this particular API call is: for ex: :eb06d8f6-5693-461b-b63c-5858fa7655e6:29cb0d19-2b68-4409-bc35-cb7160b90dd8:::CF:1.

We want to add the reason for the retry in the header_value:Now the same header would include retry reason in case its not the 0th iteration of the API operation. It would be like
:eb06d8f6-5693-461b-b63c-5858fa7655e6:29cb0d19-2b68-4409-bc35-cb7160b90dd8:::CF:1_RT. This corresponds that its retry number 1. The 0th iteration was failed due to read timeout."
Avoid mixing canonical and non-canonical when performing comparisons,13522807,Resolved,Minor,Fixed,02/Feb/23 15:38,06/Feb/23 18:29,3.3.5,The test mixes canonical and non-canonical paths and then perform comparisons.  We can avoid unexpected failures by ensuring that comparisons are always made against canonical forms.
Fix method name  of RPC.Builder#setnumReaders,13518893,Resolved,Minor,Fixed,14/Jan/23 06:03,09/Feb/23 13:30,3.3.4,
Server connection should log host name before returning VersionMismatch error,13524310,Resolved,Minor,Fixed,11/Feb/23 05:03,14/Feb/23 11:50,3.3.4,"In env with dynamically changing IP addresses, debugging issue with the logs with only IP address becomes a bit difficult at times.
{code:java}
2023-02-08 23:26:50,112 WARN  [Socket Reader #1 for port 8485] ipc.Server - Incorrect RPC Header length from {IPV4}:36556 expected length: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] got length: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] {code}
It would be better to log full hostname for the given IP address rather than only IP address."
Provide keytab file key name with ServiceStateException,13526134,Resolved,Minor,Fixed,25/Feb/23 00:10,01/Mar/23 01:35,3.4.0," 
{code:java}
util.ExitUtil - Exiting with status 1: org.apache.hadoop.service.ServiceStateException: java.io.IOException: Running in secure mode, but config doesn't have a keytab
1: org.apache.hadoop.service.ServiceStateException: java.io.IOException: Running in secure mode, but config doesn't have a keytab
  at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:264)
..
..
 {code}
 

 

When multiple downstreamers use different configs to present the same keytab file, if one of the config key gets missing or overridden as part of config generators, it becomes bit confusing for operators to realize which config is missing for a particular service, especially when keytab file value is already present with different config.

It would be nice to report config key with the stacktrace error message."
x-ms-client-request-id to have some way that identifies retry of an API.,13526296,Resolved,Minor,Fixed,27/Feb/23 08:59,05/Apr/23 13:59,3.3.5,"In case primaryRequestId in x-ms-client-request-id is empty-string, the retry's primaryRequestId has to contain last part of clientRequestId UUID."
Path.suffix raises NullPointerException,13527106,Resolved,Minor,Fixed,05/Mar/23 04:00,18/May/23 23:50,3.3.6,"Calling the Path.suffix method on root raises a NullPointerException. Tested with hadoop-client-api 3.3.2

Scenario:
{code:java}
import org.apache.hadoop.fs.*

Path root = new Path(""/"")
root.getParent == null  // true
root.suffix(""bar"")  // NPE is raised
{code}

Stack:
{code:none}
23/03/03 15:13:18 ERROR Uncaught throwable from user code: java.lang.NullPointerException
    at org.apache.hadoop.fs.Path.<init>(Path.java:104)
    at org.apache.hadoop.fs.Path.<init>(Path.java:93)
    at org.apache.hadoop.fs.Path.suffix(Path.java:361)
{code}"
Insufficient heap during full test runs in Docker container.,13529999,Resolved,Minor,Fixed,24/Mar/23 16:22,03/Apr/23 23:06,3.3.6,"During verification of releases on the 3.3 line, I often run out of heap during full test runs inside the Docker container. Let's increase the default in {{MAVEN_OPTS}} to match trunk.

Additionally, on trunk, the settings are different in Dockerfile vs. Dockerfile_aarch64. We can align those."
Support custom property for credential provider path,13523061,Open,Minor,,04/Feb/23 08:10,,3.1.3,"Hadoop allows the configuration of a credential provider path through the property ""{*}hadoop.security.credential.provider.path{*}"", and the {{Configuration#getPassword()}} method retrieves the credentials from this provider.

However, using common credential provider properties for components like Hive, HDFS, and MapReduce can cause issues when they want to configure separate JCEKS files for credentials. For example, the value in the core-site.xml property file can be overridden by the hive-site.xml property file. To resolve this, all components should share a common credential provider path and add all their credentials.

Azure storage supports account-specific credentials, and thus the credential provider should permit the configuration of separate JCEKS files for each account, such as the property ""{*}fs.azure.account.credential.provider.path.<account>.blob.core.windows.net{*}"".

To accommodate this, the {{Configuration#getPassword()}} method should accept a custom property for the credential provider path and retrieve its value. The current default property can be overridden to achieve this.
{code:java}
public char[] getPassword(String name) throws IOException {
    ......
    ......
}


public char[] getPassword(String name, String providerKey) throws IOException {                  
    ......
    ......
 }{code}
 

One Example is, Ambari [CustomServiceOrchestrator|https://github.com/apache/ambari/blob/trunk/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py#L312] service override the core-site.xml value for other component. This fix is very much needed for Ambari. 

 "
Backport of HADOOP-18493: Update jackson-databind for CVE fix for CVE-2022-42003,13530938,Resolved,Minor,Invalid,31/Mar/23 09:21,31/Mar/23 13:51,,This ticket is for jackson-databind version update to fix CVE 2022-42003.
ABFS: eTag Check to mitigate Rename Failures,13528626,Resolved,Minor,Duplicate,15/Mar/23 12:08,15/Mar/23 19:49,,"RenameFilePath on its first try receives a Request timed out error with code 500. On retrying the same operation, a Source file not found (404) error is received. 

Possible temporary mitigation: Check whether etags remain the same before and after the retry and accordingly send an Operation Successful result, instead of source file not found. For this, adding a check and an additional call to get the source eTag, in case it is not present already, to check after a rename retry whether the source and destination eTags match."
you can't launch create-release --docker from a build file,13528452,Open,Minor,,14/Mar/23 14:31,,3.3.5,"the {{create-release}} script launches docker with -i for interactivity, which
breaks when launching it from an ant build file or other scripts


{code}
        [x] 
        [x] Failed!
        [x] 
        [x] the input device is not a TTY
        [x]         3.12 real         0.29 user         0.39 sys

{code}

Fix: don't do that
"
S3A delegation token implementations to be able to update tokens from the user credentials,13523537,Open,Minor,,07/Feb/23 17:06,,3.3.5,"
Spark never renews tokens, instead it can create new ones and attach them to the current users credentials.

This means long-running S3A instances which can pick up new tokens/credentials need a way to look for new tokens in the credential chain.

Proposed
* class AbstractDelegationTokenBinding adds a CallableRaisingIOE field which can be updated with a callback
* S3ADelegationTokens to add method boolean maybeUpdateTokenFromOwner() to look for any new token and switch to it if new
* S3ADelegationTokens serviceInit() to pass the method down to the instantiated DT binding as the callback

It is up to the token binding implementation to decide what to do about it; the standard implementations will do: nothing. "
AbstractSTestS3AHugeFiles put request assert is using wrong iostatistic,13522786,Open,Minor,,02/Feb/23 12:52,,3.3.5,"The huge file test case {{AbstractSTestS3AHugeFiles.test_010_CreateHugeFile()}} asserts that the put file count > 0. But this stat measures the count of HTTP PUT requests sent to S3; if a multipart PUT was used then the count is zero. As a result, this test case can fail on bigger files than the default.

Fix. assert on OBJECT_PUT_REQUESTS_COMPLETED; this will be 1 for a simple PUT, and #of multipart blocks uploaded on a larger file. 
"
Simplify single node instructions for creating directories for Map Reduce,13520007,Resolved,Trivial,Fixed,18/Jan/23 08:59,20/Apr/23 10:43,3.3.4,"The {{mkdir}} command supports the {{-p}} option which instructs {{hdfs}} to create all the parent directories if needed. The single nose setup instructions now ask a user to create both /user and /user/<username> directories explicitly, which can be simplified to creating just the /user/<username> with the help from the -p option of mkdir"
Filesystem Spelling Mistake,13528132,Resolved,Trivial,Fixed,12/Mar/23 17:49,25/Apr/23 16:15,3.3.6,"The fs/Filesystem reports errors always containing the spelling mistake 'fileystem'
It is not the only place in Hadoop this is the case, but this is the easiest to fix."
